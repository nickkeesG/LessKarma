{"results": [{"createdAt": null, "postedAt": "2012-11-04T18:50:39.325Z", "modifiedAt": null, "url": null, "title": "A place for casual, non-karmic discussion for lesswrongers?", "slug": "a-place-for-casual-non-karmic-discussion-for-lesswrongers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:49.455Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "oXGTijwhjZB8cnJ3W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BbjodMd5dJc7nQWET/a-place-for-casual-non-karmic-discussion-for-lesswrongers", "pageUrlRelative": "/posts/BbjodMd5dJc7nQWET/a-place-for-casual-non-karmic-discussion-for-lesswrongers", "linkUrl": "https://www.lesswrong.com/posts/BbjodMd5dJc7nQWET/a-place-for-casual-non-karmic-discussion-for-lesswrongers", "postedAtFormatted": "Sunday, November 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20place%20for%20casual%2C%20non-karmic%20discussion%20for%20lesswrongers%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20place%20for%20casual%2C%20non-karmic%20discussion%20for%20lesswrongers%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBbjodMd5dJc7nQWET%2Fa-place-for-casual-non-karmic-discussion-for-lesswrongers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20place%20for%20casual%2C%20non-karmic%20discussion%20for%20lesswrongers%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBbjodMd5dJc7nQWET%2Fa-place-for-casual-non-karmic-discussion-for-lesswrongers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBbjodMd5dJc7nQWET%2Fa-place-for-casual-non-karmic-discussion-for-lesswrongers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 216, "htmlBody": "<p>I have never been on a Lesswrong meetup because they tend to take place too far away from my range in terms of distance and budget. Because of that, I don't know if those perform this function to everyone's satisfaction in such a way that what I'm suggesting here doesn't seem worth the effort. I hear that they're a lot of fun, and involve quite a bit of silliness, though; I find those cruelly lacking on Lesswrong proper, whether it be in main posts or discussion posts, and their relevant threads.&nbsp;</p>\n<p>That's why I think it would be nice to have a forum, a place to have normal discussions, where you don't have to watch that you don't say anything stupid or out-of-line lest you unexpectedly lose karma. A place to exchange jokes, frivolities, and entertainment. A place to talk about stuff that isn't rationality or singularity-related. A place to relax and enjoy the company of like-minded folks. A place to take a more personal approach to communication, with sequential rather than branching conversations. A place to make and be friends.</p>\n<p>Don't you think having that would be nice?</p>\n<p>EDIT: Also, if this place does already exist and I'm not aware of it, I humbly request that you provide me a link, for which I would be most grateful.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BbjodMd5dJc7nQWET", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 24, "extendedScore": null, "score": 1.0256964787180297e-06, "legacy": true, "legacyId": "19822", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 88, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-04T19:12:07.876Z", "modifiedAt": null, "url": null, "title": "NYC Winter Megameetup, Event RSVPs", "slug": "nyc-winter-megameetup-event-rsvps", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:23.809Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GPqBPtYXuaYERnq8J/nyc-winter-megameetup-event-rsvps", "pageUrlRelative": "/posts/GPqBPtYXuaYERnq8J/nyc-winter-megameetup-event-rsvps", "linkUrl": "https://www.lesswrong.com/posts/GPqBPtYXuaYERnq8J/nyc-winter-megameetup-event-rsvps", "postedAtFormatted": "Sunday, November 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20NYC%20Winter%20Megameetup%2C%20Event%20RSVPs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANYC%20Winter%20Megameetup%2C%20Event%20RSVPs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGPqBPtYXuaYERnq8J%2Fnyc-winter-megameetup-event-rsvps%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=NYC%20Winter%20Megameetup%2C%20Event%20RSVPs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGPqBPtYXuaYERnq8J%2Fnyc-winter-megameetup-event-rsvps", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGPqBPtYXuaYERnq8J%2Fnyc-winter-megameetup-event-rsvps", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 207, "htmlBody": "<p>I've just posted two facebook events for&nbsp;<a href=\"/meetups/en\">The Winter Solstice Megameetup</a>, the weekend of December 15th-16th.</p>\n<p>One is for the <a href=\"https://www.facebook.com/events/280327112088209/\">Solstice Celebration</a> - a Saturday night of ritual, song, stories and dance, catering to both Less Wrong-folk as well as other anyone with rational/humanist/futurist worldview they want to celebrate.</p>\n<p>The second is the official <a href=\"https://www.facebook.com/events/243475622446994/\">Less Wrong Winter Megameetup</a>, a Sunday afternoon of mingling and conversations with interesting aspiring rationalists. While there will not be a primary public discussion, people are encouraged to come with ideas about their goals for the next year. Take an opportunity to get a lot of feedback from smart, interesting people, many of whom have tackled some big projects and life changes in the past.</p>\n<p>Both events are potluck - bring some great food to share with great people.</p>\n<p>People from out of state attending both events are welcome to stay the night at Winterfell House. (You are advised to bring a sleeping bag - there is plenty of floor space but may not be enough mattresses for everyone)</p>\n<p>If you have facebook, you're encouraged to RSVP on the events so we have as accurate a headcount as possible. If not, and you haven't already indicated you're coming, please reply here to let us know which day(s) you're coming.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GPqBPtYXuaYERnq8J", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "19823", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-05T01:02:37.308Z", "modifiedAt": "2021-11-25T10:09:32.031Z", "url": null, "title": "Voting is like donating thousands of dollars to charity", "slug": "voting-is-like-donating-thousands-of-dollars-to-charity", "viewCount": null, "lastCommentedAt": "2015-10-08T07:10:23.040Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3kLjmvG4BaM6QrDnT/voting-is-like-donating-thousands-of-dollars-to-charity", "pageUrlRelative": "/posts/3kLjmvG4BaM6QrDnT/voting-is-like-donating-thousands-of-dollars-to-charity", "linkUrl": "https://www.lesswrong.com/posts/3kLjmvG4BaM6QrDnT/voting-is-like-donating-thousands-of-dollars-to-charity", "postedAtFormatted": "Monday, November 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Voting%20is%20like%20donating%20thousands%20of%20dollars%20to%20charity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AVoting%20is%20like%20donating%20thousands%20of%20dollars%20to%20charity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3kLjmvG4BaM6QrDnT%2Fvoting-is-like-donating-thousands-of-dollars-to-charity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Voting%20is%20like%20donating%20thousands%20of%20dollars%20to%20charity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3kLjmvG4BaM6QrDnT%2Fvoting-is-like-donating-thousands-of-dollars-to-charity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3kLjmvG4BaM6QrDnT%2Fvoting-is-like-donating-thousands-of-dollars-to-charity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1401, "htmlBody": "<p><strong>Summary:</strong>&nbsp; People often say that <em><a href=\"http://www.pursueaction.org/voting-is-irrational/\">voting</a> <a href=\"http://www.economist.com/blogs/buttonwood/2012/01/economics-irrationality\">is</a> <a href=\"http://www.psychologytoday.com/blog/the-scientific-fundamentalist/200911/why-do-people-vote-i\">irrational</a></em>, because the probability of affecting the outcome is so small. But the outcome itself is extremely large when you consider its impact on other people. I estimate that for most people, voting is worth a charitable donation of somewhere between $100 and $1.5 million. For me, the value came out to around $56,000. &nbsp;So I figure something on the order of $1000 is a reasonable evaluation (after all, I'm writing this post because the number turned out to be large according to this method, so regression to the mean suggests I err on the conservative side), and that's be enough to make me do it.</p>\n<p>Moreover, in swing states the value is much higher, so taking a 10% chance at convincing a friend in a swing state to vote similarly to you is probably worth thousands of expected donation dollars, too.</p>\n<p>I find this much more compelling than the typical attempts to justify voting purely in terms of signal value or the resulting sense of pride in fulfilling a civic duty. And voting for selfish reasons is still almost completely worthless, in terms of direct effect. If you're on the way to the polls only to vote for the party that will benefit <em>you</em> the most, you're better off using that time to earn $5 mowing someone's lawn. But if you're even a little altruistic... vote away!</p>\n<h3>Time for a Fermi estimate</h3>\n<p>Below is an example Fermi calculation for the value of voting in the USA. Of course, the estimates are all rough and fuzzy, so I'll be conservative, and we can adjust upward based on your opinion.</p>\n<p>I'll be estimating the value of voting in <em>marginal expected altruistic dollars</em>, the expected number of dollars being spent in a way that is in line with your altruistic preferences.<sup>1</sup> If you don't like measuring the altruistic value of the outcome in dollars, <strong>please consider making up your own measure, and keep reading</strong>. Perhaps use the number of smiles per year, or number of lives saved. Your measure doesn't have to be <a href=\"http://en.wikipedia.org/wiki/Average_and_total_utilitarianism\">total or average utilitarian</a>, either; as long as it's roughly commensurate with the size of the country, it will lead you to a similar conclusion in terms of orders of magnitude.</p>\n<p><a id=\"more\"></a></p>\n<h3>Component estimates:</h3>\n<p><strong>At least 1/(100 million) = probability estimate that my vote would affect the outcome.</strong> This is the most interesting thing to estimate. There are approximately 100 million voters in the USA, and if you assume a naive fair coin-flip model of other voters, and a naive majority-rule voting system (i.e. not the electoral college), with a fair coin deciding ties, then the probability of a vote being decisive is around <a href=\"http://www.springerlink.com/content/v7740xw683l67462/\">&radic;(2/(pi*100 million))</a> = 8/10,000.</p>\n<p>But this is too big, considering the way voters cluster: we are not independent coin flips. As well, the USA uses the electoral college system, not majority rule. So I found <a href=\"http://www.stat.columbia.edu/~gelman/research/published/decisive2.pdf\">this paper</a> by Gelman, King, and Boscardin (1998), where they simulate the electoral college using models fit to previous US elections, and find that the probability of a decisive vote came out between 1/(3 million) and 1/(100 million) for voters in most states in most elections, with most states lying very close to 1/(10 million).</p>\n<p><strong>At least 55% = my subjective credence</strong> that I know which candidate is \"better\", where I'm using the word \"better\" subjectively to mean which candidate would turn out to do the most good for others, in my view, if elected. If you don't like this, please make up your own definition of better and keep reading :) In any case, 55% is pretty conservative; it means I consider myself to have almost no information.</p>\n<p><strong>At least $100 billion = the approximate marginal altruistic value of the \"better\" candidate.</strong> I think this is also very conservative. The annual federal budget is around $3 trillion right now, making $12 trillion over a 4-year term, and Barack Obama and Mitt Romney differ on trillions of dollars in their proposed budgets. It would be pretty strange to me if, given a perfect understanding of what they'd both do, I would only care altruistically about 100 billion of those dollars, marginally speaking.</p>\n<h3>Result</h3>\n<p>I don't know which candidate would turn out \"better for the world\" in my estimation, but I'd consider myself as having at least a 55%*1/(100 million) chance of affecting the outcome in the better-for-the-world direction, and a 45%*1/(100 million) chance of affecting it in the worse-for-the-world direction, so in expectation I'm donating at least around</p>\n<p>(55%-45%)*1/(100 million)*($100 billion) = <strong>$100</strong></p>\n<p>Again, this was pretty conservative:</p>\n<ul>\n<li>I'm more like 70% sure, </li>\n<li>Being in California, Gelman et al. put my probability of a decisive vote around 1/(5 million). </li>\n<li>To me, the outcome matters more on the order of a $700 billion donation, given that Obama and Romney's budgets differ on around $7 trillion, and I figure at least 10% of that is stuff that I'd care about relative to other shifts in money I could imagine. </li>\n</ul>\n<p>That makes (70%-30%)*1/(5 million)*($700 billion) = <strong>$56,000</strong>. Going further, if you're</p>\n<ul>\n<li>90% sure, </li>\n<li>voting in Virginia -- 1/(3.5 million), and </li>\n<li>care about the whole $7 trillion dollar difference in budgets, </li>\n</ul>\n<p>you get (90%-30%)*1/(3.5 million)*($7 trillion) = <strong>$1.2 million</strong>. This is so large, it becomes a valuable use of my time to take 1% chances at convincing other people to vote... which I'm hopefully doing by writing this post.</p>\n<h3>Discussion</h3>\n<p>Now, I'm sure all these values are quite wrong in the sense that taking account everything we know about the current election would give very different answers. If anyone has a more nuanced model of the electoral college than Gelman et al, or a way of helping me better estimate how much the outcome matters to me, please post it! My $700 billion outcome value still feels a bit out-of-a-hat-ish.</p>\n<p>But the intuition to take away here is that a country is a very large operation, much larger than the number of people in it, and that's what makes voting worth it... if you care about other people. If you don't care about others, voting is probably not worth it to you. That expected $100 - $1,500,000 is going to get spread around to 300 million people... you're not expecting much of it yourself! That's a nice conclusion, isn't it? Nice people should vote, and selfish people shouldn't?</p>\n<p>Of course, <a href=\"/lw/gw/politics_is_the_mindkiller/\">politics is the mind killer</a>, and there are debates to be had about whether voting in the current system is immoral because the right thing to do is abstain in silent protest that we aren't using <a href=\"http://en.wikipedia.org/wiki/Approval_voting\">approval voting</a>, which has better properties than the current system... but I don't think that's how to get a new voting system. I think while we're making whatever efforts we can to build a better global community, it's no sacrifice to vote in the current system if it's really worth that much in expected donations.</p>\n<p>So if you weren't going to vote already, give some thought to this expected donation angle, and maybe you'll start. Maybe you'll start telling your swing state friends to vote, too. And if you do vote to experience a sense of pride in doing your civic duty, I say go ahead and keep feeling it!</p>\n<hr />\n<h3>Related reading</h3>\n<p>I've found a couple of papers by authors with similar thoughts to these:</p>\n<ul>\n<li><a href=\"http://rss.sagepub.com/cgi/framedrapidpdf/14/1/55\">Jankowski (2002)</a>, \"Buying a Lottery Ticket to Help the Poor: Altruism, Civic Duty, and Self-interest in the Decision to Vote\", and </li>\n<li><a href=\"http://works.bepress.com/aaron_edlin/42/\">Edlin, Gelman and Kaplan (2007)</a>, \"Voting as a Rational Choice: Why and How People Vote To Improve the Well-Being of Others. </li>\n</ul>\n<p>Also, just today I found this <a href=\"http://www.overcomingbias.com/2008/12/rationality-of.html\">this interesting Overcoming Bias post</a>, by Andrew Gelman as well.</p>\n<p>&nbsp;</p>\n<hr />\n<p><sup>1</sup> A nitpick, for people like me who are <a href=\"/lw/244/vnm_expected_utility_theory_uses_abuses_and/\">very particular</a> about what they mean by <a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">utility</a>: in this post, I'm calculating expected altruistic dollars, not expected utility. However, while our personal utility functions are (or would be, if we managed to have them!) certainly non-linear in the amount of money we spend on ourselves, there is a compelling argument for having the altruistic part of your utility function be approximately linear in altruistic dollars: there are just so many dollars in the world, and it's reasonable to assume utility is approximately differentiable in commodities. So on the scale of the world, your affect on how altruistic dollars are spent is small enough that you should value them approximately linearly.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"FkzScn5byCs9PxGsA": 1, "mPuSAzJN7CyrMiKrf": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3kLjmvG4BaM6QrDnT", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 78, "baseScore": 41, "extendedScore": null, "score": 9.8e-05, "legacy": true, "legacyId": "19824", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 42, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Summary:</strong>&nbsp; People often say that <em><a href=\"http://www.pursueaction.org/voting-is-irrational/\">voting</a> <a href=\"http://www.economist.com/blogs/buttonwood/2012/01/economics-irrationality\">is</a> <a href=\"http://www.psychologytoday.com/blog/the-scientific-fundamentalist/200911/why-do-people-vote-i\">irrational</a></em>, because the probability of affecting the outcome is so small. But the outcome itself is extremely large when you consider its impact on other people. I estimate that for most people, voting is worth a charitable donation of somewhere between $100 and $1.5 million. For me, the value came out to around $56,000. &nbsp;So I figure something on the order of $1000 is a reasonable evaluation (after all, I'm writing this post because the number turned out to be large according to this method, so regression to the mean suggests I err on the conservative side), and that's be enough to make me do it.</p>\n<p>Moreover, in swing states the value is much higher, so taking a 10% chance at convincing a friend in a swing state to vote similarly to you is probably worth thousands of expected donation dollars, too.</p>\n<p>I find this much more compelling than the typical attempts to justify voting purely in terms of signal value or the resulting sense of pride in fulfilling a civic duty. And voting for selfish reasons is still almost completely worthless, in terms of direct effect. If you're on the way to the polls only to vote for the party that will benefit <em>you</em> the most, you're better off using that time to earn $5 mowing someone's lawn. But if you're even a little altruistic... vote away!</p>\n<h3 id=\"Time_for_a_Fermi_estimate\">Time for a Fermi estimate</h3>\n<p>Below is an example Fermi calculation for the value of voting in the USA. Of course, the estimates are all rough and fuzzy, so I'll be conservative, and we can adjust upward based on your opinion.</p>\n<p>I'll be estimating the value of voting in <em>marginal expected altruistic dollars</em>, the expected number of dollars being spent in a way that is in line with your altruistic preferences.<sup>1</sup> If you don't like measuring the altruistic value of the outcome in dollars, <strong>please consider making up your own measure, and keep reading</strong>. Perhaps use the number of smiles per year, or number of lives saved. Your measure doesn't have to be <a href=\"http://en.wikipedia.org/wiki/Average_and_total_utilitarianism\">total or average utilitarian</a>, either; as long as it's roughly commensurate with the size of the country, it will lead you to a similar conclusion in terms of orders of magnitude.</p>\n<p><a id=\"more\"></a></p>\n<h3 id=\"Component_estimates_\">Component estimates:</h3>\n<p><strong>At least 1/(100 million) = probability estimate that my vote would affect the outcome.</strong> This is the most interesting thing to estimate. There are approximately 100 million voters in the USA, and if you assume a naive fair coin-flip model of other voters, and a naive majority-rule voting system (i.e. not the electoral college), with a fair coin deciding ties, then the probability of a vote being decisive is around <a href=\"http://www.springerlink.com/content/v7740xw683l67462/\">\u221a(2/(pi*100 million))</a> = 8/10,000.</p>\n<p>But this is too big, considering the way voters cluster: we are not independent coin flips. As well, the USA uses the electoral college system, not majority rule. So I found <a href=\"http://www.stat.columbia.edu/~gelman/research/published/decisive2.pdf\">this paper</a> by Gelman, King, and Boscardin (1998), where they simulate the electoral college using models fit to previous US elections, and find that the probability of a decisive vote came out between 1/(3 million) and 1/(100 million) for voters in most states in most elections, with most states lying very close to 1/(10 million).</p>\n<p><strong>At least 55% = my subjective credence</strong> that I know which candidate is \"better\", where I'm using the word \"better\" subjectively to mean which candidate would turn out to do the most good for others, in my view, if elected. If you don't like this, please make up your own definition of better and keep reading :) In any case, 55% is pretty conservative; it means I consider myself to have almost no information.</p>\n<p><strong>At least $100 billion = the approximate marginal altruistic value of the \"better\" candidate.</strong> I think this is also very conservative. The annual federal budget is around $3 trillion right now, making $12 trillion over a 4-year term, and Barack Obama and Mitt Romney differ on trillions of dollars in their proposed budgets. It would be pretty strange to me if, given a perfect understanding of what they'd both do, I would only care altruistically about 100 billion of those dollars, marginally speaking.</p>\n<h3 id=\"Result\">Result</h3>\n<p>I don't know which candidate would turn out \"better for the world\" in my estimation, but I'd consider myself as having at least a 55%*1/(100 million) chance of affecting the outcome in the better-for-the-world direction, and a 45%*1/(100 million) chance of affecting it in the worse-for-the-world direction, so in expectation I'm donating at least around</p>\n<p>(55%-45%)*1/(100 million)*($100 billion) = <strong>$100</strong></p>\n<p>Again, this was pretty conservative:</p>\n<ul>\n<li>I'm more like 70% sure, </li>\n<li>Being in California, Gelman et al. put my probability of a decisive vote around 1/(5 million). </li>\n<li>To me, the outcome matters more on the order of a $700 billion donation, given that Obama and Romney's budgets differ on around $7 trillion, and I figure at least 10% of that is stuff that I'd care about relative to other shifts in money I could imagine. </li>\n</ul>\n<p>That makes (70%-30%)*1/(5 million)*($700 billion) = <strong>$56,000</strong>. Going further, if you're</p>\n<ul>\n<li>90% sure, </li>\n<li>voting in Virginia -- 1/(3.5 million), and </li>\n<li>care about the whole $7 trillion dollar difference in budgets, </li>\n</ul>\n<p>you get (90%-30%)*1/(3.5 million)*($7 trillion) = <strong>$1.2 million</strong>. This is so large, it becomes a valuable use of my time to take 1% chances at convincing other people to vote... which I'm hopefully doing by writing this post.</p>\n<h3 id=\"Discussion\">Discussion</h3>\n<p>Now, I'm sure all these values are quite wrong in the sense that taking account everything we know about the current election would give very different answers. If anyone has a more nuanced model of the electoral college than Gelman et al, or a way of helping me better estimate how much the outcome matters to me, please post it! My $700 billion outcome value still feels a bit out-of-a-hat-ish.</p>\n<p>But the intuition to take away here is that a country is a very large operation, much larger than the number of people in it, and that's what makes voting worth it... if you care about other people. If you don't care about others, voting is probably not worth it to you. That expected $100 - $1,500,000 is going to get spread around to 300 million people... you're not expecting much of it yourself! That's a nice conclusion, isn't it? Nice people should vote, and selfish people shouldn't?</p>\n<p>Of course, <a href=\"/lw/gw/politics_is_the_mindkiller/\">politics is the mind killer</a>, and there are debates to be had about whether voting in the current system is immoral because the right thing to do is abstain in silent protest that we aren't using <a href=\"http://en.wikipedia.org/wiki/Approval_voting\">approval voting</a>, which has better properties than the current system... but I don't think that's how to get a new voting system. I think while we're making whatever efforts we can to build a better global community, it's no sacrifice to vote in the current system if it's really worth that much in expected donations.</p>\n<p>So if you weren't going to vote already, give some thought to this expected donation angle, and maybe you'll start. Maybe you'll start telling your swing state friends to vote, too. And if you do vote to experience a sense of pride in doing your civic duty, I say go ahead and keep feeling it!</p>\n<hr>\n<h3 id=\"Related_reading\">Related reading</h3>\n<p>I've found a couple of papers by authors with similar thoughts to these:</p>\n<ul>\n<li><a href=\"http://rss.sagepub.com/cgi/framedrapidpdf/14/1/55\">Jankowski (2002)</a>, \"Buying a Lottery Ticket to Help the Poor: Altruism, Civic Duty, and Self-interest in the Decision to Vote\", and </li>\n<li><a href=\"http://works.bepress.com/aaron_edlin/42/\">Edlin, Gelman and Kaplan (2007)</a>, \"Voting as a Rational Choice: Why and How People Vote To Improve the Well-Being of Others. </li>\n</ul>\n<p>Also, just today I found this <a href=\"http://www.overcomingbias.com/2008/12/rationality-of.html\">this interesting Overcoming Bias post</a>, by Andrew Gelman as well.</p>\n<p>&nbsp;</p>\n<hr>\n<p><sup>1</sup> A nitpick, for people like me who are <a href=\"/lw/244/vnm_expected_utility_theory_uses_abuses_and/\">very particular</a> about what they mean by <a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">utility</a>: in this post, I'm calculating expected altruistic dollars, not expected utility. However, while our personal utility functions are (or would be, if we managed to have them!) certainly non-linear in the amount of money we spend on ourselves, there is a compelling argument for having the altruistic part of your utility function be approximately linear in altruistic dollars: there are just so many dollars in the world, and it's reasonable to assume utility is approximately differentiable in commodities. So on the scale of the world, your affect on how altruistic dollars are spent is small enough that you should value them approximately linearly.</p>", "sections": [{"title": "Time for a Fermi estimate", "anchor": "Time_for_a_Fermi_estimate", "level": 1}, {"title": "Component estimates:", "anchor": "Component_estimates_", "level": 1}, {"title": "Result", "anchor": "Result", "level": 1}, {"title": "Discussion", "anchor": "Discussion", "level": 1}, {"title": "Related reading", "anchor": "Related_reading", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "206 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 212, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9weLK2AJ9JEt2Tt8f", "YCMfQoqqi2o9Tjwoa"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2012-11-05T01:02:37.308Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-05T01:23:52.009Z", "modifiedAt": null, "url": null, "title": "Does My Vote Matter?", "slug": "does-my-vote-matter", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:27.024Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "orthonormal", "createdAt": "2009-03-22T16:06:51.665Z", "isAdmin": false, "displayName": "orthonormal"}, "userId": "4fh2AAe3n7oBviyxx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6H5M7nnrp7Nrnr43L/does-my-vote-matter", "pageUrlRelative": "/posts/6H5M7nnrp7Nrnr43L/does-my-vote-matter", "linkUrl": "https://www.lesswrong.com/posts/6H5M7nnrp7Nrnr43L/does-my-vote-matter", "postedAtFormatted": "Monday, November 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Does%20My%20Vote%20Matter%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoes%20My%20Vote%20Matter%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6H5M7nnrp7Nrnr43L%2Fdoes-my-vote-matter%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Does%20My%20Vote%20Matter%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6H5M7nnrp7Nrnr43L%2Fdoes-my-vote-matter", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6H5M7nnrp7Nrnr43L%2Fdoes-my-vote-matter", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1738, "htmlBody": "<p><em>(Cross-posted from elsewhere; I thought some readers here might like it too. Please excuse the unusually informal style.)</em></p>\n<p><strong>Short answer:</strong> Actually, yes!</p>\n<p><strong>Slightly longer answer:</strong> Yes, if the election is close. You'll never get to <em>know</em> that your vote was decisive, but one vote can substantially change the odds on Election Day nonetheless. Even if the election is a foregone conclusion (or if you don't care about the major candidates), the same reasoning applies to third parties- there are thresholds that really matter to them, and if they reach those now they have a significantly better chance in the next election. And finally, local elections matter in the long run just as state or nation elections do. So, in most cases, voting is rational if you care about the outcome.</p>\n<p><strong>Full answer:</strong> Welcome! This is a nonpartisan analysis, written by a math PhD, of when and how a single vote matters in a large election. I've got a table of contents below; please skip to whatever section interests you most. And feel free to share this!</p>\n<p><strong>Sections:</strong></p>\n<p><strong>1. How can a single vote matter in a huge election?</strong></p>\n<p><strong>2. What if I know it's not going to be close?</strong></p>\n<p><strong>3. Do local elections matter?</strong></p>\n<p>In what follows, I'm going to be assuming an American-style voting system (first-past-the-post, for you voting-system buffs), but most of what I say carries over to other voting systems found around the world.</p>\n<p><strong>1. How can a single vote matter in a huge election?</strong></p>\n<p>To answer this, let's imagine a different voting system. In the land of Erewhon, voters cast their ballots for president just as they do here; but instead of decreeing that the candidate with the most votes is the winner, each vote is turned into a lottery ball, and one is chosen at random to determine the next president.</p>\n<p>While this system has its drawbacks (they get fringe candidates elected every so often, and they've had to outlaw write-in campaigns to prevent every voter from simply voting themselves in), the citizens of Erewhon agree on its main advantage: every one of them knows that their vote <em>counts</em>, that they increase the chance of their candidate winning by just that much.</p>\n<p><strong>I'm going to argue that if you would bother to vote if an election were held in Erewhon, then you should also vote&mdash;for the same reason&mdash;if the same election were held the normal way, and if it looked like the outcome might be close.</strong> That is, your effect on the outcome is about equivalent if the pre-election polls fall within the margin of error (about &plusmn;3% for each candidate, or a margin of less than 6% between two candidates, for an electorate of millions). And if the polls are nearly tied, then voting in our system might have the same effect as voting hundreds or even thousands of times in Erewhon's system!</p>\n<p>This seems counterintuitive, because we imagine that the votes of everyone else are \"locked in\" somehow, and that we're only deciding whether to add ours to the pile- in which case, the only way that it could matter is in the event that it makes or breaks an exact tie. And not only are those exceedingly rare (the largest such example I could find was <a href=\"http://www.boston.com/news/local/breaking_news/2011/02/judge_rules_wor.html\">a recent Massachusetts election for state representative</a>, in which 13,500 ballots were cast), but if the initial count for a massive election did show a margin of one or zero, we would be headed for an interminable recount. (See, for instance, the <a href=\"http://en.wikipedia.org/wiki/United_States_Senate_election_in_Minnesota,_2008\">2008 Minnesota Senate election</a>, which was decided by about 300 votes; the extensive recount delayed the winner's inauguration by about six months.) What this messes with, though, isn't your chance of helping decide the election, but your chance of <em>knowing</em> that you helped decide the election.</p>\n<p>That is, in modern elections, there's not a perfect sharp boundary between \"Candidate A wins\" and \"Candidate B wins\", but a gigantic muddle; and if A is narrowly ahead, then every additional vote for A represents a greater chance that the recount will eventually end, or end earlier, or not be contested at all; while every vote for candidate B means a greater chance that there will be a recount, or that it will go on longer, or that it might turn out victorious for B after all. You'd never know that <em>your</em> vote, which changed the lead from 412 to 413, was the straw that broke the proverbial camel's back and led the other candidate to concede, but at some point that's what happens.</p>\n<p>And even more significantly, we <em>can't</em> consider everyone else's votes as \"locked in\". If you had the ability to re-play Election Day over and over, the chaotic dynamics of everyday life would affect the vote totals. Someone makes a light about to turn red, and so she ends up at the next light behind a bumper sticker that infuriates her, and so she remembers to vote. Or someone else bumps into an old friend, and starts a conversation, and then he realizes he doesn't have enough of his lunch break left to get to the polling place anyhow. It's the <a href=\"http://en.wikipedia.org/wiki/Butterfly_effect\">butterfly effect</a> in action, and when multiplied by millions of people it leads to fluctuations in the hundreds or thousands. (The exact mechanism for this estimate is the <a href=\"http://en.wikipedia.org/wiki/Central_limit_theorem\">Central Limit Theorem</a>.) And what that means is that the margin isn't a fixed number pending your decision to vote; it's a mix of different possibilities, and your decision changes the odds as surely as adding a lottery ball to the Erewhon election does.</p>\n<p>Of course, if one candidate is polling 10 points ahead of the other, then those fluctuations will be irrelevant. If the margin is closer, then it just might have an effect; remember that polls have several sources of error in them, so you can't be exactly sure of the margin before voting actually happens. And in those cases where the actual vote turns out to be extremely close, like Minnesota in 2008, your choice to vote swings those odds just as if you'd poured in thousands of Erewhonian lottery balls with your candidate's name on them. (That is, it changes those odds by a fraction of a percent, but the ability to nudge the odds by that much in an electorate of millions is pretty impressive!)</p>\n<p><strong>2. What if I know it's not going to be close?</strong></p>\n<p>If the difference between the top candidates is outside the combined margin of error (7 points or more in a big election), then it's true that your vote won't affect the outcome of the <em>current</em> election, but it can matter greatly for the <em>next</em> one. This especially holds when there's a third party you prefer to the current major parties, but there are other relevant reasons to vote even if that's not the case.</p>\n<p>First, about third parties: the conventional wisdom that they can't win is an outright falsehood. We tend to forget that <a href=\"http://en.wikipedia.org/wiki/Ross_Perot_presidential_campaign,_1992\">Ross Perot nearly became President</a>: he held a six-point lead in June 1992 over both Bush and Clinton, and despite his candidacy crashing and burning later on (for reasons more personal than systemic), he still won nearly 20% of the popular vote in the end. In addition, there are two current Senators who were elected as independents.</p>\n<p>For a third party that's not polling well enough yet, votes <em>now</em> matter for <em>future</em> elections, and there are several significant thresholds. If they get rounded up to 1% on Election Day, then they get mentioned in election coverage. (Next, getting rounded up to 2% sounds much more impressive than getting rounded down to 1%.) If they get to 5%, then they can qualify for FEC matching funds, and double their ability to reach voters next time. And 10% is a significant number for media exposure (as well as invitations to the main debates). Higher than 20%, and we're talking about a viable candidate; there's a runaway dynamic in three-candidate races where as soon as the third party looks viable, any voters from the other parties who prefer the third party will suddenly switch (now that they're no longer worried about supporting an obvious loser), and suddenly the third party becomes the front-runner. (This kind of behavior is common in game theory in what's known as a <a href=\"http://en.wikipedia.org/wiki/Coordination_game\">coordination problem</a>.)</p>\n<p>Even if there's not a third party you prefer to the main ones, the margin of victory now helps determine what sort of candidates the parties select next time. If one party wins an election by more than 10% (this round number has a psychological hold on people, so it's what usually counts as a \"landslide victory\"), then next time that party is more likely to nominate a less moderate candidate, while the defeated party is likely to nominate a more moderate candidate. (Also, it goes without saying that you should be voting in primaries as well!)</p>\n<p>All of these thresholds can hinge just as easily on a few hundred votes, and so your vote can matter greatly. You just need to care about what happens two, or four, or six years from now.</p>\n<p><strong>3. Do local elections matter?</strong></p>\n<p>Most certainly. First, your vote affects the odds more strongly in a local election. (And, of course, there <em>can</em> be margins of zero or one votes, without recounts, in local elections!) Secondly, there are a number of important issues decided at the local level, both directly (tax and bond issues) and through representatives (especially local school boards). And finally, many big names in politics start out in local elections: <a href=\"http://en.wikipedia.org/wiki/Barack_Obama#State_Senator:_1997.E2.80.932004\">the current President began as an Illinois state senator</a>, and <a href=\"http://en.wikipedia.org/wiki/Joe_Biden#Family_and_early_political_career\">the current Vice President began as a city councilman</a>.</p>\n<p>It's more difficult to do research at the local level. <a href=\"http://votesmart.org/\">Project VoteSmart</a> may have data on some of the candidates (including the voting record of incumbents, public statements, and response to a questionnaire about their positions). Otherwise, your local newspaper may be the best bet (short of going to campaign events yourself). But a little bit of research can go a long way here.</p>\n<p><em>(<strong>Addendum:</strong> If I'd been composing this for the Less Wrong crowd, I'd have also noted that the decisions of people similar to you should be correlated, which adds another multiplier to the effectiveness of voting. I might like I bumper sticker that says \"I'm a timeless decision theorist, therefore I vote!\")</em></p>\n<p><em>(<strong>Second addendum:</strong> I'm mindful of the dangers of posting this within a few days of a major election. I've done my very best to keep from <a href=\"http://wiki.lesswrong.com/wiki/Politics_is_the_Mind-Killer\">mindkilling</a> language; I hope you do the same in the comments.)</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"FkzScn5byCs9PxGsA": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6H5M7nnrp7Nrnr43L", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 23, "extendedScore": null, "score": 5.8e-05, "legacy": true, "legacyId": "19826", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>(Cross-posted from elsewhere; I thought some readers here might like it too. Please excuse the unusually informal style.)</em></p>\n<p><strong>Short answer:</strong> Actually, yes!</p>\n<p><strong>Slightly longer answer:</strong> Yes, if the election is close. You'll never get to <em>know</em> that your vote was decisive, but one vote can substantially change the odds on Election Day nonetheless. Even if the election is a foregone conclusion (or if you don't care about the major candidates), the same reasoning applies to third parties- there are thresholds that really matter to them, and if they reach those now they have a significantly better chance in the next election. And finally, local elections matter in the long run just as state or nation elections do. So, in most cases, voting is rational if you care about the outcome.</p>\n<p><strong>Full answer:</strong> Welcome! This is a nonpartisan analysis, written by a math PhD, of when and how a single vote matters in a large election. I've got a table of contents below; please skip to whatever section interests you most. And feel free to share this!</p>\n<p><strong id=\"Sections_\">Sections:</strong></p>\n<p><strong id=\"1__How_can_a_single_vote_matter_in_a_huge_election_\">1. How can a single vote matter in a huge election?</strong></p>\n<p><strong id=\"2__What_if_I_know_it_s_not_going_to_be_close_\">2. What if I know it's not going to be close?</strong></p>\n<p><strong id=\"3__Do_local_elections_matter_\">3. Do local elections matter?</strong></p>\n<p>In what follows, I'm going to be assuming an American-style voting system (first-past-the-post, for you voting-system buffs), but most of what I say carries over to other voting systems found around the world.</p>\n<p><strong id=\"1__How_can_a_single_vote_matter_in_a_huge_election_1\">1. How can a single vote matter in a huge election?</strong></p>\n<p>To answer this, let's imagine a different voting system. In the land of Erewhon, voters cast their ballots for president just as they do here; but instead of decreeing that the candidate with the most votes is the winner, each vote is turned into a lottery ball, and one is chosen at random to determine the next president.</p>\n<p>While this system has its drawbacks (they get fringe candidates elected every so often, and they've had to outlaw write-in campaigns to prevent every voter from simply voting themselves in), the citizens of Erewhon agree on its main advantage: every one of them knows that their vote <em>counts</em>, that they increase the chance of their candidate winning by just that much.</p>\n<p><strong>I'm going to argue that if you would bother to vote if an election were held in Erewhon, then you should also vote\u2014for the same reason\u2014if the same election were held the normal way, and if it looked like the outcome might be close.</strong> That is, your effect on the outcome is about equivalent if the pre-election polls fall within the margin of error (about \u00b13% for each candidate, or a margin of less than 6% between two candidates, for an electorate of millions). And if the polls are nearly tied, then voting in our system might have the same effect as voting hundreds or even thousands of times in Erewhon's system!</p>\n<p>This seems counterintuitive, because we imagine that the votes of everyone else are \"locked in\" somehow, and that we're only deciding whether to add ours to the pile- in which case, the only way that it could matter is in the event that it makes or breaks an exact tie. And not only are those exceedingly rare (the largest such example I could find was <a href=\"http://www.boston.com/news/local/breaking_news/2011/02/judge_rules_wor.html\">a recent Massachusetts election for state representative</a>, in which 13,500 ballots were cast), but if the initial count for a massive election did show a margin of one or zero, we would be headed for an interminable recount. (See, for instance, the <a href=\"http://en.wikipedia.org/wiki/United_States_Senate_election_in_Minnesota,_2008\">2008 Minnesota Senate election</a>, which was decided by about 300 votes; the extensive recount delayed the winner's inauguration by about six months.) What this messes with, though, isn't your chance of helping decide the election, but your chance of <em>knowing</em> that you helped decide the election.</p>\n<p>That is, in modern elections, there's not a perfect sharp boundary between \"Candidate A wins\" and \"Candidate B wins\", but a gigantic muddle; and if A is narrowly ahead, then every additional vote for A represents a greater chance that the recount will eventually end, or end earlier, or not be contested at all; while every vote for candidate B means a greater chance that there will be a recount, or that it will go on longer, or that it might turn out victorious for B after all. You'd never know that <em>your</em> vote, which changed the lead from 412 to 413, was the straw that broke the proverbial camel's back and led the other candidate to concede, but at some point that's what happens.</p>\n<p>And even more significantly, we <em>can't</em> consider everyone else's votes as \"locked in\". If you had the ability to re-play Election Day over and over, the chaotic dynamics of everyday life would affect the vote totals. Someone makes a light about to turn red, and so she ends up at the next light behind a bumper sticker that infuriates her, and so she remembers to vote. Or someone else bumps into an old friend, and starts a conversation, and then he realizes he doesn't have enough of his lunch break left to get to the polling place anyhow. It's the <a href=\"http://en.wikipedia.org/wiki/Butterfly_effect\">butterfly effect</a> in action, and when multiplied by millions of people it leads to fluctuations in the hundreds or thousands. (The exact mechanism for this estimate is the <a href=\"http://en.wikipedia.org/wiki/Central_limit_theorem\">Central Limit Theorem</a>.) And what that means is that the margin isn't a fixed number pending your decision to vote; it's a mix of different possibilities, and your decision changes the odds as surely as adding a lottery ball to the Erewhon election does.</p>\n<p>Of course, if one candidate is polling 10 points ahead of the other, then those fluctuations will be irrelevant. If the margin is closer, then it just might have an effect; remember that polls have several sources of error in them, so you can't be exactly sure of the margin before voting actually happens. And in those cases where the actual vote turns out to be extremely close, like Minnesota in 2008, your choice to vote swings those odds just as if you'd poured in thousands of Erewhonian lottery balls with your candidate's name on them. (That is, it changes those odds by a fraction of a percent, but the ability to nudge the odds by that much in an electorate of millions is pretty impressive!)</p>\n<p><strong id=\"2__What_if_I_know_it_s_not_going_to_be_close_1\">2. What if I know it's not going to be close?</strong></p>\n<p>If the difference between the top candidates is outside the combined margin of error (7 points or more in a big election), then it's true that your vote won't affect the outcome of the <em>current</em> election, but it can matter greatly for the <em>next</em> one. This especially holds when there's a third party you prefer to the current major parties, but there are other relevant reasons to vote even if that's not the case.</p>\n<p>First, about third parties: the conventional wisdom that they can't win is an outright falsehood. We tend to forget that <a href=\"http://en.wikipedia.org/wiki/Ross_Perot_presidential_campaign,_1992\">Ross Perot nearly became President</a>: he held a six-point lead in June 1992 over both Bush and Clinton, and despite his candidacy crashing and burning later on (for reasons more personal than systemic), he still won nearly 20% of the popular vote in the end. In addition, there are two current Senators who were elected as independents.</p>\n<p>For a third party that's not polling well enough yet, votes <em>now</em> matter for <em>future</em> elections, and there are several significant thresholds. If they get rounded up to 1% on Election Day, then they get mentioned in election coverage. (Next, getting rounded up to 2% sounds much more impressive than getting rounded down to 1%.) If they get to 5%, then they can qualify for FEC matching funds, and double their ability to reach voters next time. And 10% is a significant number for media exposure (as well as invitations to the main debates). Higher than 20%, and we're talking about a viable candidate; there's a runaway dynamic in three-candidate races where as soon as the third party looks viable, any voters from the other parties who prefer the third party will suddenly switch (now that they're no longer worried about supporting an obvious loser), and suddenly the third party becomes the front-runner. (This kind of behavior is common in game theory in what's known as a <a href=\"http://en.wikipedia.org/wiki/Coordination_game\">coordination problem</a>.)</p>\n<p>Even if there's not a third party you prefer to the main ones, the margin of victory now helps determine what sort of candidates the parties select next time. If one party wins an election by more than 10% (this round number has a psychological hold on people, so it's what usually counts as a \"landslide victory\"), then next time that party is more likely to nominate a less moderate candidate, while the defeated party is likely to nominate a more moderate candidate. (Also, it goes without saying that you should be voting in primaries as well!)</p>\n<p>All of these thresholds can hinge just as easily on a few hundred votes, and so your vote can matter greatly. You just need to care about what happens two, or four, or six years from now.</p>\n<p><strong id=\"3__Do_local_elections_matter_1\">3. Do local elections matter?</strong></p>\n<p>Most certainly. First, your vote affects the odds more strongly in a local election. (And, of course, there <em>can</em> be margins of zero or one votes, without recounts, in local elections!) Secondly, there are a number of important issues decided at the local level, both directly (tax and bond issues) and through representatives (especially local school boards). And finally, many big names in politics start out in local elections: <a href=\"http://en.wikipedia.org/wiki/Barack_Obama#State_Senator:_1997.E2.80.932004\">the current President began as an Illinois state senator</a>, and <a href=\"http://en.wikipedia.org/wiki/Joe_Biden#Family_and_early_political_career\">the current Vice President began as a city councilman</a>.</p>\n<p>It's more difficult to do research at the local level. <a href=\"http://votesmart.org/\">Project VoteSmart</a> may have data on some of the candidates (including the voting record of incumbents, public statements, and response to a questionnaire about their positions). Otherwise, your local newspaper may be the best bet (short of going to campaign events yourself). But a little bit of research can go a long way here.</p>\n<p><em>(<strong>Addendum:</strong> If I'd been composing this for the Less Wrong crowd, I'd have also noted that the decisions of people similar to you should be correlated, which adds another multiplier to the effectiveness of voting. I might like I bumper sticker that says \"I'm a timeless decision theorist, therefore I vote!\")</em></p>\n<p><em>(<strong>Second addendum:</strong> I'm mindful of the dangers of posting this within a few days of a major election. I've done my very best to keep from <a href=\"http://wiki.lesswrong.com/wiki/Politics_is_the_Mind-Killer\">mindkilling</a> language; I hope you do the same in the comments.)</em></p>", "sections": [{"title": "Sections:", "anchor": "Sections_", "level": 1}, {"title": "1. How can a single vote matter in a huge election?", "anchor": "1__How_can_a_single_vote_matter_in_a_huge_election_", "level": 1}, {"title": "2. What if I know it's not going to be close?", "anchor": "2__What_if_I_know_it_s_not_going_to_be_close_", "level": 1}, {"title": "3. Do local elections matter?", "anchor": "3__Do_local_elections_matter_", "level": 1}, {"title": "1. How can a single vote matter in a huge election?", "anchor": "1__How_can_a_single_vote_matter_in_a_huge_election_1", "level": 1}, {"title": "2. What if I know it's not going to be close?", "anchor": "2__What_if_I_know_it_s_not_going_to_be_close_1", "level": 1}, {"title": "3. Do local elections matter?", "anchor": "3__Do_local_elections_matter_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "77 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 77, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-05T05:35:55.862Z", "modifiedAt": null, "url": null, "title": "Yes, Voting is Rational, So Please Go Vote!", "slug": "yes-voting-is-rational-so-please-go-vote", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:32.252Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "peter_hurford", "createdAt": "2011-07-19T19:05:31.793Z", "isAdmin": false, "displayName": "Peter Wildeford"}, "userId": "FMsXugZ8aB5d8nHsm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ASPeo8eDREupa5CtC/yes-voting-is-rational-so-please-go-vote", "pageUrlRelative": "/posts/ASPeo8eDREupa5CtC/yes-voting-is-rational-so-please-go-vote", "linkUrl": "https://www.lesswrong.com/posts/ASPeo8eDREupa5CtC/yes-voting-is-rational-so-please-go-vote", "postedAtFormatted": "Monday, November 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Yes%2C%20Voting%20is%20Rational%2C%20So%20Please%20Go%20Vote!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYes%2C%20Voting%20is%20Rational%2C%20So%20Please%20Go%20Vote!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FASPeo8eDREupa5CtC%2Fyes-voting-is-rational-so-please-go-vote%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Yes%2C%20Voting%20is%20Rational%2C%20So%20Please%20Go%20Vote!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FASPeo8eDREupa5CtC%2Fyes-voting-is-rational-so-please-go-vote", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FASPeo8eDREupa5CtC%2Fyes-voting-is-rational-so-please-go-vote", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1736, "htmlBody": "<p><strong>Main Idea:</strong>&nbsp;Voting is rational because (1) the benefits of having your preferred candidate win might be enough to outweigh the chance you won't swing the election, (2) there are intangible benefits to voting besides swining the election, and (3) the collective action of nonvoters matters and could have swung elections.</p>\n<p>~</p>\n<p>Currently, about <a href=\"http://en.wikipedia.org/wiki/Voter_turnout_in_the_United_States_presidential_elections\">58% of the voting age population actually vote</a>, which in 2008 was 131 million people out of a possible 225 million people. &nbsp;What this means is that there were 94 million people who didn't vote in the last election despite being able to, and we can expect this election to be the same.</p>\n<p>Is this a problem? &nbsp;Perhaps not. &nbsp;While <a href=\"http://www.utilitarian.net/singer/by/200712--.htm\">compulsory voting has an interesting appeal</a>, I generally think people should be free to decline to participate for whatever reason if they want to. &nbsp;Indeed, given <a href=\"http://www.greatplay.net/essays/youre-not-entitled-to-your-policy-opinions\">people's overconfidence and under-informed approach to issues</a>&nbsp;(for which I don't blame them and for which I am also at fault), many people may have <a href=\"http://www.amazon.com/dp/0691154449/\">a moral imperative to not vote</a>.</p>\n<p>All this aside, I want to tackle head on the pervailing view I've been seeing among many people, including myself, that \"voting is irrational\" because your vote doesn't really \"count\". &nbsp;As <a href=\"http://www.freakonomics.com/2012/10/25/we-the-sheeple-full-transcript/\">smart person Steven Levitt says</a>:</p>\n<blockquote>\n<p>Well, one good indicator of a person who&rsquo;s not so smart is if they vote in a presidential election because they think their vote might actually decide which candidate wins... there has never been and there never will be a vote cast in a presidential election that could possibly be decisive.</p>\n</blockquote>\n<p>So this leads me to wonder is voting rational? &nbsp;Are there good reasons to vote? &nbsp;In this essay, I want to reverse the argument I used to buy, <a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind\">change my mind</a>, and answer with a resounding yes. &nbsp;Voting is rational, there are good reasons to vote, and you really should go out and vote if you can.</p>\n<p>&nbsp;</p>\n<h2>The Voter's Lottery: Swinging the Vote</h2>\n<p>In the paper <a href=\"http://www.stat.columbia.edu/~gelman/research/published/probdecisive2.pdf\">\"What is the Probability Your Vote Will Make a Difference?\"</a>&nbsp;[PDF], authors Andrew Gelman, Nate Silver, and Aaron Eldin concluded that if you're voting in New Mexico, New Hampshire, Virginia, or Colorado, your vote would have about a 1 in 10 million chance of being the deciding vote in the election. &nbsp;The average state has a chance of 1 in 1 billion. &nbsp;For ll states except DC, your chance will have an upper bound of 1 in 100 billion. &nbsp;For DC voters, its closer to 1 in 1 trillion. &nbsp;(And it all ends up being the same <a href=\"http://www.stat.columbia.edu/~gelman/research/published/gelmankatzbafumi.pdf\">when you take into account recounts, litigation, etc.</a>&nbsp;[PDF])</p>\n<p>Using these odds, we can think of it like a lottery ticket and analyze the expected utility. &nbsp;By a strict view of rationality, rational decisions are those that are expected to maximize utility, which is calculated by multiplying the utility derived by the situation multiplied by the chance of that situation coming into existence.</p>\n<p>A win for your preferred candidate will affect the entire US population of 311 million people. &nbsp;As Andrew Gelman <a href=\"http://andrewgelman.com/2011/01/yes_it_can_be_r/\">argues on his blog</a>, if you think that your preferred candidate's policies are better enough than the other guys to be worth an additional $50 per person in improvement of life, which seems very plausible given the wide differences in economic platforms, that's a swing of +$15.55 billion. &nbsp;That makes your vote worth somewhere between $1555 for NM/NH/VI/CO, $15.55 for the average state, and a mere penny for the DC voter. &nbsp;This payoff matters to you if you care about the welfare of the US, and thus is <a href=\"http://www.stat.columbia.edu/~gelman/research/published/rational_final6.pdf\">best modelled as a social benefit</a>&nbsp;(PDF).</p>\n<p>&nbsp;</p>\n<h2>The Intangibles: Other Reasons to Vote</h2>\n<p>On a narrow view of the costs and benefits of voting, one would compare the expected social benefit expressed in monetary return (poor DC!) against the estimated cost of voting, and that would be that. &nbsp;However, that's not all there is to voting. &nbsp;Indeed, there are many additional benefits, beyond an expectation of impacting the election.</p>\n<p>As <a href=\"http://bigthink.com/daylight-atheism/why-you-should-be-a-voter?page=all\">Adam Lee points out</a>, voting might be a social good merely by continuing to express support for, legitimize, and perpetuate democracy as a broad institution. &nbsp;Democracy must be renewed in order to exist and the more people who vote, the more representative and legitimate the system is. &nbsp;Lee further goes on to argue that by adding your vote, you're increasing your chosen candidate's mandate by that slim amount, and doing your part in ensuring that the government stops playing solely to the extreme fringes that do vote.</p>\n<p>There are other social \"intangible\" reasons to vote as well. &nbsp;For many, including me, it feels good to be engaged and consider yourself a part of the process. &nbsp;Going to your polling place, at least as a college student, is lots of fun. &nbsp;And working to vote informed keeps myself motivated to be educated, in a way that the more abstract land of the classroom often does not. &nbsp;And as a political science / psychology student, it gives a fascinating realm of human behavior to study.</p>\n<p>Voting is also a grand opportunity to signal to yourself and others what kind of person you are. &nbsp;Like it or not, we define ourselves by how we vote (or choose not to vote). &nbsp;Whether you take up the mantle of passionate progressivism, skeptical conservatism, somewhere in between, or on some other political axis altogether, you get to stake yourself on a political identity (or a lack of one).</p>\n<p>&nbsp;</p>\n<h2>Collective Action: Your Vote Always \"Counts\"</h2>\n<p>From Gelman, Silver, and Eldin's model, it's clear your vote always has a chance of counting, even if that chance is a mere 1 in 1 trillion. &nbsp;But your vote matters in another critical sense -- imagine that you decided to not vote because you think your vote won't matter. &nbsp;Now imagine that everyone else also had that same reasoning -- everyone else also agreed voting is irrational. &nbsp;All of a sudden, no-one turns out to the polling place, and the country falls apart into chaos.</p>\n<p>Certainly your choice has no causal impact on the choices of others -- the entire country isn't going to stop voting just because you choose not to. &nbsp;But what we really have here is a classic collective action problem. &nbsp;Here's an example:</p>\n<blockquote>\n<p>The fishermen can either choose to fish normally or overfish. If all the fishermen overfish, they stand to deplete the lake and all fishermen lose their jobs. However, if just a few fishermen overfish, they get the benefit of added fish to sell, and the lake can handle the slight increase in load. So this tension is to be the fisherman that wins most by personally overfishing, while not collectively depleting the entire lake. Such problems are called <strong>collective action problems</strong> &mdash; people do well individually by defecting but do worse collectively if everyone defects. The result of a collective action problem ending in disaster is called the <strong>tragedy of the commons</strong>.</p>\n</blockquote>\n<p>Here, declining to vote is analogous to overfishing -- if too many people do it, we get a tragedy of the commons. &nbsp;But if enough people vote, you can \"free ride\" on their work by not voting yourself. &nbsp;And by refraining from voting, you hurt the group just that little bit in a collective action sense.</p>\n<p>On the flipside, no matter where you vote, you're vote does matter in the same collective action sense. &nbsp;Sure, you may be a voter in DC either redundantly supplementing or trying hopelessly fight the dominant liberal tide. &nbsp;But either way, your vote is there, and it has a marginal effect on the election that could affect the outcome.</p>\n<p>&nbsp;</p>\n<h2>Who's Missing?: How Things Could Be Different</h2>\n<p>For a good example about how things could be different in a collective action sense, imagine what would be different if all the nonvoters decided to vote instead. &nbsp;We know that there are going to be approximately 90 million non-voters. &nbsp;<a href=\"http://galesburgplanet.com/posts/20151\">According to Pew Research</a>, these undecided voters, if they were to vote, would add another 53 million votes for Obama and another 21.6 million votes for Romney. &nbsp;Going off of <a href=\"http://www.suffolk.edu/52955.html\">the USA Today - Suffolk survey</a>, we'd get +38.7M for Obama and +18M for Romney.</p>\n<p>If everyone voted, that would put Obama ahead by somewhere between 31.6M and 20.7M votes. &nbsp;If we go with <a href=\"http://fivethirtyeight.blogs.nytimes.com/\">Nate Silver's predictions</a>&nbsp;(as of Nov 3) for a 50.6% Obama / 48.3% Romney split, that would project that a pool of 131 million people voters would break 66.3M for Obama and 63.3M for Romney. &nbsp;Add in the undecided pool, and it becomes 97.9M vs. 84M -- a total trounce rather than a \"close-ish\" match. &nbsp;While it may not change the outcome in Obama/Romney, given that nonvoters are characteristically more liberal, this could have made a huge changed-the-outcome difference in Bush/Gore or Bush/Kerry.</p>\n<p>&nbsp;</p>\n<h2>The Alternative: Costs of Voting</h2>\n<p>So hopefully between (a) the expectation of voting as a lottery ticket with actually high expected value, (b) the additional \"intangible\" benefits of voting, and (c) the collective action approach, voting is not &lt;i&gt;intrinsically&lt;/i&gt; irrational. &nbsp;However, we haven't really taken into account the costs of voting.</p>\n<p>If people are voting out of a desire to help others, they might feel insufficiently confident to vote in a way that will actually help others. &nbsp;That sounds like a fair worry to me, though I'd suggest that if they even have the slightest hunch one way or another, it might be worth casting your vote just to balance out the batshit crazy people.</p>\n<p>Furthermore, they might legitimately have something better to do that's likely to impact the world far more than their vote. &nbsp;While the new availability of absentee voting would thus require you to be so busy with high-value activities that you just can't have time to fill it out, though, I suppose this is possible.</p>\n<p>Or maybe voting comes to you at an unexpectedly high hardship. &nbsp;Here, I think there are good resources to help you, but that's also understandable.</p>\n<p>But at the end of the day, voting is not only potentially and legitimately high reward, it's also surprisingly low cost for many people. &nbsp;Based on the money approach alone, you're spending like two hours max for a potential $1555 if you're in a solid swing state. &nbsp;That's damn good pay.</p>\n<p>&nbsp;</p>\n<p>Thus, in conclusion, I ask you to please find some way to be informed on the issues and strongly consider voting for whoever your preferred candidate is. &nbsp;It's truly the rational thing to do.</p>\n<p>&nbsp;</p>\n<p>(<strong>Edit: </strong>I didn't realize this would be one of some dozen essays on voting; I wrote it independently of seeing all the others. &nbsp;Sorry if there is any redundancy.)</p>\n<h5><em>(Note: This was <a href=\"http://www.greatplay.net/essays/yes-voting-is-rational-so-please-go-vote\">cross-posted from my blog</a>.)</em></h5>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ASPeo8eDREupa5CtC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": -6, "extendedScore": null, "score": -5e-06, "legacy": true, "legacyId": "19846", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<p><strong>Main Idea:</strong>&nbsp;Voting is rational because (1) the benefits of having your preferred candidate win might be enough to outweigh the chance you won't swing the election, (2) there are intangible benefits to voting besides swining the election, and (3) the collective action of nonvoters matters and could have swung elections.</p>\n<p>~</p>\n<p>Currently, about <a href=\"http://en.wikipedia.org/wiki/Voter_turnout_in_the_United_States_presidential_elections\">58% of the voting age population actually vote</a>, which in 2008 was 131 million people out of a possible 225 million people. &nbsp;What this means is that there were 94 million people who didn't vote in the last election despite being able to, and we can expect this election to be the same.</p>\n<p>Is this a problem? &nbsp;Perhaps not. &nbsp;While <a href=\"http://www.utilitarian.net/singer/by/200712--.htm\">compulsory voting has an interesting appeal</a>, I generally think people should be free to decline to participate for whatever reason if they want to. &nbsp;Indeed, given <a href=\"http://www.greatplay.net/essays/youre-not-entitled-to-your-policy-opinions\">people's overconfidence and under-informed approach to issues</a>&nbsp;(for which I don't blame them and for which I am also at fault), many people may have <a href=\"http://www.amazon.com/dp/0691154449/\">a moral imperative to not vote</a>.</p>\n<p>All this aside, I want to tackle head on the pervailing view I've been seeing among many people, including myself, that \"voting is irrational\" because your vote doesn't really \"count\". &nbsp;As <a href=\"http://www.freakonomics.com/2012/10/25/we-the-sheeple-full-transcript/\">smart person Steven Levitt says</a>:</p>\n<blockquote>\n<p>Well, one good indicator of a person who\u2019s not so smart is if they vote in a presidential election because they think their vote might actually decide which candidate wins... there has never been and there never will be a vote cast in a presidential election that could possibly be decisive.</p>\n</blockquote>\n<p>So this leads me to wonder is voting rational? &nbsp;Are there good reasons to vote? &nbsp;In this essay, I want to reverse the argument I used to buy, <a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind\">change my mind</a>, and answer with a resounding yes. &nbsp;Voting is rational, there are good reasons to vote, and you really should go out and vote if you can.</p>\n<p>&nbsp;</p>\n<h2 id=\"The_Voter_s_Lottery__Swinging_the_Vote\">The Voter's Lottery: Swinging the Vote</h2>\n<p>In the paper <a href=\"http://www.stat.columbia.edu/~gelman/research/published/probdecisive2.pdf\">\"What is the Probability Your Vote Will Make a Difference?\"</a>&nbsp;[PDF], authors Andrew Gelman, Nate Silver, and Aaron Eldin concluded that if you're voting in New Mexico, New Hampshire, Virginia, or Colorado, your vote would have about a 1 in 10 million chance of being the deciding vote in the election. &nbsp;The average state has a chance of 1 in 1 billion. &nbsp;For ll states except DC, your chance will have an upper bound of 1 in 100 billion. &nbsp;For DC voters, its closer to 1 in 1 trillion. &nbsp;(And it all ends up being the same <a href=\"http://www.stat.columbia.edu/~gelman/research/published/gelmankatzbafumi.pdf\">when you take into account recounts, litigation, etc.</a>&nbsp;[PDF])</p>\n<p>Using these odds, we can think of it like a lottery ticket and analyze the expected utility. &nbsp;By a strict view of rationality, rational decisions are those that are expected to maximize utility, which is calculated by multiplying the utility derived by the situation multiplied by the chance of that situation coming into existence.</p>\n<p>A win for your preferred candidate will affect the entire US population of 311 million people. &nbsp;As Andrew Gelman <a href=\"http://andrewgelman.com/2011/01/yes_it_can_be_r/\">argues on his blog</a>, if you think that your preferred candidate's policies are better enough than the other guys to be worth an additional $50 per person in improvement of life, which seems very plausible given the wide differences in economic platforms, that's a swing of +$15.55 billion. &nbsp;That makes your vote worth somewhere between $1555 for NM/NH/VI/CO, $15.55 for the average state, and a mere penny for the DC voter. &nbsp;This payoff matters to you if you care about the welfare of the US, and thus is <a href=\"http://www.stat.columbia.edu/~gelman/research/published/rational_final6.pdf\">best modelled as a social benefit</a>&nbsp;(PDF).</p>\n<p>&nbsp;</p>\n<h2 id=\"The_Intangibles__Other_Reasons_to_Vote\">The Intangibles: Other Reasons to Vote</h2>\n<p>On a narrow view of the costs and benefits of voting, one would compare the expected social benefit expressed in monetary return (poor DC!) against the estimated cost of voting, and that would be that. &nbsp;However, that's not all there is to voting. &nbsp;Indeed, there are many additional benefits, beyond an expectation of impacting the election.</p>\n<p>As <a href=\"http://bigthink.com/daylight-atheism/why-you-should-be-a-voter?page=all\">Adam Lee points out</a>, voting might be a social good merely by continuing to express support for, legitimize, and perpetuate democracy as a broad institution. &nbsp;Democracy must be renewed in order to exist and the more people who vote, the more representative and legitimate the system is. &nbsp;Lee further goes on to argue that by adding your vote, you're increasing your chosen candidate's mandate by that slim amount, and doing your part in ensuring that the government stops playing solely to the extreme fringes that do vote.</p>\n<p>There are other social \"intangible\" reasons to vote as well. &nbsp;For many, including me, it feels good to be engaged and consider yourself a part of the process. &nbsp;Going to your polling place, at least as a college student, is lots of fun. &nbsp;And working to vote informed keeps myself motivated to be educated, in a way that the more abstract land of the classroom often does not. &nbsp;And as a political science / psychology student, it gives a fascinating realm of human behavior to study.</p>\n<p>Voting is also a grand opportunity to signal to yourself and others what kind of person you are. &nbsp;Like it or not, we define ourselves by how we vote (or choose not to vote). &nbsp;Whether you take up the mantle of passionate progressivism, skeptical conservatism, somewhere in between, or on some other political axis altogether, you get to stake yourself on a political identity (or a lack of one).</p>\n<p>&nbsp;</p>\n<h2 id=\"Collective_Action__Your_Vote_Always__Counts_\">Collective Action: Your Vote Always \"Counts\"</h2>\n<p>From Gelman, Silver, and Eldin's model, it's clear your vote always has a chance of counting, even if that chance is a mere 1 in 1 trillion. &nbsp;But your vote matters in another critical sense -- imagine that you decided to not vote because you think your vote won't matter. &nbsp;Now imagine that everyone else also had that same reasoning -- everyone else also agreed voting is irrational. &nbsp;All of a sudden, no-one turns out to the polling place, and the country falls apart into chaos.</p>\n<p>Certainly your choice has no causal impact on the choices of others -- the entire country isn't going to stop voting just because you choose not to. &nbsp;But what we really have here is a classic collective action problem. &nbsp;Here's an example:</p>\n<blockquote>\n<p>The fishermen can either choose to fish normally or overfish. If all the fishermen overfish, they stand to deplete the lake and all fishermen lose their jobs. However, if just a few fishermen overfish, they get the benefit of added fish to sell, and the lake can handle the slight increase in load. So this tension is to be the fisherman that wins most by personally overfishing, while not collectively depleting the entire lake. Such problems are called <strong>collective action problems</strong> \u2014 people do well individually by defecting but do worse collectively if everyone defects. The result of a collective action problem ending in disaster is called the <strong>tragedy of the commons</strong>.</p>\n</blockquote>\n<p>Here, declining to vote is analogous to overfishing -- if too many people do it, we get a tragedy of the commons. &nbsp;But if enough people vote, you can \"free ride\" on their work by not voting yourself. &nbsp;And by refraining from voting, you hurt the group just that little bit in a collective action sense.</p>\n<p>On the flipside, no matter where you vote, you're vote does matter in the same collective action sense. &nbsp;Sure, you may be a voter in DC either redundantly supplementing or trying hopelessly fight the dominant liberal tide. &nbsp;But either way, your vote is there, and it has a marginal effect on the election that could affect the outcome.</p>\n<p>&nbsp;</p>\n<h2 id=\"Who_s_Missing___How_Things_Could_Be_Different\">Who's Missing?: How Things Could Be Different</h2>\n<p>For a good example about how things could be different in a collective action sense, imagine what would be different if all the nonvoters decided to vote instead. &nbsp;We know that there are going to be approximately 90 million non-voters. &nbsp;<a href=\"http://galesburgplanet.com/posts/20151\">According to Pew Research</a>, these undecided voters, if they were to vote, would add another 53 million votes for Obama and another 21.6 million votes for Romney. &nbsp;Going off of <a href=\"http://www.suffolk.edu/52955.html\">the USA Today - Suffolk survey</a>, we'd get +38.7M for Obama and +18M for Romney.</p>\n<p>If everyone voted, that would put Obama ahead by somewhere between 31.6M and 20.7M votes. &nbsp;If we go with <a href=\"http://fivethirtyeight.blogs.nytimes.com/\">Nate Silver's predictions</a>&nbsp;(as of Nov 3) for a 50.6% Obama / 48.3% Romney split, that would project that a pool of 131 million people voters would break 66.3M for Obama and 63.3M for Romney. &nbsp;Add in the undecided pool, and it becomes 97.9M vs. 84M -- a total trounce rather than a \"close-ish\" match. &nbsp;While it may not change the outcome in Obama/Romney, given that nonvoters are characteristically more liberal, this could have made a huge changed-the-outcome difference in Bush/Gore or Bush/Kerry.</p>\n<p>&nbsp;</p>\n<h2 id=\"The_Alternative__Costs_of_Voting\">The Alternative: Costs of Voting</h2>\n<p>So hopefully between (a) the expectation of voting as a lottery ticket with actually high expected value, (b) the additional \"intangible\" benefits of voting, and (c) the collective action approach, voting is not &lt;i&gt;intrinsically&lt;/i&gt; irrational. &nbsp;However, we haven't really taken into account the costs of voting.</p>\n<p>If people are voting out of a desire to help others, they might feel insufficiently confident to vote in a way that will actually help others. &nbsp;That sounds like a fair worry to me, though I'd suggest that if they even have the slightest hunch one way or another, it might be worth casting your vote just to balance out the batshit crazy people.</p>\n<p>Furthermore, they might legitimately have something better to do that's likely to impact the world far more than their vote. &nbsp;While the new availability of absentee voting would thus require you to be so busy with high-value activities that you just can't have time to fill it out, though, I suppose this is possible.</p>\n<p>Or maybe voting comes to you at an unexpectedly high hardship. &nbsp;Here, I think there are good resources to help you, but that's also understandable.</p>\n<p>But at the end of the day, voting is not only potentially and legitimately high reward, it's also surprisingly low cost for many people. &nbsp;Based on the money approach alone, you're spending like two hours max for a potential $1555 if you're in a solid swing state. &nbsp;That's damn good pay.</p>\n<p>&nbsp;</p>\n<p>Thus, in conclusion, I ask you to please find some way to be informed on the issues and strongly consider voting for whoever your preferred candidate is. &nbsp;It's truly the rational thing to do.</p>\n<p>&nbsp;</p>\n<p>(<strong>Edit: </strong>I didn't realize this would be one of some dozen essays on voting; I wrote it independently of seeing all the others. &nbsp;Sorry if there is any redundancy.)</p>\n<h5><em>(Note: This was <a href=\"http://www.greatplay.net/essays/yes-voting-is-rational-so-please-go-vote\">cross-posted from my blog</a>.)</em></h5>", "sections": [{"title": "The Voter's Lottery: Swinging the Vote", "anchor": "The_Voter_s_Lottery__Swinging_the_Vote", "level": 1}, {"title": "The Intangibles: Other Reasons to Vote", "anchor": "The_Intangibles__Other_Reasons_to_Vote", "level": 1}, {"title": "Collective Action: Your Vote Always \"Counts\"", "anchor": "Collective_Action__Your_Vote_Always__Counts_", "level": 1}, {"title": "Who's Missing?: How Things Could Be Different", "anchor": "Who_s_Missing___How_Things_Could_Be_Different", "level": 1}, {"title": "The Alternative: Costs of Voting", "anchor": "The_Alternative__Costs_of_Voting", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-05T20:09:21.438Z", "modifiedAt": null, "url": null, "title": "Please don't vote because democracy is a local optimum ", "slug": "please-don-t-vote-because-democracy-is-a-local-optimum", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:00.594Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FccGAQALm82uiJaMz/please-don-t-vote-because-democracy-is-a-local-optimum", "pageUrlRelative": "/posts/FccGAQALm82uiJaMz/please-don-t-vote-because-democracy-is-a-local-optimum", "linkUrl": "https://www.lesswrong.com/posts/FccGAQALm82uiJaMz/please-don-t-vote-because-democracy-is-a-local-optimum", "postedAtFormatted": "Monday, November 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Please%20don't%20vote%20because%20democracy%20is%20a%20local%20optimum%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APlease%20don't%20vote%20because%20democracy%20is%20a%20local%20optimum%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFccGAQALm82uiJaMz%2Fplease-don-t-vote-because-democracy-is-a-local-optimum%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Please%20don't%20vote%20because%20democracy%20is%20a%20local%20optimum%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFccGAQALm82uiJaMz%2Fplease-don-t-vote-because-democracy-is-a-local-optimum", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFccGAQALm82uiJaMz%2Fplease-don-t-vote-because-democracy-is-a-local-optimum", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 21, "htmlBody": "<p><strong>Related to: </strong><a href=\"/lw/fao/voting_is_like_donating_thousands_of_dollars_to/\">Voting is like donating thousands of dollars to charity</a>, <a href=\"/r/discussion/lw/faq/does_my_vote_matter/\">Does My Vote Matter? </a></p>\n<p>And voting adds legitimacy to it.</p>\n<p>Thank you.</p>\n<p>#annoyedbymotivatedcognition</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FccGAQALm82uiJaMz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 70, "baseScore": -19, "extendedScore": null, "score": 1.026531011158963e-06, "legacy": true, "legacyId": "19850", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 210, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3kLjmvG4BaM6QrDnT", "6H5M7nnrp7Nrnr43L"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-06T01:28:24.783Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC Show and tell meetup: Economics II", "slug": "meetup-washington-dc-show-and-tell-meetup-economics-ii-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mRJ7sbFmsTGzYzzFX/meetup-washington-dc-show-and-tell-meetup-economics-ii-0", "pageUrlRelative": "/posts/mRJ7sbFmsTGzYzzFX/meetup-washington-dc-show-and-tell-meetup-economics-ii-0", "linkUrl": "https://www.lesswrong.com/posts/mRJ7sbFmsTGzYzzFX/meetup-washington-dc-show-and-tell-meetup-economics-ii-0", "postedAtFormatted": "Tuesday, November 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20Show%20and%20tell%20meetup%3A%20Economics%20II&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20Show%20and%20tell%20meetup%3A%20Economics%20II%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmRJ7sbFmsTGzYzzFX%2Fmeetup-washington-dc-show-and-tell-meetup-economics-ii-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20Show%20and%20tell%20meetup%3A%20Economics%20II%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmRJ7sbFmsTGzYzzFX%2Fmeetup-washington-dc-show-and-tell-meetup-economics-ii-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmRJ7sbFmsTGzYzzFX%2Fmeetup-washington-dc-show-and-tell-meetup-economics-ii-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 53, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/fk'>Washington DC Show and tell meetup: Economics II</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 November 2012 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery Plaza, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This will be the second \"Show and tell\" meetup, again about economics.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/fk'>Washington DC Show and tell meetup: Economics II</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mRJ7sbFmsTGzYzzFX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.026707054132974e-06, "legacy": true, "legacyId": "19851", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Show_and_tell_meetup__Economics_II\">Discussion article for the meetup : <a href=\"/meetups/fk\">Washington DC Show and tell meetup: Economics II</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 November 2012 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery Plaza, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This will be the second \"Show and tell\" meetup, again about economics.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_Show_and_tell_meetup__Economics_II1\">Discussion article for the meetup : <a href=\"/meetups/fk\">Washington DC Show and tell meetup: Economics II</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC Show and tell meetup: Economics II", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Show_and_tell_meetup__Economics_II", "level": 1}, {"title": "Discussion article for the meetup : Washington DC Show and tell meetup: Economics II", "anchor": "Discussion_article_for_the_meetup___Washington_DC_Show_and_tell_meetup__Economics_II1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-06T05:10:46.923Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] A Few Background Posts by Robin Hanson", "slug": "seq-rerun-a-few-background-posts-by-robin-hanson", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:34.594Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fj4exLK9zNtPFWJKo/seq-rerun-a-few-background-posts-by-robin-hanson", "pageUrlRelative": "/posts/fj4exLK9zNtPFWJKo/seq-rerun-a-few-background-posts-by-robin-hanson", "linkUrl": "https://www.lesswrong.com/posts/fj4exLK9zNtPFWJKo/seq-rerun-a-few-background-posts-by-robin-hanson", "postedAtFormatted": "Tuesday, November 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20A%20Few%20Background%20Posts%20by%20Robin%20Hanson&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20A%20Few%20Background%20Posts%20by%20Robin%20Hanson%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffj4exLK9zNtPFWJKo%2Fseq-rerun-a-few-background-posts-by-robin-hanson%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20A%20Few%20Background%20Posts%20by%20Robin%20Hanson%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffj4exLK9zNtPFWJKo%2Fseq-rerun-a-few-background-posts-by-robin-hanson", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ffj4exLK9zNtPFWJKo%2Fseq-rerun-a-few-background-posts-by-robin-hanson", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 155, "htmlBody": "<p>I've held off on posting the next rerun for a few days in case anybody else suggested something new in the discussion of how to run the AI FOOM Debate. After looking at the comments (with associated votes), and after looking through the sequence myself, I've decided to rerun one post a day. Most of these posts will be from Robin Hanson and Eliezer Yudkowsky, but there are a few posts from Carl Shulman and James Miller that will be included as well. This process will start tomorrow with \"Abstraction, Not Analogy\" by Robin Hanson.</p>\n<p>Meanwhile, there are several posts written by Robin Hanson in the week or so leading up to the debate that provide a bit of background to his perspectives, which I have linked to below. They're all fairly short and relatively straightforward, so I don't think that they each merit a full blown individual discussion.</p>\n<p><a href=\"http://www.overcomingbias.com/2008/11/fund-ubertool.html\">Fund UberTool?</a></p>\n<p><a href=\"http://www.overcomingbias.com/2008/11/engelbarts-uber.html\">Engelbart As UberTool?</a></p>\n<p><a href=\"http://www.overcomingbias.com/2008/11/englebart-not-r.html\">Friendly Teams</a></p>\n<p><a href=\"http://www.overcomingbias.com/2008/11/friendliness-fa.html\">Friendliness Factors</a></p>\n<p><a href=\"http://www.overcomingbias.com/2008/11/setting-the-sta.html\">Setting the Stage</a></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fj4exLK9zNtPFWJKo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.026829453335416e-06, "legacy": true, "legacyId": "19859", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-06T05:28:16.489Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup - Fun Theory", "slug": "meetup-west-la-meetup-fun-theory", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zkxbws2sNJ8SaDrDM/meetup-west-la-meetup-fun-theory", "pageUrlRelative": "/posts/zkxbws2sNJ8SaDrDM/meetup-west-la-meetup-fun-theory", "linkUrl": "https://www.lesswrong.com/posts/zkxbws2sNJ8SaDrDM/meetup-west-la-meetup-fun-theory", "postedAtFormatted": "Tuesday, November 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%20-%20Fun%20Theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%20-%20Fun%20Theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzkxbws2sNJ8SaDrDM%2Fmeetup-west-la-meetup-fun-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%20-%20Fun%20Theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzkxbws2sNJ8SaDrDM%2Fmeetup-west-la-meetup-fun-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fzkxbws2sNJ8SaDrDM%2Fmeetup-west-la-meetup-fun-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 190, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/fl'>West LA Meetup - Fun Theory</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 November 2012 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm Wednesday, November 7th.</p>\n\n<p><strong>Where:</strong> The Westside Tavern <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion Topic:</strong> This week, let's have a serious talk about fun. I'm talking about <a href=\"http://lesswrong.com/lw/xy/the_fun_theory_sequence/\">fun theory</a>. While this set of topics leaves lots of room for wandering, there will be some specific discussion goals for the night. Until then, enjoy some of the relevant articles!</p>\n\n<p>There will be general discussion too, and there are lots of interesting <a href=\"http://lesswrong.com/recentposts/\">recent posts</a> (also check out LW's sister site, <a href=\"http://www.overcomingbias.com/\">Overcoming Bias</a> ). But don't worry if you don't have time to read any articles, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/fl'>West LA Meetup - Fun Theory</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zkxbws2sNJ8SaDrDM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.026839083017333e-06, "legacy": true, "legacyId": "19860", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Fun_Theory\">Discussion article for the meetup : <a href=\"/meetups/fl\">West LA Meetup - Fun Theory</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 November 2012 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm Wednesday, November 7th.</p>\n\n<p><strong>Where:</strong> The Westside Tavern <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"https://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters. The entrance sign says \"Lounge\".</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Discussion Topic:</strong> This week, let's have a serious talk about fun. I'm talking about <a href=\"http://lesswrong.com/lw/xy/the_fun_theory_sequence/\">fun theory</a>. While this set of topics leaves lots of room for wandering, there will be some specific discussion goals for the night. Until then, enjoy some of the relevant articles!</p>\n\n<p>There will be general discussion too, and there are lots of interesting <a href=\"http://lesswrong.com/recentposts/\">recent posts</a> (also check out LW's sister site, <a href=\"http://www.overcomingbias.com/\">Overcoming Bias</a> ). But don't worry if you don't have time to read any articles, or even if you've never read any Less Wrong! Bring a friend! The atmosphere is casual, and good, intelligent conversation with friendly people is guaranteed.</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup___Fun_Theory1\">Discussion article for the meetup : <a href=\"/meetups/fl\">West LA Meetup - Fun Theory</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup - Fun Theory", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Fun_Theory", "level": 1}, {"title": "Discussion article for the meetup : West LA Meetup - Fun Theory", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup___Fun_Theory1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["K4aGvLnHvYgX9pZHS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-06T07:51:36.429Z", "modifiedAt": null, "url": null, "title": "Meetup :  Moscow: Applied Rationality and Cognitive Biases", "slug": "meetup-moscow-applied-rationality-and-cognitive-biases-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oqiSeLrcEwitW45HL/meetup-moscow-applied-rationality-and-cognitive-biases-0", "pageUrlRelative": "/posts/oqiSeLrcEwitW45HL/meetup-moscow-applied-rationality-and-cognitive-biases-0", "linkUrl": "https://www.lesswrong.com/posts/oqiSeLrcEwitW45HL/meetup-moscow-applied-rationality-and-cognitive-biases-0", "postedAtFormatted": "Tuesday, November 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20%20Moscow%3A%20Applied%20Rationality%20and%20Cognitive%20Biases&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20%20Moscow%3A%20Applied%20Rationality%20and%20Cognitive%20Biases%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoqiSeLrcEwitW45HL%2Fmeetup-moscow-applied-rationality-and-cognitive-biases-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20%20Moscow%3A%20Applied%20Rationality%20and%20Cognitive%20Biases%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoqiSeLrcEwitW45HL%2Fmeetup-moscow-applied-rationality-and-cognitive-biases-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoqiSeLrcEwitW45HL%2Fmeetup-moscow-applied-rationality-and-cognitive-biases-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 146, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/fm'> Moscow: Applied Rationality and Cognitive Biases</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 November 2012 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Rossiya, Moscow, ulitsa Ostozhenka 14/2</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet at \u201cSubway\u201d restaurant, entrance from Lopukhinskiy pereulok. Look for a table with \u201cLW\u201d banner, I will be there from 16:00 MSK.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li><p>Applied rationality: practice. We will improve our rationality skills.</p></li>\n<li><p>Cognitive biases analysis. Here is <a href=\"https://docs.google.com/document/d/1fNqnLwnAB4oD7ePu6gK7WNZwoL4RuDyx2Nu_O5AMxpE/edit\" rel=\"nofollow\">the link to the list of biases we will work on</a> (in Russian).</p></li>\n</ul>\n\n<p>If you are going for the first time, please fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p>\n\n<p>N. B. Google shows incorrect location, please use <a href=\"http://maps.yandex.ru/?text=%D0%A0%D0%BE%D1%81%D1%81%D0%B8%D1%8F%2C%20%D0%9C%D0%BE%D1%81%D0%BA%D0%B2%D0%B0%2C%20%D1%83%D0%BB%D0%B8%D1%86%D0%B0%20%D0%9E%D1%81%D1%82%D0%BE%D0%B6%D0%B5%D0%BD%D0%BA%D0%B0%2C%2014%2F2&amp;sll=37.599139%2C55.74185&amp;ll=37.599139%2C55.741850&amp;spn=0.014377%2C0.004449&amp;z=17&amp;l=map\" rel=\"nofollow\">Yandex maps</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/fm'> Moscow: Applied Rationality and Cognitive Biases</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oqiSeLrcEwitW45HL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0269179927280832e-06, "legacy": true, "legacyId": "19872", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup____Moscow__Applied_Rationality_and_Cognitive_Biases\">Discussion article for the meetup : <a href=\"/meetups/fm\"> Moscow: Applied Rationality and Cognitive Biases</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 November 2012 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Rossiya, Moscow, ulitsa Ostozhenka 14/2</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet at \u201cSubway\u201d restaurant, entrance from Lopukhinskiy pereulok. Look for a table with \u201cLW\u201d banner, I will be there from 16:00 MSK.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li><p>Applied rationality: practice. We will improve our rationality skills.</p></li>\n<li><p>Cognitive biases analysis. Here is <a href=\"https://docs.google.com/document/d/1fNqnLwnAB4oD7ePu6gK7WNZwoL4RuDyx2Nu_O5AMxpE/edit\" rel=\"nofollow\">the link to the list of biases we will work on</a> (in Russian).</p></li>\n</ul>\n\n<p>If you are going for the first time, please fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p>\n\n<p>N. B. Google shows incorrect location, please use <a href=\"http://maps.yandex.ru/?text=%D0%A0%D0%BE%D1%81%D1%81%D0%B8%D1%8F%2C%20%D0%9C%D0%BE%D1%81%D0%BA%D0%B2%D0%B0%2C%20%D1%83%D0%BB%D0%B8%D1%86%D0%B0%20%D0%9E%D1%81%D1%82%D0%BE%D0%B6%D0%B5%D0%BD%D0%BA%D0%B0%2C%2014%2F2&amp;sll=37.599139%2C55.74185&amp;ll=37.599139%2C55.741850&amp;spn=0.014377%2C0.004449&amp;z=17&amp;l=map\" rel=\"nofollow\">Yandex maps</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup____Moscow__Applied_Rationality_and_Cognitive_Biases1\">Discussion article for the meetup : <a href=\"/meetups/fm\"> Moscow: Applied Rationality and Cognitive Biases</a></h2>", "sections": [{"title": "Discussion article for the meetup :  Moscow: Applied Rationality and Cognitive Biases", "anchor": "Discussion_article_for_the_meetup____Moscow__Applied_Rationality_and_Cognitive_Biases", "level": 1}, {"title": "Discussion article for the meetup :  Moscow: Applied Rationality and Cognitive Biases", "anchor": "Discussion_article_for_the_meetup____Moscow__Applied_Rationality_and_Cognitive_Biases1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-06T11:33:32.960Z", "modifiedAt": null, "url": null, "title": "Gauging interest for a Rio de Janeiro meetup group. ", "slug": "gauging-interest-for-a-rio-de-janeiro-meetup-group", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:33.873Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "roland", "createdAt": "2009-02-27T23:03:47.279Z", "isAdmin": false, "displayName": "roland"}, "userId": "p2C9rpg32LHrGwer8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AHm9pLrGx79Ps57hk/gauging-interest-for-a-rio-de-janeiro-meetup-group", "pageUrlRelative": "/posts/AHm9pLrGx79Ps57hk/gauging-interest-for-a-rio-de-janeiro-meetup-group", "linkUrl": "https://www.lesswrong.com/posts/AHm9pLrGx79Ps57hk/gauging-interest-for-a-rio-de-janeiro-meetup-group", "postedAtFormatted": "Tuesday, November 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Gauging%20interest%20for%20a%20Rio%20de%20Janeiro%20meetup%20group.%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGauging%20interest%20for%20a%20Rio%20de%20Janeiro%20meetup%20group.%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAHm9pLrGx79Ps57hk%2Fgauging-interest-for-a-rio-de-janeiro-meetup-group%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Gauging%20interest%20for%20a%20Rio%20de%20Janeiro%20meetup%20group.%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAHm9pLrGx79Ps57hk%2Fgauging-interest-for-a-rio-de-janeiro-meetup-group", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAHm9pLrGx79Ps57hk%2Fgauging-interest-for-a-rio-de-janeiro-meetup-group", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 40, "htmlBody": "<p>Who would be interested? Please reply to this thread or msg in private.</p>\n<p>Thanks!</p>\n<p>Edit: It's Rio de Janeiro, RJ, in Brazil.</p>\n<p>It seems it is going to happen, discussion also on fb now:</p>\n<p><a rel=\"nofollow\" href=\"http://www.facebook.com/Ierfh/posts/108940625936257?comment_id=28607&amp;ref=notif&amp;notif_t=comment_mention\">http://www.facebook.com/Ierfh/posts/108940625936257?comment_id=28607&amp;ref=notif&not;if_t=comment_mention</a></p>\n<p>&nbsp;</p>\n<p>Ok, the meetup will happen, here is the link: http://lesswrong.com/meetups/fp</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AHm9pLrGx79Ps57hk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 2, "extendedScore": null, "score": 1.0270402014653827e-06, "legacy": true, "legacyId": "19877", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-06T18:03:01.837Z", "modifiedAt": null, "url": null, "title": "[LINK] TEDx \"When creative machines overtake man\"", "slug": "link-tedx-when-creative-machines-overtake-man", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:33.933Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kawoomba", "createdAt": "2012-05-01T11:54:25.423Z", "isAdmin": false, "displayName": "Kawoomba"}, "userId": "FScr44PGNPbBCodRv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wY2SnLqSPcFCgnNna/link-tedx-when-creative-machines-overtake-man", "pageUrlRelative": "/posts/wY2SnLqSPcFCgnNna/link-tedx-when-creative-machines-overtake-man", "linkUrl": "https://www.lesswrong.com/posts/wY2SnLqSPcFCgnNna/link-tedx-when-creative-machines-overtake-man", "postedAtFormatted": "Tuesday, November 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20TEDx%20%22When%20creative%20machines%20overtake%20man%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20TEDx%20%22When%20creative%20machines%20overtake%20man%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwY2SnLqSPcFCgnNna%2Flink-tedx-when-creative-machines-overtake-man%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20TEDx%20%22When%20creative%20machines%20overtake%20man%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwY2SnLqSPcFCgnNna%2Flink-tedx-when-creative-machines-overtake-man", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwY2SnLqSPcFCgnNna%2Flink-tedx-when-creative-machines-overtake-man", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 243, "htmlBody": "<div>Video <a href=\"http://www.youtube.com/watch?v=KQ35zNlyG-o\" target=\"_blank\">here</a>, illustrated transcript <a href=\"http://www.kurzweilai.net/when-creative-machines-overtake-man\" target=\"_blank\">here</a>.</div>\n<div><br /></div>\n<div>Contains a good sketch of Schmidhuber's <a href=\"http://www.idsia.ch/~juergen/creativity.html\" target=\"_blank\">Formal Theory of Fun and Creativity</a>.</div>\n<div><br /></div>\n<div>One TEDx passage in particular stuck with me, quoted with context:</div>\n<div><br /></div>\n<blockquote>\n<div>In a few decades, such [creative] machines will have more computational power than human brains.</div>\n<div><br /></div>\n<div>This will have consequences. My kids were born around 2000. The insurance mathematicians say they are expected to see the year 2100, because they are girls.</div>\n<div><br /></div>\n<div>A substantial fraction of their lives will be spent in a world where the smartest things are not humans, but the artificial brains of an emerging robot civilization, which presumably will spread throughout the solar system and beyond (space is hostile to humans but nice to robots).</div>\n<div><br /></div>\n<div>This will change everything much more than, say, global warming, etc. But hardly any politician is aware of this development happening before our eyes. Like the water lilies which every day cover twice as much of the pond, but get noticed only a few days before the pond is full.</div>\n<div><br /></div>\n<div>My final advice: don't think of us, the humans, versus them, those future uber-robots. Instead view yourself, and humankind in general, as a stepping stone (not the last one) on the path of the universe towards more and more unfathomable complexity. Be content with that little role in the grand scheme of things.</div>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wY2SnLqSPcFCgnNna", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 5, "extendedScore": null, "score": 1.0272547250885254e-06, "legacy": true, "legacyId": "19878", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-06T22:38:36.144Z", "modifiedAt": "2021-11-25T10:09:40.535Z", "url": null, "title": "Rationality Quotes November 2012", "slug": "rationality-quotes-november-2012", "viewCount": null, "lastCommentedAt": "2014-02-22T04:26:14.315Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "a7xJQpZ55R6SxFTik", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wKWvodoAt3zRhyC4x/rationality-quotes-november-2012", "pageUrlRelative": "/posts/wKWvodoAt3zRhyC4x/rationality-quotes-november-2012", "linkUrl": "https://www.lesswrong.com/posts/wKWvodoAt3zRhyC4x/rationality-quotes-november-2012", "postedAtFormatted": "Tuesday, November 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20November%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20November%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwKWvodoAt3zRhyC4x%2Frationality-quotes-november-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20November%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwKWvodoAt3zRhyC4x%2Frationality-quotes-november-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwKWvodoAt3zRhyC4x%2Frationality-quotes-november-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 15.833333015441895px; text-align: justify;\">Here's the new thread for posting quotes, with the usual rules:</p>\n<ul style=\"padding: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 15.833333015441895px; text-align: justify;\">\n<li>Please post all quotes separately, so that they can be voted up/down separately. &nbsp;(If they are strongly related, reply to your own comments. &nbsp;If strongly ordered, then go ahead and post them together.)</li>\n<li>Do not quote yourself</li>\n<li>Do not quote comments/posts on LW/OB</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wKWvodoAt3zRhyC4x", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 10, "extendedScore": null, "score": 1.027406556034559e-06, "legacy": true, "legacyId": "19761", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 903, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2012-11-06T22:38:36.144Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-07T06:07:54.321Z", "modifiedAt": null, "url": null, "title": "Think Twice: A Response to Kevin Kelly on \u2018Thinkism\u2019", "slug": "think-twice-a-response-to-kevin-kelly-on-thinkism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:34.701Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MichaelAnissimov", "createdAt": "2009-03-21T20:49:52.763Z", "isAdmin": false, "displayName": "MichaelAnissimov"}, "userId": "tkZmAXciPjSumi4Wk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FeJzRL6PqsvrTswBk/think-twice-a-response-to-kevin-kelly-on-thinkism", "pageUrlRelative": "/posts/FeJzRL6PqsvrTswBk/think-twice-a-response-to-kevin-kelly-on-thinkism", "linkUrl": "https://www.lesswrong.com/posts/FeJzRL6PqsvrTswBk/think-twice-a-response-to-kevin-kelly-on-thinkism", "postedAtFormatted": "Wednesday, November 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Think%20Twice%3A%20A%20Response%20to%20Kevin%20Kelly%20on%20%E2%80%98Thinkism%E2%80%99&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThink%20Twice%3A%20A%20Response%20to%20Kevin%20Kelly%20on%20%E2%80%98Thinkism%E2%80%99%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFeJzRL6PqsvrTswBk%2Fthink-twice-a-response-to-kevin-kelly-on-thinkism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Think%20Twice%3A%20A%20Response%20to%20Kevin%20Kelly%20on%20%E2%80%98Thinkism%E2%80%99%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFeJzRL6PqsvrTswBk%2Fthink-twice-a-response-to-kevin-kelly-on-thinkism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFeJzRL6PqsvrTswBk%2Fthink-twice-a-response-to-kevin-kelly-on-thinkism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 42, "htmlBody": "<p>I wrote a <a href=\"http://www.acceleratingfuture.com/michael/blog/2012/11/think-twice-a-response-to-kevin-kelly-on-thinkism/\">blog post</a> responding to <a href=\"http://www.kk.org/thetechnium/archives/2008/09/thinkism.php\">Kevin Kelly</a> that I'm fairly happy about. It summarizes some of the reasons why I figure that superintelligence is likely to be a fairly big deal. If you read it, please post your comments here.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FeJzRL6PqsvrTswBk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 10, "extendedScore": null, "score": 2.1e-05, "legacy": true, "legacyId": "19894", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-07T08:03:59.499Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Abstraction, Not Analogy", "slug": "seq-rerun-abstraction-not-analogy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:34.107Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LQJ4akjhGuL6bkQwu/seq-rerun-abstraction-not-analogy", "pageUrlRelative": "/posts/LQJ4akjhGuL6bkQwu/seq-rerun-abstraction-not-analogy", "linkUrl": "https://www.lesswrong.com/posts/LQJ4akjhGuL6bkQwu/seq-rerun-abstraction-not-analogy", "postedAtFormatted": "Wednesday, November 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Abstraction%2C%20Not%20Analogy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Abstraction%2C%20Not%20Analogy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLQJ4akjhGuL6bkQwu%2Fseq-rerun-abstraction-not-analogy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Abstraction%2C%20Not%20Analogy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLQJ4akjhGuL6bkQwu%2Fseq-rerun-abstraction-not-analogy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLQJ4akjhGuL6bkQwu%2Fseq-rerun-abstraction-not-analogy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 200, "htmlBody": "<p>Today's post, <a href=\"http://www.overcomingbias.com/2008/11/abstraction-vs.html\">Abstraction, Not Analogy</a> was originally published on November 19, 2008.  A summary:</p>\n<p>&nbsp;</p>\n<blockquote>Describing certain arguments as analogies misses much of the point of those arguments. In order to generate predictions, we often ignore certain information in favor of more relevant information. These sorts of abstractions succeed or fail based on whether or not they successfully capture the important details.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through old posts in order so that people who are interested can (re-)read and discuss them. This post is the first official post in the rerun of the AI-FOOM Debate. The previous post was <a href=\"/lw/fbn/seq_rerun_a_few_background_posts_by_robin_hanson/\">A Few Background Posts by Robin Hanson</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LQJ4akjhGuL6bkQwu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0277181908038362e-06, "legacy": true, "legacyId": "19899", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fj4exLK9zNtPFWJKo", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-07T09:24:05.070Z", "modifiedAt": null, "url": null, "title": "AI risk-related improvements to the LW wiki", "slug": "ai-risk-related-improvements-to-the-lw-wiki", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:27.381Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uEj6m3Qv84jfQios7/ai-risk-related-improvements-to-the-lw-wiki", "pageUrlRelative": "/posts/uEj6m3Qv84jfQios7/ai-risk-related-improvements-to-the-lw-wiki", "linkUrl": "https://www.lesswrong.com/posts/uEj6m3Qv84jfQios7/ai-risk-related-improvements-to-the-lw-wiki", "postedAtFormatted": "Wednesday, November 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AI%20risk-related%20improvements%20to%20the%20LW%20wiki&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAI%20risk-related%20improvements%20to%20the%20LW%20wiki%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuEj6m3Qv84jfQios7%2Fai-risk-related-improvements-to-the-lw-wiki%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AI%20risk-related%20improvements%20to%20the%20LW%20wiki%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuEj6m3Qv84jfQios7%2Fai-risk-related-improvements-to-the-lw-wiki", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuEj6m3Qv84jfQios7%2Fai-risk-related-improvements-to-the-lw-wiki", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 692, "htmlBody": "<p>Back in May, Luke suggested the creation of a <a href=\"/lw/cnj/a_scholarly_ai_risk_wiki/\">scholarly AI risk wiki</a>, which was to include a large set of summary articles on topics related to AI risk, mapped out in terms of how they related to the central debates about AI risk. In response, Wei Dai <a href=\"/lw/cnj/a_scholarly_ai_risk_wiki/6oho\">suggested</a> that among other things, the existing Less Wrong wiki could be improved instead. As a result, the Singularity Institute has massively improved the LW wiki, in preparation for a more ambitious scholarly AI risk wiki. The outcome was the creation or dramatic expansion of the following articles:</p>\n<ul>\n<li><a href=\"http://wiki.lesswrong.com/wiki/5-and-10\" target=\"_blank\">5-and-10</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Acausal_Trade\" target=\"_blank\">Acausal Trade</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Acceleration_thesis\" target=\"_blank\">Acceleration thesis</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Agent\" target=\"_blank\">Agent</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/AGI_chaining\" target=\"_blank\">AGI chaining</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/AGI_skepticism\" target=\"_blank\">AGI skepticism</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/AGI_Sputnik_moment\" target=\"_blank\">AGI Sputnik moment</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/AI_advantages\" target=\"_blank\">AI advantages</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/AI_arms_race\" target=\"_blank\">AI arms race</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/AI_Boxing\" target=\"_blank\">AI Boxing</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/AI-complete\" target=\"_blank\">AI-complete</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/AI_takeoff\" target=\"_blank\">AI takeoff</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/AIXI\" target=\"_blank\">AIXI</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Algorithmic_complexity\" target=\"_blank\">Algorithmic complexity</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Anvil_problem\" target=\"_blank\">Anvil problem</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Astronomical_waste\" target=\"_blank\">Astronomical waste</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Bayesian_decision_theory\" target=\"_blank\">Bayesian decision theory</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Benevolence\" target=\"_blank\">Benevolence</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Ben_Goertzel\" target=\"_blank\">Ben Goertzel</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Bias\" target=\"_blank\">Bias</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement\" target=\"_blank\">Biological Cognitive Enhancement</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Brain-computer_interfaces\" target=\"_blank\">Brain-computer interfaces</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Carl_Shulman\" target=\"_blank\">Carl Shulman</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Causal_decision_theory\" target=\"_blank\">Causal decision theory</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Church-Turing_thesis\" target=\"_blank\">Church-Turing thesis</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Coherent_Aggregated_Volition\" target=\"_blank\">Coherent Aggregated Volition</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Coherent_Blended_Volition\" target=\"_blank\">Coherent Blended Volition</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Coherent_Extrapolated_Volition\" target=\"_blank\">Coherent Extrapolated Volition</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Computing_overhang\" target=\"_blank\">Computing overhang</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Computronium\" target=\"_blank\">Computronium</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Consequentialism\" target=\"_blank\">Consequentialism</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Counterfactual_mugging\" target=\"_blank\">Counterfactual mugging</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Creating_Friendly_AI\" target=\"_blank\">Creating Friendly AI</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Cyc\" target=\"_blank\">Cyc</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Decision_theory\" target=\"_blank\">Decision theory</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Differential_intellectual_progress\" target=\"_blank\">Differential intellectual progress</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Economic_consequences_of_AI_and_whole_brain_emulation\" target=\"_blank\">Economic consequences of AI and whole brain emulation</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Eliezer_Yudkowsky\" target=\"_blank\">Eliezer Yudkowsky</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Empathic_inference\" target=\"_blank\">Empathic inference</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Emulation_argument_for_human-level_AI\" target=\"_blank\">Emulation argument for human-level AI</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/EURISKO\" target=\"_blank\">EURISKO</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Event_horizon_thesis\" target=\"_blank\">Event horizon thesis</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Evidential_Decision_Theory\" target=\"_blank\">Evidential Decision Theory</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Evolutionary_algorithm\" target=\"_blank\">Evolutionary algorithm</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Evolutionary_argument_for_human-level_AI\" target=\"_blank\">Evolutionary argument for human-level AI</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Existential_risk\" target=\"_blank\">Existential risk</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Expected_utility\" target=\"_blank\">Expected utility</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Expected_value\" target=\"_blank\">Expected value</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Extensibility_argument_for_greater-than-human_intelligence\" target=\"_blank\">Extensibility argument for greater-than-human intelligence</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/FAI-complete\" target=\"_blank\">FAI-complete</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Fallacy\" target=\"_blank\">Fallacy</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Fragility_of_value\" target=\"_blank\">Fragility_of_value</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Friendly_AI\" target=\"_blank\">Friendly AI</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Fun_Theory\" target=\"_blank\">Fun Theory</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Future_of_Humanity_Institute\" target=\"_blank\">Future of Humanity Institute</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Game_theory\" target=\"_blank\">Game theory</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/G%C3%B6del_machine\" target=\"_blank\">G&ouml;del machine</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Great_Filter\" target=\"_blank\">Great Filter</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/History_of_AI_risk_thought\" target=\"_blank\">History of AI risk thought</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Human-AGI_integration_and_trade\" target=\"_blank\">Human-AGI integration and trade</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Induction\" target=\"_blank\">Induction</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Infinities_in_ethics\" target=\"_blank\">Infinities in ethics</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Information_hazard\" target=\"_blank\">Information hazard</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Instrumental_convergence_thesis\" target=\"_blank\">Instrumental convergence thesis</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Intelligence\" target=\"_blank\">Intelligence</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Intelligence_explosion\" target=\"_blank\">Intelligence explosion</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Jeeves_Problem\" target=\"_blank\">Jeeves Problem</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Lifespan_dilemma\" target=\"_blank\">Lifespan dilemma</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Machine_ethics\" target=\"_blank\">Machine ethics</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Machine_learning\" target=\"_blank\">Machine learning</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Malthusian_Scenarios\" target=\"_blank\">Malthusian Scenarios</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Metaethics\" target=\"_blank\">Metaethics</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Moore%27s_law\" target=\"_blank\">Moore's law</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Moral_divergence\" target=\"_blank\">Moral divergence</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Moral_uncertainty\" target=\"_blank\">Moral uncertainty</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Nanny_AI\" target=\"_blank\">Nanny AI</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Nanotechnology\" target=\"_blank\">Nanotechnology</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Neuromorphic_AI\" target=\"_blank\">Neuromorphic AI</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Nick_Bostrom\" target=\"_blank\">Nick Bostrom</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Nonperson_predicate\" target=\"_blank\">Nonperson predicate</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Observation_selection_effect\" target=\"_blank\">Observation selection effect</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Ontological_crisis\" target=\"_blank\">Ontological crisis</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Optimal_philanthropy\" target=\"_blank\">Optimal philanthropy</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Optimization_power\" target=\"_blank\">Optimization power</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Optimization_process\" target=\"_blank\">Optimization process</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Oracle_AI\" target=\"_blank\">Oracle AI</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Orthogonality_thesis\" target=\"_blank\">Orthogonality thesis</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Paperclip_maximizer\" target=\"_blank\">Paperclip maximizer</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Pascal%27s_mugging\" target=\"_blank\">Pascal's mugging</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Prediction_market\" target=\"_blank\">Prediction market</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Preference\" target=\"_blank\">Preference</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Prior_probability\" target=\"_blank\">Prior probability</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Probability_theory\" target=\"_blank\">Probability theory</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Recursive_self-improvement\" target=\"_blank\">Recursive self-improvement</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Reflective_decision_theory\" target=\"_blank\">Reflective decision theory</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Regulation_and_AI_risk\" target=\"_blank\">Regulation and AI risk</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Reinforcement_learning\" target=\"_blank\">Reinforcement learning</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Search_space\" target=\"_blank\">Search space</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Seed_AI\" target=\"_blank\">Seed AI</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Self_Indication_Assumption\" target=\"_blank\">Self Indication Assumption</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Self_Sampling_Assumption\" target=\"_blank\">Self Sampling Assumption</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Scoring_rule\" target=\"_blank\">Scoring rule</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Simulation_argument\" target=\"_blank\">Simulation argument</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Simulation_hypothesis\" target=\"_blank\">Simulation hypothesis</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Singleton\" target=\"_blank\">Singleton</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Singularitarianism\" target=\"_blank\">Singularitarianism</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Singularity\" target=\"_blank\">Singularity</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Subgoal_stomp\" target=\"_blank\">Subgoal stomp</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Superintelligence\" target=\"_blank\">Superintelligence</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Superorganism\" target=\"_blank\">Superorganism</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Technological_forecasting\" target=\"_blank\">Technological forecasting</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Technological_revolution\" target=\"_blank\">Technological revolution</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Terminal_value\" target=\"_blank\">Terminal value</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\" target=\"_blank\">Timeless decision theory</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Tool_AI\" target=\"_blank\">Tool AI</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Unfriendly_AI\" target=\"_blank\">Unfriendly AI</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Universal_intelligence\" target=\"_blank\">Universal intelligence</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Utility\" target=\"_blank\">Utility</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Utility_extraction\" target=\"_blank\">Utility extraction</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Utility_indifference\" target=\"_blank\">Utility indifference</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Value_extrapolation\" target=\"_blank\">Value extrapolation</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Value_learning\" target=\"_blank\">Value learning</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Whole_brain_emulation\" target=\"_blank\">Whole brain emulation</a></li>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Wireheading\" target=\"_blank\">Wireheading</a></li>\n</ul>\n<p>In managing the project, I focused on content over presentation, so a number of articles still have minor issues such as the grammar and style having room for improvement. It's our hope that, with the largest part of the work already done, the LW community will help improve the articles even further.</p>\n<p>Thanks to everyone who worked on these pages: Alex Altair, Adam Bales, Caleb Bell, Costanza Riccioli, Daniel Trenor, Jo&atilde;o Louren&ccedil;o, Joshua Fox, Patrick Rhodes, Pedro Chaves, Stuart Armstrong, and Steven Kaas.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uEj6m3Qv84jfQios7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 41, "baseScore": 59, "extendedScore": null, "score": 0.0005743363696095945, "legacy": true, "legacyId": "19902", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 38, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mYuMdmMmGM7fFj382"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-07T17:38:26.309Z", "modifiedAt": null, "url": null, "title": "Meetup : Meetup, Champaign IL, ", "slug": "meetup-meetup-champaign-il", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:35.428Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "aspera", "createdAt": "2012-03-20T21:28:32.004Z", "isAdmin": false, "displayName": "aspera"}, "userId": "W58nKSarCzqXj8TPS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Xof46BYibmQTdzpFp/meetup-meetup-champaign-il", "pageUrlRelative": "/posts/Xof46BYibmQTdzpFp/meetup-meetup-champaign-il", "linkUrl": "https://www.lesswrong.com/posts/Xof46BYibmQTdzpFp/meetup-meetup-champaign-il", "postedAtFormatted": "Wednesday, November 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Meetup%2C%20Champaign%20IL%2C%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Meetup%2C%20Champaign%20IL%2C%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXof46BYibmQTdzpFp%2Fmeetup-meetup-champaign-il%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Meetup%2C%20Champaign%20IL%2C%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXof46BYibmQTdzpFp%2Fmeetup-meetup-champaign-il", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXof46BYibmQTdzpFp%2Fmeetup-meetup-champaign-il", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 134, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/fn'>Meetup, Champaign IL, </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 November 2012 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Cafe Paradiso,  \t801 South Lincoln Avenue  Urbana, IL 61801</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Let's get together on Wednesday the 14th of November at 7pm at Cafe Paradiso.</p>\n\n<p>Last time we talked about wanting to discuss specific topics at the meetings, and we decided that we're going to start with Consequentialism.</p>\n\n<p>So try to read a little bit about it before next time. Possibilities include:</p>\n\n<ul>\n<li><a href=\"http://en.wikipedia.org/wiki/Consequentialism\" rel=\"nofollow\">Wikipedia</a></li>\n<li><a href=\"http://lesswrong.com/lw/b4f/sotw_check_consequentialism/\">Check Consequentialism on LessWrong</a> </li>\n<li><a href=\"http://www.raikoth.net/consequentialism.html\" rel=\"nofollow\">Consequentialism FAQ</a></li>\n<li><a href=\"http://lesswrong.com/lw/778/consequentialism_need_not_be_nearsighted/#more\">Consequentialism Need Not Be Nearsighted</a></li>\n</ul>\n\n<p>Also, think about any topics you'd be interested in eventually presenting to the rest of us. You don't need to be an expert: just interested. It can be LW related or not.</p>\n\n<p>See you then.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/fn'>Meetup, Champaign IL, </a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Xof46BYibmQTdzpFp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0280349894325248e-06, "legacy": true, "legacyId": "19904", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Meetup__Champaign_IL__\">Discussion article for the meetup : <a href=\"/meetups/fn\">Meetup, Champaign IL, </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 November 2012 07:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Cafe Paradiso,  \t801 South Lincoln Avenue  Urbana, IL 61801</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Let's get together on Wednesday the 14th of November at 7pm at Cafe Paradiso.</p>\n\n<p>Last time we talked about wanting to discuss specific topics at the meetings, and we decided that we're going to start with Consequentialism.</p>\n\n<p>So try to read a little bit about it before next time. Possibilities include:</p>\n\n<ul>\n<li><a href=\"http://en.wikipedia.org/wiki/Consequentialism\" rel=\"nofollow\">Wikipedia</a></li>\n<li><a href=\"http://lesswrong.com/lw/b4f/sotw_check_consequentialism/\">Check Consequentialism on LessWrong</a> </li>\n<li><a href=\"http://www.raikoth.net/consequentialism.html\" rel=\"nofollow\">Consequentialism FAQ</a></li>\n<li><a href=\"http://lesswrong.com/lw/778/consequentialism_need_not_be_nearsighted/#more\">Consequentialism Need Not Be Nearsighted</a></li>\n</ul>\n\n<p>Also, think about any topics you'd be interested in eventually presenting to the rest of us. You don't need to be an expert: just interested. It can be LW related or not.</p>\n\n<p>See you then.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Meetup__Champaign_IL__1\">Discussion article for the meetup : <a href=\"/meetups/fn\">Meetup, Champaign IL, </a></h2>", "sections": [{"title": "Discussion article for the meetup : Meetup, Champaign IL, ", "anchor": "Discussion_article_for_the_meetup___Meetup__Champaign_IL__", "level": 1}, {"title": "Discussion article for the meetup : Meetup, Champaign IL, ", "anchor": "Discussion_article_for_the_meetup___Meetup__Champaign_IL__1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xypbWhzEEw4ZsRK9i"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-07T19:26:31.047Z", "modifiedAt": null, "url": null, "title": "Meetup :St.Louis MO: Psycology of memorization, Thiel Fellow James Koppel, and games", "slug": "meetup-st-louis-mo-psycology-of-memorization-thiel-fellow", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:35.541Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "beoShaffer", "createdAt": "2011-05-29T15:52:29.240Z", "isAdmin": false, "displayName": "beoShaffer"}, "userId": "589WwYp3jytZqATFL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oF6ubqgNABXvEg8oY/meetup-st-louis-mo-psycology-of-memorization-thiel-fellow", "pageUrlRelative": "/posts/oF6ubqgNABXvEg8oY/meetup-st-louis-mo-psycology-of-memorization-thiel-fellow", "linkUrl": "https://www.lesswrong.com/posts/oF6ubqgNABXvEg8oY/meetup-st-louis-mo-psycology-of-memorization-thiel-fellow", "postedAtFormatted": "Wednesday, November 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3ASt.Louis%20MO%3A%20Psycology%20of%20memorization%2C%20Thiel%20Fellow%20James%20Koppel%2C%20and%20games&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3ASt.Louis%20MO%3A%20Psycology%20of%20memorization%2C%20Thiel%20Fellow%20James%20Koppel%2C%20and%20games%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoF6ubqgNABXvEg8oY%2Fmeetup-st-louis-mo-psycology-of-memorization-thiel-fellow%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3ASt.Louis%20MO%3A%20Psycology%20of%20memorization%2C%20Thiel%20Fellow%20James%20Koppel%2C%20and%20games%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoF6ubqgNABXvEg8oY%2Fmeetup-st-louis-mo-psycology-of-memorization-thiel-fellow", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoF6ubqgNABXvEg8oY%2Fmeetup-st-louis-mo-psycology-of-memorization-thiel-fellow", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 167, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/fo\">Psycology of memorization, Thiel Fellow James Koppel, and games</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">24 November 2012 02:30:00PM (-0600)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">St.Louis Mo, <a href=\"http://www.kaldiscoffee.com/LOCATIONS/Demun/tabid/109/List/1/CategoryID/155/Level/a/Default.aspx\">Kaldi's Coffee Clayton branch</a></span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>&nbsp;</p>\n<div style=\"font-size: 14px; color: #666666; line-height: 1.35em; font-family: verdana, arial, sans-serif; width: 100%; height: 100%; background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: #ffffff; font-weight: normal; padding: 0px; margin: 0px;\">\n<p style=\"margin-top: 0px; margin-right: 0px; margin-left: 0px; margin-bottom: 0.7em; font-family: inherit; font-size: 1em; padding: 0px;\">I'm planing to do a presentation with optional exercises on the psychology of memorization with an emphasis on learning names. I plan on focusing on techniques that haven't already received much attention on Less Wrong, but will have materiel on things like SRS prepared in case anyone isn't already familiar with them.<br />Also, if you'd like to talk to Thiel Fellow and Singularity summit speaker James Koppel now's your chance. &nbsp;He's told me that will be attending, and since he no longer lives in the St.Louis area you might not get another chance.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-left: 0px; margin-bottom: 0.7em; font-family: inherit; font-size: 1em; padding: 0px;\">I'll also be bringing some games for anyone who wants to stick around after the presentation. &nbsp;Feel free to bring some of your own.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-left: 0px; margin-bottom: 0.7em; font-family: inherit; font-size: 1em; padding: 0px;\">cross-posted from Less Wrong</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-left: 0px; margin-bottom: 0.7em; font-family: inherit; font-size: 1em; padding: 0px;\">-edited for writing style and updated location</p>\n</div>\n<p>&nbsp;</p>\n<p>cross-posted from http://www.meetup.com/lesswrong/</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/fo\">Psycology of memorization, Thiel Fellow James Koppel, and games</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oF6ubqgNABXvEg8oY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.0280946126428011e-06, "legacy": true, "legacyId": "19906", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Psycology_of_memorization__Thiel_Fellow_James_Koppel__and_games\">Discussion article for the meetup : <a href=\"/meetups/fo\">Psycology of memorization, Thiel Fellow James Koppel, and games</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">24 November 2012 02:30:00PM (-0600)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">St.Louis Mo, <a href=\"http://www.kaldiscoffee.com/LOCATIONS/Demun/tabid/109/List/1/CategoryID/155/Level/a/Default.aspx\">Kaldi's Coffee Clayton branch</a></span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>&nbsp;</p>\n<div style=\"font-size: 14px; color: #666666; line-height: 1.35em; font-family: verdana, arial, sans-serif; width: 100%; height: 100%; background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: #ffffff; font-weight: normal; padding: 0px; margin: 0px;\">\n<p style=\"margin-top: 0px; margin-right: 0px; margin-left: 0px; margin-bottom: 0.7em; font-family: inherit; font-size: 1em; padding: 0px;\">I'm planing to do a presentation with optional exercises on the psychology of memorization with an emphasis on learning names. I plan on focusing on techniques that haven't already received much attention on Less Wrong, but will have materiel on things like SRS prepared in case anyone isn't already familiar with them.<br>Also, if you'd like to talk to Thiel Fellow and Singularity summit speaker James Koppel now's your chance. &nbsp;He's told me that will be attending, and since he no longer lives in the St.Louis area you might not get another chance.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-left: 0px; margin-bottom: 0.7em; font-family: inherit; font-size: 1em; padding: 0px;\">I'll also be bringing some games for anyone who wants to stick around after the presentation. &nbsp;Feel free to bring some of your own.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-left: 0px; margin-bottom: 0.7em; font-family: inherit; font-size: 1em; padding: 0px;\">cross-posted from Less Wrong</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-left: 0px; margin-bottom: 0.7em; font-family: inherit; font-size: 1em; padding: 0px;\">-edited for writing style and updated location</p>\n</div>\n<p>&nbsp;</p>\n<p>cross-posted from http://www.meetup.com/lesswrong/</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Psycology_of_memorization__Thiel_Fellow_James_Koppel__and_games1\">Discussion article for the meetup : <a href=\"/meetups/fo\">Psycology of memorization, Thiel Fellow James Koppel, and games</a></h2>", "sections": [{"title": "Discussion article for the meetup : Psycology of memorization, Thiel Fellow James Koppel, and games", "anchor": "Discussion_article_for_the_meetup___Psycology_of_memorization__Thiel_Fellow_James_Koppel__and_games", "level": 1}, {"title": "Discussion article for the meetup : Psycology of memorization, Thiel Fellow James Koppel, and games", "anchor": "Discussion_article_for_the_meetup___Psycology_of_memorization__Thiel_Fellow_James_Koppel__and_games1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-07T21:19:19.244Z", "modifiedAt": null, "url": null, "title": "Checklist of Rationality Habits", "slug": "checklist-of-rationality-habits", "viewCount": null, "lastCommentedAt": "2018-05-29T22:55:37.405Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ttGbpJQ8shBi8hDhh/checklist-of-rationality-habits", "pageUrlRelative": "/posts/ttGbpJQ8shBi8hDhh/checklist-of-rationality-habits", "linkUrl": "https://www.lesswrong.com/posts/ttGbpJQ8shBi8hDhh/checklist-of-rationality-habits", "postedAtFormatted": "Wednesday, November 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Checklist%20of%20Rationality%20Habits&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AChecklist%20of%20Rationality%20Habits%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FttGbpJQ8shBi8hDhh%2Fchecklist-of-rationality-habits%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Checklist%20of%20Rationality%20Habits%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FttGbpJQ8shBi8hDhh%2Fchecklist-of-rationality-habits", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FttGbpJQ8shBi8hDhh%2Fchecklist-of-rationality-habits", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3251, "htmlBody": "<div>As you may know, the <a href=\"http://appliedrationality.org/\">Center for Applied Rationality</a> has run several workshops, each teaching content similar to that in the <a href=\"http://wiki.lesswrong.com/wiki/Sequences#Core_Sequences\">core sequences</a>, but made more <a href=\"/lw/47j/make_your_training_useful/\">practical</a>, and more into <a href=\"/lw/5kz/the_5second_level/\">fine-grained</a> habits.</div>\n<div><br /></div>\n<div>Below is the checklist of rationality habits we have been using in the minicamps' opening session. &nbsp;It was co-written by Eliezer, myself, and a number of others at CFAR. &nbsp;As mentioned below, the goal is not to assess how \"rational\" you are, but, rather, to develop a personal shopping list of habits to consider developing. &nbsp;We generated it by asking ourselves, not what rationality content it's useful to <em>understand</em>, but what rationality-related actions (or thinking habits) it's useful to actually <em>do</em>.</div>\n<div><br /></div>\n<div>I hope you find it useful; I certainly have. &nbsp;Comments and suggestions are most welcome; it remains a work in progress.&nbsp;(It's also available as a <a href=\" https://docs.google.com/open?id=0B7x1QWa2vNf5WmJ1ekZqNVVJR28\">pdf</a>.)&nbsp;</div>\n<div><a id=\"more\"></a><br /></div>\n<div>---</div>\n<div><br /></div>\n<div>This checklist is meant for your personal use so you can have a wish-list of rationality habits, and so that you can see if you're acquiring good habits over the next year&mdash;it's not meant to be a way to get a 'how rational are you?' score, but, rather, a way to notice specific habits you might want to develop. &nbsp;For each item, you might ask yourself: did you last use this habit...</div>\n<div style=\"font-style: normal; \">\n<ul>\n<li><em>Never</em></li>\n<li><em>Today/yesterday</em></li>\n<li><em>Last week</em></li>\n<li><em>Last month</em></li>\n<li><em>Last year</em></li>\n<li><em>Before the last year</em></li>\n</ul>\n</div>\n<div><br /></div>\n<div><ol>\n<li><strong>Reacting to evidence / surprises / arguments you haven't heard before; flagging beliefs for examination.</strong><ol>\n<li>When I see something odd - something that doesn't fit with what I'd ordinarily expect, given my other beliefs - I successfully notice, promote it to conscious attention and think \"I notice that I am confused\" or some equivalent thereof. <em>(Example: You think that your flight is scheduled to depart on Thursday. On Tuesday, you get an email from Travelocity advising you to prepare for your flight &ldquo;tomorrow&rdquo;, which seems wrong. Do you successfully raise this anomaly to the level of conscious attention? (Based on the experience of an actual LWer who failed to notice confusion at this point and missed their plane flight.)<br />\n<div style=\"font-style: normal; \">\n<ul>\n<br /> \n</ul>\n</div>\n</em></li>\n<li>When somebody says something that isn't quite clear enough for me to visualize, I notice this and ask for examples. <em>(Recent example from Eliezer: A mathematics student said they were studying \"stacks\". I asked for an example of a stack. They said that the integers could form a stack. I asked for an example of something that was not a stack.) (Recent example from Anna: Cat said that her boyfriend was very competitive. I asked her for an example of \"very competitive.\" She said that when he&rsquo;s driving and the person next to him revs their engine, he must be the one to leave the intersection first&mdash;and when he&rsquo;s the passenger he gets mad at the driver when they don&rsquo;t react similarly.)<br /><em>\n<div style=\"font-style: normal; \"><br /><br /></div>\n<div style=\"font-style: normal; \">\n<ul>\n</ul>\n</div>\n</em></em></li>\n<li>I notice when my mind is arguing for a side (instead of evaluating which side to choose), and flag this as an error mode. <em>(Recent example from Anna: Noticed myself explaining to myself why outsourcing my clothes shopping does make sense, rather than evaluating whether to do it.)<br /><br /><em>\n<div style=\"font-style: normal; \"><br /></div>\n<div style=\"font-style: normal; \">\n<ul>\n</ul>\n</div>\n</em></em></li>\n<li>I notice my mind flinching away from a thought; and when I notice, I flag that area as requiring more deliberate exploration. <em>(Recent example from Anna: I have a failure mode where, when I feel socially uncomfortable, I try to make others feel mistaken so that I will feel less vulnerable. Pulling this thought into words required repeated conscious effort, as my mind kept wanting to just drop the subject.)<br /><br /><em>\n<div style=\"font-style: normal; \"><br /></div>\n<div style=\"font-style: normal; \">\n<ul>\n</ul>\n</div>\n</em></em></li>\n<li>I consciously attempt to welcome bad news, or at least not push it away. <em>(Recent example from Eliezer: At a brainstorming session for future Singularity Summits, one issue raised was that we hadn't really been asking for money at previous ones. My brain was offering resistance, so I applied the \"bad news is good news\" pattern to rephrase this as, \"This point doesn't change the fixed amount of money we raised in past years, so it is good news because it implies that we can fix the strategy and do better next year.\")<br /><br /><em>\n<div style=\"font-style: normal; \"><br /></div>\n<div style=\"font-style: normal; \">\n<ul>\n</ul>\n</div>\n</em></em></li>\n</ol></li>\n<li><strong>Questioning and analyzing beliefs (after they come to your attention).</strong><ol>\n<li>I notice when I'm not being curious. <em>(Recent example from Anna: Whenever someone criticizes me, I usually find myself thinking defensively at first, and have to visualize the world in which the criticism is true, and the world in which it's false, to convince myself that I actually want to know. For example, someone criticized us for providing inadequate prior info on what statistics we'd gather for the Rationality Minicamp; and I had to visualize the consequences of [explaining to myself, internally, why I couldn&rsquo;t have done any better given everything else I had to do], vs. the possible consequences of [visualizing how it might've been done better, so as to update my action-patterns for next time], to snap my brain out of defensive-mode and into should-we-do-that-differently mode.)<br /><br /><em>\n<div style=\"font-style: normal; \"><br /></div>\n<div style=\"font-style: normal; \">\n<ul>\n</ul>\n</div>\n</em></em></li>\n<li>I look for the actual, historical causes of my beliefs, emotions, and habits; and when doing so, I can suppress my mind's search for justifications, or set aside justifications that weren't the actual, historical causes of my thoughts. <em>(Recent example from Anna: When it turned out that we couldn't rent the Minicamp location I thought I was going to get, I found lots and lots of reasons to blame the person who was supposed to get it; but realized that most of my emotion came from the fear of being blamed myself for a cost overrun.)<br /><br /><em>\n<div style=\"font-style: normal; \"><br /></div>\n<div style=\"font-style: normal; \">\n<ul>\n</ul>\n</div>\n</em></em></li>\n<li>I try to think of a concrete example that I can use to follow abstract arguments or proof steps. <em>(Classic example: Richard Feynman being disturbed that Brazilian physics students didn't know that a \"material with an index\" meant a material such as water. If someone talks about a proof over all integers, do you try it with the number 17? If your thoughts are circling around your roommate being messy, do you try checking your reasoning against the specifics of a particular occasion when they were messy?)<br /><br /><em>\n<div style=\"font-style: normal; \"><br /></div>\n<div style=\"font-style: normal; \">\n<ul>\n</ul>\n</div>\n</em></em></li>\n<li>When I'm trying to distinguish between two (or more) hypotheses using a piece of evidence, I visualize the world where hypothesis #1 holds, and try to consider the prior probability I'd have assigned to the evidence in that world, then visualize the world where hypothesis #2 holds; and see if the evidence seems more likely or more specifically predicted in one world than the other <em>(Historical example: During the Amanda Knox murder case, after many hours of police interrogation, Amanda Knox turned some cartwheels in her cell. The prosecutor argued that she was celebrating the murder. Would you, confronted with this argument, try to come up with a way to make the same evidence fit her innocence? Or would you first try visualizing an innocent detainee, then a guilty detainee, to ask with what frequency you think such people turn cartwheels during detention, to see if the likelihoods were skewed in one direction or the other?)<br /><br /><em>\n<div style=\"font-style: normal; \"><br /></div>\n<div style=\"font-style: normal; \">\n<ul>\n</ul>\n</div>\n</em></em></li>\n<li>I try to consciously assess prior probabilities and compare them to the apparent strength of evidence. <em>(Recent example from Eliezer: Used it in a conversation about apparent evidence for parapsychology, saying that for this I wanted p &lt; 0.0001, like they use in physics, rather than p &lt; 0.05, before I started paying attention at all.)<br /><br /><em>\n<div style=\"font-style: normal; \"><br /></div>\n<div style=\"font-style: normal; \">\n<ul>\n</ul>\n</div>\n</em></em></li>\n<li>When I encounter evidence that's insufficient to make me \"change my mind\" (substantially change beliefs/policies), but is still more likely to occur in world X than world Y, I try to update my probabilities at least a little. <em>(Recent example from Anna: Realized I should somewhat update my beliefs about being a good driver after someone else knocked off my side mirror, even though it was legally and probably actually their fault&mdash;even so, the accident is still more likely to occur in worlds where my bad-driver parameter is higher.)<br /><br /><em><em>\n<div style=\"font-style: normal; \"><br /></div>\n<div style=\"font-style: normal; \">\n<ul>\n</ul>\n</div>\n</em></em></em></li>\n</ol></li>\n<li><em><em><em>\n<div style=\"font-style: normal; \"><strong>Handling inner conflicts; when different parts of you are pulling in different directions, you want different things that seem incompatible; responses to stress.</strong></div>\n</em></em></em><ol>\n<li>I notice when I and my brain seem to believe different things (a belief-vs-anticipation divergence), and when this happens I pause and ask which of us is right. <em>(Recent example from Anna: Jumping off the Stratosphere Hotel in Las Vegas in a wire-guided fall. I knew it was safe based on 40,000 data points of people doing it without significant injury, but to persuade my brain I had to visualize 2 times the population of my college jumping off and surviving. Also, my brain sometimes seems much more pessimistic, especially about social things, than I am, and is almost always wrong.)<br /><br /><em><em><em>\n<div style=\"font-style: normal; \"><br /></div>\n<div style=\"font-style: normal; \">\n<ul>\n</ul>\n</div>\n</em></em></em></em></li>\n<li>When facing a difficult decision, I try to reframe it in a way that will reduce, or at least switch around, the biases that might be influencing it. <em>(Recent example from Anna's brother: Trying to decide whether to move to Silicon Valley and look for a higher-paying programming job, he tried a reframe to avoid the status quo bias: If he was living in Silicon Valley already, would he accept a $70K pay cut to move to Santa Barbara with his college friends? (Answer: No.))<br /><br /><em><em><em>\n<div style=\"font-style: normal; \"><br /></div>\n<div style=\"font-style: normal; \">\n<ul>\n</ul>\n</div>\n</em></em></em></em></li>\n<li>When facing a difficult decision, I check which considerations are consequentialist - which considerations are actually about future consequences. <em>(Recent example from Eliezer: I bought a $1400 mattress in my quest for sleep, over the Internet hence much cheaper than the mattress I tried in the store, but non-returnable. When the new mattress didn't seem to work too well once I actually tried sleeping nights on it, this was making me reluctant to spend even more money trying another mattress. I reminded myself that the $1400 was a sunk cost rather than a future consequence, and didn't change the importance and scope of future better sleep at stake (occurring once per day and a large effect size each day).<br /><br /><em><em><em>\n<div style=\"font-style: normal; \"><br /></div>\n<div style=\"font-style: normal; \">\n<ul>\n</ul>\n</div>\n</em></em></em></em></li>\n</ol></li>\n<li><strong>What you do when you find your thoughts, or an argument, going in circles or not getting anywhere.</strong><ol>\n<li>I try to find a concrete prediction that the different beliefs, or different people, definitely disagree about, just to make sure the disagreement is real/empirical. <em>(Recent example from Michael Smith: Someone was worried that rationality training might be \"fake\", and I asked if they could think of a particular prediction they'd make about the results of running the rationality units, that was different from mine, given that it was \"fake\".)<br /><br /><em><em><em><em>\n<div style=\"font-style: normal; \"><br /></div>\n<div style=\"font-style: normal; \">\n<ul>\n</ul>\n</div>\n</em></em></em></em></em></li>\n<li>I try to come up with an experimental test, whose possible results would either satisfy me (if it's an internal argument) or that my friends can agree on (if it's a group discussion). <em>(This is how we settled the running argument over what to call the Center for Applied Rationality&mdash;Julia went out and tested alternate names on around 120 people.)<br /><br /><em><em><em><em>\n<div style=\"font-style: normal; \"><br /></div>\n<div style=\"font-style: normal; \">\n<ul>\n</ul>\n</div>\n</em></em></em></em></em></li>\n<li>If I find my thoughts circling around a particular word, I try to taboo the word, i.e., think without using that word or any of its synonyms or equivalent concepts. (E.g. wondering whether you're \"smart enough\", whether your partner is \"inconsiderate\", or if you're \"trying to do the right thing\".) <em>(Recent example from Anna: Advised someone to stop spending so much time wondering if they or other people were justified; was told that they were trying to do the right thing; and asked them to taboo the word 'trying' and talk about how their thought-patterns were actually behaving.)<br /><br /><em><em><em><em>\n<div style=\"font-style: normal; \"><br /></div>\n<div style=\"font-style: normal; \">\n<ul>\n</ul>\n</div>\n</em></em></em></em></em></li>\n</ol></li>\n<li><strong>Noticing and flagging behaviors (habits, strategies) for review and revision.</strong><ol>\n<li>I consciously think about information-value when deciding whether to try something new, or investigate something that I'm doubtful about. <em>(Recent example from Eliezer: Ordering a $20 exercise ball to see if sitting on it would improve my alertness and/or back muscle strain.) (Non-recent example from Eliezer: After several months of procrastination, and due to Anna nagging me about the value of information, finally trying out what happens when I write with a paired partner; and finding that my writing productivity went up by a factor of four, literally, measured in words per day.)<br /><br /><em><em><em><em>\n<div style=\"font-style: normal; \"><br /></div>\n<div style=\"font-style: normal; \">\n<ul>\n</ul>\n</div>\n</em></em></em></em></em></li>\n<li>I quantify consequences&mdash;how often, how long, how intense. (<em>Recent example from Anna: When we had Julia take on the task of figuring out the Center's name, I worried that a certain person would be offended by not being in control of the loop, and had to consciously evaluate how improbable this was, how little he'd probably be offended, and how short the offense would probably last, to get my brain to stop worrying.) (Plus 3 real cases we've observed in the last year: Someone switching careers is afraid of what a parent will think, and has to consciously evaluate how much emotional pain the parent will experience, for how long before they acclimate, to realize that this shouldn't be a dominant consideration.)<br /><br /><em><em><em><em>\n<div style=\"font-style: normal; \"><br /></div>\n<div style=\"font-style: normal; \">\n<ul>\n</ul>\n</div>\n</em></em></em></em></em></li>\n</ol></li>\n<li><strong>Revising strategies, forming new habits, implementing new behavior patterns.</strong><ol>\n<li>I notice when something is negatively reinforcing a behavior I want to repeat. <em>(Recent example from Anna: I noticed that every time I hit 'Send' on an email, I was visualizing all the ways the recipient might respond poorly or something else might go wrong, negatively reinforcing the behavior of sending emails. I've (a) stopped doing that (b) installed a habit of smiling each time I hit 'Send' (which provides my brain a jolt of positive reinforcement). This has resulted in strongly reduced procrastination about emails.)<br /><br /><em><em><em><em>\n<div style=\"font-style: normal; \"><br /></div>\n<div style=\"font-style: normal; \">\n<ul>\n</ul>\n</div>\n</em></em></em></em></em></li>\n<li>I talk to my friends or deliberately use other social commitment mechanisms on myself. <em>(Recent example from Anna: Using grapefruit juice to keep up brain glucose, I had some juice left over when work was done. I looked at Michael Smith and jokingly said, \"But if I don't drink this now, it will have been wasted!\" to prevent the sunk cost fallacy.) (Example from Eliezer: When I was having trouble getting to sleep, I (a) talked to Anna about the dumb reasoning my brain was using for staying up later, and (b) set up a system with Luke where I put a + in my daily work log every night I showered by my target time for getting to sleep on schedule, and a &mdash; every time I didn't.)<br /><br /><em><em><em><em>\n<div style=\"font-style: normal; \"><br /></div>\n<div style=\"font-style: normal; \">\n<ul>\n</ul>\n</div>\n</em></em></em></em></em></li>\n<li>To establish a new habit, I reward my inner pigeon for executing the habit. <em>(Example from Eliezer: Multiple observers reported a long-term increase in my warmth / niceness several months after... 3 repeats of 4-hour writing sessions during which, in passing, I was rewarded with an M&amp;M (and smiles) each time I complimented someone, i.e., remembered to say out loud a nice thing I thought.) (Recent example from Anna: Yesterday I rewarded myself using a smile and happy gesture for noticing that I was doing a string of low-priority tasks without doing the metacognition for putting the top priorities on top. Noticing a mistake is a good habit, which I&rsquo;ve been training myself to reward, instead of just feeling bad.)<br /><br /><em><em><em><em>\n<div style=\"font-style: normal; \"><br /></div>\n<div style=\"font-style: normal; \">\n<ul>\n</ul>\n</div>\n</em></em></em></em></em></li>\n<li>I try not to treat myself as if I have magic free will; I try to set up influences (habits, situations, etc.) on the way I behave, not just rely on my will to make it so. <em>(Example from Alicorn: I avoid learning politicians&rsquo; positions on gun control, because I have strong emotional reactions to the subject which I don&rsquo;t endorse.) (Recent example from Anna: I bribed Carl to get me to write in my journal every night.)<br /><br /><em><em><em><em>\n<div style=\"font-style: normal; \"><br /></div>\n<div style=\"font-style: normal; \">\n<ul>\n</ul>\n</div>\n</em></em></em></em></em></li>\n<li>I use the outside view on myself. <em>(Recent example from Anna: I like to call my parents once per week, but hadn't done it in a couple of weeks. My brain said, \"I shouldn't call now because I'm busy today.\" My other brain replied, \"Outside view, is this really an unusually busy day and will we actually be less busy tomorrow?\")<br /><br /><em><em><em><em>\n<div style=\"font-style: normal; \">\n<ul>\n</ul>\n</div>\n</em></em></em></em></em></li>\n</ol></li>\n</ol></div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2, "fkABsGCJZ6y9qConW": 3, "X7v7Fyp9cgBYaMe2e": 2, "2oWPnnnzMbiAxWfbs": 2, "5f5c37ee1b5cdee568cfb163": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ttGbpJQ8shBi8hDhh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 133, "baseScore": 182, "extendedScore": null, "score": 0.000422, "legacy": true, "legacyId": "19875", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 182, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 189, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7F5jo5LD9FD7DpxCX", "JcpzFpPBSmzuksmWM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-08T06:09:14.223Z", "modifiedAt": null, "url": null, "title": "PROPOSAL: LessWrong for Teenagers", "slug": "proposal-lesswrong-for-teenagers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:02.157Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ZsH6rgLCquwn8g6AS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9AmBxFmj2f2WMxPYa/proposal-lesswrong-for-teenagers", "pageUrlRelative": "/posts/9AmBxFmj2f2WMxPYa/proposal-lesswrong-for-teenagers", "linkUrl": "https://www.lesswrong.com/posts/9AmBxFmj2f2WMxPYa/proposal-lesswrong-for-teenagers", "postedAtFormatted": "Thursday, November 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20PROPOSAL%3A%20LessWrong%20for%20Teenagers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APROPOSAL%3A%20LessWrong%20for%20Teenagers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9AmBxFmj2f2WMxPYa%2Fproposal-lesswrong-for-teenagers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=PROPOSAL%3A%20LessWrong%20for%20Teenagers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9AmBxFmj2f2WMxPYa%2Fproposal-lesswrong-for-teenagers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9AmBxFmj2f2WMxPYa%2Fproposal-lesswrong-for-teenagers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 425, "htmlBody": "<p><em>What</em><em> about an online group for high schoolers devoted to refining the art of human rationality?</em></p>\n<p>Discovering LessWrong had a profound effect on me, shedding light on the way I study thought processes and helping me with a more rational approach. As a teenager in high school, I wish I could share LessWrong's teachings and philosophies with others at my level.&nbsp;</p>\n<p><span style=\"white-space:pre\"> </span>It would be awesome if we could create a list for the interests of LessWrong readers who are in their teens/in high school. I think this would allow a rational online community such as LessWrong to help develop more rationalists whether by outlining some plans to start rationality clubs in high school or discussing ways teenagers an approach rationality. I also think it would help more timid readers to express themselves and talk with other teenagers about common interests (adults could be allowed in to, if they are deemed appropriate for the community). Correct me if I'm wrong, but rationality training should start as soon as possible in the development process and what better age group to target than teenagers? Adolescence is a crucial transitional phase psychologically, biologically and culturally. I would love to see more collected articles on the evolution of rationality in the amazing, flexible mind of an adolescent. If the goal of this blog is to train humans to be rational-minded, more importance should be allocated to training teenagers. I do not think it hasn't happened yet for want of need among teenagers and if we concentrate some resources, gather a list of interested individuals and garner some interest we can make this work. This article is a good example of something that could be distributed in the proposed group:</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">For&nbsp;</span><strong style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">LW readers under 20</strong><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">: Note that&nbsp;</span><strong style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">the Thiel Fellowships (20 under 20) are now open</strong><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">&nbsp;for their next round of applications, and as they put it, \"you have a huge readership of folk who would make great applicants\". More info&nbsp;</span><a style=\"color: #8a8a8b; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \" href=\"http://us5.campaign-archive1.com/?u=acaba706ab5c95042f36f6e5c&amp;id=59d5da85a4&amp;e=c872586ed7\">here</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \">. (from&nbsp;</span>http://lesswrong.com/lw/f9r/weekly_lw_meetups_austin_berlin_cambridge_uk/)</p>\n</blockquote>\n<p>There is also this LessWrong Highschoolers Facebook group created by Curtis SerVaas:</p>\n<p>https://www.facebook.com/groups/201577993258819/</p>\n<p><span style=\"white-space:pre\"> </span>I recently Skyped (not officially a verb yet?) Anna Salamon who is the Executive Director of CFAR (Center for Applied Rationality). We had begun to develop this proposal. She is on the e-mail list and will be involved as a quasi-supervisor person.&nbsp;You can reach her at&nbsp;anna [at] appliedrationality [dot] org.&nbsp;Drop me a one-line e-mail with your name, age, and situation at [deleted] if you'd like to join the list. Speak up! Teenagers should be the subject of concentrated effort on LessWrong. We are the future, help us to reach the fruits of human rationality.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"7ow6EFpypbH4hzFuz": 1, "izp6eeJJEg9v5zcur": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9AmBxFmj2f2WMxPYa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 31, "extendedScore": null, "score": 7.9e-05, "legacy": true, "legacyId": "19889", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-08T08:30:59.538Z", "modifiedAt": null, "url": null, "title": "[LINK] Was Intrade being manipulated?", "slug": "link-was-intrade-being-manipulated", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:35.719Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielVarga", "createdAt": "2009-09-16T22:21:30.125Z", "isAdmin": false, "displayName": "DanielVarga"}, "userId": "rqE4DaRxHwBpQXj96", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/57Ea77yuNAL89XiBo/link-was-intrade-being-manipulated", "pageUrlRelative": "/posts/57Ea77yuNAL89XiBo/link-was-intrade-being-manipulated", "linkUrl": "https://www.lesswrong.com/posts/57Ea77yuNAL89XiBo/link-was-intrade-being-manipulated", "postedAtFormatted": "Thursday, November 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Was%20Intrade%20being%20manipulated%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Was%20Intrade%20being%20manipulated%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F57Ea77yuNAL89XiBo%2Flink-was-intrade-being-manipulated%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Was%20Intrade%20being%20manipulated%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F57Ea77yuNAL89XiBo%2Flink-was-intrade-being-manipulated", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F57Ea77yuNAL89XiBo%2Flink-was-intrade-being-manipulated", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 81, "htmlBody": "<p><a href=\"http://www.overcomingbias.com/2012/11/was-intrade-being-manipulated-over-the-last-month.html\">Overcoming Bias: Was Intrade being manipulated in the last month?</a></p>\n<p>The link is good apropos, but the question that interests me is a bit more general. It seems to me that the apparent failure of Intrade to function as a predictor/knowledge aggregator before the US presidential elections was an important natural experiment. What do you think about the&nbsp;explanations and implications? I don't think it's too hard to discuss the knowledge aggregation issues without being bogged down in the specificities of US politics.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "57Ea77yuNAL89XiBo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 9, "extendedScore": null, "score": 1.0285275620429547e-06, "legacy": true, "legacyId": "19928", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-08T15:28:13.860Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Whence Your Abstractions?", "slug": "seq-rerun-whence-your-abstractions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:37.157Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZCrWkddZpZHgRku6f/seq-rerun-whence-your-abstractions", "pageUrlRelative": "/posts/ZCrWkddZpZHgRku6f/seq-rerun-whence-your-abstractions", "linkUrl": "https://www.lesswrong.com/posts/ZCrWkddZpZHgRku6f/seq-rerun-whence-your-abstractions", "postedAtFormatted": "Thursday, November 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Whence%20Your%20Abstractions%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Whence%20Your%20Abstractions%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZCrWkddZpZHgRku6f%2Fseq-rerun-whence-your-abstractions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Whence%20Your%20Abstractions%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZCrWkddZpZHgRku6f%2Fseq-rerun-whence-your-abstractions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZCrWkddZpZHgRku6f%2Fseq-rerun-whence-your-abstractions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 184, "htmlBody": "<p>Today's post, <a href=\"/lw/w1/whence_your_abstractions/\">Whence Your Abstractions?</a> was originally published on 20 November 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Whence_Your_Abstractions.3F\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Figuring out how to place concepts in categories is an important part of the problem. Before we classify AI into the same group as human intelligence, farming, and industry, we need to think about why we want to put them into that same category.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fcr/seq_rerun_abstraction_not_analogy/\">Abstraction, Not Analogy</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZCrWkddZpZHgRku6f", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.028757966473427e-06, "legacy": true, "legacyId": "19933", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YyYwRxajPkAyajKXx", "LQJ4akjhGuL6bkQwu", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-08T15:40:52.935Z", "modifiedAt": null, "url": null, "title": "SI is coming to Oxford, looking for hosts, trying to keep costs down ", "slug": "si-is-coming-to-oxford-looking-for-hosts-trying-to-keep", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:35.248Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "malo", "createdAt": "2011-12-28T20:01:23.182Z", "isAdmin": false, "displayName": "Malo"}, "userId": "DA863LaqrSGNFs5w5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hYLqXzFiSZSv86W8t/si-is-coming-to-oxford-looking-for-hosts-trying-to-keep", "pageUrlRelative": "/posts/hYLqXzFiSZSv86W8t/si-is-coming-to-oxford-looking-for-hosts-trying-to-keep", "linkUrl": "https://www.lesswrong.com/posts/hYLqXzFiSZSv86W8t/si-is-coming-to-oxford-looking-for-hosts-trying-to-keep", "postedAtFormatted": "Thursday, November 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20SI%20is%20coming%20to%20Oxford%2C%20looking%20for%20hosts%2C%20trying%20to%20keep%20costs%20down%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASI%20is%20coming%20to%20Oxford%2C%20looking%20for%20hosts%2C%20trying%20to%20keep%20costs%20down%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhYLqXzFiSZSv86W8t%2Fsi-is-coming-to-oxford-looking-for-hosts-trying-to-keep%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=SI%20is%20coming%20to%20Oxford%2C%20looking%20for%20hosts%2C%20trying%20to%20keep%20costs%20down%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhYLqXzFiSZSv86W8t%2Fsi-is-coming-to-oxford-looking-for-hosts-trying-to-keep", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhYLqXzFiSZSv86W8t%2Fsi-is-coming-to-oxford-looking-for-hosts-trying-to-keep", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 226, "htmlBody": "<p>The Singularity Institute is coming to Oxford, England for <a title=\"The Fifth Conference on Artificial General Intelligence\" href=\"http://agi-conference.org/2012/\">AGI-2012</a>!</p>\n<p>AGI-2012 runs December 8th&ndash;11th, but many of us will be arriving as early as the <strong>6th</strong> or 7th, and some will be leaving as late as the <strong>16th</strong>.&nbsp;We are looking for generous members of the Less Wrong community who would be willing to host us during our visit.</p>\n<h2><strong>Who's coming?</strong></h2>\n<ul>\n<li><a href=\"http://intelligence.org/team/\">Luke Muehlhauser</a></li>\n<li><a href=\"http://intelligence.org/team/#helm\" target=\"_blank\">Louie Helm</a></li>\n<li><a href=\"http://intelligence.org/team/#shulman\">Carl Shulman</a></li>\n<li><a href=\"http://intelligence.org/team/#altair\">Alex Altair</a></li>\n<li><a href=\"http://intelligence.org/team/#malo\">Malo Bourgon</a></li>\n<li>Anja Heinisch</li>\n</ul>\n<h2><strong>Why you might want to host us?</strong></h2>\n<ul>\n<li>You want to help SI save some money.</li>\n<li>You want to see&nbsp;<a title=\"Luke Meuhlhauser Facts\" href=\"http://squid314.livejournal.com/330825.html\">Luke Meuhlhauser fact #8</a>&nbsp;for yourself.</li>\n<li>You want to hangout with us, which is great, because we also want to hangout with you.</li>\n<li>You want to marvel at Louie's boyish good looks in person.</li>\n</ul>\n<div><br /></div>\n<div>If you live close to Oxford and have a spare bed, a futon, a comfortable couch, a not so comfortable couch, an inflatable mattress etc.&mdash;and would be willing to put one or more of us up for one or more nights&mdash;please email&nbsp;<a href=\"mailto:malo@intelligence.org\">malo@intelligence.org</a>&nbsp;with the following information:</div>\n<ul>\n<li>your location,</li>\n<li>what dates you can host one or more of us, and</li>\n<li>how many of us you're able/willing to put up with :)</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hYLqXzFiSZSv86W8t", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 8, "extendedScore": null, "score": 1.0287649540616135e-06, "legacy": true, "legacyId": "19932", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>The Singularity Institute is coming to Oxford, England for <a title=\"The Fifth Conference on Artificial General Intelligence\" href=\"http://agi-conference.org/2012/\">AGI-2012</a>!</p>\n<p>AGI-2012 runs December 8th\u201311th, but many of us will be arriving as early as the <strong>6th</strong> or 7th, and some will be leaving as late as the <strong>16th</strong>.&nbsp;We are looking for generous members of the Less Wrong community who would be willing to host us during our visit.</p>\n<h2 id=\"Who_s_coming_\"><strong>Who's coming?</strong></h2>\n<ul>\n<li><a href=\"http://intelligence.org/team/\">Luke Muehlhauser</a></li>\n<li><a href=\"http://intelligence.org/team/#helm\" target=\"_blank\">Louie Helm</a></li>\n<li><a href=\"http://intelligence.org/team/#shulman\">Carl Shulman</a></li>\n<li><a href=\"http://intelligence.org/team/#altair\">Alex Altair</a></li>\n<li><a href=\"http://intelligence.org/team/#malo\">Malo Bourgon</a></li>\n<li>Anja Heinisch</li>\n</ul>\n<h2 id=\"Why_you_might_want_to_host_us_\"><strong>Why you might want to host us?</strong></h2>\n<ul>\n<li>You want to help SI save some money.</li>\n<li>You want to see&nbsp;<a title=\"Luke Meuhlhauser Facts\" href=\"http://squid314.livejournal.com/330825.html\">Luke Meuhlhauser fact #8</a>&nbsp;for yourself.</li>\n<li>You want to hangout with us, which is great, because we also want to hangout with you.</li>\n<li>You want to marvel at Louie's boyish good looks in person.</li>\n</ul>\n<div><br></div>\n<div>If you live close to Oxford and have a spare bed, a futon, a comfortable couch, a not so comfortable couch, an inflatable mattress etc.\u2014and would be willing to put one or more of us up for one or more nights\u2014please email&nbsp;<a href=\"mailto:malo@intelligence.org\">malo@intelligence.org</a>&nbsp;with the following information:</div>\n<ul>\n<li>your location,</li>\n<li>what dates you can host one or more of us, and</li>\n<li>how many of us you're able/willing to put up with :)</li>\n</ul>", "sections": [{"title": "Who's coming?", "anchor": "Who_s_coming_", "level": 1}, {"title": "Why you might want to host us?", "anchor": "Why_you_might_want_to_host_us_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-08T17:11:10.674Z", "modifiedAt": null, "url": null, "title": "FAI, FIA, and singularity politics", "slug": "fai-fia-and-singularity-politics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:35.215Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mitchell_Porter", "createdAt": "2009-05-28T02:36:19.394Z", "isAdmin": false, "displayName": "Mitchell_Porter"}, "userId": "fjERoRhgjipqw3z2b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zWNyEM9xmK7fbbxpD/fai-fia-and-singularity-politics", "pageUrlRelative": "/posts/zWNyEM9xmK7fbbxpD/fai-fia-and-singularity-politics", "linkUrl": "https://www.lesswrong.com/posts/zWNyEM9xmK7fbbxpD/fai-fia-and-singularity-politics", "postedAtFormatted": "Thursday, November 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20FAI%2C%20FIA%2C%20and%20singularity%20politics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFAI%2C%20FIA%2C%20and%20singularity%20politics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzWNyEM9xmK7fbbxpD%2Ffai-fia-and-singularity-politics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=FAI%2C%20FIA%2C%20and%20singularity%20politics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzWNyEM9xmK7fbbxpD%2Ffai-fia-and-singularity-politics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzWNyEM9xmK7fbbxpD%2Ffai-fia-and-singularity-politics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1889, "htmlBody": "<p>In discussing scenarios of the future, I speak of \"slow futures\" and \"fast futures\". A fast future is exemplified by what is now called a <a href=\"http://wiki.lesswrong.com/wiki/AI_takeoff\">hard takeoff</a> singularity: something bootstraps its way to superhuman intelligence in a short time. A slow future is a continuation of history as we know it: decades pass and the world changes, with new politics, culture, and technology. To some extent the <a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\">Hanson vs Yudkowsky debate</a> was about slow vs fast; Robin's future is fast-moving, but on the way there, there's never an event in which some single \"agent\" becomes all-powerful by getting ahead of all others. <br /><br />The Singularity Institute does many things, but I take its core agenda to be about a fast scenario. The theoretical objective is to design an AI which would still be friendly if it became all-powerful. There is also the practical objective of ensuring that the first AI across the self-enhancement threshold <em>is</em> friendly. One way to do that is to be the one who makes it, but that's asking a lot. Another way is to have enough FAI design and FAI theory out there, that the people who <em>do</em> win the mind race will have known about it and will have taken it into consideration. Then there are mixed strategies, such as working on FAI theory while liaising with known AI projects that are contenders in the race and whose principals are receptive to the idea of friendliness. <br /><br />I recently <a href=\"/lw/eyl/thinking_soberly_about_the_context_and/\">criticised</a> a lot of the ideas that circulate in conjunction with the concept of friendly AI. The \"sober\" ideas and the \"extreme\" ideas have a certain correlation with slow-future and fast-future scenarios, respectively. The sober future is a slow one where AIs exist and posthumanity expands into space, but history, politics, and finitude aren't transcended. The extreme future is a fast one where one day the ingredients for a hard takeoff are brought together in one place, an artificial god is born, and, depending on its inclinations and on the nature of reality, something transcendental happens: everyone uploads to the Planck scale, our local overmind reaches out to other realities, we \"live forever and remember it afterwards\". <br /><br />Although I have criticised such transcendentalism, saying that it should not be the default expectation of the future, I do think that the \"hard takeoff\" and the \"all-powerful agent\" would be among the strategic considerations in an ideal plan for the future, though in a rather broader sense than is usually discussed. The reason is that if one day Earth is being ruled by, say, a coalition of AIs with a particular value system, with natural humans reduced to the status of wildlife, then the functional equivalent of a singularity has occurred, even if these AIs have no intention of going on to conquer the galaxy; and I regard <em>that</em> as a quite conceivable scenario. It is fantastic (in the sense of mind-boggling), but it's not transcendental. All the scenario implies is that the human race is no longer at the top of the heap; it has successors and they are now in charge. <br /><br />But we can view those successors as, collectively, the \"all-powerful agent\" that has replaced human hegemony. And we can regard the events, whatever they were, that first gave the original such entities their unbeatable advantage in power, as the \"hard takeoff\" of this scenario. So even a slow, sober future scenario can issue in a singularity where the basic premises and motivations of existing FAI research apply. It's just that one might need to be imaginative in anticipating how they are realized. <br /><br />For example, perhaps hegemonic superintelligence could emerge, not from a single powerful AI research program, but from a particular clique of networked neurohackers who have the right combination of collaborative tools, brain interfaces, and concrete plans for achieving transhuman intelligence. They might go on to build an army of AIs, and subdue the world that way, but the crucial steps which made them the winners in the mind race, and which determined what they would do with their victory, would lie in their methods of brain modification, enhancement, and interfacing, and in the ends to which they applied those methods. <br /><br />In such a scenario, we could speak of \"FIA\" - friendly intelligence augmentation. A basic idea of existing FAI discourse is that the true human utility function needs to be determined, and then the values that make an AI human-friendly would be extrapolated from that. Similar thinking can be applied to the prospect of brain modification and intelligence increase in human beings. Human brains work a certain way, modified or augmented human brains will work in specifically different ways, and we should want to know which modifications are genuinely enhancements, what sort of modifications stabilize value and which ones destabilize value, and so on. <br /><br />If there was a mature and sophisticated culture of preparing for the singularity, then there would be FAI research, FIA research, and a lot of communication between the two fields. (For example, researchers in both fields need to figure out how the human brain works.) Instead, the biggest enthusiasts of FAI are a futurist subculture with a lot of conceptual baggage, and FIA is nonexistent. However, we can at least start thinking and discussing about how this broader culture of research into \"friendly minds\" could take shape. <br /><br />Despite its flaws, the Singularity Institute stands alone as an organization concerned with the fast future scenario, the hard takeoff. I have argued that a sober futurology, while forecasting a slowly evolving future for some time to come, must ultimately concern itself with the emergence of a posthuman power arising from some cognitive technology, whether that is AI, neurotechnology, or a combination of these. So I have asked myself who, among \"slow futurists\", is best equipped to develop an outlook and a plan which is sober and realistic, yet also visionary enough to accommodate the really overwhelming responsibility of designing the architecture of friendly posthuman minds capable of managing a future that we would want. <br /><br />At the moment, my favorites in this respect are the various branches, scattered around the world, of the <a href=\"http://www.facebook.com/groups/longevity.party/\">Longevity Party</a> that was started in Russia a few months ago. (It shouldn't be confused with \"Evolution 2045\", a big-budget rival backed by an Internet entrepreneur, that especially promotes mind uploading. For some reason, transhumanist politics has begun to stir in that country.) If the Singularity Institute falls short of the ideal, then the \"longevity parties\" are even further away from living up to their ambitious agenda. Outside of Russia, they are mostly just small Facebook groups; the most basic issues of policy and practice are still being worked out; no-one involved has much of a history of political achievement. <br /><br />Nonetheless, if there were no prospect of singularity but otherwise science and technology were advancing as they are, the agenda here looks just about ideal. People age and decline until it kills them, an extrapolation of biomedical knowledge suggests this is not a law of nature but just a sign of primitive technology, and the Longevity Party exists to rectify this situation. It's visionary and despite the current immaturity and growing pains, an effective longevity politics must arise one day, simply because the advance of technology will force the issue on us! The human race cannot currently muster enough will to live, to openly make rejuvenation a political goal, but the incremental pursuit of health and well-being is taking us in that direction anyway. <br /><br />There's a vacuum of authority and intention in the realm of life extension, and transhuman technology generally, and these would-be longevity politicians are stepping into that vacuum. I don't think they are ready for all the issues that transhuman power entails, but the process has to start somewhere. Faced with the infinite possibilities of technological transformation, the basic affirmation of the desire to live as well as reality permits, can serve as a founding principle against which to judge attitudes and approaches for all the more complicated \"issues\" that arise in a world where anyone can become anything. <br /><br />Maria Konovalenko, a biomedical researcher and one of the prime movers behind the Russian Longevity Party, wrote an <a href=\"http://mariakonovalenko.wordpress.com/2012/09/10/i-am-an-aging-fighter-because-life-is-the-main-human-right-demand-and-desire/\">essay</a> setting out her version of how the world ought to work. You'll notice that she manages to include friendly AI on her agenda. This is another example, a humble beginning, of the sort of conceptual development which I think needs to happen. The sort of approach to FAI that Eliezer has pioneered needs a context, a broader culture concerned with FIA and the interplay between neuroscience and pure AI, and we need realistic yet visionary political thinking which encompasses both the shocking potentials of a slow future, above all rejuvenation and the conquest of aging, and the singularity imperative. <br /><br />Unless there is simply a catastrophe, one day someone, some thing, some coalition will wield transhuman power. It may begin as a corporation, or as a specific technological research subculture, or as the peak political body in a sovereign state. Perhaps it will be part of a broader global culture of \"competitors in the mind race\" who know about each other and recognize each other as contenders for the first across the line. Perhaps there will be coalitions in the race: contenders who agree on the need for friendliness and the form it should take, and others who are pursuing private power, or who are just pushing AI ahead without too much concern for the transformation of the world that will result. Perhaps there will be a war as one contender begins to visibly pull ahead, and others resort to force to stop them. <br /><br />But without a final and total catastrophe, however much slow history there remains ahead of us, eventually someone or something will \"win\", and after that the world will be reshaped according to its values and priorities. We don't need to imagine this as \"tiling the universe\"; it should be enough to think of it as a ubiquitous posthuman political order, in which all intelligent agents are either kept so powerless as to not be a threat, or managed and modified so as to be reliably friendly to whatever the governing civilizational values are. I see no alternative to this if we are looking for a stable long-term way of living in which ultimate technological powers exist; the ultimate powers of coercion and destruction can't be left lying around, to be taken up by entities with arbitrary values. <br /><br />So the supreme challenge is to conceive of a social and technological order where that power exists, and is used, but it's still a world that we want to live in. FAI is part of the answer, but so is FIA, and so is the development of political concepts and projects which can encompass such an agenda. The Singularity Institute and the Longevity Party are fledgling institutions, and if they live they will surely, eventually, form ties with older and more established bodies; but right now, they seem to be the crucial nuclei of the theoretical research and the political vision that we need.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zWNyEM9xmK7fbbxpD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 17, "extendedScore": null, "score": 1.028814829052681e-06, "legacy": true, "legacyId": "19934", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["czxjKohS7RQjkBiSD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-08T18:15:15.281Z", "modifiedAt": null, "url": null, "title": "Nov 16-18: Rationality for Entrepreneurs", "slug": "nov-16-18-rationality-for-entrepreneurs", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:39.193Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AnnaSalamon", "createdAt": "2009-02-27T04:25:14.013Z", "isAdmin": false, "displayName": "AnnaSalamon"}, "userId": "pnFbJAtNHGDK8PHQx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7NwSsZ7WbswZYg3ax/nov-16-18-rationality-for-entrepreneurs", "pageUrlRelative": "/posts/7NwSsZ7WbswZYg3ax/nov-16-18-rationality-for-entrepreneurs", "linkUrl": "https://www.lesswrong.com/posts/7NwSsZ7WbswZYg3ax/nov-16-18-rationality-for-entrepreneurs", "postedAtFormatted": "Thursday, November 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Nov%2016-18%3A%20Rationality%20for%20Entrepreneurs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANov%2016-18%3A%20Rationality%20for%20Entrepreneurs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7NwSsZ7WbswZYg3ax%2Fnov-16-18-rationality-for-entrepreneurs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Nov%2016-18%3A%20Rationality%20for%20Entrepreneurs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7NwSsZ7WbswZYg3ax%2Fnov-16-18-rationality-for-entrepreneurs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7NwSsZ7WbswZYg3ax%2Fnov-16-18-rationality-for-entrepreneurs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1683, "htmlBody": "<p><a href=\"http://appliedrationality.org/\">CFAR</a> is taking LW-style rationality into the world, this month, with a new kind of rationality camp: <a href=\"http://appliedrationality.org/entrepreneurs/\">Rationality for Entrepreneurs</a>. &nbsp;It is aimed at ambitious, relatively successful folk (regardless of whether they are familiar with LW), who like analytic thinking and care about making practical real-world projects work. &nbsp;Some will be paying for themselves; others will be covered by their companies. &nbsp;</p>\r\n<p>If you'd like to learn rationality in a more practical context, consider <a href=\"http://appliedrationality.org/apply/\">applying</a>. &nbsp;Also, if you were hoping to introduce rationality and related ideas to a friend/acquaintance who fits the bill, please talk to them about the workshop, both for their sake and to strengthen the rationality community.</p>\r\n<p>The price will be out of reach for some: the workshop costs $3.9k. &nbsp;But there <strong>is</strong> a money-back guarantee. &nbsp;Some partial scholarships may be available. This fee buys participants:</p>\r\n<ul>\r\n<li>Four nights and three days at a retreat center, with small classes, interactive exercises, and much opportunity for unstructured conversation that applies the material at meals and during the evenings (room and board is included);</li>\r\n<li>One instructor for every three participants;&nbsp;</li>\r\n<li>Six weeks of Skype/phone and email follow-up, to help participants make the material into regular habits, and navigate real-life business and personal situations with these tools.</li>\r\n</ul>\r\n<p>CFAR is planning future camps which are more directly targeted at a Less Wrong audience (like our previous camps), so don&rsquo;t worry if this camp doesn&rsquo;t seem like the right fit for you (because of cost, interests, etc.). &nbsp;There will be others. &nbsp;But if you or someone you know does have an entrepreneurial bent[1], then we strongly recommend applying to this camp rather than waiting. &nbsp;Attendees will be surrounded by other ambitious, successful, practically-minded folks, learn from materials that have been tailored to entrepreneurial issues, and receive extensive follow-up to help apply what they&rsquo;ve learned to their businesses and personal lives.</p>\r\n<p>Our schedule is below.</p>\r\n<p>(See also the thread about the camp on <a href=\"http://news.ycombinator.com/item?id=4751584\">Hacker News</a>.)<a id=\"more\"></a></p>\r\n<p><img src=\"http://gallery.mailchimp.com/e190f95c12790256f7d34fcc1/images/Screen_Shot_2012_11_06_at_3.14.43_PMc1b543454c11.png\" alt=\"\" /></p>\r\n<h2 style=\"text-align: center;\">WORKSHOP SCHEDULE: NOVEMBER 16-18, 2012</h2>\r\n<hr />\r\n<p>&nbsp;</p>\r\n<h2><em>Friday, November 16: Know the World</em></h2>\r\n<table border=\"1\" cellspacing=\"1\" cellpadding=\"5\" frame=\"box\" rules=\"rows\">\r\n<tbody>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">8:00 am</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Breakfast</strong></span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">8:45 am</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Opening Session</strong>. Rationality doesn't mean Spock - the common themes that will bind the workshop together - building accurate models of the world, figuring out which actions lead to preferred outcomes, and managing our brain's internal resources and algorithms. Plus all the practical stuff; how to get the most out of the workshop; and betting games we'll be playing throughout. (Also, this will be the only time in the program where an instructor speaks for more than 5 sequential minutes - the rest is all activities, exercises, and interactions.)</span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">9:50 am</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Deliberate performance: Explicit predictions</strong>. Experiment shows that practice doesn't always lead to learning - you can spend 60 hours per week working and not improve skill! Research into 'deliberate performance' shows that simple changes can vastly increase the power of practice. To discover quickly where our implicit beliefs are right or wrong, we'll get into the habit of making conscious, explicit predictions during work.</span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">11:00 am</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Curiosity</strong>. When it comes to finding out the truth about your life or business, there's no substitute for feeling genuine curiosity about questions. Learn to notice when you're arguing a question without feeling interested in the answer. Make yourself more likeable by being more curious during conversation. Visualize your uncertainty and use simple techniques to remind yourself of what you don't know.</span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">12:00 pm</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Lunch</strong></span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">1:00 pm</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Bayes's Rule: How much to change your mind</strong> 3-person workshops (so you can choose your own technical level), as we examine the powerful and simple rules for weighing the strength of evidence, and the qualitative takeaways for deciding when to change your mind in everyday life.</span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">3:00 pm</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Break</strong></span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">3:40 pm</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Professed belief and anticipated experience</strong> What we say loudly that we believe isn't always what our brain expects to see. Learn how to routinely notice the difference between claiming that your political candidate will win the election, and being willing to bet money on their winning. How to motivate yourself without trying to deny what your brain already knows.</span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">4:50 pm</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Quantified trust and advice-taking</strong> Are you updating on others' advice too much or too little? Surprising theorems show that rational agents shouldn't be able to predict when they'll disagree. We'll play a game that will show you whether you're updating too much or too little on other people's opinions, with plenty of fast feedback for practice. (Don't worry - it's still possible to disagree with the majority if you know something they don't.)</span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">6:00 pm</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Dinner</strong></span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">7:30 pm</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Beeminder, a Tool for Installing New Behaviors</strong> When you&rsquo;re trying to turn plans for change (including plans you form this weekend) into things you actually do, it helps to have the right tool to keep your attention on your goal and your progress towards it. Many of us have found Beeminder to be a convenient, intuitive, motivating software tool for establishing new behaviors, so we have invited Daniel Reeves, co-founder of Beeminder, to explain how to use it most effectively. </span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">7:50 pm</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Overcoming Procrastination</strong> Guest speaker Geoff Anders presents techniques his organization has used to overcome procrastination and maintain 75 hours/week of productive work time per person.</span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">Nighttime</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Informal discussion</strong> Past participants have reported that some of their most valuable hours were the evenings chatting with instructors and other participants about places where their lives and businesses are stuck, and which techniques might apply to them. We've ensured there's an instructor for every three participants, and kept nighttimes free for open discussion.</span></td>\r\n</tr>\r\n</tbody>\r\n</table>\r\n<p>&nbsp;</p>\r\n<h2><em>Saturday, November 17: Know Your Self</em></h2>\r\n<p>&nbsp;</p>\r\n<table border=\"1\" cellspacing=\"1\" cellpadding=\"5\" frame=\"box\" rules=\"rows\">\r\n<tbody>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">8:00 am</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Breakfast</strong></span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">9:00 am</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Goal-factoring: Fungible goods in everyday life</strong> Figure out all of the goals being achieved by your daily behaviors and brainstorm how they can be achieved least expensively. Are you incurring large costs to buy a friend's goodwill on one occasion, while declining to purchase it much more cheaply the next day?</span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">10:10 am</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Expected value of information</strong> Quantify and guess the expected value of new information you can easily gather, and new policies you can try out to see what happens. Find the $20 investments that might return $2,000 of value, and learn to try the affordable experiments that 'probably won't work'.</span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">11:10 am</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Break</strong></span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">12:00 pm</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Lunch</strong></span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">1:00 pm</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Panic</strong> Physiological techniques for noticing and controlling your body&rsquo;s instinctive fight-or-flight response so that you can remain calm, clear-minded, and open to new information in stressful situations.</span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">2:10 pm</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Overcoming aversions: Comfort Zone Expansion </strong>In a staggeringly huge universe of possible actions, we usually limit ourselves to choosing from among a tiny subset that are known and comfortable. What are some simple things you've somehow \"never gotten around\" to doing? Would it be all that painful to try doing some of them? Sometimes the most useful actions are those we've never tried before. </span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">3:10 pm</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Break</strong></span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">3:40 pm</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Expanding social comfort zones </strong>Our social ranges are often constricted to what we feel comfortable doing - and startups need to be comfortable asking venture capitalists for 10 million dollars. Figure out what your brain is afraid will happen, and do simple experiments to find out what actually happens.</span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">4:50 pm</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Productive arguments </strong>Frames of mind for noticing valuable information even when it feels like bad news, and even when it&rsquo;s presented by someone who isn't going out of their way to make you like them. A family of practical techniques for keeping debates collaborative and truthfinding (or deciding when to give up).</span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">6:00 pm</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Dinner</strong></span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">7:00 pm</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Comfort Zone Expansion - In the Field</strong> Hone your new appreciation for slightly uncomfortable social actions by testing them with total strangers. Surprises and entertainment guaranteed! (No, really, this unit is always popular afterward.)</span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">Nighttime</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Informal discussion</strong> Past participants have reported that some of their most valuable hours were the evenings chatting with instructors and other participants about places where their lives and businesses are stuck, and which techniques might apply to them. We've ensured there's an instructor for every three participants, and kept nighttimes free for open discussion.</span></td>\r\n</tr>\r\n</tbody>\r\n</table>\r\n<p>&nbsp;</p>\r\n<h2><em>Sunday, November 18: The Big Picture</em></h2>\r\n<p>&nbsp;</p>\r\n<table border=\"1\" cellspacing=\"1\" cellpadding=\"5\" frame=\"box\" rules=\"rows\">\r\n<tbody>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">8:00 am</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Breakfast</strong></span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">9:00 am</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>From lifehacking to life-programming</strong> Guest speaker Yan Zhang explains how to use the principles of deliberate performance and the behavioral psychology of games to restructure your life for maximum fun and skill acquisition.</span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">10:10 am</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>How to actually change your mind</strong> Status quo bias, sunk costs, motivated skepticism - there's a long litany of experimentally discovered biases that prevent us from changing our minds. Learn to beat some of the worst obstacles to pivoting your business, dropping the hire who predictably won&rsquo;t work out, or admitting that your cofounder was right all along. (This will make you surprisingly popular compared to never changing your mind. Really.)</span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">11:10 am</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Break</strong></span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">12:00 pm</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Lunch</strong></span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">1:00 pm</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Expected value of thinking styles</strong> You spend thousands of hours running various default thought processes, some that are curious and accumulate knowledge, some of which are making up more and more reasons why you don't need to worry about that project coming up tomorrow. Try some simple value calculations for which thought processes are most helpful, and learn to spend more background time in those.</span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">2:10 pm</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Optimizing self-presentation</strong> Guest speaker Liron Shapira describes the social presentation signals you send every moment whether you intend to or not, and how to make them say what you mean. </span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">3:10 pm</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Break</strong></span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">3:40 pm</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Installing Habits</strong> Use some of the simplest and best-tested principles in experimental psychology to reinforce the behaviors you want to want, and avoid punishing yourself for doing things you want to do again. Operant conditioning advises us to pay attention to immediate rewards and punishments (like thinking of everything that could go wrong, the moment after you press 'Send' on an email). Learn to train your &ldquo;inner pigeon&rdquo;.</span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">4:50 pm</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Closing session</strong>&nbsp;Connections between the units that wouldn't have made sense previously (e.g., taking an algorithm-based view of yourself; asking which algorithms you want to be running). &nbsp;Set-up for the six weeks of follow-up, and for applying the material in your daily life.</span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">6:00 pm</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Dinner</strong></span></td>\r\n</tr>\r\n<tr>\r\n<td><span style=\"color: #000000; font-size: small;\">Nighttime</span></td>\r\n<td><span style=\"color: #000000; font-size: small;\"><strong>Party</strong> Fun, decompression, and further discussion.</span></td>\r\n</tr>\r\n</tbody>\r\n</table>\r\n<p>&nbsp;</p>\r\n<hr />\r\n<p>Note: Ordering shown is typical rather than actual. To allow each session to have only 6 participants, each session is held at multiple times during the day. (Session groups will be frequently remixed; everyone gets a chance to meet everyone else.)</p>\r\n<hr />\r\n<h2><a href=\"http://appliedrationality.org/entrepreneurs\">Click here for the workshop overview</a></h2>\r\n<h2><a href=\"http://appliedrationality.org/apply/\">Click here to apply</a></h2>\r\n<p>[1] Many attending the camp will be entrepreneurs; but there will also be folks from finance, programmers, managers, and others who combine analytic thinking with an interest in ambitious, real-world projects.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"DQHWBcKeiLnyh9za9": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7NwSsZ7WbswZYg3ax", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 44, "extendedScore": null, "score": 1.0288502245941998e-06, "legacy": true, "legacyId": "19879", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><a href=\"http://appliedrationality.org/\">CFAR</a> is taking LW-style rationality into the world, this month, with a new kind of rationality camp: <a href=\"http://appliedrationality.org/entrepreneurs/\">Rationality for Entrepreneurs</a>. &nbsp;It is aimed at ambitious, relatively successful folk (regardless of whether they are familiar with LW), who like analytic thinking and care about making practical real-world projects work. &nbsp;Some will be paying for themselves; others will be covered by their companies. &nbsp;</p>\n<p>If you'd like to learn rationality in a more practical context, consider <a href=\"http://appliedrationality.org/apply/\">applying</a>. &nbsp;Also, if you were hoping to introduce rationality and related ideas to a friend/acquaintance who fits the bill, please talk to them about the workshop, both for their sake and to strengthen the rationality community.</p>\n<p>The price will be out of reach for some: the workshop costs $3.9k. &nbsp;But there <strong>is</strong> a money-back guarantee. &nbsp;Some partial scholarships may be available. This fee buys participants:</p>\n<ul>\n<li>Four nights and three days at a retreat center, with small classes, interactive exercises, and much opportunity for unstructured conversation that applies the material at meals and during the evenings (room and board is included);</li>\n<li>One instructor for every three participants;&nbsp;</li>\n<li>Six weeks of Skype/phone and email follow-up, to help participants make the material into regular habits, and navigate real-life business and personal situations with these tools.</li>\n</ul>\n<p>CFAR is planning future camps which are more directly targeted at a Less Wrong audience (like our previous camps), so don\u2019t worry if this camp doesn\u2019t seem like the right fit for you (because of cost, interests, etc.). &nbsp;There will be others. &nbsp;But if you or someone you know does have an entrepreneurial bent[1], then we strongly recommend applying to this camp rather than waiting. &nbsp;Attendees will be surrounded by other ambitious, successful, practically-minded folks, learn from materials that have been tailored to entrepreneurial issues, and receive extensive follow-up to help apply what they\u2019ve learned to their businesses and personal lives.</p>\n<p>Our schedule is below.</p>\n<p>(See also the thread about the camp on <a href=\"http://news.ycombinator.com/item?id=4751584\">Hacker News</a>.)<a id=\"more\"></a></p>\n<p><img src=\"http://gallery.mailchimp.com/e190f95c12790256f7d34fcc1/images/Screen_Shot_2012_11_06_at_3.14.43_PMc1b543454c11.png\" alt=\"\"></p>\n<h2 style=\"text-align: center;\" id=\"WORKSHOP_SCHEDULE__NOVEMBER_16_18__2012\">WORKSHOP SCHEDULE: NOVEMBER 16-18, 2012</h2>\n<hr>\n<p>&nbsp;</p>\n<h2 id=\"Friday__November_16__Know_the_World\"><em>Friday, November 16: Know the World</em></h2>\n<table border=\"1\" cellspacing=\"1\" cellpadding=\"5\" frame=\"box\" rules=\"rows\">\n<tbody>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">8:00 am</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Breakfast</strong></span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">8:45 am</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Opening Session</strong>. Rationality doesn't mean Spock - the common themes that will bind the workshop together - building accurate models of the world, figuring out which actions lead to preferred outcomes, and managing our brain's internal resources and algorithms. Plus all the practical stuff; how to get the most out of the workshop; and betting games we'll be playing throughout. (Also, this will be the only time in the program where an instructor speaks for more than 5 sequential minutes - the rest is all activities, exercises, and interactions.)</span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">9:50 am</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Deliberate performance: Explicit predictions</strong>. Experiment shows that practice doesn't always lead to learning - you can spend 60 hours per week working and not improve skill! Research into 'deliberate performance' shows that simple changes can vastly increase the power of practice. To discover quickly where our implicit beliefs are right or wrong, we'll get into the habit of making conscious, explicit predictions during work.</span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">11:00 am</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Curiosity</strong>. When it comes to finding out the truth about your life or business, there's no substitute for feeling genuine curiosity about questions. Learn to notice when you're arguing a question without feeling interested in the answer. Make yourself more likeable by being more curious during conversation. Visualize your uncertainty and use simple techniques to remind yourself of what you don't know.</span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">12:00 pm</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Lunch</strong></span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">1:00 pm</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Bayes's Rule: How much to change your mind</strong> 3-person workshops (so you can choose your own technical level), as we examine the powerful and simple rules for weighing the strength of evidence, and the qualitative takeaways for deciding when to change your mind in everyday life.</span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">3:00 pm</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Break</strong></span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">3:40 pm</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Professed belief and anticipated experience</strong> What we say loudly that we believe isn't always what our brain expects to see. Learn how to routinely notice the difference between claiming that your political candidate will win the election, and being willing to bet money on their winning. How to motivate yourself without trying to deny what your brain already knows.</span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">4:50 pm</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Quantified trust and advice-taking</strong> Are you updating on others' advice too much or too little? Surprising theorems show that rational agents shouldn't be able to predict when they'll disagree. We'll play a game that will show you whether you're updating too much or too little on other people's opinions, with plenty of fast feedback for practice. (Don't worry - it's still possible to disagree with the majority if you know something they don't.)</span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">6:00 pm</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Dinner</strong></span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">7:30 pm</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Beeminder, a Tool for Installing New Behaviors</strong> When you\u2019re trying to turn plans for change (including plans you form this weekend) into things you actually do, it helps to have the right tool to keep your attention on your goal and your progress towards it. Many of us have found Beeminder to be a convenient, intuitive, motivating software tool for establishing new behaviors, so we have invited Daniel Reeves, co-founder of Beeminder, to explain how to use it most effectively. </span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">7:50 pm</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Overcoming Procrastination</strong> Guest speaker Geoff Anders presents techniques his organization has used to overcome procrastination and maintain 75 hours/week of productive work time per person.</span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">Nighttime</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Informal discussion</strong> Past participants have reported that some of their most valuable hours were the evenings chatting with instructors and other participants about places where their lives and businesses are stuck, and which techniques might apply to them. We've ensured there's an instructor for every three participants, and kept nighttimes free for open discussion.</span></td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<h2 id=\"Saturday__November_17__Know_Your_Self\"><em>Saturday, November 17: Know Your Self</em></h2>\n<p>&nbsp;</p>\n<table border=\"1\" cellspacing=\"1\" cellpadding=\"5\" frame=\"box\" rules=\"rows\">\n<tbody>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">8:00 am</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Breakfast</strong></span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">9:00 am</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Goal-factoring: Fungible goods in everyday life</strong> Figure out all of the goals being achieved by your daily behaviors and brainstorm how they can be achieved least expensively. Are you incurring large costs to buy a friend's goodwill on one occasion, while declining to purchase it much more cheaply the next day?</span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">10:10 am</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Expected value of information</strong> Quantify and guess the expected value of new information you can easily gather, and new policies you can try out to see what happens. Find the $20 investments that might return $2,000 of value, and learn to try the affordable experiments that 'probably won't work'.</span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">11:10 am</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Break</strong></span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">12:00 pm</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Lunch</strong></span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">1:00 pm</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Panic</strong> Physiological techniques for noticing and controlling your body\u2019s instinctive fight-or-flight response so that you can remain calm, clear-minded, and open to new information in stressful situations.</span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">2:10 pm</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Overcoming aversions: Comfort Zone Expansion </strong>In a staggeringly huge universe of possible actions, we usually limit ourselves to choosing from among a tiny subset that are known and comfortable. What are some simple things you've somehow \"never gotten around\" to doing? Would it be all that painful to try doing some of them? Sometimes the most useful actions are those we've never tried before. </span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">3:10 pm</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Break</strong></span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">3:40 pm</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Expanding social comfort zones </strong>Our social ranges are often constricted to what we feel comfortable doing - and startups need to be comfortable asking venture capitalists for 10 million dollars. Figure out what your brain is afraid will happen, and do simple experiments to find out what actually happens.</span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">4:50 pm</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Productive arguments </strong>Frames of mind for noticing valuable information even when it feels like bad news, and even when it\u2019s presented by someone who isn't going out of their way to make you like them. A family of practical techniques for keeping debates collaborative and truthfinding (or deciding when to give up).</span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">6:00 pm</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Dinner</strong></span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">7:00 pm</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Comfort Zone Expansion - In the Field</strong> Hone your new appreciation for slightly uncomfortable social actions by testing them with total strangers. Surprises and entertainment guaranteed! (No, really, this unit is always popular afterward.)</span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">Nighttime</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Informal discussion</strong> Past participants have reported that some of their most valuable hours were the evenings chatting with instructors and other participants about places where their lives and businesses are stuck, and which techniques might apply to them. We've ensured there's an instructor for every three participants, and kept nighttimes free for open discussion.</span></td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<h2 id=\"Sunday__November_18__The_Big_Picture\"><em>Sunday, November 18: The Big Picture</em></h2>\n<p>&nbsp;</p>\n<table border=\"1\" cellspacing=\"1\" cellpadding=\"5\" frame=\"box\" rules=\"rows\">\n<tbody>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">8:00 am</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Breakfast</strong></span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">9:00 am</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>From lifehacking to life-programming</strong> Guest speaker Yan Zhang explains how to use the principles of deliberate performance and the behavioral psychology of games to restructure your life for maximum fun and skill acquisition.</span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">10:10 am</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>How to actually change your mind</strong> Status quo bias, sunk costs, motivated skepticism - there's a long litany of experimentally discovered biases that prevent us from changing our minds. Learn to beat some of the worst obstacles to pivoting your business, dropping the hire who predictably won\u2019t work out, or admitting that your cofounder was right all along. (This will make you surprisingly popular compared to never changing your mind. Really.)</span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">11:10 am</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Break</strong></span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">12:00 pm</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Lunch</strong></span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">1:00 pm</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Expected value of thinking styles</strong> You spend thousands of hours running various default thought processes, some that are curious and accumulate knowledge, some of which are making up more and more reasons why you don't need to worry about that project coming up tomorrow. Try some simple value calculations for which thought processes are most helpful, and learn to spend more background time in those.</span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">2:10 pm</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Optimizing self-presentation</strong> Guest speaker Liron Shapira describes the social presentation signals you send every moment whether you intend to or not, and how to make them say what you mean. </span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">3:10 pm</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Break</strong></span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">3:40 pm</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Installing Habits</strong> Use some of the simplest and best-tested principles in experimental psychology to reinforce the behaviors you want to want, and avoid punishing yourself for doing things you want to do again. Operant conditioning advises us to pay attention to immediate rewards and punishments (like thinking of everything that could go wrong, the moment after you press 'Send' on an email). Learn to train your \u201cinner pigeon\u201d.</span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">4:50 pm</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Closing session</strong>&nbsp;Connections between the units that wouldn't have made sense previously (e.g., taking an algorithm-based view of yourself; asking which algorithms you want to be running). &nbsp;Set-up for the six weeks of follow-up, and for applying the material in your daily life.</span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">6:00 pm</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Dinner</strong></span></td>\n</tr>\n<tr>\n<td><span style=\"color: #000000; font-size: small;\">Nighttime</span></td>\n<td><span style=\"color: #000000; font-size: small;\"><strong>Party</strong> Fun, decompression, and further discussion.</span></td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<hr>\n<p>Note: Ordering shown is typical rather than actual. To allow each session to have only 6 participants, each session is held at multiple times during the day. (Session groups will be frequently remixed; everyone gets a chance to meet everyone else.)</p>\n<hr>\n<h2 id=\"Click_here_for_the_workshop_overview\"><a href=\"http://appliedrationality.org/entrepreneurs\">Click here for the workshop overview</a></h2>\n<h2 id=\"Click_here_to_apply\"><a href=\"http://appliedrationality.org/apply/\">Click here to apply</a></h2>\n<p>[1] Many attending the camp will be entrepreneurs; but there will also be folks from finance, programmers, managers, and others who combine analytic thinking with an interest in ambitious, real-world projects.</p>", "sections": [{"title": "WORKSHOP SCHEDULE: NOVEMBER 16-18, 2012", "anchor": "WORKSHOP_SCHEDULE__NOVEMBER_16_18__2012", "level": 1}, {"title": "Friday, November 16: Know the World", "anchor": "Friday__November_16__Know_the_World", "level": 1}, {"title": "Saturday, November 17: Know Your Self", "anchor": "Saturday__November_17__Know_Your_Self", "level": 1}, {"title": "Sunday, November 18: The Big Picture", "anchor": "Sunday__November_18__The_Big_Picture", "level": 1}, {"title": "Click here for the workshop overview", "anchor": "Click_here_for_the_workshop_overview", "level": 1}, {"title": "Click here to apply", "anchor": "Click_here_to_apply", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "58 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 67, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-09T00:24:27.758Z", "modifiedAt": null, "url": null, "title": "Meetup : Rio de Janeiro Meetup", "slug": "meetup-rio-de-janeiro-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "roland", "createdAt": "2009-02-27T23:03:47.279Z", "isAdmin": false, "displayName": "roland"}, "userId": "p2C9rpg32LHrGwer8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9hewscA3JkuxbqNsZ/meetup-rio-de-janeiro-meetup", "pageUrlRelative": "/posts/9hewscA3JkuxbqNsZ/meetup-rio-de-janeiro-meetup", "linkUrl": "https://www.lesswrong.com/posts/9hewscA3JkuxbqNsZ/meetup-rio-de-janeiro-meetup", "postedAtFormatted": "Friday, November 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Rio%20de%20Janeiro%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Rio%20de%20Janeiro%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9hewscA3JkuxbqNsZ%2Fmeetup-rio-de-janeiro-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Rio%20de%20Janeiro%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9hewscA3JkuxbqNsZ%2Fmeetup-rio-de-janeiro-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9hewscA3JkuxbqNsZ%2Fmeetup-rio-de-janeiro-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 63, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/fp'>Rio de Janeiro Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 November 2012 08:00:00PM (-0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Rio de Janeiro</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It will be at Pizzaria Parm\u00ea in Largo do Machado(Rua do Catete, 311 - Ljs. 112 / 1)\n<a href=\"https://maps.google.com/maps?q=largo+do+machado&amp;hl=en&amp;ll=-22.930806%2C-43.177662&amp;spn=0.001885%2C0.00397&amp;hq=largo+do+machado&amp;t=m&amp;z=19&amp;layer=c&amp;cbll=-22.930709%2C-43.177663&amp;panoid=-aQXwYgavBIzN3pCPX-Tvg&amp;cbp=12%2C107.62%2C%2C0%2C0\" rel=\"nofollow\">link</a></p>\n\n<p>I will place a paper written with \"LessWrong.com Meet-up\" on the table so you can find us.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/fp'>Rio de Janeiro Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9hewscA3JkuxbqNsZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.029054214403376e-06, "legacy": true, "legacyId": "19935", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Rio_de_Janeiro_Meetup\">Discussion article for the meetup : <a href=\"/meetups/fp\">Rio de Janeiro Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 November 2012 08:00:00PM (-0200)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Rio de Janeiro</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>It will be at Pizzaria Parm\u00ea in Largo do Machado(Rua do Catete, 311 - Ljs. 112 / 1)\n<a href=\"https://maps.google.com/maps?q=largo+do+machado&amp;hl=en&amp;ll=-22.930806%2C-43.177662&amp;spn=0.001885%2C0.00397&amp;hq=largo+do+machado&amp;t=m&amp;z=19&amp;layer=c&amp;cbll=-22.930709%2C-43.177663&amp;panoid=-aQXwYgavBIzN3pCPX-Tvg&amp;cbp=12%2C107.62%2C%2C0%2C0\" rel=\"nofollow\">link</a></p>\n\n<p>I will place a paper written with \"LessWrong.com Meet-up\" on the table so you can find us.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Rio_de_Janeiro_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/fp\">Rio de Janeiro Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Rio de Janeiro Meetup", "anchor": "Discussion_article_for_the_meetup___Rio_de_Janeiro_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Rio de Janeiro Meetup", "anchor": "Discussion_article_for_the_meetup___Rio_de_Janeiro_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-09T03:26:17.544Z", "modifiedAt": null, "url": null, "title": "On counting and addition", "slug": "on-counting-and-addition", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:37.527Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Anatoly_Vorobey", "createdAt": "2009-03-22T09:13:04.364Z", "isAdmin": false, "displayName": "Anatoly_Vorobey"}, "userId": "gEQxcSsKD5bqjna3M", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5tm3eZtSWYYM4ySeT/on-counting-and-addition", "pageUrlRelative": "/posts/5tm3eZtSWYYM4ySeT/on-counting-and-addition", "linkUrl": "https://www.lesswrong.com/posts/5tm3eZtSWYYM4ySeT/on-counting-and-addition", "postedAtFormatted": "Friday, November 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20counting%20and%20addition&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20counting%20and%20addition%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5tm3eZtSWYYM4ySeT%2Fon-counting-and-addition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20counting%20and%20addition%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5tm3eZtSWYYM4ySeT%2Fon-counting-and-addition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5tm3eZtSWYYM4ySeT%2Fon-counting-and-addition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1230, "htmlBody": "<p>When mathematical truths and their applicability to the physical world are discussed, there's a certain kind of flawed (in my opinion) thinking that is often employed, and it comes out in sentences like \"rocks behave isomorphically to integers, while clouds don't\", or \"two apples plus two apples is four apples, unless someone stole one from the bowl\", and so on. I'll try to explain why I consider such reasoning flawed, and what are the more suitable descriptions of what's going on with the apples and the rocks and the clouds and the drops of water and such.</p>\n<p>Two mistakes combine together and create a shared state of confusion; the mistakes themselves are almost independent and I think they stand or fall separately.</p>\n<p>1. The first mistake is conflating <em>counting</em> things and <em>bringing things together</em> spatially. One usually goes with the other because that's how we learn to count, by looking at groups of things located closely together - but it doesn't have to be this way. When we say \"take 2 apples and add 2 more apples, now count - you have 4 apples\", we automatically imagine the two additional apples being brought in and placed next to the two original ones, but that's just a mental crutch you can easily do without. Let me show you: suppose I ask you to consider two sheep in England and two more sheep in Australia - how many sheep are there together? You see the English sheep in your mind's eye, there they are standing, and you count one, two. Now please resist the temptation to teleport the two Australian sheep and place them next in sequence. Instead, just fly with your mind's eye all the way to Australia in a split second and home on that field - there they are - and continue counting: three, four. There, you just counted 2+2 sheep without bringing them together in space, in real life or your imagination. There's nothing to it and you do it all the time - if someone asks you to count the number of chairs in a large and busy-looking living room, you don't bring them together in your mind, you just gaze-travel over the room and count them off one by one.</p>\n<p>Now consider the implication of that to clouds or other such objects. Suppose someone tells you \"well, apples obey the law of arithmetics, but one drop of water plus another drop of water equals one larger drop of water, not two\" - it should be clear that they are naively conflating spatial movement and adding/counting. Adding two apples and two apples is not a spatial operation of bringing them together, it's a mental act of <em>viewing them as one whole collection</em> that can be counted. It's just that bringing them spatially together, in reality or your imagimation, is the easiest way to carry that \"viewing\" out. For drops of water or clouds, the spatial operation becomes a distraction, so just resist it. One cloud plus one cloud most certainly equals two clouds: just count them in your mind, here's one and there's two, maybe drifting next to each other, or maybe they're on separate continents. You don't have to <em>merge</em> them - nothing about \"+\" says you do.</p>\n<p>2. The second mistake is a sort of a map-territory confusion where we naively grant the territory the power of holding discrete objects for our needs. It may be helpful to realize and to remember that at least on our macro scale of reality, on the scale of things we perceive with our senses, discrete, separate objects are a feature of the map, not the territory; they exist in your mind, not the reality. In the reality, there's just a lot of atoms everywhere (I'm simplifying; to be more thorough, think of elementary particles and many-worlds if you feel like it, and the virtual vacuum particles and so on) with no \"natural\" way of separating them into different conglomerates.</p>\n<p>Normally, this really isn't an important point to insist on, and there's no harm whatsoever in just thinking of reality as being made up of objects like apples and clouds and whatnot. But imagine now an argument over an issue like \"Was 2+2=4 before humans were around to invent that equation? I believe that long before humans, on an Earth devoid of life, when two rocks rolled onto a beach with three rocks already on it, there were five rocks altogether on the beach\". What's really happening in that story? Well, there's the spatial confusion dealt with above, when we find it easiest to imagine the two rocks to be added rolling into the scene. But even before that, zoom in on those three rocks on the beach. What are they? What business do you have saying there's a \"rock\" there? There's a bunch of atoms of various kinds, and lots of other atoms (of air, sand, etc.) around and next to those, with somewhat different densities and behavior, but no definite boundary between any of them. On the nanoscale, there's constant exchange of atoms everywhere. To say \"this is a discrete object, a rock\" you need to be able to somehow ignore this inherently fuzzy boundary, maybe by picking a scale and smoothing out all the fine details below it - in human experience, the crudeness of our senses does that job for us, but our senses aren't there in that picture. The reality doesn't know or care about \"rocks\" - there's just atoms.</p>\n<p>This isn't to say, I find it important to note, that the existence of three rocks on that beach is somehow a \"subjective\" claim. Imagine that there are alien nanobots everywhere on that Earth, recording everything faithfully on the nanoscale and storing the data somewhere for someone to discover; a billion years later, you come along, parse the data, reconstruct the scene - and of course you'll then recognize that there were three rocks on the beach. The fact that there are three rocks on that beach is an objective fact of nature; it's just that the <em>meaning</em> of that statement relies on the procedure of discretization being carried out, on someone or something defining what we consider a single discrete object and how we isolate it; and nature won't do that for you. You do that with your brain. That's why counting - and addition - are inherently mental acts carried out on mental constructs; on the map, not the territory.</p>\n<p>All this is not to say that math is \"invented\" rather than \"discovered\"; I think my analysis is silent on that, and both platonism and formalism remain possible. It's just that an example of five rocks on the beach before humans are around is not helpful in resolving that question - without human-style discretization you can't meaningfully say it was five rocks there, rather than just a bunch of atoms. It may still be true - I certainly believe so - that the laws that govern the behavior of discrete objects, though they are normally mental entities, are universal and 2+2 was 4 before humans were around and in any alien mentality.</p>\n<p>(It remains to acknowledge that reality may be discrete on the micro level - spacetime itself may be discrete, we can certainly speak of single photons, etc. This is irrelevant on the level of our everyday perceptions, however - the level of apples, rocks, clouds and so on).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LnEEs8xGooYmQ8iLA": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5tm3eZtSWYYM4ySeT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 47, "extendedScore": null, "score": 1.0291547027012059e-06, "legacy": true, "legacyId": "19942", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-09T04:59:58.349Z", "modifiedAt": null, "url": null, "title": "Meetup : Berlin Meetup", "slug": "meetup-berlin-meetup-6", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:37.706Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "blob", "createdAt": "2011-12-09T17:52:34.152Z", "isAdmin": false, "displayName": "blob"}, "userId": "3Yvqo9A3euExjqhsi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FEJoMZaYFLeBuxQzA/meetup-berlin-meetup-6", "pageUrlRelative": "/posts/FEJoMZaYFLeBuxQzA/meetup-berlin-meetup-6", "linkUrl": "https://www.lesswrong.com/posts/FEJoMZaYFLeBuxQzA/meetup-berlin-meetup-6", "postedAtFormatted": "Friday, November 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berlin%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berlin%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFEJoMZaYFLeBuxQzA%2Fmeetup-berlin-meetup-6%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berlin%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFEJoMZaYFLeBuxQzA%2Fmeetup-berlin-meetup-6", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFEJoMZaYFLeBuxQzA%2Fmeetup-berlin-meetup-6", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 99, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/fq'>Berlin Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 November 2012 07:30:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Ming Dynastie, Br\u00fcckenstra\u00dfe 6, 10179 Berlin</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>WARNING: We might meet at <a href=\"http://www.c-base.org/\" rel=\"nofollow\">c-base</a> instead which is also at S Jannowitzbruecke. If it works out, the change of location will be announced on the <a href=\"https://groups.google.com/forum/?fromgroups#!forum/lw-berlin\" rel=\"nofollow\">mailing list</a> and here at least a day in advance.</p>\n\n<p>Our plans for this time are:</p>\n\n<ul>\n<li>Discuss plans and make public commitments</li>\n<li>Discuss answers to <a href=\"http://lesswrong.com/lw/f9l/2012_less_wrong_censussurvey_now_open/\">LW census survey</a></li>\n<li>Self-improvement open space</li>\n</ul>\n\n<p>As usual, I'll be there slightly early and bring a sign.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/fq'>Berlin Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FEJoMZaYFLeBuxQzA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.0292064818824833e-06, "legacy": true, "legacyId": "19943", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berlin_Meetup\">Discussion article for the meetup : <a href=\"/meetups/fq\">Berlin Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 November 2012 07:30:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Ming Dynastie, Br\u00fcckenstra\u00dfe 6, 10179 Berlin</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>WARNING: We might meet at <a href=\"http://www.c-base.org/\" rel=\"nofollow\">c-base</a> instead which is also at S Jannowitzbruecke. If it works out, the change of location will be announced on the <a href=\"https://groups.google.com/forum/?fromgroups#!forum/lw-berlin\" rel=\"nofollow\">mailing list</a> and here at least a day in advance.</p>\n\n<p>Our plans for this time are:</p>\n\n<ul>\n<li>Discuss plans and make public commitments</li>\n<li>Discuss answers to <a href=\"http://lesswrong.com/lw/f9l/2012_less_wrong_censussurvey_now_open/\">LW census survey</a></li>\n<li>Self-improvement open space</li>\n</ul>\n\n<p>As usual, I'll be there slightly early and bring a sign.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berlin_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/fq\">Berlin Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berlin Meetup", "anchor": "Discussion_article_for_the_meetup___Berlin_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Berlin Meetup", "anchor": "Discussion_article_for_the_meetup___Berlin_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bJiyYJeCyh4HcKHub"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-09T05:25:49.367Z", "modifiedAt": null, "url": null, "title": "XKCD - Frequentist vs. Bayesians", "slug": "xkcd-frequentist-vs-bayesians", "viewCount": null, "lastCommentedAt": "2020-06-18T19:54:11.199Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "brilee", "createdAt": "2009-11-24T14:36:56.816Z", "isAdmin": false, "displayName": "brilee"}, "userId": "bbMiGjzXWpEqRMwe6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mpTEEffWYE6ZAs7id/xkcd-frequentist-vs-bayesians", "pageUrlRelative": "/posts/mpTEEffWYE6ZAs7id/xkcd-frequentist-vs-bayesians", "linkUrl": "https://www.lesswrong.com/posts/mpTEEffWYE6ZAs7id/xkcd-frequentist-vs-bayesians", "postedAtFormatted": "Friday, November 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20XKCD%20-%20Frequentist%20vs.%20Bayesians&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AXKCD%20-%20Frequentist%20vs.%20Bayesians%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmpTEEffWYE6ZAs7id%2Fxkcd-frequentist-vs-bayesians%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=XKCD%20-%20Frequentist%20vs.%20Bayesians%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmpTEEffWYE6ZAs7id%2Fxkcd-frequentist-vs-bayesians", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmpTEEffWYE6ZAs7id%2Fxkcd-frequentist-vs-bayesians", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 36, "htmlBody": "<p><a href=\"http://xkcd.com/1132/\">http://xkcd.com/1132/</a></p>\n<p>&nbsp;</p>\n<p>Is this a fair representation of frequentists versus bayesians? I feel like every time the topic comes up, 'Bayesian statistics' is an applause light for me, and I'm not sure why I'm supposed to be applauding.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LhX3F2SvGDarZCuh6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mpTEEffWYE6ZAs7id", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 24, "extendedScore": null, "score": 1.0292207706259065e-06, "legacy": true, "legacyId": "19949", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 91, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-09T05:37:00.426Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] AI Go Foom", "slug": "seq-rerun-ai-go-foom", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:36.057Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bQ2qQHEsvmyNx7JTn/seq-rerun-ai-go-foom", "pageUrlRelative": "/posts/bQ2qQHEsvmyNx7JTn/seq-rerun-ai-go-foom", "linkUrl": "https://www.lesswrong.com/posts/bQ2qQHEsvmyNx7JTn/seq-rerun-ai-go-foom", "postedAtFormatted": "Friday, November 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20AI%20Go%20Foom&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20AI%20Go%20Foom%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbQ2qQHEsvmyNx7JTn%2Fseq-rerun-ai-go-foom%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20AI%20Go%20Foom%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbQ2qQHEsvmyNx7JTn%2Fseq-rerun-ai-go-foom", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbQ2qQHEsvmyNx7JTn%2Fseq-rerun-ai-go-foom", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 145, "htmlBody": "<p>Today's post, <a href=\"http://www.overcomingbias.com/2008/11/ai-go-foom.html\">AI Go Foom</a> was originally published on November 19, 2008.  A summary:</p>\n<p>&nbsp;</p>\n<blockquote>Robin Hanson's attempt to summarize Eliezer Yudkowsky's position.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fdp/seq_rerun_whence_your_abstractions/\">Whence Your Abstractions?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bQ2qQHEsvmyNx7JTn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.0292269528567346e-06, "legacy": true, "legacyId": "19952", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZCrWkddZpZHgRku6f", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-09T16:12:27.399Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Austin, Berlin, Cambridge UK, Champaign IL, Durham NC (2), Washington DC", "slug": "weekly-lw-meetups-austin-berlin-cambridge-uk-champaign-il", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:30.668Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jhBXeFbxEJhRGg9eR/weekly-lw-meetups-austin-berlin-cambridge-uk-champaign-il", "pageUrlRelative": "/posts/jhBXeFbxEJhRGg9eR/weekly-lw-meetups-austin-berlin-cambridge-uk-champaign-il", "linkUrl": "https://www.lesswrong.com/posts/jhBXeFbxEJhRGg9eR/weekly-lw-meetups-austin-berlin-cambridge-uk-champaign-il", "postedAtFormatted": "Friday, November 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Austin%2C%20Berlin%2C%20Cambridge%20UK%2C%20Champaign%20IL%2C%20Durham%20NC%20(2)%2C%20Washington%20DC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Austin%2C%20Berlin%2C%20Cambridge%20UK%2C%20Champaign%20IL%2C%20Durham%20NC%20(2)%2C%20Washington%20DC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjhBXeFbxEJhRGg9eR%2Fweekly-lw-meetups-austin-berlin-cambridge-uk-champaign-il%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Austin%2C%20Berlin%2C%20Cambridge%20UK%2C%20Champaign%20IL%2C%20Durham%20NC%20(2)%2C%20Washington%20DC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjhBXeFbxEJhRGg9eR%2Fweekly-lw-meetups-austin-berlin-cambridge-uk-champaign-il", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjhBXeFbxEJhRGg9eR%2Fweekly-lw-meetups-austin-berlin-cambridge-uk-champaign-il", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 510, "htmlBody": "<p><strong>This summary was posted to LW main on Nov 2nd, and has been moved to discussion. The following week's summary is <a href=\"/lw/fei/weekly_lw_meetups_austin_berlin_champaign/\">here</a>.</strong></p>\n<p>For <strong>LW readers under 20</strong>: Note that <strong>the Thiel Fellowships (20 under 20) are now open</strong> for their next round of applications, and as they put it, \"you have a huge readership of folk who would make great applicants\". More info <a href=\"http://us5.campaign-archive1.com/?u=acaba706ab5c95042f36f6e5c&amp;id=59d5da85a4&amp;e=c872586ed7\">here</a>.</p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/fd\">Durham HPMoR discussion, ch 12-14:&nbsp;<span class=\"date\">03 November 2012 11:00AM</span></a></li>\n<li><a href=\"/meetups/f3\">Berlin Meetup:&nbsp;<span class=\"date\">03 November 2012 08:30PM</span></a></li>\n<li><a href=\"/meetups/ff\">Washington DC Social Meetup:&nbsp;<span class=\"date\">04 November 2012 04:23PM</span></a></li>\n<li><a href=\"/meetups/fh\">Champaign, IL meetup:&nbsp;<span class=\"date\">06 November 2012 09:00PM</span></a></li>\n<li><a href=\"/meetups/fe\">Durham LW: Technical explanation, meta:&nbsp;<span class=\"date\">08 November 2012 08:00PM</span></a></li>\n<li><a href=\"/meetups/fa\">Skepticon 5:&nbsp;<span class=\"date\">10 November 2012 01:06PM</span></a></li>\n<li><a href=\"/meetups/fb\">First Meetup- Cleveland and Akron, Ohio:&nbsp;<span class=\"date\">17 November 2012 08:03PM</span></a></li>\n<li><a href=\"/meetups/fc\">18/11 London Meetup:&nbsp;<span class=\"date\">18 November 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/el\">Sofia, Bulgaria Meetup:&nbsp;<span class=\"date\">09 December 2012 05:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">03 November 2018 01:30PM</span></a></li>\n<li><a href=\"/meetups/fg\">Cambridge UK Weekly Meetup:&nbsp;<span class=\"date\">03 November 2012 07:47PM</span></a></li>\n<li><a href=\"/meetups/en\">Winter Solstice Megameetup - NYC:&nbsp;<span class=\"date\">15 December 2012 05:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong></strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jhBXeFbxEJhRGg9eR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0295783146962822e-06, "legacy": true, "legacyId": "19791", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["kfDRzBhJWWzk47XmR", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-09T22:10:45.473Z", "modifiedAt": null, "url": null, "title": "Meetup : (Seattle) Conscientiousness", "slug": "meetup-seattle-conscientiousness", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2qGnuGi5cAZYwGjPF/meetup-seattle-conscientiousness", "pageUrlRelative": "/posts/2qGnuGi5cAZYwGjPF/meetup-seattle-conscientiousness", "linkUrl": "https://www.lesswrong.com/posts/2qGnuGi5cAZYwGjPF/meetup-seattle-conscientiousness", "postedAtFormatted": "Friday, November 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20(Seattle)%20Conscientiousness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20(Seattle)%20Conscientiousness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2qGnuGi5cAZYwGjPF%2Fmeetup-seattle-conscientiousness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20(Seattle)%20Conscientiousness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2qGnuGi5cAZYwGjPF%2Fmeetup-seattle-conscientiousness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2qGnuGi5cAZYwGjPF%2Fmeetup-seattle-conscientiousness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 111, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/fr'>(Seattle) Conscientiousness</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 November 2012 04:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1900 NE 68th St. Seattle, WA 98115</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Three short conscientiousness related presentations.</p>\n\n<ul>\n<li>Andrew will give a short presentation on what psychologists know about conscientiousness (one of the big-5 personality traits), </li>\n<li>Ben will talk a bit about the book \"Getting Things Done\".</li>\n<li>I will briefly talk about Vitamin D supplementation research.</li>\n</ul>\n\n<p>We'll have some discussion and socializing and order dinner. I'm interested in talking about how to work on conscientiousness related skills (organization of various sorts, motivation etc.).</p>\n\n<p>See you there!</p>\n\n<p>If you need help: 360 602 1069</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/fr'>(Seattle) Conscientiousness</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2qGnuGi5cAZYwGjPF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.0297765253895612e-06, "legacy": true, "legacyId": "19964", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup____Seattle__Conscientiousness\">Discussion article for the meetup : <a href=\"/meetups/fr\">(Seattle) Conscientiousness</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 November 2012 04:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1900 NE 68th St. Seattle, WA 98115</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Three short conscientiousness related presentations.</p>\n\n<ul>\n<li>Andrew will give a short presentation on what psychologists know about conscientiousness (one of the big-5 personality traits), </li>\n<li>Ben will talk a bit about the book \"Getting Things Done\".</li>\n<li>I will briefly talk about Vitamin D supplementation research.</li>\n</ul>\n\n<p>We'll have some discussion and socializing and order dinner. I'm interested in talking about how to work on conscientiousness related skills (organization of various sorts, motivation etc.).</p>\n\n<p>See you there!</p>\n\n<p>If you need help: 360 602 1069</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup____Seattle__Conscientiousness1\">Discussion article for the meetup : <a href=\"/meetups/fr\">(Seattle) Conscientiousness</a></h2>", "sections": [{"title": "Discussion article for the meetup : (Seattle) Conscientiousness", "anchor": "Discussion_article_for_the_meetup____Seattle__Conscientiousness", "level": 1}, {"title": "Discussion article for the meetup : (Seattle) Conscientiousness", "anchor": "Discussion_article_for_the_meetup____Seattle__Conscientiousness1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-10T00:24:57.882Z", "modifiedAt": null, "url": null, "title": "Meetup : Toronto THINK", "slug": "meetup-toronto-think-1", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Giles", "createdAt": "2011-02-11T02:30:16.999Z", "isAdmin": false, "displayName": "Giles"}, "userId": "H347ba3KZMP8XoDt3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QvwSm3E5QiaQS3n4D/meetup-toronto-think-1", "pageUrlRelative": "/posts/QvwSm3E5QiaQS3n4D/meetup-toronto-think-1", "linkUrl": "https://www.lesswrong.com/posts/QvwSm3E5QiaQS3n4D/meetup-toronto-think-1", "postedAtFormatted": "Saturday, November 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Toronto%20THINK&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Toronto%20THINK%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQvwSm3E5QiaQS3n4D%2Fmeetup-toronto-think-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Toronto%20THINK%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQvwSm3E5QiaQS3n4D%2Fmeetup-toronto-think-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQvwSm3E5QiaQS3n4D%2Fmeetup-toronto-think-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 236, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/fs'>Toronto THINK</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 November 2012 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">7 Hart House Circle, Toronto</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>In the Hart House Reading Room (the room across from the reception desk with the purple walls)</p>\n\n<p>Sustainability and the environment are hot topics - global warming, resource depletion, biodiversity loss, overpopulation, water resources, pollution, deforestation. But as individuals, what should we be doing about it? Should we be trying to minimize our own environmental impact, or encouraging our friends to do so? Or should we be lobbying for change at the political level? Just how much do things need to change? Is it already too late? And, importantly, which issues should be getting most of our attention and which do we think are distractions?</p>\n\n<p>This will be the focus of next week's discussion.</p>\n\n<p>In addition to this, we're starting a project to save a life! One of the best ways we know to go about this comes to our attention via the charity evaluator GiveWell. If their research is to be believed, the Against Malaria Foundation can save a life for around $2000. But we need to get the money somehow, and we need to decide whether we trust GiveWell's methodology and whether there are better options available. All things to discuss at the next meetup!</p>\n\n<p>Note: <a href=\"http://www.thehighimpactnetwork.org/\" rel=\"nofollow\">THINK</a> is not directly LW-affiliated but I've been told to post our meetups here anyhow :-)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/fs'>Toronto THINK</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QvwSm3E5QiaQS3n4D", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.0298507853800164e-06, "legacy": true, "legacyId": "19967", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Toronto_THINK\">Discussion article for the meetup : <a href=\"/meetups/fs\">Toronto THINK</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 November 2012 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">7 Hart House Circle, Toronto</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>In the Hart House Reading Room (the room across from the reception desk with the purple walls)</p>\n\n<p>Sustainability and the environment are hot topics - global warming, resource depletion, biodiversity loss, overpopulation, water resources, pollution, deforestation. But as individuals, what should we be doing about it? Should we be trying to minimize our own environmental impact, or encouraging our friends to do so? Or should we be lobbying for change at the political level? Just how much do things need to change? Is it already too late? And, importantly, which issues should be getting most of our attention and which do we think are distractions?</p>\n\n<p>This will be the focus of next week's discussion.</p>\n\n<p>In addition to this, we're starting a project to save a life! One of the best ways we know to go about this comes to our attention via the charity evaluator GiveWell. If their research is to be believed, the Against Malaria Foundation can save a life for around $2000. But we need to get the money somehow, and we need to decide whether we trust GiveWell's methodology and whether there are better options available. All things to discuss at the next meetup!</p>\n\n<p>Note: <a href=\"http://www.thehighimpactnetwork.org/\" rel=\"nofollow\">THINK</a> is not directly LW-affiliated but I've been told to post our meetups here anyhow :-)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Toronto_THINK1\">Discussion article for the meetup : <a href=\"/meetups/fs\">Toronto THINK</a></h2>", "sections": [{"title": "Discussion article for the meetup : Toronto THINK", "anchor": "Discussion_article_for_the_meetup___Toronto_THINK", "level": 1}, {"title": "Discussion article for the meetup : Toronto THINK", "anchor": "Discussion_article_for_the_meetup___Toronto_THINK1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-10T03:23:13.120Z", "modifiedAt": null, "url": null, "title": "Bug: The \"Load all comments\" link doesn't work", "slug": "bug-the-load-all-comments-link-doesn-t-work", "viewCount": null, "lastCommentedAt": "2012-11-11T18:18:23.954Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eugine_Nier", "createdAt": "2010-09-19T04:54:48.475Z", "isAdmin": false, "displayName": "Eugine_Nier"}, "userId": "DuGWafuKMcBx8uXWY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hcDD8nxu9iAnpPfnw/bug-the-load-all-comments-link-doesn-t-work", "pageUrlRelative": "/posts/hcDD8nxu9iAnpPfnw/bug-the-load-all-comments-link-doesn-t-work", "linkUrl": "https://www.lesswrong.com/posts/hcDD8nxu9iAnpPfnw/bug-the-load-all-comments-link-doesn-t-work", "postedAtFormatted": "Saturday, November 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bug%3A%20The%20%22Load%20all%20comments%22%20link%20doesn't%20work&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABug%3A%20The%20%22Load%20all%20comments%22%20link%20doesn't%20work%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhcDD8nxu9iAnpPfnw%2Fbug-the-load-all-comments-link-doesn-t-work%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bug%3A%20The%20%22Load%20all%20comments%22%20link%20doesn't%20work%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhcDD8nxu9iAnpPfnw%2Fbug-the-load-all-comments-link-doesn-t-work", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhcDD8nxu9iAnpPfnw%2Fbug-the-load-all-comments-link-doesn-t-work", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 18, "htmlBody": "<p>The \"Load all comments\" link that's at the bottom of articles with more than 500 comments doesn't work.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hcDD8nxu9iAnpPfnw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 2, "extendedScore": null, "score": 4e-06, "legacy": true, "legacyId": "19975", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2012-11-10T03:23:13.120Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-10T04:32:02.524Z", "modifiedAt": null, "url": null, "title": "If MWI is correct, should we expect to experience Quantum Torment?", "slug": "if-mwi-is-correct-should-we-expect-to-experience-quantum", "viewCount": null, "lastCommentedAt": "2019-04-19T16:08:20.446Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Furcas", "createdAt": "2009-03-06T01:46:02.371Z", "isAdmin": false, "displayName": "Furcas"}, "userId": "op2xhKHCebioDyCxb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xCG6kXXmYwKCYHeif/if-mwi-is-correct-should-we-expect-to-experience-quantum", "pageUrlRelative": "/posts/xCG6kXXmYwKCYHeif/if-mwi-is-correct-should-we-expect-to-experience-quantum", "linkUrl": "https://www.lesswrong.com/posts/xCG6kXXmYwKCYHeif/if-mwi-is-correct-should-we-expect-to-experience-quantum", "postedAtFormatted": "Saturday, November 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20If%20MWI%20is%20correct%2C%20should%20we%20expect%20to%20experience%20Quantum%20Torment%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIf%20MWI%20is%20correct%2C%20should%20we%20expect%20to%20experience%20Quantum%20Torment%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxCG6kXXmYwKCYHeif%2Fif-mwi-is-correct-should-we-expect-to-experience-quantum%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=If%20MWI%20is%20correct%2C%20should%20we%20expect%20to%20experience%20Quantum%20Torment%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxCG6kXXmYwKCYHeif%2Fif-mwi-is-correct-should-we-expect-to-experience-quantum", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxCG6kXXmYwKCYHeif%2Fif-mwi-is-correct-should-we-expect-to-experience-quantum", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 166, "htmlBody": "<p>If the many worlds of the Many Worlds Interpretation of quantum mechanics are real, there's at least a good chance that Quantum Immortality is real as well: All conscious beings should expect to experience the next moment in at least one Everett branch even if they stop existing in all other branches, and the moment after that in at least one other branch, and so on forever.</p>\n<p>However, the transition from life to death isn't usually a binary change. For most people it happens slowly as your brain and the rest of your body deteriorates, often painfully.</p>\n<p>Doesn't it follow that each of us should expect to keep living in this state of constant degradation and suffering for a very, very long time, perhaps forever?</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>I don't know much about quantum mechanics, so I don't have anything to contribute to this discussion. I'm just terrified, and I'd like, not to be reassured by well-meaning lies, but to know the truth. How likely is it that Quantum Torment is real?</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xCG6kXXmYwKCYHeif", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 5, "extendedScore": null, "score": 1.029987524033967e-06, "legacy": true, "legacyId": "19976", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 74, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-10T05:39:14.662Z", "modifiedAt": null, "url": null, "title": "How many of you doing Khan Academy?", "slug": "how-many-of-you-doing-khan-academy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:32.478Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hackerkiba", "createdAt": "2010-09-17T02:45:39.493Z", "isAdmin": false, "displayName": "hackerkiba"}, "userId": "LssGgJYPEsFHPwyLy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NaCC7MNort7b94Zru/how-many-of-you-doing-khan-academy", "pageUrlRelative": "/posts/NaCC7MNort7b94Zru/how-many-of-you-doing-khan-academy", "linkUrl": "https://www.lesswrong.com/posts/NaCC7MNort7b94Zru/how-many-of-you-doing-khan-academy", "postedAtFormatted": "Saturday, November 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20many%20of%20you%20doing%20Khan%20Academy%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20many%20of%20you%20doing%20Khan%20Academy%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNaCC7MNort7b94Zru%2Fhow-many-of-you-doing-khan-academy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20many%20of%20you%20doing%20Khan%20Academy%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNaCC7MNort7b94Zru%2Fhow-many-of-you-doing-khan-academy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNaCC7MNort7b94Zru%2Fhow-many-of-you-doing-khan-academy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 57, "htmlBody": "<p>Hello, I was wondering: how many of you are doing khanacademy?</p>\n<p>&nbsp;</p>\n<p>Right now, I am about 2 weeks away from completing khanacademy altogether. 334 out of 380 concepts are completed, assuming that the KA team don't add any new content during the time period.</p>\n<p>&nbsp;</p>\n<p>Are you guys brushing up your math on KA, and if so what's your progress?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NaCC7MNort7b94Zru", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 5, "extendedScore": null, "score": 1.030024248827523e-06, "legacy": true, "legacyId": "19977", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-10T05:50:28.467Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Optimization and the Singularity", "slug": "seq-rerun-optimization-and-the-singularity-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8Nms7aHbnGvrBX4em/seq-rerun-optimization-and-the-singularity-0", "pageUrlRelative": "/posts/8Nms7aHbnGvrBX4em/seq-rerun-optimization-and-the-singularity-0", "linkUrl": "https://www.lesswrong.com/posts/8Nms7aHbnGvrBX4em/seq-rerun-optimization-and-the-singularity-0", "postedAtFormatted": "Saturday, November 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Optimization%20and%20the%20Singularity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Optimization%20and%20the%20Singularity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8Nms7aHbnGvrBX4em%2Fseq-rerun-optimization-and-the-singularity-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Optimization%20and%20the%20Singularity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8Nms7aHbnGvrBX4em%2Fseq-rerun-optimization-and-the-singularity-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8Nms7aHbnGvrBX4em%2Fseq-rerun-optimization-and-the-singularity-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 168, "htmlBody": "<p>Today's post, <a href=\"/lw/rk/optimization_and_the_singularity/\">Optimization and the Singularity</a> was originally published on 23 June 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>An introduction to optimization processes and why Yudkowsky thinks that a singularity would be far more powerful than calculations based on human progress would suggest.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fe8/seq_rerun_ai_go_foom/\">AI Go Foom</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8Nms7aHbnGvrBX4em", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "19978", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HFTn3bAT6uXSNwv4m", "bQ2qQHEsvmyNx7JTn", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-10T06:01:49.592Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Eliezer's Meta-Level Determinism", "slug": "seq-rerun-eliezer-s-meta-level-determinism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:37.293Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/H4DQ4kk62D6e3tont/seq-rerun-eliezer-s-meta-level-determinism", "pageUrlRelative": "/posts/H4DQ4kk62D6e3tont/seq-rerun-eliezer-s-meta-level-determinism", "linkUrl": "https://www.lesswrong.com/posts/H4DQ4kk62D6e3tont/seq-rerun-eliezer-s-meta-level-determinism", "postedAtFormatted": "Saturday, November 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Eliezer's%20Meta-Level%20Determinism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Eliezer's%20Meta-Level%20Determinism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH4DQ4kk62D6e3tont%2Fseq-rerun-eliezer-s-meta-level-determinism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Eliezer's%20Meta-Level%20Determinism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH4DQ4kk62D6e3tont%2Fseq-rerun-eliezer-s-meta-level-determinism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH4DQ4kk62D6e3tont%2Fseq-rerun-eliezer-s-meta-level-determinism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 175, "htmlBody": "<p>Today's post, <a href=\"http://www.overcomingbias.com/2008/06/eliezers-meta-l.html\">Eliezer&rsquo;s Meta-Level Determinism</a> was originally published on June 23, 2008.  A summary:</p>\n<p>&nbsp;</p>\n<blockquote>Eliezer has suggested that the most dramatic changes in the history of life are the result of events that changed the optimization process at work. This is experimentally testable, and the predictions it offers seem to be false.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fe8/seq_rerun_ai_go_foom/\">AI Go Foom</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "H4DQ4kk62D6e3tont", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.030037220969967e-06, "legacy": true, "legacyId": "19980", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bQ2qQHEsvmyNx7JTn", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-10T13:15:49.624Z", "modifiedAt": null, "url": null, "title": "NKCDT: The Big Bang Theory", "slug": "nkcdt-the-big-bang-theory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:39.649Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "oXGTijwhjZB8cnJ3W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gz6D4bx2TLcqtNwWt/nkcdt-the-big-bang-theory", "pageUrlRelative": "/posts/gz6D4bx2TLcqtNwWt/nkcdt-the-big-bang-theory", "linkUrl": "https://www.lesswrong.com/posts/gz6D4bx2TLcqtNwWt/nkcdt-the-big-bang-theory", "postedAtFormatted": "Saturday, November 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20NKCDT%3A%20The%20Big%20Bang%20Theory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANKCDT%3A%20The%20Big%20Bang%20Theory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgz6D4bx2TLcqtNwWt%2Fnkcdt-the-big-bang-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=NKCDT%3A%20The%20Big%20Bang%20Theory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgz6D4bx2TLcqtNwWt%2Fnkcdt-the-big-bang-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgz6D4bx2TLcqtNwWt%2Fnkcdt-the-big-bang-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 404, "htmlBody": "<p><strong id=\"internal-source-marker_0.23429366503842175\" style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal; \"></strong></p>\n<blockquote>\n<p style=\"margin: 0pt 19pt 0pt 12pt; text-align: justify; \" dir=\"ltr\"><strong id=\"internal-source-marker_0.23429366503842175\" style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal; \"><span style=\"font-size: 11px; font-family: Verdana; background-color: #f7f7f8; vertical-align: baseline; white-space: pre-wrap; \">Hi, Welcome to the first </span><span style=\"font-size: 11px; font-family: Verdana; background-color: #f7f7f8; font-weight: bold; vertical-align: baseline; white-space: pre-wrap; \">Non-Karmic-Casual-Discussion-Thread.</span></strong></p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\"><strong id=\"internal-source-marker_0.23429366503842175\" style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal; \"><span style=\"font-size: 11px; font-family: Verdana; background-color: transparent; vertical-align: baseline; white-space: pre-wrap; \">This is a place for [purpose of thread goes here].</span></strong></p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\"><strong id=\"internal-source-marker_0.23429366503842175\" style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal; \"><span style=\"font-size: 11px; font-family: Verdana; background-color: transparent; vertical-align: baseline; white-space: pre-wrap; \">In order to create a causal non karmic environment for every one we ask that you</span></strong></p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\"><strong id=\"internal-source-marker_0.23429366503842175\" style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal; \"><span style=\"font-size: 11px; font-family: Verdana; background-color: transparent; vertical-align: baseline; white-space: pre-wrap; \">-Do not upvote or downvote any zero karma posts</span></strong></p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\"><strong id=\"internal-source-marker_0.23429366503842175\" style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal; \"><span style=\"font-size: 11px; font-family: Verdana; background-color: transparent; vertical-align: baseline; white-space: pre-wrap; \">-If you see a vote with positive karma, downvote it towards zero, even if it&rsquo;s a good post</span></strong></p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\"><strong id=\"internal-source-marker_0.23429366503842175\" style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal; \"><span style=\"font-size: 11px; font-family: Verdana; background-color: transparent; vertical-align: baseline; white-space: pre-wrap; \">-If you see a vote with negative karma, upvote it towards zero, even if it&rsquo;s a weak post</span></strong></p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\"><strong id=\"internal-source-marker_0.23429366503842175\" style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal; \"><span style=\"font-size: 11px; font-family: Verdana; background-color: transparent; vertical-align: baseline; white-space: pre-wrap; \">-Please be polite and respectful to other users</span></strong></p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\"><strong id=\"internal-source-marker_0.23429366503842175\" style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal; \"><span style=\"font-size: 11px; font-family: Verdana; background-color: transparent; vertical-align: baseline; white-space: pre-wrap; \">-Have fun!&rdquo;</span></strong></p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\">&nbsp;</p>\n</blockquote>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\">&nbsp;</p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\">This is my first attempt at starting a casual conversation on LW where people don't have to worry about winning or losing points, and can just relax and have social fun together.</p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\">&nbsp;</p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\">So, Big Bang Theory. That series got me wondering. It seems to be about \"geeks\", and not the basement-dwelling variety either; they're highly successful and accomplished professionals, each in their own field. One of them has been an astronaut, even. And yet, everything they ever accomplish amounts to absolutely nothing in terms of social recognition or even in terms of personal happiness. And the thing is, it doesn't even get better for their \"normal\" counterparts, who are just as miserable and petty.</p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\">&nbsp;</p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\">Consider, then; how would being rationalists would affect the characters on this show? The writing of the show relies a lot on laughing at people rather than with them; would rationalist characters subvert that? And how would that rationalist outlook express itself given their personalities? (After all, notice how amazingly different from each other Yudkowsky, Hanson, and Alicorn are, just to name a few; they emphasize rather different things, and take different approaches to both truth-testing and problem-solving).</p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\">Note: this discussion does not need to be about rationalism. It can be a casual, normal discussion about the series. Relax and enjoy yourselves.</p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\">&nbsp;</p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\">But the reason I brought up that series is that its characters are excellent examples of high intelligence hampered by immense irrationality. The apex of this is represented by Dr. Sheldon Cooper, who is, essentially, a complete fundamentalist over every single thing in his life; he applies this attitude to everything, right down to people's favorite flavor of pudding: Raj is \"axiomatically wrong\" to prefer tapioca, because the best pudding is chocolate. Period. This attitude makes him a far, far worse scientist than he thinks, as he refuses to even consider any criticism of his methods or results.&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gz6D4bx2TLcqtNwWt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 59, "baseScore": -18, "extendedScore": null, "score": 1.030277505976973e-06, "legacy": true, "legacyId": "19989", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"internal-source-marker_0.23429366503842175\" style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal; \"></strong></p>\n<blockquote>\n<p style=\"margin: 0pt 19pt 0pt 12pt; text-align: justify; \" dir=\"ltr\"><strong id=\"Hi__Welcome_to_the_first_Non_Karmic_Casual_Discussion_Thread_\" style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal; \"><span style=\"font-size: 11px; font-family: Verdana; background-color: #f7f7f8; vertical-align: baseline; white-space: pre-wrap; \">Hi, Welcome to the first </span><span style=\"font-size: 11px; font-family: Verdana; background-color: #f7f7f8; font-weight: bold; vertical-align: baseline; white-space: pre-wrap; \">Non-Karmic-Casual-Discussion-Thread.</span></strong></p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\"><strong id=\"This_is_a_place_for__purpose_of_thread_goes_here__\" style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal; \"><span style=\"font-size: 11px; font-family: Verdana; background-color: transparent; vertical-align: baseline; white-space: pre-wrap; \">This is a place for [purpose of thread goes here].</span></strong></p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\"><strong id=\"In_order_to_create_a_causal_non_karmic_environment_for_every_one_we_ask_that_you\" style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal; \"><span style=\"font-size: 11px; font-family: Verdana; background-color: transparent; vertical-align: baseline; white-space: pre-wrap; \">In order to create a causal non karmic environment for every one we ask that you</span></strong></p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\"><strong id=\"_Do_not_upvote_or_downvote_any_zero_karma_posts\" style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal; \"><span style=\"font-size: 11px; font-family: Verdana; background-color: transparent; vertical-align: baseline; white-space: pre-wrap; \">-Do not upvote or downvote any zero karma posts</span></strong></p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\"><strong id=\"_If_you_see_a_vote_with_positive_karma__downvote_it_towards_zero__even_if_it_s_a_good_post\" style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal; \"><span style=\"font-size: 11px; font-family: Verdana; background-color: transparent; vertical-align: baseline; white-space: pre-wrap; \">-If you see a vote with positive karma, downvote it towards zero, even if it\u2019s a good post</span></strong></p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\"><strong id=\"_If_you_see_a_vote_with_negative_karma__upvote_it_towards_zero__even_if_it_s_a_weak_post\" style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal; \"><span style=\"font-size: 11px; font-family: Verdana; background-color: transparent; vertical-align: baseline; white-space: pre-wrap; \">-If you see a vote with negative karma, upvote it towards zero, even if it\u2019s a weak post</span></strong></p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\"><strong id=\"_Please_be_polite_and_respectful_to_other_users\" style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal; \"><span style=\"font-size: 11px; font-family: Verdana; background-color: transparent; vertical-align: baseline; white-space: pre-wrap; \">-Please be polite and respectful to other users</span></strong></p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\"><strong id=\"_Have_fun__\" style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal; \"><span style=\"font-size: 11px; font-family: Verdana; background-color: transparent; vertical-align: baseline; white-space: pre-wrap; \">-Have fun!\u201d</span></strong></p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\">&nbsp;</p>\n</blockquote>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\">&nbsp;</p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\">This is my first attempt at starting a casual conversation on LW where people don't have to worry about winning or losing points, and can just relax and have social fun together.</p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\">&nbsp;</p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\">So, Big Bang Theory. That series got me wondering. It seems to be about \"geeks\", and not the basement-dwelling variety either; they're highly successful and accomplished professionals, each in their own field. One of them has been an astronaut, even. And yet, everything they ever accomplish amounts to absolutely nothing in terms of social recognition or even in terms of personal happiness. And the thing is, it doesn't even get better for their \"normal\" counterparts, who are just as miserable and petty.</p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\">&nbsp;</p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\">Consider, then; how would being rationalists would affect the characters on this show? The writing of the show relies a lot on laughing at people rather than with them; would rationalist characters subvert that? And how would that rationalist outlook express itself given their personalities? (After all, notice how amazingly different from each other Yudkowsky, Hanson, and Alicorn are, just to name a few; they emphasize rather different things, and take different approaches to both truth-testing and problem-solving).</p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\">Note: this discussion does not need to be about rationalism. It can be a casual, normal discussion about the series. Relax and enjoy yourselves.</p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\">&nbsp;</p>\n<p style=\"margin: 0pt 19pt 0pt 12pt; \" dir=\"ltr\">But the reason I brought up that series is that its characters are excellent examples of high intelligence hampered by immense irrationality. The apex of this is represented by Dr. Sheldon Cooper, who is, essentially, a complete fundamentalist over every single thing in his life; he applies this attitude to everything, right down to people's favorite flavor of pudding: Raj is \"axiomatically wrong\" to prefer tapioca, because the best pudding is chocolate. Period. This attitude makes him a far, far worse scientist than he thinks, as he refuses to even consider any criticism of his methods or results.&nbsp;</p>\n<p>&nbsp;</p>", "sections": [{"title": "Hi, Welcome to the first Non-Karmic-Casual-Discussion-Thread.", "anchor": "Hi__Welcome_to_the_first_Non_Karmic_Casual_Discussion_Thread_", "level": 1}, {"title": "This is a place for [purpose of thread goes here].", "anchor": "This_is_a_place_for__purpose_of_thread_goes_here__", "level": 1}, {"title": "In order to create a causal non karmic environment for every one we ask that you", "anchor": "In_order_to_create_a_causal_non_karmic_environment_for_every_one_we_ask_that_you", "level": 1}, {"title": "-Do not upvote or downvote any zero karma posts", "anchor": "_Do_not_upvote_or_downvote_any_zero_karma_posts", "level": 1}, {"title": "-If you see a vote with positive karma, downvote it towards zero, even if it\u2019s a good post", "anchor": "_If_you_see_a_vote_with_positive_karma__downvote_it_towards_zero__even_if_it_s_a_good_post", "level": 1}, {"title": "-If you see a vote with negative karma, upvote it towards zero, even if it\u2019s a weak post", "anchor": "_If_you_see_a_vote_with_negative_karma__upvote_it_towards_zero__even_if_it_s_a_weak_post", "level": 1}, {"title": "-Please be polite and respectful to other users", "anchor": "_Please_be_polite_and_respectful_to_other_users", "level": 1}, {"title": "-Have fun!\u201d", "anchor": "_Have_fun__", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "207 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 207, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-10T14:21:25.105Z", "modifiedAt": null, "url": null, "title": "Cryonics as Charity", "slug": "cryonics-as-charity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:43.638Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jkaufman", "createdAt": "2010-11-04T21:42:19.863Z", "isAdmin": false, "displayName": "jefftk"}, "userId": "TtEoCrFeowCGb6rFK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JHTjdCe78t52NC9EQ/cryonics-as-charity", "pageUrlRelative": "/posts/JHTjdCe78t52NC9EQ/cryonics-as-charity", "linkUrl": "https://www.lesswrong.com/posts/JHTjdCe78t52NC9EQ/cryonics-as-charity", "postedAtFormatted": "Saturday, November 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cryonics%20as%20Charity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACryonics%20as%20Charity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJHTjdCe78t52NC9EQ%2Fcryonics-as-charity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cryonics%20as%20Charity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJHTjdCe78t52NC9EQ%2Fcryonics-as-charity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJHTjdCe78t52NC9EQ%2Fcryonics-as-charity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 242, "htmlBody": "<p>Imagine you accept the main idea of cryonics, that if we freeze brains future technology is likely to be able to extract the encoded information and revive the person digitally. [1] While this currently costs about $120K/person, if we did it routinely to everyone it could probably get down below $1000/person. Which is interesting: the current cost of averting a death is around <a href=\"http://www.givewell.org/international/top-charities/AMF#Costperlifesaved\">$1700</a>, but someone who doesn't die of malaria is still going to die of old age, so you can't really say their death was \"averted\". While someone who is revived after being frozen wouldn't live eternally, they might get to experience thousands of years of subjective life. In terms of life-years, getting cryonics to be really cheap and paying for people to get it sounds like it beats <a href=\"http://www.givewell.org/international/top-charities/\">GiveWell's top charities</a>.</p>\n<p>Aside from not agreeing that cryonics is likely to work, however, I don't think the value is actually all that high. A future world which would revive large numbers of people we freeze today would be massively different from the current world economically, but would still have constraints. There would be some number of digital people that could run simultaneously on whatever people-emulating hardware they have. Preserving additional 21st century minds would give future people the option to run revived people instead of new people, but I don't think it affects the overall number emulated.</p>\n<p><br /> [1] I <a href=\"/lw/b93/brain_preservation/\">don't think this is likely</a>.</p>\n<p><em><small>I also posted this <a href=\"http://www.jefftk.com/news/2012-11-10\">on my blog</a>.</small></em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZnHkaTkxukegSrZqE": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JHTjdCe78t52NC9EQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 6, "extendedScore": null, "score": 1.0303138293583568e-06, "legacy": true, "legacyId": "19990", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["nk9928vPqoeAMrTh6"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-10T15:50:07.292Z", "modifiedAt": null, "url": null, "title": "Detecting Web baloney with your nose?", "slug": "detecting-web-baloney-with-your-nose-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:37.546Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "uzalud", "createdAt": "2011-07-16T12:16:46.510Z", "isAdmin": false, "displayName": "uzalud"}, "userId": "LtYeek58ACZWdRy8n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ctr8f9cnabaQuLAbw/detecting-web-baloney-with-your-nose-0", "pageUrlRelative": "/posts/Ctr8f9cnabaQuLAbw/detecting-web-baloney-with-your-nose-0", "linkUrl": "https://www.lesswrong.com/posts/Ctr8f9cnabaQuLAbw/detecting-web-baloney-with-your-nose-0", "postedAtFormatted": "Saturday, November 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Detecting%20Web%20baloney%20with%20your%20nose%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADetecting%20Web%20baloney%20with%20your%20nose%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCtr8f9cnabaQuLAbw%2Fdetecting-web-baloney-with-your-nose-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Detecting%20Web%20baloney%20with%20your%20nose%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCtr8f9cnabaQuLAbw%2Fdetecting-web-baloney-with-your-nose-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCtr8f9cnabaQuLAbw%2Fdetecting-web-baloney-with-your-nose-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 159, "htmlBody": "<p>Is there a useful heuristic for detecting rationally-challenged texts (as in Web pages, forum posts, facebook comments) which takes relatively superficial attributes such as formatting choices, spelling errors, etc. as input? Something a casual Internet reader may use to detect <em>possibly&nbsp;</em>unworthy content so they can suspend their belief and research the matter further. Let's call them \"text smells\"&nbsp;(analogue to&nbsp;<a href=\"http://en.wikipedia.org/wiki/Code_smell\">code smells</a>),&nbsp;<a href=\"http://www.abovetopsecret.com/forum/thread882111/pg1\">like</a>:</p>\n<ol>\n<li>too much emphasis in text (ALL CAPS, bold, color, exclamations, etc.);</li>\n<li>walls of text;</li>\n<li>little concrete data/links/references;</li>\n<li>too much&nbsp;irrelevant&nbsp;data and references;</li>\n<li>poor spelling and grammar;</li>\n<li>obvious half-truths and misinformation.</li>\n</ol>\n<p>Since many crackpots, pseudoscientific con artists, and conspiracy theorists seem to have cleaned up their Web sites in recent years, I wonder do these low-cost baloney detection tools might be of real value. Does anyone know of any studies or analyses of correlation between these basic metrics and the actual quality of the content? Can you think of some other smells typical of Web baloney?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ctr8f9cnabaQuLAbw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -4, "extendedScore": null, "score": 1.0303629552686114e-06, "legacy": true, "legacyId": "19966", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-11T01:20:12.711Z", "modifiedAt": null, "url": null, "title": "Struck with a belief in Alien presence", "slug": "struck-with-a-belief-in-alien-presence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:39.132Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "LQxXyeYY8tAyoJ4m9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/STdfo8dKxaznnE66C/struck-with-a-belief-in-alien-presence", "pageUrlRelative": "/posts/STdfo8dKxaznnE66C/struck-with-a-belief-in-alien-presence", "linkUrl": "https://www.lesswrong.com/posts/STdfo8dKxaznnE66C/struck-with-a-belief-in-alien-presence", "postedAtFormatted": "Sunday, November 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Struck%20with%20a%20belief%20in%20Alien%20presence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStruck%20with%20a%20belief%20in%20Alien%20presence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSTdfo8dKxaznnE66C%2Fstruck-with-a-belief-in-alien-presence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Struck%20with%20a%20belief%20in%20Alien%20presence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSTdfo8dKxaznnE66C%2Fstruck-with-a-belief-in-alien-presence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSTdfo8dKxaznnE66C%2Fstruck-with-a-belief-in-alien-presence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 189, "htmlBody": "<p>Recently I've been struck with a belief in Aliens being present on this Earth. It happened after I watched <a href=\"http://www.youtube.com/watch?v=wtBnB8ONiJA\" target=\"_self\">this documenary</a> (and subsequently <a href=\"http://www.youtube.com/playlist?list=PLjEhkqEuqXzEDEWQ3aQhqT8ToAZJ1mLmU\" target=\"_self\">several others</a>). My feeling of belief is not particular interesting in itself - I could be lunatic or otherwise&nbsp;psychological&nbsp;dysfunctional. What I'm interested in knowing is to what extend other people, who consider themselves rationalists, feel belief in the existence of aliens on this earth, after watching this documentary. Is anyone willing to try and watch it and then report back?</p>\n<p>Another question arising in this matter is how to treat evidence of extraordinary things. Should one require 'extraordinary evidence for extraordinary claims'? I somehow feel that this notion is misguided - it discriminates evidence <em>prior</em> to observation. That is not the right time to start discriminating. At most we should ascribe a prior probability of zero and then do some Bayesian updating to get a posterior. Hmm, if no one has seen a black swan and some bayesian thinking person then sees a black swan a) in the distance or b) up front, what will his a posterior probability of the existence of black swans then be?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "STdfo8dKxaznnE66C", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": -13, "extendedScore": null, "score": -2e-05, "legacy": true, "legacyId": "19993", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 113, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-11T06:50:53.589Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Observing Optimization", "slug": "seq-rerun-observing-optimization", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:36.504Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Py24sqJXFSaBQxugJ/seq-rerun-observing-optimization", "pageUrlRelative": "/posts/Py24sqJXFSaBQxugJ/seq-rerun-observing-optimization", "linkUrl": "https://www.lesswrong.com/posts/Py24sqJXFSaBQxugJ/seq-rerun-observing-optimization", "postedAtFormatted": "Sunday, November 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Observing%20Optimization&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Observing%20Optimization%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPy24sqJXFSaBQxugJ%2Fseq-rerun-observing-optimization%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Observing%20Optimization%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPy24sqJXFSaBQxugJ%2Fseq-rerun-observing-optimization", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPy24sqJXFSaBQxugJ%2Fseq-rerun-observing-optimization", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 165, "htmlBody": "<p>Today's post, <a href=\"/lw/w2/observing_optimization/\">Observing Optimization</a> was originally published on 21 November 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Observing_Optimization\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Trying to derive predictions from a theory that says that sexual reproduction increases the rate of evolution is more difficult than it first appears.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/ff0/seq_rerun_eliezers_metalevel_determinism/\">Eliezer's Meta-Level Determinism</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Py24sqJXFSaBQxugJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 6, "extendedScore": null, "score": 1.0308620590800214e-06, "legacy": true, "legacyId": "19994", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GbGzP4LZTBN8dyd8c", "H4DQ4kk62D6e3tont", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-11T10:43:45.282Z", "modifiedAt": null, "url": null, "title": "Meetup : Paderborn Meetup, November 22th", "slug": "meetup-paderborn-meetup-november-22th", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:58.527Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Just_existing", "createdAt": "2012-01-16T18:40:07.392Z", "isAdmin": false, "displayName": "Just_existing"}, "userId": "o89m5G8Hk2tbpKTxg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TdEWNrxrd6ZnPB6s5/meetup-paderborn-meetup-november-22th", "pageUrlRelative": "/posts/TdEWNrxrd6ZnPB6s5/meetup-paderborn-meetup-november-22th", "linkUrl": "https://www.lesswrong.com/posts/TdEWNrxrd6ZnPB6s5/meetup-paderborn-meetup-november-22th", "postedAtFormatted": "Sunday, November 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Paderborn%20Meetup%2C%20November%2022th&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Paderborn%20Meetup%2C%20November%2022th%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTdEWNrxrd6ZnPB6s5%2Fmeetup-paderborn-meetup-november-22th%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Paderborn%20Meetup%2C%20November%2022th%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTdEWNrxrd6ZnPB6s5%2Fmeetup-paderborn-meetup-november-22th", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTdEWNrxrd6ZnPB6s5%2Fmeetup-paderborn-meetup-november-22th", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 87, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ft'>Paderborn Meetup, November 22th</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 November 2012 07:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Gownsmen's Pub, Uni Paderborn, Warburger Stra\u00dfe 100, Paderborn</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The first meetup in this city ever.</p>\n\n<p>There is lots of rationality stuff to talk about and the more people come, the more interesting people will be there to meet.</p>\n\n<p>Even if you are new to Lesswrong and have only read some articles, don't be to shy to \nshow up.</p>\n\n<p>I will have a sign.</p>\n\n<p>Everybody is welcome!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ft'>Paderborn Meetup, November 22th</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TdEWNrxrd6ZnPB6s5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0309911537007499e-06, "legacy": true, "legacyId": "19995", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Paderborn_Meetup__November_22th\">Discussion article for the meetup : <a href=\"/meetups/ft\">Paderborn Meetup, November 22th</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 November 2012 07:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Gownsmen's Pub, Uni Paderborn, Warburger Stra\u00dfe 100, Paderborn</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The first meetup in this city ever.</p>\n\n<p>There is lots of rationality stuff to talk about and the more people come, the more interesting people will be there to meet.</p>\n\n<p>Even if you are new to Lesswrong and have only read some articles, don't be to shy to \nshow up.</p>\n\n<p>I will have a sign.</p>\n\n<p>Everybody is welcome!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Paderborn_Meetup__November_22th1\">Discussion article for the meetup : <a href=\"/meetups/ft\">Paderborn Meetup, November 22th</a></h2>", "sections": [{"title": "Discussion article for the meetup : Paderborn Meetup, November 22th", "anchor": "Discussion_article_for_the_meetup___Paderborn_Meetup__November_22th", "level": 1}, {"title": "Discussion article for the meetup : Paderborn Meetup, November 22th", "anchor": "Discussion_article_for_the_meetup___Paderborn_Meetup__November_22th1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-11T18:54:32.073Z", "modifiedAt": null, "url": null, "title": "Questions from potential donors to Giving What We Can, 80,000 Hours and EAA", "slug": "questions-from-potential-donors-to-giving-what-we-can-80-000", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:57.701Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Giles", "createdAt": "2011-02-11T02:30:16.999Z", "isAdmin": false, "displayName": "Giles"}, "userId": "H347ba3KZMP8XoDt3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vsxREZsCi24KMJRQk/questions-from-potential-donors-to-giving-what-we-can-80-000", "pageUrlRelative": "/posts/vsxREZsCi24KMJRQk/questions-from-potential-donors-to-giving-what-we-can-80-000", "linkUrl": "https://www.lesswrong.com/posts/vsxREZsCi24KMJRQk/questions-from-potential-donors-to-giving-what-we-can-80-000", "postedAtFormatted": "Sunday, November 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Questions%20from%20potential%20donors%20to%20Giving%20What%20We%20Can%2C%2080%2C000%20Hours%20and%20EAA&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuestions%20from%20potential%20donors%20to%20Giving%20What%20We%20Can%2C%2080%2C000%20Hours%20and%20EAA%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvsxREZsCi24KMJRQk%2Fquestions-from-potential-donors-to-giving-what-we-can-80-000%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Questions%20from%20potential%20donors%20to%20Giving%20What%20We%20Can%2C%2080%2C000%20Hours%20and%20EAA%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvsxREZsCi24KMJRQk%2Fquestions-from-potential-donors-to-giving-what-we-can-80-000", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvsxREZsCi24KMJRQk%2Fquestions-from-potential-donors-to-giving-what-we-can-80-000", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 988, "htmlBody": "<p><strong>UPDATE: <a href=\"/r/discussion/lw/fkz/responses_to_questions_on_donating_to_80k_gwwc/\">Will has responded here</a>.</strong></p>\n<p><a href=\"/lw/fej/giving_what_we_can_80000_hours_and_metacharity/\">The Centre for Effective Altruism has recently opened up for donations</a>. This is the umbrella organisation for <a href=\"http://www.givingwhatwecan.org/\">Giving What We Can</a> (GWWC), <a href=\"http://80000hours.org/\">80,000 Hours</a> (80K) and <a href=\"http://www.effectiveanimalactivism.org/\">Effective Animal Activism</a> (EAA, which I think is still formally part of 80K). I responded to Will Crouch in an email with a list of questions. You're welcome to add to the list in the comments and I'll make sure the questions get to him.</p>\n<p>Will seems pretty cool about the whole transparency thing so any answers will hopefully make their way onto the LW Discussion board. Will (and Ben Todd) will try to take the time to give accurate and well-formed answers where possible.</p>\n<p>CEA is based in the UK so figures are in GBP.</p>\n<p><a id=\"more\"></a>1. What would you do with different funding levels?<br />- The \"what would my money buy\" section of the donate page gives me a vague idea<br />- It's easier to understand information of the form \"If we got &pound;50,000 over the next year we'd do this, if we got &pound;100,000 over the next year we'd do this and this\"<br /> - I understand if you're unable to produce that kind of information<br />- That kind of information is particularly useful for evaluating \"did things go as well as they were supposed to, given the amount of funding we actually received\"<br /> - Particularly useful for me as I'm trying to train myself to identify funding opportunities. So I'll need feedback as to whether I got it right!<br /><br />2. How much money are you expecting?<br />- You say on the donate page that you've spent less than &pound;20,000<br /> - This must have come from somewhere - how much of that particular source of funding is still available?<br />- Are you getting any other emails like this one? <strong>(Edit: i.e. which are offering donations, possibly conditional on answering some questions first)</strong><br />- Do you have any plans (perhaps in the longer term) to chase after big money e.g. Peter Thiel?<br /> - Are you expecting to get a lot of donations back from people who've received advice and help from 80K?<br />- I sort of get the impression that there's a lot of good feeling towards 80K at least within the effective altruist community, and for that reason it might not be too hard for you to find funds. To what extent do you agree with this?<br /> <br />3. Which is more useful, regular donations or lump sums?<br /><br />4. If you had funds to hire an extra person, do you know how that person would be?<br />- How important is it to find talented people to work for you?<br />- Are you trying to find someone from the top 5%? The top 1%?<br /> <br />5. Where do you see the delineation between what CEA does and what other effective altruist orgs do?<br />- THINK<br />- Global Catastrophic Risk Institute<br />- Center for Applied Rationality<br />- Future of Humanity Institute<br /> - GiveWell<br /><br />6. How much personal connection and communication is there between CEA and these orgs?<br /><br />7. How do you assess the credibility of money pledged?<br /><br />8. How are you measuring money moved/pledged?<br /> - GiveWell give the impression of being particularly careful about this<br />- GW also communicate it very openly, including a breakdown of how much money goes to which cause. Are you planning to communicate it in a similar way?<br /> - How much money do you expect to move next year?<br />- How much of your \"money moved\" will also get counted as GiveWell's \"money moved\"? (This is particularly relevant when using money moved figures to estimate the size of the effective giving sector as a whole)<br /> <br />9. You do a bunch of different but related things - website content, speaker events, career counseling,<br />- Do you imagine yourself specialising in just one of these in the future?<br />- Are you at the stage of experimenting to find out which activity is the most effective?<br /> - Is there synergy between them? (e.g. if career advice sessions and website content are both a lot more effective if you're also doing the other one)<br /><br />10. I have very little idea about what the 80K community is like or how exactly you invest in it<br /> - in what ways does your team interact with your community, other than one-on-one career advice and hosting speaker events?<br />- do you invest in members' skills such as critical thinking and the ability to evaluate organisations?<br /> - what other skills and qualities do you want to develop in your members, and how do you plan to go about it?<br />- to what extent do you think talents and abilities are inherent (or at least beyond your control), and to what extent are they trainable?<br /> <br />11. Is 80K planning activity in any new physical locations?<br />- If so, where?<br />- If there's already a THINK community in that location, what do you imagine the relationship between THINK and 80K looking like?<br /> <br />12. What does 80K/CEA plan in the way of self-evaluation?<br /><br />13. What are you planning in the way of financial transparency?<br /><br />14. You said in your LW post that you have \"much more information available\" on GWWC's impact.<br /> <br />15. There's apparently a lot of interest in xrisk among 80K members. Do you know why this is?<br /><br />16. Do you know of any other organisations that do anything similar to what you do? (other than ones I've already mentioned). In particular any groups that give career advice to philanthropists.</p>\n<p><strong>UPDATE: <a href=\"http://lesswrong.com/r/discussion/lw/fkz/responses_to_questions_on_donating_to_80k_gwwc/\">Will has responded here</a>.</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"se3XDuQ4xbeWvu4eF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vsxREZsCi24KMJRQk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 17, "extendedScore": null, "score": 1.0312633277094335e-06, "legacy": true, "legacyId": "19996", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"UPDATE__Will_has_responded_here_\">UPDATE: <a href=\"/r/discussion/lw/fkz/responses_to_questions_on_donating_to_80k_gwwc/\">Will has responded here</a>.</strong></p>\n<p><a href=\"/lw/fej/giving_what_we_can_80000_hours_and_metacharity/\">The Centre for Effective Altruism has recently opened up for donations</a>. This is the umbrella organisation for <a href=\"http://www.givingwhatwecan.org/\">Giving What We Can</a> (GWWC), <a href=\"http://80000hours.org/\">80,000 Hours</a> (80K) and <a href=\"http://www.effectiveanimalactivism.org/\">Effective Animal Activism</a> (EAA, which I think is still formally part of 80K). I responded to Will Crouch in an email with a list of questions. You're welcome to add to the list in the comments and I'll make sure the questions get to him.</p>\n<p>Will seems pretty cool about the whole transparency thing so any answers will hopefully make their way onto the LW Discussion board. Will (and Ben Todd) will try to take the time to give accurate and well-formed answers where possible.</p>\n<p>CEA is based in the UK so figures are in GBP.</p>\n<p><a id=\"more\"></a>1. What would you do with different funding levels?<br>- The \"what would my money buy\" section of the donate page gives me a vague idea<br>- It's easier to understand information of the form \"If we got \u00a350,000 over the next year we'd do this, if we got \u00a3100,000 over the next year we'd do this and this\"<br> - I understand if you're unable to produce that kind of information<br>- That kind of information is particularly useful for evaluating \"did things go as well as they were supposed to, given the amount of funding we actually received\"<br> - Particularly useful for me as I'm trying to train myself to identify funding opportunities. So I'll need feedback as to whether I got it right!<br><br>2. How much money are you expecting?<br>- You say on the donate page that you've spent less than \u00a320,000<br> - This must have come from somewhere - how much of that particular source of funding is still available?<br>- Are you getting any other emails like this one? <strong>(Edit: i.e. which are offering donations, possibly conditional on answering some questions first)</strong><br>- Do you have any plans (perhaps in the longer term) to chase after big money e.g. Peter Thiel?<br> - Are you expecting to get a lot of donations back from people who've received advice and help from 80K?<br>- I sort of get the impression that there's a lot of good feeling towards 80K at least within the effective altruist community, and for that reason it might not be too hard for you to find funds. To what extent do you agree with this?<br> <br>3. Which is more useful, regular donations or lump sums?<br><br>4. If you had funds to hire an extra person, do you know how that person would be?<br>- How important is it to find talented people to work for you?<br>- Are you trying to find someone from the top 5%? The top 1%?<br> <br>5. Where do you see the delineation between what CEA does and what other effective altruist orgs do?<br>- THINK<br>- Global Catastrophic Risk Institute<br>- Center for Applied Rationality<br>- Future of Humanity Institute<br> - GiveWell<br><br>6. How much personal connection and communication is there between CEA and these orgs?<br><br>7. How do you assess the credibility of money pledged?<br><br>8. How are you measuring money moved/pledged?<br> - GiveWell give the impression of being particularly careful about this<br>- GW also communicate it very openly, including a breakdown of how much money goes to which cause. Are you planning to communicate it in a similar way?<br> - How much money do you expect to move next year?<br>- How much of your \"money moved\" will also get counted as GiveWell's \"money moved\"? (This is particularly relevant when using money moved figures to estimate the size of the effective giving sector as a whole)<br> <br>9. You do a bunch of different but related things - website content, speaker events, career counseling,<br>- Do you imagine yourself specialising in just one of these in the future?<br>- Are you at the stage of experimenting to find out which activity is the most effective?<br> - Is there synergy between them? (e.g. if career advice sessions and website content are both a lot more effective if you're also doing the other one)<br><br>10. I have very little idea about what the 80K community is like or how exactly you invest in it<br> - in what ways does your team interact with your community, other than one-on-one career advice and hosting speaker events?<br>- do you invest in members' skills such as critical thinking and the ability to evaluate organisations?<br> - what other skills and qualities do you want to develop in your members, and how do you plan to go about it?<br>- to what extent do you think talents and abilities are inherent (or at least beyond your control), and to what extent are they trainable?<br> <br>11. Is 80K planning activity in any new physical locations?<br>- If so, where?<br>- If there's already a THINK community in that location, what do you imagine the relationship between THINK and 80K looking like?<br> <br>12. What does 80K/CEA plan in the way of self-evaluation?<br><br>13. What are you planning in the way of financial transparency?<br><br>14. You said in your LW post that you have \"much more information available\" on GWWC's impact.<br> <br>15. There's apparently a lot of interest in xrisk among 80K members. Do you know why this is?<br><br>16. Do you know of any other organisations that do anything similar to what you do? (other than ones I've already mentioned). In particular any groups that give career advice to philanthropists.</p>\n<p><strong id=\"UPDATE__Will_has_responded_here_1\">UPDATE: <a href=\"http://lesswrong.com/r/discussion/lw/fkz/responses_to_questions_on_donating_to_80k_gwwc/\">Will has responded here</a>.</strong></p>", "sections": [{"title": "UPDATE: Will has responded here.", "anchor": "UPDATE__Will_has_responded_here_", "level": 1}, {"title": "UPDATE: Will has responded here.", "anchor": "UPDATE__Will_has_responded_here_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "22 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rC2EBbhmCzJCSyysN", "FCiMtrsM8mcmBtfTR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-11T21:09:13.023Z", "modifiedAt": null, "url": null, "title": "Meetup : Pittsburgh: Value of Information", "slug": "meetup-pittsburgh-value-of-information", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KatjaGrace", "createdAt": "2009-02-27T14:15:22.378Z", "isAdmin": false, "displayName": "KatjaGrace"}, "userId": "jRRYAy2mQAHy2Mq3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MwmMzgAGNw6zAX7qX/meetup-pittsburgh-value-of-information", "pageUrlRelative": "/posts/MwmMzgAGNw6zAX7qX/meetup-pittsburgh-value-of-information", "linkUrl": "https://www.lesswrong.com/posts/MwmMzgAGNw6zAX7qX/meetup-pittsburgh-value-of-information", "postedAtFormatted": "Sunday, November 11th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Pittsburgh%3A%20Value%20of%20Information&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Pittsburgh%3A%20Value%20of%20Information%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMwmMzgAGNw6zAX7qX%2Fmeetup-pittsburgh-value-of-information%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Pittsburgh%3A%20Value%20of%20Information%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMwmMzgAGNw6zAX7qX%2Fmeetup-pittsburgh-value-of-information", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMwmMzgAGNw6zAX7qX%2Fmeetup-pittsburgh-value-of-information", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 37, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/fu'>Pittsburgh: Value of Information</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">13 November 2012 06:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">EatUnique, S Craig St, Pittsburgh</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Phone 412-304-6258 if you can't find us</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/fu'>Pittsburgh: Value of Information</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MwmMzgAGNw6zAX7qX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0313380414092391e-06, "legacy": true, "legacyId": "19998", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh__Value_of_Information\">Discussion article for the meetup : <a href=\"/meetups/fu\">Pittsburgh: Value of Information</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">13 November 2012 06:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">EatUnique, S Craig St, Pittsburgh</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Phone 412-304-6258 if you can't find us</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh__Value_of_Information1\">Discussion article for the meetup : <a href=\"/meetups/fu\">Pittsburgh: Value of Information</a></h2>", "sections": [{"title": "Discussion article for the meetup : Pittsburgh: Value of Information", "anchor": "Discussion_article_for_the_meetup___Pittsburgh__Value_of_Information", "level": 1}, {"title": "Discussion article for the meetup : Pittsburgh: Value of Information", "anchor": "Discussion_article_for_the_meetup___Pittsburgh__Value_of_Information1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-12T03:08:50.983Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Life's Story Continues", "slug": "seq-rerun-life-s-story-continues", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:37.220Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rLF95zQieKnGrQMNK/seq-rerun-life-s-story-continues", "pageUrlRelative": "/posts/rLF95zQieKnGrQMNK/seq-rerun-life-s-story-continues", "linkUrl": "https://www.lesswrong.com/posts/rLF95zQieKnGrQMNK/seq-rerun-life-s-story-continues", "postedAtFormatted": "Monday, November 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Life's%20Story%20Continues&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Life's%20Story%20Continues%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrLF95zQieKnGrQMNK%2Fseq-rerun-life-s-story-continues%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Life's%20Story%20Continues%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrLF95zQieKnGrQMNK%2Fseq-rerun-life-s-story-continues", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrLF95zQieKnGrQMNK%2Fseq-rerun-life-s-story-continues", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 164, "htmlBody": "<p>Today's post, <a href=\"/lw/w3/lifes_story_continues/\">Life's Story Continues</a> was originally published on 21 November 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Life.27s_Story_Continues\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A discussion of some of the classical big steps in the evolution of life, and how they relate to the idea of optimization.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/ffe/seq_rerun_observing_optimization/\">Observing Optimization</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rLF95zQieKnGrQMNK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.031537590535374e-06, "legacy": true, "legacyId": "20011", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RXLoo3pXgY2hjdXrg", "Py24sqJXFSaBQxugJ", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-12T16:03:54.715Z", "modifiedAt": null, "url": null, "title": "Digging the Bull's Horn", "slug": "digging-the-bull-s-horn", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:37.510Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gworley", "createdAt": "2009-03-26T17:18:20.404Z", "isAdmin": false, "displayName": "G Gordon Worley III"}, "userId": "gjoi5eBQob27Lww62", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LqjH7J5sy9TpGhyBR/digging-the-bull-s-horn", "pageUrlRelative": "/posts/LqjH7J5sy9TpGhyBR/digging-the-bull-s-horn", "linkUrl": "https://www.lesswrong.com/posts/LqjH7J5sy9TpGhyBR/digging-the-bull-s-horn", "postedAtFormatted": "Monday, November 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Digging%20the%20Bull's%20Horn&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADigging%20the%20Bull's%20Horn%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLqjH7J5sy9TpGhyBR%2Fdigging-the-bull-s-horn%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Digging%20the%20Bull's%20Horn%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLqjH7J5sy9TpGhyBR%2Fdigging-the-bull-s-horn", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLqjH7J5sy9TpGhyBR%2Fdigging-the-bull-s-horn", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 480, "htmlBody": "<p>Some time ago I learned of the metaphor of 'digging the bull's horn'. This might sound a little strange, since horns are mostly hollow, but imagine a bull's horn used to store black powder. In the beginning the work is easy and you can scoop out a lot powder with very little effort. As you dig down, though, each scoop yields less powder as you dig into the narrow part of the horn until the only way you can get out more powder is to turn the horn over a dump it out.</p>\n<p>It's often the same way with learning. When you start out in a subject there is a lot to be learned (both in quantity of material you have not yet seen and in quantity of benefits you have to gain from the information), but as you dig deeper into a subject the useful insights come less often or are more limited in scope. Eventually you dig down so far that the only way to learn more is to discover new things that no one has yet learned (to stretch the metaphor, you have to add your own powder back to dig out).</p>\n<p>It's useful to know that you're digging the bull's horn when learning because, unless you really enjoy a subject or have some reason to believe that contributing to it is worthwhile, you can know in advance that most of the really valuable insights you'll gain will come early on. If you want to benefit from knowing about as much stuff as possible, you'll often want to stop actively pursuing a subject unless you want to make a career out of it.</p>\n<p>But, for a few subjects, this isn't true. Sometimes, as you continue to learn the last few hard things that don't seem to provide big, broadly-useful insights, you manage to accumulate a critical level of knowledge about the subject that opens up a whole new world of insights to you that were previously hidden. To push the metaphor, you eventually dig so deep that you come out the other side to find a huge pile of powder.</p>\n<p>The Way seems to be one of those subjects you can dig past the end of: there are some people who have mastered The Way to such an extent that they have access to a huge range of benefits not available to those still digging the horn. But when it comes to other subjects, how do you know? Great insights could be hiding beyond currently obscure fields of study because no one has bothered to dig deep enough. Aside from having clear examples of people who came out the other side to give us reason to believe it's worth while to deep really deep on some subjects, is there any way we can make a good prediction about what subjects may be worth digging to the end of the bull's horn?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LqjH7J5sy9TpGhyBR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": -10, "extendedScore": null, "score": 1.031967881322066e-06, "legacy": true, "legacyId": "20022", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-12T17:23:24.348Z", "modifiedAt": null, "url": null, "title": "SIA fears (expected) infinity", "slug": "sia-fears-expected-infinity", "viewCount": null, "lastCommentedAt": "2020-12-02T14:32:12.766Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/99isjR8W4JGZdro7x/sia-fears-expected-infinity", "pageUrlRelative": "/posts/99isjR8W4JGZdro7x/sia-fears-expected-infinity", "linkUrl": "https://www.lesswrong.com/posts/99isjR8W4JGZdro7x/sia-fears-expected-infinity", "postedAtFormatted": "Monday, November 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20SIA%20fears%20(expected)%20infinity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASIA%20fears%20(expected)%20infinity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F99isjR8W4JGZdro7x%2Fsia-fears-expected-infinity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=SIA%20fears%20(expected)%20infinity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F99isjR8W4JGZdro7x%2Fsia-fears-expected-infinity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F99isjR8W4JGZdro7x%2Fsia-fears-expected-infinity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 217, "htmlBody": "<p>It's well known that the <a href=\"http://en.wikipedia.org/wiki/Self-Indication_Assumption\">Self-Indication Assumption</a> (SIA) has problems with infinite populations (one of the reasons I strongly recommend not using the probability as the fundamental object of interest, but instead the decision, as in&nbsp;<a href=\"/lw/891/anthropic_decision_theory_i_sleeping_beauty_and/\">anthropic decision theory</a>).</p>\n<p>SIA also has problems with arbitrarily large finite populations, at least in some cases. What cases are these? Imagine that we had these (non-anthropic) probabilities for various populations:</p>\n<p style=\"padding-left: 30px;\">p<sub>0</sub>, p<sub>1</sub>, p<sub>2</sub>, p<sub>3</sub>, p<sub>4</sub>...</p>\n<p>Now let us apply the anthropic correction from SIA; before renormalising, we have these weights for different population levels:</p>\n<p style=\"padding-left: 30px;\">0, p<sub>1</sub>, 2p<sub>2</sub>, 3p<sub>3</sub>, 4p<sub>4</sub>...</p>\n<p>To renormalise, we need to divide by the sum&nbsp;0 + p<sub>1</sub>&nbsp;+ 2p<sub>2</sub>&nbsp;+ 3p<sub>3</sub>&nbsp;+ 4p<sub>4</sub>... This is actually the expected population!&nbsp;(note: we are using the population as a proxy for the size of the reference class of agents who are subjectively indistinguishable from us; see <a href=\"/r/discussion/lw/f7d/sia_conditional_probability_and_jaan_tallinns/\">this post</a> for more details)</p>\n<p>So using SIA is possible if and only if the (non-anthropic) expected population is finite (and non-zero).</p>\n<p>Note that it is possible for the anthropic expected population to be infinite! For instance if p<sub>j</sub> is C/j<sup>3</sup>, for some constant C, then the non-anthropic expected population is finite (being the infinite sum of C/j<sup>2</sup>). However once we have done the SIA correction, we can see that the SIA-corrected expected population is infinite (being the infinite sum of some constant times 1/j).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "99isjR8W4JGZdro7x", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 20, "extendedScore": null, "score": 4.5e-05, "legacy": true, "legacyId": "20023", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["svhbnSdxW3XmFXXTK", "hqET2rKbLeZFPD6uM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-12T17:24:44.043Z", "modifiedAt": null, "url": null, "title": "SIA, conditional probability and Jaan Tallinn's simulation tree", "slug": "sia-conditional-probability-and-jaan-tallinn-s-simulation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:56.251Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hqET2rKbLeZFPD6uM/sia-conditional-probability-and-jaan-tallinn-s-simulation", "pageUrlRelative": "/posts/hqET2rKbLeZFPD6uM/sia-conditional-probability-and-jaan-tallinn-s-simulation", "linkUrl": "https://www.lesswrong.com/posts/hqET2rKbLeZFPD6uM/sia-conditional-probability-and-jaan-tallinn-s-simulation", "postedAtFormatted": "Monday, November 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20SIA%2C%20conditional%20probability%20and%20Jaan%20Tallinn's%20simulation%20tree&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASIA%2C%20conditional%20probability%20and%20Jaan%20Tallinn's%20simulation%20tree%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhqET2rKbLeZFPD6uM%2Fsia-conditional-probability-and-jaan-tallinn-s-simulation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=SIA%2C%20conditional%20probability%20and%20Jaan%20Tallinn's%20simulation%20tree%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhqET2rKbLeZFPD6uM%2Fsia-conditional-probability-and-jaan-tallinn-s-simulation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhqET2rKbLeZFPD6uM%2Fsia-conditional-probability-and-jaan-tallinn-s-simulation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2005, "htmlBody": "<p>If you're going to use anthropic probability, use&nbsp;the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Self-Indication_Assumption\">self indication assumption</a>&nbsp;(SIA) - it's by far the most sensible way of doing things.</p>\n<p>Now, I&nbsp;am of the strong belief that probabilities in anthropic problems (such as the <a href=\"http://en.wikipedia.org/wiki/Sleeping_Beauty_problem\">Sleeping Beauty problem</a>) are not meaningful - only your decisions matter. And you can have different probability theories but still always reach the decisions if you have different theories as to who bears the responsibility of the actions of your copies, or how much you value them - see <a href=\"/lw/891/anthropic_decision_theory_i_sleeping_beauty_and/\">anthropic decision theory</a>&nbsp;(ADT).</p>\n<p>But that's a minority position - most people still use anthropic probabilities, so it's worth taking a more through look at what SIA does and doesn't tell you about population sizes and conditional probability.</p>\n<p>This post will aim to clarify some issues with SIA, especially concerning Jaan Tallinn's simulation-tree model which he <a href=\"http://fora.tv/2012/10/14/Jaan_Tallinn_Why_Now_A_Quest_in_Metaphysics\">presented</a> in&nbsp;exquisite&nbsp;story format at the recent <a href=\"http://singularitysummit.com/\">singularity summit</a>. I'll be assuming basic familiarity with SIA, and will run away screaming from any questions concerning infinity. SIA fears infinity (in a shameless self plug, I'll mention that anthropic decision theory runs into far less problems with infinities; for instance a bounded utility function is a sufficient - but not necessary - condition to ensure that ADT give you sensible answers even with infinitely many copies).</p>\n<p>But onwards and upwards with SIA! To not-quite-infinity and below!</p>\n<p>&nbsp;</p>\n<h2>SIA does not (directly) predict large populations</h2>\n<p>One error people often make with SIA is to assume that it predicts a large population. It doesn't - at least not directly. What SIA predicts is that there will be a large number of agents that are subjectively indistinguishable from you. You can call these subjectively indistinguishable agents the \"minimal&nbsp;reference&nbsp;class\" - it is a great advantage of SIA that it will continue to make sense for any reference class you choose (as long as it contains the minimal reference class).</p>\n<p>The SIA's impact on the total population is indirect: if the size of the total population is correlated with that of the minimal reference class, SIA will predict a large population. A correlation is not implausible: for instance, if there are a lot of humans around, then the probability that one of them is you is much larger. If there are a lot of intelligent life forms around, then the chance that humans exist is higher, and so on.</p>\n<p>In most cases, we don't run into problems with assuming that SIA predicts large populations. But we have to bear in mind that the effect is indirect, and the effect can and does break down in many cases. For instance imagine that&nbsp;you knew you had evolved on some planet, but for some odd reason, didn't know whether your planet had a ring system or not. You have managed to figure out that the evolution of life on planets with ring systems is independent of the evolution of life on planets without. Since you don't know which situation you're in, SIA instructs you to increase the probability of life on ringed and on non-ringed planets (so far, so good - SIA is predicting generally larger populations).</p>\n<p>And then one day you look up at the sky and see:<a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<p><img src=\"http://images.lesswrong.com/t3_f7d_0.png?v=f333746e9108010bc925811483fea7a8\" alt=\"\" width=\"480\" height=\"376\" /></p>\n<p>&nbsp;</p>\n<p>After seeing this, SIA still increases your probability estimate for large populations on ringed planets - but your estimate for non-ringed planets reverts to the naive baseline. SIA can affect variables only as much as they are correlated with the size of the minimal reference class - as variables become more independent of this, SIA's effect becomes weaker.</p>\n<p>&nbsp;</p>\n<h2>Once you know how all about yourselves, SIA is powerless</h2>\n<p>Designate by P the naive probability that you might have, before considering any anthropic factors. Let P<sub>SIA</sub> be the anthropic probability, adjusted by SIA. If N is a random variable that counts the size of the minimal reference class, then SIA's adjustment for any random variable X is:</p>\n<p style=\"padding-left: 60px;\"><strong>P<sub>SIA</sub>(N=n and X=x) </strong>=<strong> nP(N=n and X=x)/Norm</strong></p>\n<p>This is just rephrasing the fact that we weight the worlds by the size of the minimal reference class. The normalization term \"Norm\" is simply a constant that divides all P<sub>SIA</sub> probabilities to ensure they sum to 1 after doing the above adjustment (Norm is equal to the <a href=\"/r/discussion/lw/fg7/sia_fears_expected_infinity/\">expected value of N</a>). If X is independent of N (and hence P(X=x) doesn't have any anthropic correction from SIA, and equals P<sub>SIA</sub>(X=x)), then conditional probability follows the rule:</p>\n<p style=\"padding-left: 60px;\"><strong>P<sub>SIA</sub>(N=n|X=x)</strong> = P<sub>SIA</sub>(N=n and X=x)/P<sub>SIA</sub>(X=x) = <strong>n</strong>P(N=n and X=x)/(Norm*P(X=x)) = <strong>nP(N=n|X=x)/Norm</strong></p>\n<p>If X isn't independent of N, then P<sub>SIA</sub>(X=x) is not necessarily the same as P(X=x), and the above equality need not hold. In most cases there will still be an anthropic correction, hence:</p>\n<ul>\n<li>In general, the probability of anthropic variables conditioned upon other variables will change under SIA.</li>\n</ul>\n<p>The picture is very different, however, if we condition on anthropic variables! For instance if we condition on N itself, then&nbsp;P<sub>SIA</sub> is exactly the same as P, as there are two factors of n (and two Norms) cancelling out:</p>\n<p style=\"padding-left: 60px;\"><strong>P<sub>SIA</sub>(X=x|N=n)</strong> = P<sub>SIA</sub>(N=n and X=x)/P<sub>SIA</sub>(N=n) = <strong>n</strong>P(N=n and X=x)/<strong>n</strong>P(N=n) = <strong>P(X=x|N=n)</strong></p>\n<p>Hence:</p>\n<ul>\n<li>The probability of any variable conditioned on the size of the minimal reference class, is unaltered by SIA.</li>\n</ul>\n<p>So if we condition on the size of our reference class, there are no longer <em>any</em> anthropic effects.</p>\n<p>I myself was somewhat confused by this (see <a href=\"/lw/eel/sia_doomsday/\">here</a> and <a href=\"/lw/ef3/the_real_sia_doomsday/\">here</a>). But one implication is that,&nbsp;if you know the past human population, you expect the current human population to be above trend. Ditto if you know the future population: you then expect the current population to be higher than that information would&nbsp;naively&nbsp;imply (this gives you a poor-man's <a href=\"http://en.wikipedia.org/wiki/Doomsday_argument\">Doomsday argument</a>).&nbsp;But if, on the other hand, you know your current population, then you expect future and past populations to be given by naive probabilities, without anthropic adjustment.</p>\n<p>To illustrate this counterintuitive idea, take a toy model, with three populations at three steps (entitled \"past population\", \"present population\" and \"future population\"). Imagine that the past population was 3, and that the population either goes up by one, goes down by one, or stays the same, with equal (naive) probability. So we now have nine equally probable population series: (3,2,1), (3,3,3), (3,4,4), (3,4,5), etc...</p>\n<p>We live in the present, and we will adjust these series with SIA; thus we have three series (3,4,3), (3,4,4) and (3,4,5), each with probability 4/27, three series (3,3,4), (3,3,3) and (3,3,2), with probability 3/27, and three series (3,2,3), (3,2,2) and (3,2,1), with probability 2/27. Then if I give you the past and future population values, you expect the current population to be higher than the average: (3,?,3) is more likely to be (3,4,3), (3,?,2) is more likely to be (3,3,2), and so on. The only exceptions to this are&nbsp;(3,?,5) and (3,?,1), which each have only one possible value.</p>\n<p>However, if I give you the current population, then all possible future populations are equally likely: (3,4,?) is equally likely to end with 3, 4, or 5, as the sequences&nbsp;(3,4,3), (3,4,4) and (3,4,5) are equally likely. And that is exactly what naive probability would tell us. By conditioning on current population (indirectly conditioning on the size of the minimal reference class), we've erased any anthropic effects.</p>\n<p>&nbsp;</p>\n<h2>Jaan Tallinn's tree</h2>\n<p>If you haven't already, go an watch Jaan Tallinn's&nbsp;<a href=\"http://fora.tv/2012/10/14/Jaan_Tallinn_Why_Now_A_Quest_in_Metaphysics\">presentation</a>. It's beautiful. And don't come back until you're done that.</p>\n<p>Right, back already? Jaan proposes a model in which superintelligences try and produce other superintelligences by evolving civilizations through a process of tree search. Essentially evolving one civilization up to superintelligence/singularity, rewinding the program a little bit, and then running it forwards again with small differences. This would cut down on calculations and avoid having to simulate each civilization's history separately, from birth to singularity.</p>\n<p>Then if we asked \"if we are simulated, where are we likely to exist in this tree process?\", then SIA gives you a clear answer: at the final stage, just before the singularity. Because that's where most of the simulated beings will exist.</p>\n<p>But the real question is \"if we are simulated, then given that we observe ourselves to be at a certain stage of human civilization, how likely is it that we are at the final stage?\" This is a&nbsp;subtly&nbsp;different question. And we've already seen that conditioning on variables tied to our existence - in this case, our observation of what stage of civilization we are at - can give very different answers from what we might expect.</p>\n<p>To see this, consider five different scenarios, or worlds. W<sub>1</sub> is the world described in Jaan Tallinn's talk: a branching process of simulations, with us the final stage before the singularity. In our model, it has eight civilizations at the same stage as us. Then we have world W<sub>2</sub>, which is the same as W<sub>1</sub>, except that there's still another stage remaining before the singularity: we add extra simulations at a date beyond us (which means that W<sub>2</sub> contains more simulations that W<sub>1</sub>). Then we have W<sub>3</sub>, which is structurally the same as W<sub>2</sub>, but has the same total number of simulations as W<sub>1</sub>. The following table shows these worlds, with a dotted rectangle around civilizations at our level of development. Under each world is the label, then a row showing the number of civilizations at our level, then a row showing the number of civilizations at every stage.</p>\n<p><img src=\"http://images.lesswrong.com/t3_f7d_6.png?v=a017c7e1eeea4329389e362de7f90a42\" alt=\"\" width=\"705\" height=\"494\" /></p>\n<p>But maybe the tree model is wrong, and the real world is linear - either because the superintelligences want to simulate us thoroughly along one complete history, or maybe because most agents are in real histories and not simulations. We can the consider world W<sub>4</sub>, which is linear and has the same number of civilizations <em>at our stage</em> as W<sub>1</sub> does, and W<sub>5</sub>, which is linear and has the same <em>total number</em> of civilizations as W1 does:</p>\n<p><img src=\"http://images.lesswrong.com/t3_f7d_7.png?v=782e05c123bd22fbfebda21d7a27d998\" alt=\"\" width=\"700\" height=\"626\" /></p>\n<p>What is the impact of SIA? Well, it decreases the probabilities of worlds W<sub>3</sub> and W<sub>5</sub>, but leaves the relative probabilities of W<sub>1</sub>, W<sub>2</sub> and W<sub>4</sub> intact. So if we want to say that W<sub>1</sub> is more likely than the others - that we are indeed in the last stages of a simulation tree search - we can only do so because of a prior, not because of SIA.</p>\n<p>What kind of prior could make this work? Well, we notice that W<sub>2</sub> and W<sub>4</sub> have a higher total population that W<sub>1</sub>. This feels like it should penalise them. So a reasonable prior would be to consider total population size and type of world are independent variables. Then if we have a prior over population that diminishes as the population increases, we get our desired result: worlds like W<sub>2</sub> and W<sub>4</sub> are unlikely because of their large populations, and worlds like W<sub>3</sub> and W<sub>5</sub> are unlikely because of SIA.</p>\n<p>Is this enough? Not quite. Our priors need to do still more work before we're home and dry. Imagine we had a theory that superintelligences only simulated people close to the singularity, and used general statistical methods for the previous history. This is the kind of world that would score highly by SIA, while still having a low total population. So we better make them very unlikely a priori, lest they swamp the model. This is&nbsp;not an unreasonable assumption, but is a necessary one.</p>\n<p>So far, we've been using \"total population at our current era\" as a constant multiple of N, the size of the minimal reference class, and using the first as a proxy for the second. But the ratio between them can change in different worlds. We could imagine a superintelligence that only produced <em>human</em> civilizations. This would get a tremendous boost by SIA over other&nbsp;possibilities where it has to simulate a lot of useless aliens. Or we could go to the extreme, and imagine a superintelligence dedicated manically to simulating you, and only you, over and over again. Does that possibility feel unlikely to you? Good, because our extreme prior unlikelyhood is the only thing from preventing SIA blowing that scenario up until it dominates all other possibilities.</p>\n<p>&nbsp;</p>\n<h2>Conclusion</h2>\n<p>So anthropic reasoning is subtle, and&nbsp;especially&nbsp;when we are conditioning on facts relevant to our existence (such as observations in our world), we need to follow the mathematics rather than our intuitions - and count on our priors to do a lot of the heavy lifting. And avoid infinity.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hqET2rKbLeZFPD6uM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 18, "extendedScore": null, "score": 3.7e-05, "legacy": true, "legacyId": "19705", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>If you're going to use anthropic probability, use&nbsp;the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Self-Indication_Assumption\">self indication assumption</a>&nbsp;(SIA) - it's by far the most sensible way of doing things.</p>\n<p>Now, I&nbsp;am of the strong belief that probabilities in anthropic problems (such as the <a href=\"http://en.wikipedia.org/wiki/Sleeping_Beauty_problem\">Sleeping Beauty problem</a>) are not meaningful - only your decisions matter. And you can have different probability theories but still always reach the decisions if you have different theories as to who bears the responsibility of the actions of your copies, or how much you value them - see <a href=\"/lw/891/anthropic_decision_theory_i_sleeping_beauty_and/\">anthropic decision theory</a>&nbsp;(ADT).</p>\n<p>But that's a minority position - most people still use anthropic probabilities, so it's worth taking a more through look at what SIA does and doesn't tell you about population sizes and conditional probability.</p>\n<p>This post will aim to clarify some issues with SIA, especially concerning Jaan Tallinn's simulation-tree model which he <a href=\"http://fora.tv/2012/10/14/Jaan_Tallinn_Why_Now_A_Quest_in_Metaphysics\">presented</a> in&nbsp;exquisite&nbsp;story format at the recent <a href=\"http://singularitysummit.com/\">singularity summit</a>. I'll be assuming basic familiarity with SIA, and will run away screaming from any questions concerning infinity. SIA fears infinity (in a shameless self plug, I'll mention that anthropic decision theory runs into far less problems with infinities; for instance a bounded utility function is a sufficient - but not necessary - condition to ensure that ADT give you sensible answers even with infinitely many copies).</p>\n<p>But onwards and upwards with SIA! To not-quite-infinity and below!</p>\n<p>&nbsp;</p>\n<h2 id=\"SIA_does_not__directly__predict_large_populations\">SIA does not (directly) predict large populations</h2>\n<p>One error people often make with SIA is to assume that it predicts a large population. It doesn't - at least not directly. What SIA predicts is that there will be a large number of agents that are subjectively indistinguishable from you. You can call these subjectively indistinguishable agents the \"minimal&nbsp;reference&nbsp;class\" - it is a great advantage of SIA that it will continue to make sense for any reference class you choose (as long as it contains the minimal reference class).</p>\n<p>The SIA's impact on the total population is indirect: if the size of the total population is correlated with that of the minimal reference class, SIA will predict a large population. A correlation is not implausible: for instance, if there are a lot of humans around, then the probability that one of them is you is much larger. If there are a lot of intelligent life forms around, then the chance that humans exist is higher, and so on.</p>\n<p>In most cases, we don't run into problems with assuming that SIA predicts large populations. But we have to bear in mind that the effect is indirect, and the effect can and does break down in many cases. For instance imagine that&nbsp;you knew you had evolved on some planet, but for some odd reason, didn't know whether your planet had a ring system or not. You have managed to figure out that the evolution of life on planets with ring systems is independent of the evolution of life on planets without. Since you don't know which situation you're in, SIA instructs you to increase the probability of life on ringed and on non-ringed planets (so far, so good - SIA is predicting generally larger populations).</p>\n<p>And then one day you look up at the sky and see:<a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<p><img src=\"http://images.lesswrong.com/t3_f7d_0.png?v=f333746e9108010bc925811483fea7a8\" alt=\"\" width=\"480\" height=\"376\"></p>\n<p>&nbsp;</p>\n<p>After seeing this, SIA still increases your probability estimate for large populations on ringed planets - but your estimate for non-ringed planets reverts to the naive baseline. SIA can affect variables only as much as they are correlated with the size of the minimal reference class - as variables become more independent of this, SIA's effect becomes weaker.</p>\n<p>&nbsp;</p>\n<h2 id=\"Once_you_know_how_all_about_yourselves__SIA_is_powerless\">Once you know how all about yourselves, SIA is powerless</h2>\n<p>Designate by P the naive probability that you might have, before considering any anthropic factors. Let P<sub>SIA</sub> be the anthropic probability, adjusted by SIA. If N is a random variable that counts the size of the minimal reference class, then SIA's adjustment for any random variable X is:</p>\n<p style=\"padding-left: 60px;\"><strong>P<sub>SIA</sub>(N=n and X=x) </strong>=<strong> nP(N=n and X=x)/Norm</strong></p>\n<p>This is just rephrasing the fact that we weight the worlds by the size of the minimal reference class. The normalization term \"Norm\" is simply a constant that divides all P<sub>SIA</sub> probabilities to ensure they sum to 1 after doing the above adjustment (Norm is equal to the <a href=\"/r/discussion/lw/fg7/sia_fears_expected_infinity/\">expected value of N</a>). If X is independent of N (and hence P(X=x) doesn't have any anthropic correction from SIA, and equals P<sub>SIA</sub>(X=x)), then conditional probability follows the rule:</p>\n<p style=\"padding-left: 60px;\"><strong>P<sub>SIA</sub>(N=n|X=x)</strong> = P<sub>SIA</sub>(N=n and X=x)/P<sub>SIA</sub>(X=x) = <strong>n</strong>P(N=n and X=x)/(Norm*P(X=x)) = <strong>nP(N=n|X=x)/Norm</strong></p>\n<p>If X isn't independent of N, then P<sub>SIA</sub>(X=x) is not necessarily the same as P(X=x), and the above equality need not hold. In most cases there will still be an anthropic correction, hence:</p>\n<ul>\n<li>In general, the probability of anthropic variables conditioned upon other variables will change under SIA.</li>\n</ul>\n<p>The picture is very different, however, if we condition on anthropic variables! For instance if we condition on N itself, then&nbsp;P<sub>SIA</sub> is exactly the same as P, as there are two factors of n (and two Norms) cancelling out:</p>\n<p style=\"padding-left: 60px;\"><strong>P<sub>SIA</sub>(X=x|N=n)</strong> = P<sub>SIA</sub>(N=n and X=x)/P<sub>SIA</sub>(N=n) = <strong>n</strong>P(N=n and X=x)/<strong>n</strong>P(N=n) = <strong>P(X=x|N=n)</strong></p>\n<p>Hence:</p>\n<ul>\n<li>The probability of any variable conditioned on the size of the minimal reference class, is unaltered by SIA.</li>\n</ul>\n<p>So if we condition on the size of our reference class, there are no longer <em>any</em> anthropic effects.</p>\n<p>I myself was somewhat confused by this (see <a href=\"/lw/eel/sia_doomsday/\">here</a> and <a href=\"/lw/ef3/the_real_sia_doomsday/\">here</a>). But one implication is that,&nbsp;if you know the past human population, you expect the current human population to be above trend. Ditto if you know the future population: you then expect the current population to be higher than that information would&nbsp;naively&nbsp;imply (this gives you a poor-man's <a href=\"http://en.wikipedia.org/wiki/Doomsday_argument\">Doomsday argument</a>).&nbsp;But if, on the other hand, you know your current population, then you expect future and past populations to be given by naive probabilities, without anthropic adjustment.</p>\n<p>To illustrate this counterintuitive idea, take a toy model, with three populations at three steps (entitled \"past population\", \"present population\" and \"future population\"). Imagine that the past population was 3, and that the population either goes up by one, goes down by one, or stays the same, with equal (naive) probability. So we now have nine equally probable population series: (3,2,1), (3,3,3), (3,4,4), (3,4,5), etc...</p>\n<p>We live in the present, and we will adjust these series with SIA; thus we have three series (3,4,3), (3,4,4) and (3,4,5), each with probability 4/27, three series (3,3,4), (3,3,3) and (3,3,2), with probability 3/27, and three series (3,2,3), (3,2,2) and (3,2,1), with probability 2/27. Then if I give you the past and future population values, you expect the current population to be higher than the average: (3,?,3) is more likely to be (3,4,3), (3,?,2) is more likely to be (3,3,2), and so on. The only exceptions to this are&nbsp;(3,?,5) and (3,?,1), which each have only one possible value.</p>\n<p>However, if I give you the current population, then all possible future populations are equally likely: (3,4,?) is equally likely to end with 3, 4, or 5, as the sequences&nbsp;(3,4,3), (3,4,4) and (3,4,5) are equally likely. And that is exactly what naive probability would tell us. By conditioning on current population (indirectly conditioning on the size of the minimal reference class), we've erased any anthropic effects.</p>\n<p>&nbsp;</p>\n<h2 id=\"Jaan_Tallinn_s_tree\">Jaan Tallinn's tree</h2>\n<p>If you haven't already, go an watch Jaan Tallinn's&nbsp;<a href=\"http://fora.tv/2012/10/14/Jaan_Tallinn_Why_Now_A_Quest_in_Metaphysics\">presentation</a>. It's beautiful. And don't come back until you're done that.</p>\n<p>Right, back already? Jaan proposes a model in which superintelligences try and produce other superintelligences by evolving civilizations through a process of tree search. Essentially evolving one civilization up to superintelligence/singularity, rewinding the program a little bit, and then running it forwards again with small differences. This would cut down on calculations and avoid having to simulate each civilization's history separately, from birth to singularity.</p>\n<p>Then if we asked \"if we are simulated, where are we likely to exist in this tree process?\", then SIA gives you a clear answer: at the final stage, just before the singularity. Because that's where most of the simulated beings will exist.</p>\n<p>But the real question is \"if we are simulated, then given that we observe ourselves to be at a certain stage of human civilization, how likely is it that we are at the final stage?\" This is a&nbsp;subtly&nbsp;different question. And we've already seen that conditioning on variables tied to our existence - in this case, our observation of what stage of civilization we are at - can give very different answers from what we might expect.</p>\n<p>To see this, consider five different scenarios, or worlds. W<sub>1</sub> is the world described in Jaan Tallinn's talk: a branching process of simulations, with us the final stage before the singularity. In our model, it has eight civilizations at the same stage as us. Then we have world W<sub>2</sub>, which is the same as W<sub>1</sub>, except that there's still another stage remaining before the singularity: we add extra simulations at a date beyond us (which means that W<sub>2</sub> contains more simulations that W<sub>1</sub>). Then we have W<sub>3</sub>, which is structurally the same as W<sub>2</sub>, but has the same total number of simulations as W<sub>1</sub>. The following table shows these worlds, with a dotted rectangle around civilizations at our level of development. Under each world is the label, then a row showing the number of civilizations at our level, then a row showing the number of civilizations at every stage.</p>\n<p><img src=\"http://images.lesswrong.com/t3_f7d_6.png?v=a017c7e1eeea4329389e362de7f90a42\" alt=\"\" width=\"705\" height=\"494\"></p>\n<p>But maybe the tree model is wrong, and the real world is linear - either because the superintelligences want to simulate us thoroughly along one complete history, or maybe because most agents are in real histories and not simulations. We can the consider world W<sub>4</sub>, which is linear and has the same number of civilizations <em>at our stage</em> as W<sub>1</sub> does, and W<sub>5</sub>, which is linear and has the same <em>total number</em> of civilizations as W1 does:</p>\n<p><img src=\"http://images.lesswrong.com/t3_f7d_7.png?v=782e05c123bd22fbfebda21d7a27d998\" alt=\"\" width=\"700\" height=\"626\"></p>\n<p>What is the impact of SIA? Well, it decreases the probabilities of worlds W<sub>3</sub> and W<sub>5</sub>, but leaves the relative probabilities of W<sub>1</sub>, W<sub>2</sub> and W<sub>4</sub> intact. So if we want to say that W<sub>1</sub> is more likely than the others - that we are indeed in the last stages of a simulation tree search - we can only do so because of a prior, not because of SIA.</p>\n<p>What kind of prior could make this work? Well, we notice that W<sub>2</sub> and W<sub>4</sub> have a higher total population that W<sub>1</sub>. This feels like it should penalise them. So a reasonable prior would be to consider total population size and type of world are independent variables. Then if we have a prior over population that diminishes as the population increases, we get our desired result: worlds like W<sub>2</sub> and W<sub>4</sub> are unlikely because of their large populations, and worlds like W<sub>3</sub> and W<sub>5</sub> are unlikely because of SIA.</p>\n<p>Is this enough? Not quite. Our priors need to do still more work before we're home and dry. Imagine we had a theory that superintelligences only simulated people close to the singularity, and used general statistical methods for the previous history. This is the kind of world that would score highly by SIA, while still having a low total population. So we better make them very unlikely a priori, lest they swamp the model. This is&nbsp;not an unreasonable assumption, but is a necessary one.</p>\n<p>So far, we've been using \"total population at our current era\" as a constant multiple of N, the size of the minimal reference class, and using the first as a proxy for the second. But the ratio between them can change in different worlds. We could imagine a superintelligence that only produced <em>human</em> civilizations. This would get a tremendous boost by SIA over other&nbsp;possibilities where it has to simulate a lot of useless aliens. Or we could go to the extreme, and imagine a superintelligence dedicated manically to simulating you, and only you, over and over again. Does that possibility feel unlikely to you? Good, because our extreme prior unlikelyhood is the only thing from preventing SIA blowing that scenario up until it dominates all other possibilities.</p>\n<p>&nbsp;</p>\n<h2 id=\"Conclusion\">Conclusion</h2>\n<p>So anthropic reasoning is subtle, and&nbsp;especially&nbsp;when we are conditioning on facts relevant to our existence (such as observations in our world), we need to follow the mathematics rather than our intuitions - and count on our priors to do a lot of the heavy lifting. And avoid infinity.</p>", "sections": [{"title": "SIA does not (directly) predict large populations", "anchor": "SIA_does_not__directly__predict_large_populations", "level": 1}, {"title": "Once you know how all about yourselves, SIA is powerless", "anchor": "Once_you_know_how_all_about_yourselves__SIA_is_powerless", "level": 1}, {"title": "Jaan Tallinn's tree", "anchor": "Jaan_Tallinn_s_tree", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["svhbnSdxW3XmFXXTK", "99isjR8W4JGZdro7x", "dpajrqGjYrExQq8MN", "DNhFQJb5gkWoZrBBD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-12T17:33:47.929Z", "modifiedAt": null, "url": null, "title": "Gap in understanding of Logical Pinpointing", "slug": "gap-in-understanding-of-logical-pinpointing", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:38.499Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Incorrect", "createdAt": "2011-05-19T06:36:41.700Z", "isAdmin": false, "displayName": "Incorrect"}, "userId": "YKd5C3yr2o6NtJgtP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rnsG5qRecfo38eKZ8/gap-in-understanding-of-logical-pinpointing", "pageUrlRelative": "/posts/rnsG5qRecfo38eKZ8/gap-in-understanding-of-logical-pinpointing", "linkUrl": "https://www.lesswrong.com/posts/rnsG5qRecfo38eKZ8/gap-in-understanding-of-logical-pinpointing", "postedAtFormatted": "Monday, November 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Gap%20in%20understanding%20of%20Logical%20Pinpointing&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGap%20in%20understanding%20of%20Logical%20Pinpointing%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrnsG5qRecfo38eKZ8%2Fgap-in-understanding-of-logical-pinpointing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Gap%20in%20understanding%20of%20Logical%20Pinpointing%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrnsG5qRecfo38eKZ8%2Fgap-in-understanding-of-logical-pinpointing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrnsG5qRecfo38eKZ8%2Fgap-in-understanding-of-logical-pinpointing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 172, "htmlBody": "<p>I had originally found myself very confused by how the second order axiom of induction restricted PA to a single model, leading to <a href=\"/lw/f4e/logical_pinpointing/7qv6\">this discussion</a> where I thought doing so would violate the incompleteness theorem.</p>\n<p>What I misunderstood is that while the axiom schema of induction effectively quantifies over properties definable in PA, the SOL version quantifies over ALL properties, including those you can't even define in PA.</p>\n<p>This seems like a really subtle point that wasn't obvious to me from the article, knowing FOL but not SOL. I actually realized my mistake when I saw that Eliezer was representing properties as sets in the visual diagram.</p>\n<p>Anyway, I thought others might benefit by learning from my mistake. Also, I'd like to point out that this definition of the natural numbers yields no procedure that lets you produce theorems of PA as second-order logic has no computable complete definition of provability. Just use the axiom schema of induction instead of the second order axiom of induction and you will be able to produce theorems though.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rnsG5qRecfo38eKZ8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 11, "extendedScore": null, "score": 1.0320178042877668e-06, "legacy": true, "legacyId": "20024", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-12T21:51:53.899Z", "modifiedAt": null, "url": null, "title": "A hypothesis concerning discounting.", "slug": "a-hypothesis-concerning-discounting", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:39.608Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "abramdemski", "createdAt": "2009-03-12T06:07:25.510Z", "isAdmin": false, "displayName": "abramdemski"}, "userId": "Q7NW4XaWQmfPfdcFj", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iCLW3eg7j9kvzo75b/a-hypothesis-concerning-discounting", "pageUrlRelative": "/posts/iCLW3eg7j9kvzo75b/a-hypothesis-concerning-discounting", "linkUrl": "https://www.lesswrong.com/posts/iCLW3eg7j9kvzo75b/a-hypothesis-concerning-discounting", "postedAtFormatted": "Monday, November 12th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20hypothesis%20concerning%20discounting.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20hypothesis%20concerning%20discounting.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiCLW3eg7j9kvzo75b%2Fa-hypothesis-concerning-discounting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20hypothesis%20concerning%20discounting.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiCLW3eg7j9kvzo75b%2Fa-hypothesis-concerning-discounting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiCLW3eg7j9kvzo75b%2Fa-hypothesis-concerning-discounting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 408, "htmlBody": "<p>Humans have a value function which is <a href=\"/lw/6c/akrasia_hyperbolic_discounting_and_picoeconomics/\">inconsistent over time</a>, discounting roughly with proportion to distance in the future, so that we discount more steeply as an event approaches. This is why we stay up late, ignore the alarm, put off work until close to a deadline, et cetera et cetera.</p>\n<p>Yet hyperbolic discounting <a href=\"/lw/8om/does_hyperbolic_discounting_really_exist/\">appears to go away as we mature</a>. I believe this is a result of cognitive mechanisms for maintaining consistency. Cognitive dissonance is painful for us. The consistency mechanism seems to explain some of our irrational behaviour, such as the sunk cost fallacy. It provides a way for us to stick with plans which we previously made, avoiding preference changes due to hyperbolic discounting.</p>\n<p>If a hyperbolically discounting agent could perfectly self-modify, it would fix its hyperbola to a specific point in time, resulting in an agent whose discounting would flatten out over the remainder of its life. Perhaps our consistency mechanism approximates this result; but far from perfectly. We can also resolve the inconsistency in a different way, by accepting a specific discount rate. Rather than forcing our future selves to conform to our present preferences, resulting in a gradually flattening function, our present selves may instead accept our future preferences in order to resolve the inconsistency.</p>\n<p>Given the difficulty of forcing our future selves to accept a flat distribution, we accept that we will steeply discount in the future as we do in the present. This resolution is popular in some circles; we are often told to \"live in the present\" or \"seize the day\". In the extreme case, there is the belief (often associated with mystics) that the present moment is infinitely more important than anything else; the discount factor has collapsed to 0. While this view is intellectually coherent, it seems to be biologically impossible; we will keep taking actions based on future consequences even if we think we are only doing what we desire in the moment. Nonetheless, I suppose we can approach very high discount factors.</p>\n<p>Based on this model, my suspicion is that we can approach any discount factor as a self-consistent equilibrium-- it is possible that we learn to make and keep very long-term plans, approximating a very low discount, but it is also possible that we learn to live in the present, or learn anything between these two. The consistency mechanism will want to find a fixed point, but which fixed-point we reach will depend on factors outside these mechanisms.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iCLW3eg7j9kvzo75b", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.0321611760857284e-06, "legacy": true, "legacyId": "20025", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["geNZ6ZpfFce5intER", "MDSt7vSqFPZRzk26B"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-13T01:17:35.115Z", "modifiedAt": null, "url": null, "title": "Rationalist subreddit", "slug": "rationalist-subreddit", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:37.964Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "nerfhammer", "createdAt": "2009-07-21T19:45:50.831Z", "isAdmin": false, "displayName": "nerfhammer"}, "userId": "TnRcc3ezfxzQs7Phn", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XekWb253YS2arn5Tt/rationalist-subreddit", "pageUrlRelative": "/posts/XekWb253YS2arn5Tt/rationalist-subreddit", "linkUrl": "https://www.lesswrong.com/posts/XekWb253YS2arn5Tt/rationalist-subreddit", "postedAtFormatted": "Tuesday, November 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationalist%20subreddit&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationalist%20subreddit%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXekWb253YS2arn5Tt%2Frationalist-subreddit%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationalist%20subreddit%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXekWb253YS2arn5Tt%2Frationalist-subreddit", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXekWb253YS2arn5Tt%2Frationalist-subreddit", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 32, "htmlBody": "<p>I'm experimenting with a new rationalist subreddit on reddit.com</p>\n<p><a href=\"http://www.reddit.com/r/rationalisthmus\">/r/rationalisthmus</a></p>\n<p>Starting with links populated from the <a href=\"http://planetrationalist.com\">planet rationalist</a> data set - to which I've added about 50 new sources in the past few weeks.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XekWb253YS2arn5Tt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 4, "extendedScore": null, "score": 1.0322754584519602e-06, "legacy": true, "legacyId": "20026", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-13T02:13:12.719Z", "modifiedAt": null, "url": null, "title": "Coursera course on critical thinking", "slug": "coursera-course-on-critical-thinking", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:00.275Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LmgHhzR6m6N3r5YiX/coursera-course-on-critical-thinking", "pageUrlRelative": "/posts/LmgHhzR6m6N3r5YiX/coursera-course-on-critical-thinking", "linkUrl": "https://www.lesswrong.com/posts/LmgHhzR6m6N3r5YiX/coursera-course-on-critical-thinking", "postedAtFormatted": "Tuesday, November 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Coursera%20course%20on%20critical%20thinking&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACoursera%20course%20on%20critical%20thinking%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLmgHhzR6m6N3r5YiX%2Fcoursera-course-on-critical-thinking%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Coursera%20course%20on%20critical%20thinking%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLmgHhzR6m6N3r5YiX%2Fcoursera-course-on-critical-thinking", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLmgHhzR6m6N3r5YiX%2Fcoursera-course-on-critical-thinking", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 419, "htmlBody": "<p>Coursera has an upcoming course on critical thinking called \"Think Again: How to Reason and Argue\". A bunch of the locals here in Columbus will be taking it, and I thought some other LWers might also be interested. If you decide to sign up, let us know, and we might get a discussion group going, or something.</p>\n<p>Linky:&nbsp;https://www.coursera.org/course/thinkagain&nbsp;</p>\n<p>Some info from the website:</p>\n<blockquote>\n<h2><strong>About the Course:</strong>&nbsp;</h2>\n<p>Reasoning is important. &nbsp;This course will teach you how to do it well. &nbsp;You will learn some simple but vital rules to follow in thinking about any topic at all and some common and tempting mistakes to avoid in reasoning. &nbsp;We will discuss how to identify, analyze, and evaluate arguments by other people (including politicians, used car salesmen, and teachers) and how to construct arguments of your own in order to help you decide what to believe or what to do. These skills will be useful in dealing with whatever matters most to you.</p>\n<h2><strong>Course Syllabus</strong></h2>\n<p><strong>Week One:</strong>&nbsp;How to Spot an Argument (and separate it from surrounding verbiage)</p>\n<p><strong>Week Two:</strong>&nbsp;How to Untangle an Argument (or break it into parts and tell what different parts are doing)</p>\n<p><strong>Week Three:</strong>&nbsp;How to Reconstruct an Argument (or arrange its parts to show how they are connected in a structure)&nbsp;</p>\n<p><strong>Week Four:</strong>&nbsp;How to Evaluate an Argument Deductively (or determine whether its conclusion follows validly from its premises) &ndash; Part 1: Propositional Logic</p>\n<p><strong>Week Five: &nbsp;</strong>How to Evaluate an Argument Deductively (or determine whether its conclusion follows validly from its premises) &ndash; Part 2: Quantificational Logic</p>\n<p><strong>Week Six:</strong>&nbsp;How to Evaluate an Argument Inductively (or assess whether its premises provide enough reason to believe its conclusion) &ndash; Part 1: Statistical Generalization and Application</p>\n<p><strong>Week Seven:</strong>&nbsp;How to Evaluate an Argument Inductively (or assess whether its premises provide enough reason to believe its conclusion) &ndash; Part 2: Causal Reasoning</p>\n<p><strong>Week Eight:</strong>&nbsp;&nbsp;How to Evaluate an Argument Inductively (or assess whether its premises provide enough reason to believe its conclusion) &ndash; Part 3: Probability&nbsp;</p>\n<p><strong>Week Nine:</strong>&nbsp;&nbsp;How to Evaluate an Argument Inductively (or assess whether its premises provide enough reason to believe its conclusion) &ndash; Part 4: Decisions</p>\n<p><strong>Week Ten:</strong>&nbsp;How to Mess Up an Argument (or commit common but tempting fallacies) &ndash; Part 1: Vagueness and Ambiguity</p>\n<p><strong>Week Eleven:</strong>&nbsp;How to Mess Up an Argument (or commit common but tempting fallacies) &ndash; Part 2: Irrelevance</p>\n<p><strong>Week Twelve:</strong>&nbsp;How to Mess Up an Argument (or commit common but tempting fallacies) &ndash; Part 3: Vacuity</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LmgHhzR6m6N3r5YiX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 9, "extendedScore": null, "score": 3.1e-05, "legacy": true, "legacyId": "20033", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Coursera has an upcoming course on critical thinking called \"Think Again: How to Reason and Argue\". A bunch of the locals here in Columbus will be taking it, and I thought some other LWers might also be interested. If you decide to sign up, let us know, and we might get a discussion group going, or something.</p>\n<p>Linky:&nbsp;https://www.coursera.org/course/thinkagain&nbsp;</p>\n<p>Some info from the website:</p>\n<blockquote>\n<h2 id=\"About_the_Course__\"><strong>About the Course:</strong>&nbsp;</h2>\n<p>Reasoning is important. &nbsp;This course will teach you how to do it well. &nbsp;You will learn some simple but vital rules to follow in thinking about any topic at all and some common and tempting mistakes to avoid in reasoning. &nbsp;We will discuss how to identify, analyze, and evaluate arguments by other people (including politicians, used car salesmen, and teachers) and how to construct arguments of your own in order to help you decide what to believe or what to do. These skills will be useful in dealing with whatever matters most to you.</p>\n<h2 id=\"Course_Syllabus\"><strong>Course Syllabus</strong></h2>\n<p><strong>Week One:</strong>&nbsp;How to Spot an Argument (and separate it from surrounding verbiage)</p>\n<p><strong>Week Two:</strong>&nbsp;How to Untangle an Argument (or break it into parts and tell what different parts are doing)</p>\n<p><strong>Week Three:</strong>&nbsp;How to Reconstruct an Argument (or arrange its parts to show how they are connected in a structure)&nbsp;</p>\n<p><strong>Week Four:</strong>&nbsp;How to Evaluate an Argument Deductively (or determine whether its conclusion follows validly from its premises) \u2013 Part 1: Propositional Logic</p>\n<p><strong>Week Five: &nbsp;</strong>How to Evaluate an Argument Deductively (or determine whether its conclusion follows validly from its premises) \u2013 Part 2: Quantificational Logic</p>\n<p><strong>Week Six:</strong>&nbsp;How to Evaluate an Argument Inductively (or assess whether its premises provide enough reason to believe its conclusion) \u2013 Part 1: Statistical Generalization and Application</p>\n<p><strong>Week Seven:</strong>&nbsp;How to Evaluate an Argument Inductively (or assess whether its premises provide enough reason to believe its conclusion) \u2013 Part 2: Causal Reasoning</p>\n<p><strong>Week Eight:</strong>&nbsp;&nbsp;How to Evaluate an Argument Inductively (or assess whether its premises provide enough reason to believe its conclusion) \u2013 Part 3: Probability&nbsp;</p>\n<p><strong>Week Nine:</strong>&nbsp;&nbsp;How to Evaluate an Argument Inductively (or assess whether its premises provide enough reason to believe its conclusion) \u2013 Part 4: Decisions</p>\n<p><strong>Week Ten:</strong>&nbsp;How to Mess Up an Argument (or commit common but tempting fallacies) \u2013 Part 1: Vagueness and Ambiguity</p>\n<p><strong>Week Eleven:</strong>&nbsp;How to Mess Up an Argument (or commit common but tempting fallacies) \u2013 Part 2: Irrelevance</p>\n<p><strong>Week Twelve:</strong>&nbsp;How to Mess Up an Argument (or commit common but tempting fallacies) \u2013 Part 3: Vacuity</p>\n</blockquote>", "sections": [{"title": "About the Course:\u00a0", "anchor": "About_the_Course__", "level": 1}, {"title": "Course Syllabus", "anchor": "Course_Syllabus", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-13T03:45:57.989Z", "modifiedAt": null, "url": null, "title": "Simulationist sympathetic magic", "slug": "simulationist-sympathetic-magic", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:38.971Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mqGsDivpMrjey5k5b/simulationist-sympathetic-magic", "pageUrlRelative": "/posts/mqGsDivpMrjey5k5b/simulationist-sympathetic-magic", "linkUrl": "https://www.lesswrong.com/posts/mqGsDivpMrjey5k5b/simulationist-sympathetic-magic", "postedAtFormatted": "Tuesday, November 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Simulationist%20sympathetic%20magic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASimulationist%20sympathetic%20magic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmqGsDivpMrjey5k5b%2Fsimulationist-sympathetic-magic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Simulationist%20sympathetic%20magic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmqGsDivpMrjey5k5b%2Fsimulationist-sympathetic-magic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmqGsDivpMrjey5k5b%2Fsimulationist-sympathetic-magic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 338, "htmlBody": "<p>While I've been wrestling with the inspiration needed to turn my fanfiction into actual fiction rather than just an author's tract... I have had an unrelated but fun thought, which I'm throwing into the breeze here for any improvements that can be suggested.</p>\n<p>&nbsp;</p>\n<p>If our universe is more likely to be a simulation, a reconstruction of the past by our descendants than the base level of reality; then that reconstruction is likely to be imperfect, based mainly on surviving records (and memories of anyone whose brain survives intact long enough for upload-style scanning).</p>\n<p>Therefore, if somebody precommits to only leave behind records which correspond to particular events... then it seems plausible (to within the bounds of 'the brain is a quantum computer' levels of plausibility) that those events become more likely to be experienced. For example, if a protagonist were to precommit to mentioning that during their walk, a bird landed right in their hand to eat a bread crust, whether or not such an event actually happened; then the probability that they will then experience a bird landing in their hand increases.<a id=\"more\"></a></p>\n<p>There are, of course, extreme limits on what can be accomplished with such trickery, even in theory. Violating the known laws of physics is right out, as are events dramatic enough to leave behind more traces than their own memories and journals. It also seems highly recommended for someone who wishes to try this to precommit to never leaving a trace that they <em>are</em> going to try it, as that would leave a record the future simulationists could use to discount their less-probable reports.</p>\n<p>&nbsp;</p>\n<p>So - what additional thoughts could be added to the above to make it more plausible, at least to those who've heard of simulationism in the first place?</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>(And since it seems more likely than not that someone will ask: No I've never tried using such simulationist sympathetic magic myself, and since I still question the basic assumptions behind such mass-simulation in the first place, I have no intention of trying it in the future, either.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mqGsDivpMrjey5k5b", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 21, "extendedScore": null, "score": 5.5e-05, "legacy": true, "legacyId": "20037", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-13T05:10:38.251Z", "modifiedAt": null, "url": null, "title": "Short introductory materials for a rationality meetup", "slug": "short-introductory-materials-for-a-rationality-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:40.063Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dolores1984", "createdAt": "2012-04-27T01:13:58.517Z", "isAdmin": false, "displayName": "Dolores1984"}, "userId": "4atqsmycH3WC4Cf5u", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yruyv49ArgE7nacj2/short-introductory-materials-for-a-rationality-meetup", "pageUrlRelative": "/posts/yruyv49ArgE7nacj2/short-introductory-materials-for-a-rationality-meetup", "linkUrl": "https://www.lesswrong.com/posts/yruyv49ArgE7nacj2/short-introductory-materials-for-a-rationality-meetup", "postedAtFormatted": "Tuesday, November 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Short%20introductory%20materials%20for%20a%20rationality%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShort%20introductory%20materials%20for%20a%20rationality%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyruyv49ArgE7nacj2%2Fshort-introductory-materials-for-a-rationality-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Short%20introductory%20materials%20for%20a%20rationality%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyruyv49ArgE7nacj2%2Fshort-introductory-materials-for-a-rationality-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fyruyv49ArgE7nacj2%2Fshort-introductory-materials-for-a-rationality-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 148, "htmlBody": "<p>So, I and a few other people are starting a Bayesian Conspiracy chapter at my university (New Mexico Tech). &nbsp;We're trying to put together a short (three page) introductory packet to give to new members. &nbsp;We'd like the packet to introduce people to what rationality is, what it's useful for, and some of the basic techniques. &nbsp;We'd like it to be as readable and palatable as possible, to avoid the intimidation factor of simply pointing people at the Sequences, which are not particularly friendly to a casual reader. &nbsp;<br /><br />I'm compiling some materials of my own for this purpose, but before I get too excited, I thought I ought to check if any of the other meetups had or knew of something along these lines already created. &nbsp;If not, we'll post our packet on our website for other meetups to use as they see fit. &nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yruyv49ArgE7nacj2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 1.0324049721269053e-06, "legacy": true, "legacyId": "20039", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-13T05:44:40.330Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Emulations Go Foom", "slug": "seq-rerun-emulations-go-foom", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:38.154Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/paZDEWb4PSS2ftGHP/seq-rerun-emulations-go-foom", "pageUrlRelative": "/posts/paZDEWb4PSS2ftGHP/seq-rerun-emulations-go-foom", "linkUrl": "https://www.lesswrong.com/posts/paZDEWb4PSS2ftGHP/seq-rerun-emulations-go-foom", "postedAtFormatted": "Tuesday, November 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Emulations%20Go%20Foom&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Emulations%20Go%20Foom%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpaZDEWb4PSS2ftGHP%2Fseq-rerun-emulations-go-foom%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Emulations%20Go%20Foom%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpaZDEWb4PSS2ftGHP%2Fseq-rerun-emulations-go-foom", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpaZDEWb4PSS2ftGHP%2Fseq-rerun-emulations-go-foom", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 153, "htmlBody": "<p>Today's post, <a href=\"http://www.overcomingbias.com/2008/11/emulations-go-f.html\">Emulations Go Foom</a> was originally published on November 22, 2008.  A summary:</p>\n<p>&nbsp;</p>\n<blockquote>A description of what Robin Hanson thinks is the most likely scenario for a intelligence takeoff.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/ffv/seq_rerun_lifes_story_continues/\">Life's Story Continues</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jQytxyauJ7kPhhGj3": 2, "5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "paZDEWb4PSS2ftGHP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0324238884827898e-06, "legacy": true, "legacyId": "20044", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rLF95zQieKnGrQMNK", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-13T10:30:15.543Z", "modifiedAt": null, "url": null, "title": "CFAR and SI MOOCs: a Great Opportunity", "slug": "cfar-and-si-moocs-a-great-opportunity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:56.163Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wrongnesslessness", "createdAt": "2012-01-11T03:37:34.797Z", "isAdmin": false, "displayName": "Wrongnesslessness"}, "userId": "Ro3FnMjhZKfnfkCgZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8zMFSTW8P63vfEpGB/cfar-and-si-moocs-a-great-opportunity", "pageUrlRelative": "/posts/8zMFSTW8P63vfEpGB/cfar-and-si-moocs-a-great-opportunity", "linkUrl": "https://www.lesswrong.com/posts/8zMFSTW8P63vfEpGB/cfar-and-si-moocs-a-great-opportunity", "postedAtFormatted": "Tuesday, November 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20CFAR%20and%20SI%20MOOCs%3A%20a%20Great%20Opportunity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACFAR%20and%20SI%20MOOCs%3A%20a%20Great%20Opportunity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8zMFSTW8P63vfEpGB%2Fcfar-and-si-moocs-a-great-opportunity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=CFAR%20and%20SI%20MOOCs%3A%20a%20Great%20Opportunity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8zMFSTW8P63vfEpGB%2Fcfar-and-si-moocs-a-great-opportunity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8zMFSTW8P63vfEpGB%2Fcfar-and-si-moocs-a-great-opportunity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 251, "htmlBody": "<p><a title=\"http://en.wikipedia.org/wiki/Massive_open_online_course\" href=\"http://en.wikipedia.org/wiki/Massive_open_online_course\" target=\"_blank\">Massive open online</a> <a title=\"http://www.class-central.com/\" href=\"http://www.class-central.com/\" target=\"_blank\">courses</a> seem to be <a title=\"http://www.blog.class-central.com/growth-of-moocs/\" href=\"http://www.blog.class-central.com/growth-of-moocs/\" target=\"_blank\">marching towards total world domination</a>&nbsp;like some kind of educational singularity (at least in the case of <a title=\"https://www.coursera.org/\" href=\"https://www.coursera.org/\" target=\"_blank\">Coursera</a>). At the same time, there are still relatively few courses available, and each new added course is a small happening in the growing MOOC community.</p>\n<p>Needless to say, this seems like a perfect opportunity for SI and CFAR to advance their goals via this new education medium. Some people seem to have already seen the potential and taken advantage of it:</p>\n<blockquote>\n<p>One interesting trend that can be seen is companies offering MOOCs to increase the adoption of their tools/technologies. We have seem this with 10gen offering&nbsp;<a style=\"color: #000000; text-decoration: initial; padding-bottom: 0px; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: #cccccc; font-family: futura-pt, 'Helvetica Neue'; font-size: 16px; line-height: 34px;\" title=\"10gen Mongo courses\" href=\"http://education.10gen.com/\" target=\"_blank\">Mongo courses</a>&nbsp;and to a lesser extent with Coursera&rsquo;s&nbsp;<a style=\"color: #000000; text-decoration: initial; padding-bottom: 0px; border-bottom-width: 1px; border-bottom-style: solid; border-bottom-color: #cccccc; font-family: futura-pt, 'Helvetica Neue'; font-size: 16px; line-height: 34px;\" title=\"Functional Programming in Scala\" href=\"https://www.coursera.org/course/progfun\" target=\"_blank\">&lsquo;Functional Programming in Scala&rsquo;</a>&nbsp;taught by Martin Odersky</p>\n</blockquote>\n<p>(from the above link to the&nbsp;<a title=\"http://www.blog.class-central.com/growth-of-moocs/\" href=\"http://www.blog.class-central.com/growth-of-moocs/\" target=\"_blank\">Class Central Blog</a>)</p>\n<p>&nbsp;</p>\n<p>So the question is, are there any online courses already planned by CFAR and/or SI? And if not, when will it happen?</p>\n<p>&nbsp;</p>\n<p><strong>Edit:</strong> This is not a \"yes or no\" question,&nbsp;albeit formulated as one. I've searched the archives and did not find any mention of MOOCs as a potentially crucial device for spreading our views. If any such courses are already being developed or at least planned, I'll be happy to move this post to the open thread, as some have requested, or delete it entirely. If not, please view this as a request for discussion and brainstorming.</p>\n<p>P.S.: Sorry, I don't have the time to write a good article on this topic.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"X7v7Fyp9cgBYaMe2e": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8zMFSTW8P63vfEpGB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 19, "extendedScore": null, "score": 1.032582641719716e-06, "legacy": true, "legacyId": "20050", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-13T18:39:42.946Z", "modifiedAt": null, "url": null, "title": "Group rationality diary, 11/13/12", "slug": "group-rationality-diary-11-13-12", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:58.623Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cata", "createdAt": "2010-06-02T18:13:22.408Z", "isAdmin": false, "displayName": "cata"}, "userId": "X9jdpCokhLjCMZEc3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KfCvFDDGiZAtTboBa/group-rationality-diary-11-13-12", "pageUrlRelative": "/posts/KfCvFDDGiZAtTboBa/group-rationality-diary-11-13-12", "linkUrl": "https://www.lesswrong.com/posts/KfCvFDDGiZAtTboBa/group-rationality-diary-11-13-12", "postedAtFormatted": "Tuesday, November 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20rationality%20diary%2C%2011%2F13%2F12&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20rationality%20diary%2C%2011%2F13%2F12%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKfCvFDDGiZAtTboBa%2Fgroup-rationality-diary-11-13-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20rationality%20diary%2C%2011%2F13%2F12%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKfCvFDDGiZAtTboBa%2Fgroup-rationality-diary-11-13-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKfCvFDDGiZAtTboBa%2Fgroup-rationality-diary-11-13-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 193, "htmlBody": "<p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">T<span style=\"color: #333333;\">his is the public group instrumental rationality diary for the week of October 29th. It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<div id=\"entry_t3_drj\" class=\"content clear\" style=\"font-family: Arial, Helvetica, sans-serif; text-align: justify; font-size: 12px; line-height: 18px;\">\n<div class=\"md\">\n<ul style=\"padding: 0px; line-height: 19px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\">Thanks to everyone who contributes!</p>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\"><a style=\"color: #8a8a8b;\" href=\"/lw/f7e/group_rationality_diary_102912/\">Previous diary</a>;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">archive of prior diaries</a>.</p>\n</div>\n</div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KfCvFDDGiZAtTboBa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.032854823600176e-06, "legacy": true, "legacyId": "20052", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xeYzNEvNRagBQPQBY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-13T21:56:12.609Z", "modifiedAt": null, "url": null, "title": "How can I reduce existential risk from AI?", "slug": "how-can-i-reduce-existential-risk-from-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:08.332Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qARBe3jBodrdPeRE6/how-can-i-reduce-existential-risk-from-ai", "pageUrlRelative": "/posts/qARBe3jBodrdPeRE6/how-can-i-reduce-existential-risk-from-ai", "linkUrl": "https://www.lesswrong.com/posts/qARBe3jBodrdPeRE6/how-can-i-reduce-existential-risk-from-ai", "postedAtFormatted": "Tuesday, November 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20can%20I%20reduce%20existential%20risk%20from%20AI%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20can%20I%20reduce%20existential%20risk%20from%20AI%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqARBe3jBodrdPeRE6%2Fhow-can-i-reduce-existential-risk-from-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20can%20I%20reduce%20existential%20risk%20from%20AI%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqARBe3jBodrdPeRE6%2Fhow-can-i-reduce-existential-risk-from-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqARBe3jBodrdPeRE6%2Fhow-can-i-reduce-existential-risk-from-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2265, "htmlBody": "<p>Suppose you think that <a href=\"http://www.existential-risk.org/concept.pdf\">reducing the risk of human extinction</a> is the highest-value thing you can do. Or maybe you want to reduce \"x-risk\" because you're already a <a href=\"http://knowyourmeme.com/memes/first-world-problems\">comfortable First-Worlder</a> like me and so you might as well do something <a href=\"http://www.youtube.com/watch?v=YDsLKEado_o\">epic and cool</a>, or because you like the <a href=\"http://www.fhi.ox.ac.uk/\">community</a> <a href=\"/\">of</a> <a href=\"http://singularity.org\">people</a> who are doing it already, or whatever.</p>\n<p>Suppose also that you think <a href=\"/lw/di4/reply_to_holden_on_the_singularity_institute/#ai-risk\">AI is the most pressing x-risk</a>, because (1) mitigating AI risk could mitigate all other existential risks, but not vice-versa, and because (2) AI is plausibly the first existential risk that will occur.</p>\n<p>In that case, what should you do? How can you reduce AI x-risk?</p>\n<p>It's complicated, but I get this question a lot, so let me try to provide <em>some</em> kind of answer.</p>\n<p>&nbsp;</p>\n<h2>Meta-work, strategy work, and direct work</h2>\n<p>When you're facing a problem and you don't know what to do about it, there are two things you can do:</p>\n<p><strong>1</strong>. <em>Meta-work</em>: Amass wealth and other resources. Build your community. Make yourself stronger. Meta-work of this sort will be useful regardless of which \"direct work\" interventions turn out to be useful for tackling the problem you face. Meta-work also empowers you to do strategic work.</p>\n<p><strong>2</strong>. <em>Strategy work</em>: Purchase a better strategic understanding of the problem you're facing, so you can see more clearly what should be done. Usually, this will consist of getting smart and self-critical people to honestly assess the strategic situation, build models, make predictions about the effects of different possible interventions, and so on. If done well, these analyses can shed light on which kinds of \"direct work\" will help you deal with the problem you're trying to solve.</p>\n<p>When you have enough strategic insight to have discovered <em>some</em> interventions that you're confident will help you tackle the problem you're facing, then you can also engage in:</p>\n<p><strong>3</strong>. <em>Direct work</em>: Directly attack the problem you're facing, whether this involves technical research, political action, particular kinds of technological development, or something else.</p>\n<p>Thinking with these categories can be useful even though the lines between them are fuzzy. For example, you might have to do some basic awareness-raising in order to amass funds for your cause, and then once you've spent those funds on strategy work, your strategy work might tell you that a specific form of awareness-raising is useful for political action that counts as \"direct work.\" Also, some forms of strategy work can feel like direct work, depending on the type of problem you're tackling.</p>\n<p><a id=\"more\"></a></p>\n<h2><br /></h2>\n<h2>Meta-work for AI x-risk reduction</h2>\n<p><a href=\"http://www.amazon.com/The-Personal-MBA-Master-Business/dp/1591845572/\">Make money</a>. <a href=\"http://wiki.lesswrong.com/wiki/The_Science_of_Winning_at_Life\">Become stronger</a>. <a href=\"http://www.amazon.com/Never-Eat-Alone-Secrets-Relationship/dp/0385512058/\">Build a community</a>, <a href=\"http://probloggerbook.com/\">an audience</a>, a <a href=\"http://www.amazon.com/Change-Heart-Psychology-Spreading-Social/dp/159056233X/\">movement</a>. Store your accumulated resources in yourself, in your community, in a <a href=\"http://en.wikipedia.org/wiki/Donor_advised_fund\">donor-advised fund</a>, or in an organization that can advance your causes better than you can as an individual.</p>\n<ol>\n<li>\n<p><strong>Make money</strong>: In the past 10 years, many people have chosen to start businesses or careers that (1) will predictably generate significant wealth they can spend on AI x-risk reduction, (2) will be enjoyable enough to \"stick with it,\" and (3) will not create large negative externalities. But certainly, the AI x-risk reduction community needs a lot <em>more</em> people to do this! If you want advice, the folks at <a href=\"http://80000hours.org/\">80,000 Hours</a> are the experts on \"ethical careers\" of this sort.</p>\n</li>\n<li>\n<p><strong>Become stronger</strong>: Sometimes it makes sense to focus on <em>improving</em> your <a href=\"/lw/3w3/how_to_beat_procrastination/\">productivity</a>, your <a href=\"/lw/5me/scholarship_how_to_do_it_efficiently/\">research skills</a>, your <a href=\"/lw/86a/rhetoric_for_the_good/\">writing skills</a>, your <a href=\"/lw/5p6/how_and_why_to_granularize/\">social skills</a>, etc. before you begin <em>using</em> those skills to achieve your goals. Example: <a href=\"/user/Vladimir_Nesov/\">Vladimir Nesov</a> has done <a href=\"/lw/3l/counterfactual_mugging/\">some original research</a>, but mostly he has spent the last few years improving his math skills before diving into original research full-time.</p>\n</li>\n<li>\n<p><strong>Build a community / a movement</strong>: Individuals <em>can</em> change the world, but communities and movements can do even better, if they're well-coordinated. Read <em><a href=\"http://www.amazon.com/Change-Heart-Psychology-Spreading-Social/dp/159056233X/\">What Psychology Can Teach Us About Spreading Social Change</a></em>. <a href=\"/lw/crs/how_to_run_a_successful_less_wrong_meetup/\">Launch (or improve) a Less Wrong group</a>. Join a <a href=\"http://www.thehighimpactnetwork.org/\">THINK group</a>. Help grow and improve the existing online communities that tend to have high rates of interest in x-risk reduction: <a href=\"/\">LessWrong</a>, <a href=\"http://singularityvolunteers.org/\">Singularity Volunteers</a>, and <a href=\"http://80000hours.org/\">80,000 Hours</a>. Help write <a href=\"/lw/cr0/short_primers_on_crucial_topics/\">short primers on crucial topics</a>. To reach a different (and perhaps wealthier, more influential) audience, maybe do help with something like the <a href=\"http://singularitysummit.com/\">Singularity Summit</a>.</p>\n</li>\n<li>\n<p><strong>Develop related skills in humanity</strong>. In other words, \"make humanity stronger in ways that are almost certainly helpful for reducing AI x-risk\" (though strategic research may reveal they are not nearly the <em>most helpful</em> ways to reduce AI x-risk). This might include, for example, getting better at risk analysis with regard to other catastrophic risks, or improving our generalized forecasting abilities by making wider use of prediction markets.</p>\n</li>\n<li>\n<p><strong>Fund a person or organization doing (3) or (4) above</strong>. The <a href=\"http://intelligence.org/donate/\">Singularity Institute</a> probably does more AI x-risk movement building than anyone, followed by the <a href=\"http://www.fhi.ox.ac.uk/donate\">Future of Humanity Institute</a>. There are lots of organizations doing things that plausibly fall under (4).</p>\n</li>\n</ol>\n<p>Note that if you mostly contribute to meta work, you want to also donate a small sum (say, $15/mo) to strategy work or direct work. If you <em>only</em> contribute to meta work for a while, an outside view (around SI, anyway) suggests there's a good chance you'll never manage to <em>ever</em> do anything non-meta. A perfect Bayesian agent might not optimize this way, but <a href=\"/lw/6py/optimal_philanthropy_for_human_beings/\">optimal philanthropy for human beings</a> works differently.</p>\n<p>&nbsp;</p>\n<h2>Strategy work for AI x-risk reduction</h2>\n<p>How can we improve our ability to do <a href=\"/lw/9ao/longterm_technological_forecasting/\">long-term technological forecasting</a>? Is AGI more likely to be safe if developed sooner (<a href=\"http://jetpress.org/v22/goertzel-pitt.pdf\">Goertzel &amp; Pitt 2012</a>) or later (<a href=\"http://intelligence.org/files/IE-EI.pdf\">Muehlhauser &amp; Salamon 2012</a>)? How likely is <a href=\"http://wiki.lesswrong.com/wiki/AI_takeoff\">hard takeoff vs. soft takeoff</a>? Could we use caged <a href=\"http://wiki.lesswrong.com/wiki/Artificial_general_intelligence\">AGIs</a> or <a href=\"http://wiki.lesswrong.com/wiki/Whole_brain_emulation\">WBEs</a> to develop safe AGIs or WBEs? How might we reduce the chances of an AGI arms race (<a href=\"http://intelligence.org/files/ArmsControl.pdf\">Shulman 2009</a>)? Which interventions should we prioritize now, to reduce AI x-risk?</p>\n<p>These questions and <a href=\"/r/discussion/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/\">many others</a> have received scant written analysis &mdash; unless you count the kind of written analysis that is (1) written with much vagueness and ambiguity, (2) written in the author's own idiosyncratic vocabulary, (3) written with few citations to related work, and is (4) spread across a variety of non-linear blog articles, forum messages, and mailing list postings. (The trouble with that kind of written analysis is that it is mostly <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6k30\">impenetrable</a> or undiscoverable to most researchers, especially the ones who are very busy because they are highly productive and don't have time to comb through 1,000 messy blog posts.)</p>\n<p>Here, then, is how you might help with strategy work for AI x-risk reduction:</p>\n<ol>\n<li>\n<p><strong>Consolidate and clarify the strategy work currently only available in a disorganized, idiosyncratic form</strong>. This makes it easier for researchers around the world to understand the current state of play, and build on it. Examples include <a href=\"http://www.imprint.co.uk/singularity.pdf\">Chalmers (2010)</a>, <a href=\"http://intelligence.org/files/SaME.pdf\">Muehlhauser &amp; Helm (2012)</a>, <a href=\"http://intelligence.org/files/IE-EI.pdf\">Muehlhauser &amp; Salamon (2012)</a>, <a href=\"http://intelligence.org/files/AGI-HMM.pdf\">Yampolskiy &amp; Fox (2012)</a>, and (much of) <a href=\"http://nickbostrom.com/\">Nick Bostrom</a>'s forthcoming scholarly monograph on machine superintelligence.</p>\n</li>\n<li>\n<p><strong>Write new strategic analyses</strong>. Examples include <a href=\"http://intelligence.org/files/AIPosNegFactor.pdf\">Yudkowsky (2008)</a>, <a href=\"http://intelligence.org/files/CoalescingMinds.pdf\">Sotala &amp; Valpola (2012)</a>, <a href=\"http://intelligence.org/files/SoftwareLimited.pdf\">Shulman &amp; Sandberg (2010)</a>, <a href=\"http://intelligence.org/files/WBE-Superorgs.pdf\">Shulman (2010)</a>, <a href=\"http://intelligence.org/files/ArmsControl.pdf\">Shulman &amp; Armstrong (2009)</a>, <a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\">Bostrom (2012)</a>, <a href=\"http://www.nickbostrom.com/ethics/ai.pdf\">Bostrom (2003)</a>, <a href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\">Omohundro (2008)</a>, <a href=\"http://jetpress.org/v22/goertzel-pitt.htm\">Goertzel &amp; Pitt (2012)</a>, <a href=\"http://cecs.louisville.edu/ry/LeakproofingtheSingularity.pdf\">Yampolskiy (2012)</a>, and some Less Wrong posts: <a href=\"/r/discussion/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/\">Muehlhauser (2012)</a>, <a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">Yudkowsky (2012)</a>, <a href=\"/lw/f6o/original_research_on_less_wrong/\">etc</a>. See <a href=\"http://tinyurl.com/ai-risk-research\">here</a> for a list of desired strategic analyses (among other desired articles).</p>\n</li>\n<li>\n<p><strong>Assist with (1) or (2), above</strong>. This is what SI's \"<a href=\"/lw/bke/the_singularity_institute_still_needs_remote/\">remote researchers</a>\" tend to do, along with many <a href=\"http://singularityvolunteers.org/\">SI volunteers</a>. Often, there are \"chunks\" of research that can be broken off and handed to people who are not an article's core authors, e.g. \"Please track down many examples of the 'wrong wish' trope so I can use a vivid example in my paper\" or \"Please review and summarize the part of the machine ethics literature that has to do with learning preferences from examples.\"</p>\n</li>\n<li>\n<p><strong>Provide resources and platforms that make it easier for researchers to contribute to strategy work</strong>. Things like my <a href=\"http://intelligence.org/files/AIR-Bibliography2012.pdf\">AI risk bibliography</a> and <a href=\"http://tinyurl.com/ai-risk-research\">list of forthcoming and desired articles on AI risk</a> make it easier for researchers to find relevant work, and to know what projects would be helpful to take on. SI's <a href=\"https://bitbucket.org/singinst/si-bibliography/src\">public BibTeX file</a> and <a href=\"http://www.mendeley.com/groups/1965711/singularity-institute-referece-list/\">Mendeley group</a> make it easier for researchers to find relevant papers. The <a href=\"http://agi-conference.org/\">AGI conference</a>, and volumes like <em><a href=\"http://www.amazon.com/Singularity-Hypotheses-Scientific-Philosophical-Assessment/dp/3642325599/\">Singularity Hypotheses</a></em>, provide publishing venues for researchers in this fledgling field. <a href=\"/lw/fcu/ai_riskrelated_improvements_to_the_lw_wiki/\">Recent improvements</a> to the Less Wrong wiki will hopefully make it easier for researchers to understand the (relatively new) concepts relevant to AI x-risk strategy work. A <a href=\"/lw/cnj/a_scholarly_ai_risk_wiki/\">scholarly AI risk wiki</a> would be even better. It would also help to find editors of prestigious journals who are open to publishing well-written AGI risk papers, so that university researchers can publish on these topics without hurting their chances to get tenure.</p>\n</li>\n<li>\n<p><strong>Fund a person or organization doing any of the above</strong>. Again, the most obvious choices are the <a href=\"http://intelligence.org/donate/\">Singularity Institute</a> or the <a href=\"http://www.fhi.ox.ac.uk/donate\">Future of Humanity Institute</a>. Most of the articles and \"resources\" above were produced by either SI or FHI. SI offers more opportunities for (3). The AGI conference is organized by <a href=\"http://agi-conference.org/2012/contact/\">Ben Goertzel</a> and others, who are of course always looking for sponsors for the AGI conference.</p>\n</li>\n</ol>\n<h2><br /></h2>\n<h2>Direct work for AI x-risk reduction</h2>\n<p>We are still at an early stage in doing strategy work on AI x-risk reduction. Because of this, most researchers in the field feel pretty uncertain about which interventions would be most helpful for reducing AI x-risk. Thus, they focus on strategic research, so they can purchase more confidence about which interventions would be helpful.</p>\n<p>Despite this uncertainty, I'll list some interventions that at least <em>some</em> people have proposed for mitigating AI x-risk, focusing on the interventions that are actionable today.</p>\n<ul>\n<li>\n<p><strong>Safe AGI research?</strong> Many proposals have been made for developing AGI designs with internal motivations beneficial to humans &mdash; including Friendly AI (<a href=\"http://intelligence.org/files/AIPosNegFactor.pdf\">Yudkowsky 2008</a>) and GOLEM (<a href=\"http://goertzel.org/GOLEM.pdf\">Goertzel 2010</a>) &mdash; but researchers disagree about which approaches are most promising (<a href=\"http://intelligence.org/files/SaME.pdf\">Muehlhauser &amp; Helm 2012</a>; <a href=\"http://jetpress.org/v22/goertzel-pitt.htm\">Goertzel &amp; Pitt 2012</a>).</p>\n</li>\n<li>\n<p><strong>AI boxing research?</strong> Many proposals have been made for confining AGIs (<a href=\"http://cecs.louisville.edu/ry/LeakproofingtheSingularity.pdf\">Yampolskiy 2012</a>; <a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Armstrong et al. 2012</a>). But such research programs may end up being fruitless, since it may be that a superintelligence will always be able to think its way out of any confinement designed by a human-level intelligence.</p>\n</li>\n<li>\n<p><strong>AI safety promotion?</strong> One may write about the importance of AI safety, persuade AI safety researchers to take up a greater concern for safety, and so on.</p>\n</li>\n<li>\n<p><strong>Regulate AGI development? Or not?</strong> <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/11/Hughes-Relinquishment-or-Regulation.pdf\">Hughes (2001)</a> and <a href=\"http://www.synesisjournal.com/vol2_g/2011_2_44-50_Daley.pdf\">Daley (2011)</a> call for regulation of AGI development. To bring this about, citizens could petition their governments and try to persuade decision-makers. <a href=\"http://www.law.northwestern.edu/lawreview/colloquy/2010/12/LRColl2010n12McGinnis.pdf\">McGinnis (2010)</a> and <a href=\"http://jetpress.org/v22/goertzel-pitt.htm\">Goertzel &amp; Pitt (2012)</a>, however, oppose AGI regulation.</p>\n</li>\n<li>\n<p><strong>Accelerate AGI? Or not?</strong> <a href=\"http://intelligence.org/files/IE-EI.pdf\">Muehlhauser &amp; Salamon (2012)</a> recommend accelerating AGI safety research relative to AGI capabilities research, so that the first AGIs have a better chance of being designed to be safe. <a href=\"http://jetpress.org/v22/goertzel-pitt.htm\">Goertzel &amp; Pitt (2012)</a>, in contrast, argue that \"the pace of AGI progress is sufficiently slow that practical work towards human-level AGI is in no danger of outpacing associated ethical theorizing,\" and argue that AGI development will be safest if it happens sooner rather than later.</p>\n</li>\n<li>\n<p><strong>Accelerate WBE? Or not?</strong> Participants in a 2011 workshop <a href=\"http://intelligence.org/files/SS11Workshop.pdf\">concluded</a> that accelerating WBE probably increases AI x-risk, but <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/11/Koene-Embracing-competitive-balance.pdf\">Koene (2012)</a> argues that WBE is safer than trying to create safe AGI.</p>\n</li>\n<li>\n<p><strong>Ban AGI and hardware development? Or not?</strong> <a href=\"http://www.wired.com/wired/archive/8.04/joy.html\">Joy (2000)</a> famously advocated a strategy of relinquishment, and <a href=\"http://berglas.org/Articles/AIKillGrandchildren/AIKillGrandchildren.html\">Berglas (2009)</a> goes so far as to suggest we abandon further computing power development. Most people, of course, disagree. In any case, it is doubtful that groups with such views could overcome the economic and military incentives for further computing and AGI development.</p>\n</li>\n<li>\n<p><strong>Foster positive values?</strong> <a href=\"http://www.amazon.com/The-Singularity-Is-Near-Transcend/dp/0143037889/\">Kurzweil (2005)</a> and others argue that one way to increase the odds that AGIs will behave ethically is to increase the chances that the particular humans who create them are moral. Thus, one might reduce AI x-risk by developing training and technology for moral enhancement (<a href=\"http://www.philosophy.ox.ac.uk/__data/assets/pdf_file/0003/9390/Finalsubmittedcorrected09april_08.pdf\">Persson and Savulescu 2008</a>).</p>\n</li>\n<li>\n<p><strong>Cognitive enhancement?</strong> Some have wondered whether the problem of safe AGI is so difficult that it will require cognitive enhancement for humans to solve it. Meanwhile, others worry that cognitive enhancement will only accelerate the development of dangerous technologies (<a href=\"http://www.philosophy.ox.ac.uk/__data/assets/pdf_file/0003/9390/Finalsubmittedcorrected09april_08.pdf\">Persson and Savulescu 2008</a>)</p>\n</li>\n</ul>\n<p>Besides engaging in these interventions directly, one may of course help to fund them. I don't currently know of a group pushing for AGI development regulations, or for banning AGI development. You could accelerate AGI by investing in AGI-related companies, or you could accelerate AGI safety research (and AI boxing research) relative to AGI capabilities research by funding <a href=\"http://intelligence.org/donate/\">SI</a> or <a href=\"http://www.fhi.ox.ac.uk/donate\">FHI</a>, who also probably do the most AI safety promotion work. You could fund research on moral enhancement or cognitive enhancement by offering grants for such research. Or, if you think \"low-tech\" cognitive enhancement is promising, you could fund organizations like <a href=\"http://www.lumosity.com/\">Lumosity</a> (brain training) or the <a href=\"http://appliedrationality.org/\">Center for Applied Rationality</a> (rationality training).</p>\n<p>&nbsp;</p>\n<h2>Conclusion</h2>\n<p>This is a brief guide to what you can do to reduce existential risk from AI. A longer guide could describe the available interventions in more detail, and present the arguments for and against each one. But that is \"strategic work,\" and requires lots of time (and therefore money) to produce.</p>\n<p><small>My thanks to <a href=\"/user/michaelcurzi/\">Michael Curzi</a> for inspiring this post.</small></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Rz5jb3cYHTSRmqNnN": 2, "ZFrgTgzwEfStg26JL": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qARBe3jBodrdPeRE6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 53, "baseScore": 63, "extendedScore": null, "score": 0.000145, "legacy": true, "legacyId": "19997", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 63, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Suppose you think that <a href=\"http://www.existential-risk.org/concept.pdf\">reducing the risk of human extinction</a> is the highest-value thing you can do. Or maybe you want to reduce \"x-risk\" because you're already a <a href=\"http://knowyourmeme.com/memes/first-world-problems\">comfortable First-Worlder</a> like me and so you might as well do something <a href=\"http://www.youtube.com/watch?v=YDsLKEado_o\">epic and cool</a>, or because you like the <a href=\"http://www.fhi.ox.ac.uk/\">community</a> <a href=\"/\">of</a> <a href=\"http://singularity.org\">people</a> who are doing it already, or whatever.</p>\n<p>Suppose also that you think <a href=\"/lw/di4/reply_to_holden_on_the_singularity_institute/#ai-risk\">AI is the most pressing x-risk</a>, because (1) mitigating AI risk could mitigate all other existential risks, but not vice-versa, and because (2) AI is plausibly the first existential risk that will occur.</p>\n<p>In that case, what should you do? How can you reduce AI x-risk?</p>\n<p>It's complicated, but I get this question a lot, so let me try to provide <em>some</em> kind of answer.</p>\n<p>&nbsp;</p>\n<h2 id=\"Meta_work__strategy_work__and_direct_work\">Meta-work, strategy work, and direct work</h2>\n<p>When you're facing a problem and you don't know what to do about it, there are two things you can do:</p>\n<p><strong>1</strong>. <em>Meta-work</em>: Amass wealth and other resources. Build your community. Make yourself stronger. Meta-work of this sort will be useful regardless of which \"direct work\" interventions turn out to be useful for tackling the problem you face. Meta-work also empowers you to do strategic work.</p>\n<p><strong>2</strong>. <em>Strategy work</em>: Purchase a better strategic understanding of the problem you're facing, so you can see more clearly what should be done. Usually, this will consist of getting smart and self-critical people to honestly assess the strategic situation, build models, make predictions about the effects of different possible interventions, and so on. If done well, these analyses can shed light on which kinds of \"direct work\" will help you deal with the problem you're trying to solve.</p>\n<p>When you have enough strategic insight to have discovered <em>some</em> interventions that you're confident will help you tackle the problem you're facing, then you can also engage in:</p>\n<p><strong>3</strong>. <em>Direct work</em>: Directly attack the problem you're facing, whether this involves technical research, political action, particular kinds of technological development, or something else.</p>\n<p>Thinking with these categories can be useful even though the lines between them are fuzzy. For example, you might have to do some basic awareness-raising in order to amass funds for your cause, and then once you've spent those funds on strategy work, your strategy work might tell you that a specific form of awareness-raising is useful for political action that counts as \"direct work.\" Also, some forms of strategy work can feel like direct work, depending on the type of problem you're tackling.</p>\n<p><a id=\"more\"></a></p>\n<h2><br></h2>\n<h2 id=\"Meta_work_for_AI_x_risk_reduction\">Meta-work for AI x-risk reduction</h2>\n<p><a href=\"http://www.amazon.com/The-Personal-MBA-Master-Business/dp/1591845572/\">Make money</a>. <a href=\"http://wiki.lesswrong.com/wiki/The_Science_of_Winning_at_Life\">Become stronger</a>. <a href=\"http://www.amazon.com/Never-Eat-Alone-Secrets-Relationship/dp/0385512058/\">Build a community</a>, <a href=\"http://probloggerbook.com/\">an audience</a>, a <a href=\"http://www.amazon.com/Change-Heart-Psychology-Spreading-Social/dp/159056233X/\">movement</a>. Store your accumulated resources in yourself, in your community, in a <a href=\"http://en.wikipedia.org/wiki/Donor_advised_fund\">donor-advised fund</a>, or in an organization that can advance your causes better than you can as an individual.</p>\n<ol>\n<li>\n<p><strong>Make money</strong>: In the past 10 years, many people have chosen to start businesses or careers that (1) will predictably generate significant wealth they can spend on AI x-risk reduction, (2) will be enjoyable enough to \"stick with it,\" and (3) will not create large negative externalities. But certainly, the AI x-risk reduction community needs a lot <em>more</em> people to do this! If you want advice, the folks at <a href=\"http://80000hours.org/\">80,000 Hours</a> are the experts on \"ethical careers\" of this sort.</p>\n</li>\n<li>\n<p><strong>Become stronger</strong>: Sometimes it makes sense to focus on <em>improving</em> your <a href=\"/lw/3w3/how_to_beat_procrastination/\">productivity</a>, your <a href=\"/lw/5me/scholarship_how_to_do_it_efficiently/\">research skills</a>, your <a href=\"/lw/86a/rhetoric_for_the_good/\">writing skills</a>, your <a href=\"/lw/5p6/how_and_why_to_granularize/\">social skills</a>, etc. before you begin <em>using</em> those skills to achieve your goals. Example: <a href=\"/user/Vladimir_Nesov/\">Vladimir Nesov</a> has done <a href=\"/lw/3l/counterfactual_mugging/\">some original research</a>, but mostly he has spent the last few years improving his math skills before diving into original research full-time.</p>\n</li>\n<li>\n<p><strong>Build a community / a movement</strong>: Individuals <em>can</em> change the world, but communities and movements can do even better, if they're well-coordinated. Read <em><a href=\"http://www.amazon.com/Change-Heart-Psychology-Spreading-Social/dp/159056233X/\">What Psychology Can Teach Us About Spreading Social Change</a></em>. <a href=\"/lw/crs/how_to_run_a_successful_less_wrong_meetup/\">Launch (or improve) a Less Wrong group</a>. Join a <a href=\"http://www.thehighimpactnetwork.org/\">THINK group</a>. Help grow and improve the existing online communities that tend to have high rates of interest in x-risk reduction: <a href=\"/\">LessWrong</a>, <a href=\"http://singularityvolunteers.org/\">Singularity Volunteers</a>, and <a href=\"http://80000hours.org/\">80,000 Hours</a>. Help write <a href=\"/lw/cr0/short_primers_on_crucial_topics/\">short primers on crucial topics</a>. To reach a different (and perhaps wealthier, more influential) audience, maybe do help with something like the <a href=\"http://singularitysummit.com/\">Singularity Summit</a>.</p>\n</li>\n<li>\n<p><strong>Develop related skills in humanity</strong>. In other words, \"make humanity stronger in ways that are almost certainly helpful for reducing AI x-risk\" (though strategic research may reveal they are not nearly the <em>most helpful</em> ways to reduce AI x-risk). This might include, for example, getting better at risk analysis with regard to other catastrophic risks, or improving our generalized forecasting abilities by making wider use of prediction markets.</p>\n</li>\n<li>\n<p><strong>Fund a person or organization doing (3) or (4) above</strong>. The <a href=\"http://intelligence.org/donate/\">Singularity Institute</a> probably does more AI x-risk movement building than anyone, followed by the <a href=\"http://www.fhi.ox.ac.uk/donate\">Future of Humanity Institute</a>. There are lots of organizations doing things that plausibly fall under (4).</p>\n</li>\n</ol>\n<p>Note that if you mostly contribute to meta work, you want to also donate a small sum (say, $15/mo) to strategy work or direct work. If you <em>only</em> contribute to meta work for a while, an outside view (around SI, anyway) suggests there's a good chance you'll never manage to <em>ever</em> do anything non-meta. A perfect Bayesian agent might not optimize this way, but <a href=\"/lw/6py/optimal_philanthropy_for_human_beings/\">optimal philanthropy for human beings</a> works differently.</p>\n<p>&nbsp;</p>\n<h2 id=\"Strategy_work_for_AI_x_risk_reduction\">Strategy work for AI x-risk reduction</h2>\n<p>How can we improve our ability to do <a href=\"/lw/9ao/longterm_technological_forecasting/\">long-term technological forecasting</a>? Is AGI more likely to be safe if developed sooner (<a href=\"http://jetpress.org/v22/goertzel-pitt.pdf\">Goertzel &amp; Pitt 2012</a>) or later (<a href=\"http://intelligence.org/files/IE-EI.pdf\">Muehlhauser &amp; Salamon 2012</a>)? How likely is <a href=\"http://wiki.lesswrong.com/wiki/AI_takeoff\">hard takeoff vs. soft takeoff</a>? Could we use caged <a href=\"http://wiki.lesswrong.com/wiki/Artificial_general_intelligence\">AGIs</a> or <a href=\"http://wiki.lesswrong.com/wiki/Whole_brain_emulation\">WBEs</a> to develop safe AGIs or WBEs? How might we reduce the chances of an AGI arms race (<a href=\"http://intelligence.org/files/ArmsControl.pdf\">Shulman 2009</a>)? Which interventions should we prioritize now, to reduce AI x-risk?</p>\n<p>These questions and <a href=\"/r/discussion/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/\">many others</a> have received scant written analysis \u2014 unless you count the kind of written analysis that is (1) written with much vagueness and ambiguity, (2) written in the author's own idiosyncratic vocabulary, (3) written with few citations to related work, and is (4) spread across a variety of non-linear blog articles, forum messages, and mailing list postings. (The trouble with that kind of written analysis is that it is mostly <a href=\"/lw/cbs/thoughts_on_the_singularity_institute_si/6k30\">impenetrable</a> or undiscoverable to most researchers, especially the ones who are very busy because they are highly productive and don't have time to comb through 1,000 messy blog posts.)</p>\n<p>Here, then, is how you might help with strategy work for AI x-risk reduction:</p>\n<ol>\n<li>\n<p><strong>Consolidate and clarify the strategy work currently only available in a disorganized, idiosyncratic form</strong>. This makes it easier for researchers around the world to understand the current state of play, and build on it. Examples include <a href=\"http://www.imprint.co.uk/singularity.pdf\">Chalmers (2010)</a>, <a href=\"http://intelligence.org/files/SaME.pdf\">Muehlhauser &amp; Helm (2012)</a>, <a href=\"http://intelligence.org/files/IE-EI.pdf\">Muehlhauser &amp; Salamon (2012)</a>, <a href=\"http://intelligence.org/files/AGI-HMM.pdf\">Yampolskiy &amp; Fox (2012)</a>, and (much of) <a href=\"http://nickbostrom.com/\">Nick Bostrom</a>'s forthcoming scholarly monograph on machine superintelligence.</p>\n</li>\n<li>\n<p><strong>Write new strategic analyses</strong>. Examples include <a href=\"http://intelligence.org/files/AIPosNegFactor.pdf\">Yudkowsky (2008)</a>, <a href=\"http://intelligence.org/files/CoalescingMinds.pdf\">Sotala &amp; Valpola (2012)</a>, <a href=\"http://intelligence.org/files/SoftwareLimited.pdf\">Shulman &amp; Sandberg (2010)</a>, <a href=\"http://intelligence.org/files/WBE-Superorgs.pdf\">Shulman (2010)</a>, <a href=\"http://intelligence.org/files/ArmsControl.pdf\">Shulman &amp; Armstrong (2009)</a>, <a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\">Bostrom (2012)</a>, <a href=\"http://www.nickbostrom.com/ethics/ai.pdf\">Bostrom (2003)</a>, <a href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\">Omohundro (2008)</a>, <a href=\"http://jetpress.org/v22/goertzel-pitt.htm\">Goertzel &amp; Pitt (2012)</a>, <a href=\"http://cecs.louisville.edu/ry/LeakproofingtheSingularity.pdf\">Yampolskiy (2012)</a>, and some Less Wrong posts: <a href=\"/r/discussion/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/\">Muehlhauser (2012)</a>, <a href=\"/lw/cze/reply_to_holden_on_tool_ai/\">Yudkowsky (2012)</a>, <a href=\"/lw/f6o/original_research_on_less_wrong/\">etc</a>. See <a href=\"http://tinyurl.com/ai-risk-research\">here</a> for a list of desired strategic analyses (among other desired articles).</p>\n</li>\n<li>\n<p><strong>Assist with (1) or (2), above</strong>. This is what SI's \"<a href=\"/lw/bke/the_singularity_institute_still_needs_remote/\">remote researchers</a>\" tend to do, along with many <a href=\"http://singularityvolunteers.org/\">SI volunteers</a>. Often, there are \"chunks\" of research that can be broken off and handed to people who are not an article's core authors, e.g. \"Please track down many examples of the 'wrong wish' trope so I can use a vivid example in my paper\" or \"Please review and summarize the part of the machine ethics literature that has to do with learning preferences from examples.\"</p>\n</li>\n<li>\n<p><strong>Provide resources and platforms that make it easier for researchers to contribute to strategy work</strong>. Things like my <a href=\"http://intelligence.org/files/AIR-Bibliography2012.pdf\">AI risk bibliography</a> and <a href=\"http://tinyurl.com/ai-risk-research\">list of forthcoming and desired articles on AI risk</a> make it easier for researchers to find relevant work, and to know what projects would be helpful to take on. SI's <a href=\"https://bitbucket.org/singinst/si-bibliography/src\">public BibTeX file</a> and <a href=\"http://www.mendeley.com/groups/1965711/singularity-institute-referece-list/\">Mendeley group</a> make it easier for researchers to find relevant papers. The <a href=\"http://agi-conference.org/\">AGI conference</a>, and volumes like <em><a href=\"http://www.amazon.com/Singularity-Hypotheses-Scientific-Philosophical-Assessment/dp/3642325599/\">Singularity Hypotheses</a></em>, provide publishing venues for researchers in this fledgling field. <a href=\"/lw/fcu/ai_riskrelated_improvements_to_the_lw_wiki/\">Recent improvements</a> to the Less Wrong wiki will hopefully make it easier for researchers to understand the (relatively new) concepts relevant to AI x-risk strategy work. A <a href=\"/lw/cnj/a_scholarly_ai_risk_wiki/\">scholarly AI risk wiki</a> would be even better. It would also help to find editors of prestigious journals who are open to publishing well-written AGI risk papers, so that university researchers can publish on these topics without hurting their chances to get tenure.</p>\n</li>\n<li>\n<p><strong>Fund a person or organization doing any of the above</strong>. Again, the most obvious choices are the <a href=\"http://intelligence.org/donate/\">Singularity Institute</a> or the <a href=\"http://www.fhi.ox.ac.uk/donate\">Future of Humanity Institute</a>. Most of the articles and \"resources\" above were produced by either SI or FHI. SI offers more opportunities for (3). The AGI conference is organized by <a href=\"http://agi-conference.org/2012/contact/\">Ben Goertzel</a> and others, who are of course always looking for sponsors for the AGI conference.</p>\n</li>\n</ol>\n<h2><br></h2>\n<h2 id=\"Direct_work_for_AI_x_risk_reduction\">Direct work for AI x-risk reduction</h2>\n<p>We are still at an early stage in doing strategy work on AI x-risk reduction. Because of this, most researchers in the field feel pretty uncertain about which interventions would be most helpful for reducing AI x-risk. Thus, they focus on strategic research, so they can purchase more confidence about which interventions would be helpful.</p>\n<p>Despite this uncertainty, I'll list some interventions that at least <em>some</em> people have proposed for mitigating AI x-risk, focusing on the interventions that are actionable today.</p>\n<ul>\n<li>\n<p><strong>Safe AGI research?</strong> Many proposals have been made for developing AGI designs with internal motivations beneficial to humans \u2014 including Friendly AI (<a href=\"http://intelligence.org/files/AIPosNegFactor.pdf\">Yudkowsky 2008</a>) and GOLEM (<a href=\"http://goertzel.org/GOLEM.pdf\">Goertzel 2010</a>) \u2014 but researchers disagree about which approaches are most promising (<a href=\"http://intelligence.org/files/SaME.pdf\">Muehlhauser &amp; Helm 2012</a>; <a href=\"http://jetpress.org/v22/goertzel-pitt.htm\">Goertzel &amp; Pitt 2012</a>).</p>\n</li>\n<li>\n<p><strong>AI boxing research?</strong> Many proposals have been made for confining AGIs (<a href=\"http://cecs.louisville.edu/ry/LeakproofingtheSingularity.pdf\">Yampolskiy 2012</a>; <a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Armstrong et al. 2012</a>). But such research programs may end up being fruitless, since it may be that a superintelligence will always be able to think its way out of any confinement designed by a human-level intelligence.</p>\n</li>\n<li>\n<p><strong>AI safety promotion?</strong> One may write about the importance of AI safety, persuade AI safety researchers to take up a greater concern for safety, and so on.</p>\n</li>\n<li>\n<p><strong>Regulate AGI development? Or not?</strong> <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/11/Hughes-Relinquishment-or-Regulation.pdf\">Hughes (2001)</a> and <a href=\"http://www.synesisjournal.com/vol2_g/2011_2_44-50_Daley.pdf\">Daley (2011)</a> call for regulation of AGI development. To bring this about, citizens could petition their governments and try to persuade decision-makers. <a href=\"http://www.law.northwestern.edu/lawreview/colloquy/2010/12/LRColl2010n12McGinnis.pdf\">McGinnis (2010)</a> and <a href=\"http://jetpress.org/v22/goertzel-pitt.htm\">Goertzel &amp; Pitt (2012)</a>, however, oppose AGI regulation.</p>\n</li>\n<li>\n<p><strong>Accelerate AGI? Or not?</strong> <a href=\"http://intelligence.org/files/IE-EI.pdf\">Muehlhauser &amp; Salamon (2012)</a> recommend accelerating AGI safety research relative to AGI capabilities research, so that the first AGIs have a better chance of being designed to be safe. <a href=\"http://jetpress.org/v22/goertzel-pitt.htm\">Goertzel &amp; Pitt (2012)</a>, in contrast, argue that \"the pace of AGI progress is sufficiently slow that practical work towards human-level AGI is in no danger of outpacing associated ethical theorizing,\" and argue that AGI development will be safest if it happens sooner rather than later.</p>\n</li>\n<li>\n<p><strong>Accelerate WBE? Or not?</strong> Participants in a 2011 workshop <a href=\"http://intelligence.org/files/SS11Workshop.pdf\">concluded</a> that accelerating WBE probably increases AI x-risk, but <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/11/Koene-Embracing-competitive-balance.pdf\">Koene (2012)</a> argues that WBE is safer than trying to create safe AGI.</p>\n</li>\n<li>\n<p><strong>Ban AGI and hardware development? Or not?</strong> <a href=\"http://www.wired.com/wired/archive/8.04/joy.html\">Joy (2000)</a> famously advocated a strategy of relinquishment, and <a href=\"http://berglas.org/Articles/AIKillGrandchildren/AIKillGrandchildren.html\">Berglas (2009)</a> goes so far as to suggest we abandon further computing power development. Most people, of course, disagree. In any case, it is doubtful that groups with such views could overcome the economic and military incentives for further computing and AGI development.</p>\n</li>\n<li>\n<p><strong>Foster positive values?</strong> <a href=\"http://www.amazon.com/The-Singularity-Is-Near-Transcend/dp/0143037889/\">Kurzweil (2005)</a> and others argue that one way to increase the odds that AGIs will behave ethically is to increase the chances that the particular humans who create them are moral. Thus, one might reduce AI x-risk by developing training and technology for moral enhancement (<a href=\"http://www.philosophy.ox.ac.uk/__data/assets/pdf_file/0003/9390/Finalsubmittedcorrected09april_08.pdf\">Persson and Savulescu 2008</a>).</p>\n</li>\n<li>\n<p><strong>Cognitive enhancement?</strong> Some have wondered whether the problem of safe AGI is so difficult that it will require cognitive enhancement for humans to solve it. Meanwhile, others worry that cognitive enhancement will only accelerate the development of dangerous technologies (<a href=\"http://www.philosophy.ox.ac.uk/__data/assets/pdf_file/0003/9390/Finalsubmittedcorrected09april_08.pdf\">Persson and Savulescu 2008</a>)</p>\n</li>\n</ul>\n<p>Besides engaging in these interventions directly, one may of course help to fund them. I don't currently know of a group pushing for AGI development regulations, or for banning AGI development. You could accelerate AGI by investing in AGI-related companies, or you could accelerate AGI safety research (and AI boxing research) relative to AGI capabilities research by funding <a href=\"http://intelligence.org/donate/\">SI</a> or <a href=\"http://www.fhi.ox.ac.uk/donate\">FHI</a>, who also probably do the most AI safety promotion work. You could fund research on moral enhancement or cognitive enhancement by offering grants for such research. Or, if you think \"low-tech\" cognitive enhancement is promising, you could fund organizations like <a href=\"http://www.lumosity.com/\">Lumosity</a> (brain training) or the <a href=\"http://appliedrationality.org/\">Center for Applied Rationality</a> (rationality training).</p>\n<p>&nbsp;</p>\n<h2 id=\"Conclusion\">Conclusion</h2>\n<p>This is a brief guide to what you can do to reduce existential risk from AI. A longer guide could describe the available interventions in more detail, and present the arguments for and against each one. But that is \"strategic work,\" and requires lots of time (and therefore money) to produce.</p>\n<p><small>My thanks to <a href=\"/user/michaelcurzi/\">Michael Curzi</a> for inspiring this post.</small></p>", "sections": [{"title": "Meta-work, strategy work, and direct work", "anchor": "Meta_work__strategy_work__and_direct_work", "level": 1}, {"title": "Meta-work for AI x-risk reduction", "anchor": "Meta_work_for_AI_x_risk_reduction", "level": 1}, {"title": "Strategy work for AI x-risk reduction", "anchor": "Strategy_work_for_AI_x_risk_reduction", "level": 1}, {"title": "Direct work for AI x-risk reduction", "anchor": "Direct_work_for_AI_x_risk_reduction", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "92 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 92, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RWo4LwFzpHNQCTcYt", "37sHjeisS9uJufi4u", "SiGY7aah56HvGXxBJ", "jTk9m75y2bpujwRfb", "mg6jDEuQEjBGtibX7", "qMuAazqwJvkvo8teR", "LFfizpc5YupK7pWNv", "hEqsWLm5zQtsPevd3", "eXdGr2LeHYEgMh9qX", "i2XoqtYEykc4XWp9B", "sizjfDgCgAsuLJQmm", "jTkmEGWM4dJAfE62W", "T5WjwPRPHy8mPrJcJ", "uEj6m3Qv84jfQios7", "mYuMdmMmGM7fFj382"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-13T23:58:50.742Z", "modifiedAt": null, "url": null, "title": "Meetup : Berkeley meetup: Deliberate performance", "slug": "meetup-berkeley-meetup-deliberate-performance", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fbWhCrdJL9rPfu2wg/meetup-berkeley-meetup-deliberate-performance", "pageUrlRelative": "/posts/fbWhCrdJL9rPfu2wg/meetup-berkeley-meetup-deliberate-performance", "linkUrl": "https://www.lesswrong.com/posts/fbWhCrdJL9rPfu2wg/meetup-berkeley-meetup-deliberate-performance", "postedAtFormatted": "Tuesday, November 13th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berkeley%20meetup%3A%20Deliberate%20performance&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berkeley%20meetup%3A%20Deliberate%20performance%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfbWhCrdJL9rPfu2wg%2Fmeetup-berkeley-meetup-deliberate-performance%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berkeley%20meetup%3A%20Deliberate%20performance%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfbWhCrdJL9rPfu2wg%2Fmeetup-berkeley-meetup-deliberate-performance", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfbWhCrdJL9rPfu2wg%2Fmeetup-berkeley-meetup-deliberate-performance", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 205, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/fv'>Berkeley meetup: Deliberate performance</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 November 2012 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This week's Berkeley meetup will be a structured discussion about getting better at skills. One way to get better at skills (like aikido) is deliberate practice. Another way to get better at skills (like making friends) is deliberate performance, which is what you do when practicing isn't an option. (If you meet an especially cool person, you don't want to try your risky bold untested friend-getting technique, because you actually want to make friends with them.) Very generally, the tools are</p>\n\n<ul>\n<li>making predictions</li>\n<li>experimenting</li>\n<li>analyzing what happened</li>\n<li>doing post-mortems on near-misses</li>\n<li>getting experts to critique your technique</li>\n</ul>\n\n<p>This is horribly abstract; at the meetup we'll have a structured discussion about how this could actually work in practice, and what we have been actually doing to become skilled. You don't have to read these linked articles to attend the meetup,</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Practice_(learning_method\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Practice_(learning_method</a>)#Deliberate_practice</p>\n\n<p><a href=\"http://peterfadde.com/Research/Deliberate_Performance-PI-1011.pdf\" rel=\"nofollow\">http://peterfadde.com/Research/Deliberate_Performance-PI-1011.pdf</a></p>\n\n<p>but if you've read this book I really hope you attend:</p>\n\n<p><a href=\"http://www.amazon.com/Cambridge-Expertise-Performance-Handbooks-Psychology/dp/0521600812\" rel=\"nofollow\">http://www.amazon.com/Cambridge-Expertise-Performance-Handbooks-Psychology/dp/0521600812</a></p>\n\n<p>Doors open at 7pm, and the meetup begins at 7:30pm. For directions to Zendo, see the mailing list:</p>\n\n<p><a href=\"http://groups.google.com/group/bayarealesswrong\" rel=\"nofollow\">http://groups.google.com/group/bayarealesswrong</a></p>\n\n<p>or call me at:</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/fv'>Berkeley meetup: Deliberate performance</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fbWhCrdJL9rPfu2wg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0330323568569646e-06, "legacy": true, "legacyId": "20053", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berkeley_meetup__Deliberate_performance\">Discussion article for the meetup : <a href=\"/meetups/fv\">Berkeley meetup: Deliberate performance</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 November 2012 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This week's Berkeley meetup will be a structured discussion about getting better at skills. One way to get better at skills (like aikido) is deliberate practice. Another way to get better at skills (like making friends) is deliberate performance, which is what you do when practicing isn't an option. (If you meet an especially cool person, you don't want to try your risky bold untested friend-getting technique, because you actually want to make friends with them.) Very generally, the tools are</p>\n\n<ul>\n<li>making predictions</li>\n<li>experimenting</li>\n<li>analyzing what happened</li>\n<li>doing post-mortems on near-misses</li>\n<li>getting experts to critique your technique</li>\n</ul>\n\n<p>This is horribly abstract; at the meetup we'll have a structured discussion about how this could actually work in practice, and what we have been actually doing to become skilled. You don't have to read these linked articles to attend the meetup,</p>\n\n<p><a href=\"http://en.wikipedia.org/wiki/Practice_(learning_method\" rel=\"nofollow\">http://en.wikipedia.org/wiki/Practice_(learning_method</a>)#Deliberate_practice</p>\n\n<p><a href=\"http://peterfadde.com/Research/Deliberate_Performance-PI-1011.pdf\" rel=\"nofollow\">http://peterfadde.com/Research/Deliberate_Performance-PI-1011.pdf</a></p>\n\n<p>but if you've read this book I really hope you attend:</p>\n\n<p><a href=\"http://www.amazon.com/Cambridge-Expertise-Performance-Handbooks-Psychology/dp/0521600812\" rel=\"nofollow\">http://www.amazon.com/Cambridge-Expertise-Performance-Handbooks-Psychology/dp/0521600812</a></p>\n\n<p>Doors open at 7pm, and the meetup begins at 7:30pm. For directions to Zendo, see the mailing list:</p>\n\n<p><a href=\"http://groups.google.com/group/bayarealesswrong\" rel=\"nofollow\">http://groups.google.com/group/bayarealesswrong</a></p>\n\n<p>or call me at:</p>\n\n<p><a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berkeley_meetup__Deliberate_performance1\">Discussion article for the meetup : <a href=\"/meetups/fv\">Berkeley meetup: Deliberate performance</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berkeley meetup: Deliberate performance", "anchor": "Discussion_article_for_the_meetup___Berkeley_meetup__Deliberate_performance", "level": 1}, {"title": "Discussion article for the meetup : Berkeley meetup: Deliberate performance", "anchor": "Discussion_article_for_the_meetup___Berkeley_meetup__Deliberate_performance1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-14T01:45:56.555Z", "modifiedAt": null, "url": null, "title": "Meetup : Chicago Organizational Meeting and Open Discussion", "slug": "meetup-chicago-organizational-meeting-and-open-discussion", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nic_Smith", "createdAt": "2009-10-23T03:32:46.312Z", "isAdmin": false, "displayName": "Nic_Smith"}, "userId": "XP9GcTgRGLBCnf9ih", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iyYquFGHZPEt8epxu/meetup-chicago-organizational-meeting-and-open-discussion", "pageUrlRelative": "/posts/iyYquFGHZPEt8epxu/meetup-chicago-organizational-meeting-and-open-discussion", "linkUrl": "https://www.lesswrong.com/posts/iyYquFGHZPEt8epxu/meetup-chicago-organizational-meeting-and-open-discussion", "postedAtFormatted": "Wednesday, November 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Chicago%20Organizational%20Meeting%20and%20Open%20Discussion&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Chicago%20Organizational%20Meeting%20and%20Open%20Discussion%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiyYquFGHZPEt8epxu%2Fmeetup-chicago-organizational-meeting-and-open-discussion%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Chicago%20Organizational%20Meeting%20and%20Open%20Discussion%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiyYquFGHZPEt8epxu%2Fmeetup-chicago-organizational-meeting-and-open-discussion", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiyYquFGHZPEt8epxu%2Fmeetup-chicago-organizational-meeting-and-open-discussion", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 78, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/fw'>Chicago Organizational Meeting and Open Discussion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 November 2012 01:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">360 N. Michigan Ave., Chicago, IL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>\"This is an organizational meeting planned for every other month to discuss the group's progress and ideas for possible future meetups. Afterwards, we will have open discussion...\" -- <a href=\"http://www.meetup.com/Less-Wrong-Chicago/events/83542572/\" rel=\"nofollow\">http://www.meetup.com/Less-Wrong-Chicago/events/83542572/</a></p>\n\n<p>This location is the same Corner Bakery that we previously had the weekly meetings at.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/fw'>Chicago Organizational Meeting and Open Discussion</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iyYquFGHZPEt8epxu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.033091947352884e-06, "legacy": true, "legacyId": "20055", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Chicago_Organizational_Meeting_and_Open_Discussion\">Discussion article for the meetup : <a href=\"/meetups/fw\">Chicago Organizational Meeting and Open Discussion</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 November 2012 01:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">360 N. Michigan Ave., Chicago, IL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>\"This is an organizational meeting planned for every other month to discuss the group's progress and ideas for possible future meetups. Afterwards, we will have open discussion...\" -- <a href=\"http://www.meetup.com/Less-Wrong-Chicago/events/83542572/\" rel=\"nofollow\">http://www.meetup.com/Less-Wrong-Chicago/events/83542572/</a></p>\n\n<p>This location is the same Corner Bakery that we previously had the weekly meetings at.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Chicago_Organizational_Meeting_and_Open_Discussion1\">Discussion article for the meetup : <a href=\"/meetups/fw\">Chicago Organizational Meeting and Open Discussion</a></h2>", "sections": [{"title": "Discussion article for the meetup : Chicago Organizational Meeting and Open Discussion", "anchor": "Discussion_article_for_the_meetup___Chicago_Organizational_Meeting_and_Open_Discussion", "level": 1}, {"title": "Discussion article for the meetup : Chicago Organizational Meeting and Open Discussion", "anchor": "Discussion_article_for_the_meetup___Chicago_Organizational_Meeting_and_Open_Discussion1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-14T01:48:17.569Z", "modifiedAt": null, "url": null, "title": "[Link] \"An OKCupid Profile of a Rationalist\"", "slug": "link-an-okcupid-profile-of-a-rationalist", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:30.210Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Athrelon", "createdAt": "2012-04-01T14:59:49.764Z", "isAdmin": false, "displayName": "Athrelon"}, "userId": "XbQcJde2pocn9RLLX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hZmSchAZtTG2HYZe4/link-an-okcupid-profile-of-a-rationalist", "pageUrlRelative": "/posts/hZmSchAZtTG2HYZe4/link-an-okcupid-profile-of-a-rationalist", "linkUrl": "https://www.lesswrong.com/posts/hZmSchAZtTG2HYZe4/link-an-okcupid-profile-of-a-rationalist", "postedAtFormatted": "Wednesday, November 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20%22An%20OKCupid%20Profile%20of%20a%20Rationalist%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20%22An%20OKCupid%20Profile%20of%20a%20Rationalist%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhZmSchAZtTG2HYZe4%2Flink-an-okcupid-profile-of-a-rationalist%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20%22An%20OKCupid%20Profile%20of%20a%20Rationalist%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhZmSchAZtTG2HYZe4%2Flink-an-okcupid-profile-of-a-rationalist", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhZmSchAZtTG2HYZe4%2Flink-an-okcupid-profile-of-a-rationalist", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 575, "htmlBody": "<div id=\"entry_t3_fh2\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div>\n<p>The rationalist <a href=\"http://marginalrevolution.com/marginalrevolution/2012/11/assorted-links-609.html\">in question</a>, of course, is our very own EY.</p>\n<p>Quotes giving a reasonable sample of the spectrum of reactions:</p>\n<blockquote>\n<p>Epic Fail on the e-harmony profile. He&rsquo;s over-signalling intelligence. There&rsquo;s a good paper about how much to optimally signal, like when you have a PhD to put it on your business card or not. This guy is going around giving out business cards that read Prof. Dr. John Doe, PhD, MA, BA. He won&rsquo;t be getting laid any time soon.</p>\n</blockquote>\n<blockquote>\n<p>His profile is probably very effective for aspergery girls who like reading the kinds of things that appear on LessWrong. Yudkowsky is basically a celebrity within a small niche of hyper-nerdy rationalists, so I doubt he has much trouble getting laid by girls in that community.</p>\n</blockquote>\n<blockquote>\n<p>You make it sound like a cult leader or something....And reading the profile again with that lens, it actually makes a lot of sense.</p>\n</blockquote>\n<blockquote>\n<p>I was about to agree [that the profile is oversharing], but then come to think of it, I realize I have an orgasm denial fetish, too. It&rsquo;s an aroused preference that never escaped to my non-aroused self-consciousness.</p>\n</blockquote>\n<p>Why is this important to consider?&nbsp;</p>\n<p>LessWrong as a community is dedicated to trying to \"raise the sanity waterline,\" and its most respected members in particular put a lot of resources into outreach, via CFAR, HPMoR, and maintaining this site.&nbsp; But a big factor in how people perceive our brand of rationality is about image.&nbsp; If we're serious about raising the sanity waterline, that means image management - or at least avoiding <em>active image malpractice</em> - is something we should enthusiastically embrace as a way to achieve our goals. [1]</p>\n<p>This is also a valuable exercise in considering the outside view.&nbsp; Marginal Revolution is already a fairly <a href=\"http://www.foreignpolicy.com/articles/2011/01/02/ideas_weird_science\">WEIRD</a> site, focused on abstract economic issues.&nbsp; If any major blog is likely to be sympathetic to our cultural quirks, this would be it.&nbsp; Yet a plurality of commenters reacted negatively.&nbsp;</p>\n<p>To the extent that we didn't notice anything strange about LW's figurehead having this OKCupid profile, LW either failed at calibrating mainstream reaction, or failed at consequentialism and realizing the drag this would have on our other recruitment efforts.&nbsp; In <a href=\"/lw/ds4/article_about_lw_faith_hope_and_singularity/73jm?context=4#comments\">our last discussion</a>, there were only a few commenters raising concerns, and the consensus of the thread was that it was harmless and had no PR consequences worth noting.</p>\n<p>As one commenter cogently put it,</p>\n<blockquote>\n<p>I&rsquo;m not saying that he&rsquo;s trying to make a statement with this, I&rsquo;m saying that he is making a statement about this whether he&rsquo;s trying to or not. Ideas have consequences for how we live our lives, and that Eliezer has a public, identifiable profile up where he talks about his sexual fetishes is not some sort of randomly occurring event with no relationship to his other ideas.</p>\n</blockquote>\n<p>I'd argue the same reasoning applies to the community at large, not just EY specifically.</p>\n<p>[1] From Anna's excellent <a href=\"/lesswrong.com/lw/fc3/checklist_of_rationality_habits\">article</a>: 5. I consciously attempt to welcome bad news, or at least not push it away. <em>(Recent example from Eliezer: At a brainstorming session for future Singularity Summits, one issue raised was that we hadn't really been asking for money at previous ones. My brain was offering resistance, so I applied the \"bad news is good news\" pattern to rephrase this as, \"This point doesn't change the fixed amount of money we raised in past years, <strong>so it is good news because it implies that we can fix the strategy and do better next year</strong>.\")</em></p>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hZmSchAZtTG2HYZe4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 54, "baseScore": -35, "extendedScore": null, "score": -7.9e-05, "legacy": true, "legacyId": "20054", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": -4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-14T04:05:38.614Z", "modifiedAt": null, "url": null, "title": "Universal agents and utility functions", "slug": "universal-agents-and-utility-functions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:00.119Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Anja", "createdAt": "2012-11-03T19:59:14.080Z", "isAdmin": false, "displayName": "Anja"}, "userId": "ptRyuMRZHMt6KdDtZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/q3mZNmvqBtnG2nQre/universal-agents-and-utility-functions", "pageUrlRelative": "/posts/q3mZNmvqBtnG2nQre/universal-agents-and-utility-functions", "linkUrl": "https://www.lesswrong.com/posts/q3mZNmvqBtnG2nQre/universal-agents-and-utility-functions", "postedAtFormatted": "Wednesday, November 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Universal%20agents%20and%20utility%20functions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUniversal%20agents%20and%20utility%20functions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq3mZNmvqBtnG2nQre%2Funiversal-agents-and-utility-functions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Universal%20agents%20and%20utility%20functions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq3mZNmvqBtnG2nQre%2Funiversal-agents-and-utility-functions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq3mZNmvqBtnG2nQre%2Funiversal-agents-and-utility-functions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1873, "htmlBody": "<p>I'm Anja Heinisch, the new visiting fellow at SI. I've been researching replacing AIXI's reward system with a proper utility function. Here I will describe my AIXI+utility function model, address concerns about restricting the model to bounded or finite utility, and analyze some of the implications of modifiable utility functions, e.g. <a title=\"wireheading\" href=\"http://wiki.lesswrong.com/wiki/Wireheading\">wireheading</a> and <a title=\"dynamic inconsistency\" href=\"http://en.wikipedia.org/wiki/Dynamic_inconsistency\">dynamic consistency</a>. Comments, questions and advice (especially about related research and material) will be highly appreciated.</p>\n<h3>Introduction to AIXI</h3>\n<p><a title=\"AIXI gentle\" href=\"http://www.hutter1.net/ai/aixigentle.pdf\">Marcus Hutter's (2003)</a>&nbsp;universal agent AIXI &nbsp;addresses the problem of rational action in a (partially) unknown computable universe, given infinite computing power and a halting oracle.&nbsp;The agent interacts with its environment in discrete time cycles, producing an action-perception sequence&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"y_1x_1y_2x_2\\dots\" src=\"http://www.codecogs.com/png.latex?y_1x_1y_2x_2\\dots\" alt=\"\" align=\"bottom\" />&nbsp;with actions (agent outputs) &nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"y_i\" src=\"http://www.codecogs.com/png.latex?y_i\" alt=\"\" align=\"bottom\" />&nbsp;and perceptions (environment outputs) &nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"x_i\" src=\"http://www.codecogs.com/png.latex?x_i\" alt=\"\" align=\"bottom\" />&nbsp;chosen from finite sets&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\mathcal{Y}\" src=\"http://www.codecogs.com/png.latex?\\mathcal{Y}\" alt=\"\" align=\"bottom\" />&nbsp;and&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\mathcal{X}\" src=\"http://www.codecogs.com/png.latex?\\mathcal{X}\" alt=\"\" align=\"bottom\" />.&nbsp;The perceptions are pairs&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"x_i=(o_i,r_i)\" src=\"http://www.codecogs.com/png.latex?x_i=(o_i,r_i)\" alt=\"\" align=\"bottom\" />, where&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"o_i\" src=\"http://www.codecogs.com/png.latex?o_i\" alt=\"\" align=\"bottom\" />&nbsp;is the observation<em>&nbsp;</em>part and&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"r_i\" src=\"http://www.codecogs.com/png.latex?r_i\" alt=\"\" align=\"bottom\" />&nbsp;denotes a reward<em>.</em>&nbsp;At time k the agent chooses its next action&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\.{y}_k\" src=\"http://www.codecogs.com/png.latex?\\.{y}_k\" alt=\"\" align=\"bottom\" />&nbsp;according to the expectimax principle:</p>\n<p><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\.{y}_k=\\textrm{arg}\\max_{y_k}\\sum_{x_k}\\max_{y_{k+1}}\\sum_{x_{k+1}}\\dots\\max_{y_{m_k}}\\sum_{x_{m_k}}(r_k+\\dots+r_{m_k})M(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k}).\" src=\"http://www.codecogs.com/png.latex?\\.{y}_k=\\textrm{arg}\\max_{y_k}\\sum_{x_k}\\max_{y_{k+1}}\\sum_{x_{k+1}}\\dots\\max_{y_{m_k}}\\sum_{x_{m_k}}(r_k+\\dots+r_{m_k})M(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k}).\" alt=\"\" align=\"bottom\" /></p>\n<p>Here M denotes the updated <a title=\"Solomonoff tutorial\" href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/\">Solomonoff prior</a> summing over all programs&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"q\\in Q_k\" src=\"http://www.codecogs.com/png.latex?q\\in%20Q_k\" alt=\"\" align=\"bottom\" />&nbsp;that are consistent with the history<em> &nbsp;</em><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\.{y}\\.{x}_{&lt;k}\" src=\"http://www.codecogs.com/png.latex?\\.{y}\\.{x}_{%3Ck}\" alt=\"\" align=\"bottom\" /><sup><a href=\"#Notation\">[1]</a><a name=\"re:Notation\"></a></sup>&nbsp;and which will, when run on the universal Turing machine T with successive inputs&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"y_{k:m_k}\" src=\"http://www.codecogs.com/png.latex?y_{k:m_k}\" alt=\"\" align=\"bottom\" />, compute outputs&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\underline{x}_{k:m_k}\" src=\"http://www.codecogs.com/png.latex?\\underline{x}_{k:m_k}\" alt=\"\" align=\"bottom\" />, i.e.</p>\n<h3><img style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal;\" title=\"M(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k})=\\sum_{q\\in%20Q_k,\\:%20T(q,y_{k:m_k})=x_{k:m_k}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!2^{-\\ell(q)}\" src=\"http://www.codecogs.com/png.latex?M(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k})=\\sum_{q\\in%20Q_k,\\:%20T(q,y_{k:m_k})=x_{k:m_k}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!2^{-\\ell(q)}\" alt=\"\" align=\"bottom\" /></h3>\n<p>AIXI is a <a title=\"Cartesian dualism\" href=\"http://en.wikipedia.org/wiki/Dualism_(philosophy_of_mind)\">dualistic</a>&nbsp;framework in the sense that the algorithm that constitutes the agent is not part of the environment, since it is not computable.&nbsp;Even considering that any running implementation of AIXI would have to be computable, AIXI accurately simulating AIXI accurately simulating AIXI ad infinitem doesn't really seem feasible. Potential consequences of this separation of mind and matter include <a title=\"Anvil problem\" href=\"http://wiki.lesswrong.com/wiki/Anvil_problem\">difficulties</a>&nbsp;the agent may have <a title=\"Existential despair\" href=\"/lw/8qy/aixi_and_existential_despair/\">predicting the effects of its actions</a> on the world.&nbsp;</p>\n<h3>Utility vs rewards</h3>\n<p>So, why is it a bad idea to work with a reward system? Say the AIXI agent is rewarded whenever a human called Bob pushes a button. Then a sufficiently smart AIXI will figure out that instead of furthering Bob&rsquo;s goals it can also threaten or deceive Bob into pushing the button, or get another human to replace Bob. On the other hand, if the reward is computed in a little box somewhere and then displayed on a screen, it might still be possible to reprogram the box or find a side channel attack. Intuitively you probably&nbsp;wouldn't&nbsp;even blame the agent for doing that -- people try to game the system all the time.&nbsp;</p>\n<p>You can visualize AIXI's computation as maximizing bars displayed on this screen; the agent is unable to connect the bars to any pattern in the environment, they are just there. It wants them to be as high as possible and it will utilize any means at its disposal. For a more detailed analysis of the problems arising through reinforcement learning, see <a title=\"Value Learning\" href=\"http://www.danieldewey.net/learning-what-to-value.pdf\">Dewey (2011)</a>.</p>\n<p>Is there a way to bind the optimization process to actual patterns in the environment? To design a framework in which the screen informs the agent about the patterns it should optimize for? The answer is, yes, we can just define a utility function</p>\n<p><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"U:\\bigcup_{m_k=1}^\\infty(\\mathcal{Y}\\times\\mathcal{X})^{m_k}\\rightarrow\\mathbb{R}\" src=\"http://www.codecogs.com/png.latex?U:\\bigcup_{m_k=1}^\\infty(\\mathcal{Y}\\times\\mathcal{X})^{m_k}\\rightarrow\\mathbb{R}\" alt=\"\" align=\"bottom\" /></p>\n<p>that assigns a value&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"U(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k})\" src=\"http://www.codecogs.com/png.latex?U(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k})\" alt=\"\" align=\"bottom\" />&nbsp;to every possible future history&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k}\" src=\"http://www.codecogs.com/png.latex?\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k}\" alt=\"\" align=\"bottom\" />&nbsp;and use it to replace the reward system in the agent specification:</p>\n<p><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"{y}_k=\\textrm{arg}\\max_{y_k}\\sum_{x_k}\\max_{y_{k+1}}\\sum_{x_{k+1}}\\dots\\max_{y_{m_k}}\\sum_{x_{m_k}}U(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k})M(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k}).\" src=\"http://www.codecogs.com/png.latex?{y}_k=\\textrm{arg}\\max_{y_k}\\sum_{x_k}\\max_{y_{k+1}}\\sum_{x_{k+1}}\\dots\\max_{y_{m_k}}\\sum_{x_{m_k}}U(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k})M(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k}).\" alt=\"\" align=\"bottom\" /></p>\n<p>When I say \"we can just define\" I am actually referring to the <em>really hard</em> question of how to recognize and describe the patterns we value in the universe. Contrasted with the necessity to specify rewards in the original AIXI framework, this is a strictly harder problem, because the utility function has to be known ahead of time and the reward system can always be represented in the framework of utility functions by setting</p>\n<p><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"U(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k})=r_k+\\dots+r_{m_k}.\" src=\"http://www.codecogs.com/png.latex?U(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k})=r_k+\\dots+r_{m_k}.\" alt=\"\" align=\"bottom\" /></p>\n<p>For the same reasons, this is also a strictly safer approach.</p>\n<h3>Infinite utility</h3>\n<p style=\"font-size: small; font-weight: normal;\">The original AIXI framework must necessarily place upper and lower bound on the rewards that are achievable, because the rewards are part of the perceptions and&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\mathcal{X}\" src=\"http://www.codecogs.com/png.latex?\\mathcal{X}\" alt=\"\" align=\"bottom\" />&nbsp;is finite. The utility function approach does not have this problem, as the expected utility&nbsp;</p>\n<p style=\"font-size: small; font-weight: normal;\"><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\sum_{x_i}u(\\.{y}\\.{x}_{&lt;k}y\\underline{x}_{k:i})M(\\.{y}\\.{x}_{&lt;k}y\\underline{x}_{k:i})\" src=\"http://www.codecogs.com/png.latex?\\sum_{x_i}u(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:i})M(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:i})\" alt=\"\" align=\"bottom\" /></p>\n<p style=\"font-size: small; font-weight: normal;\">is always finite as long as we stick to a finite set of possible perceptions, even if the utility function is not bounded. Relaxing this constraint and allowing&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\mathcal{X}\" src=\"http://www.codecogs.com/png.latex?\\mathcal{X}\" alt=\"\" align=\"bottom\" />&nbsp;to be infinite and the utility to be unbounded creates divergence of expected utility (for a proof see&nbsp;<a title=\"Divergence\" href=\"http://arxiv.org/PS_cache/arxiv/pdf/0712/0712.4318v1.pdf\">d</a><a title=\"Divergence\" href=\"http://arxiv.org/PS_cache/arxiv/pdf/0712/0712.4318v1.pdf\">e Blanc 2008</a>).&nbsp;This closely corresponds to the question of how to be a consequentialist in an infinite universe, discussed by&nbsp;<a title=\"Infinite Ethics\" href=\"http://www.nickbostrom.com/ethics/infinite.pdf\">Bostrom (2011)</a>. The underlying problem here is that (using the&nbsp;<a title=\"infinities\" href=\"http://wiki.lesswrong.com/wiki/Quick_reference_guide_to_the_infinite\">standard approach to infinities</a>) these expected utilities will become incomparable. One possible solution to this problem could be to use a larger subfield than&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\mathbb{R}\" src=\"http://www.codecogs.com/png.latex?\\mathbb{R}\" alt=\"\" align=\"bottom\" />&nbsp;of the&nbsp;<a title=\"Surreals\" href=\"http://en.wikipedia.org/wiki/Surreal_number\">surreal numbers</a>, my favorite<sup><a href=\"#Hyperreals\">[2]</a><a name=\"re:Hyperreals\"></a></sup>&nbsp;so far being the&nbsp;<a title=\"Levi-Civita\" href=\"http://en.wikipedia.org/wiki/Levi-Civita_field\">Levi-Civita field</a>&nbsp;generated by the infinitesimal&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\varepsilon\" src=\"http://www.codecogs.com/png.latex?\\varepsilon\" alt=\"\" align=\"bottom\" />:</p>\n<p style=\"font-size: small; font-weight: normal;\"><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\mathcal{R}=\\{\\sum_{q\\in\\mathbb{Q}}a_q\\varepsilon^q|a_q\\in\\mathbb{R}\\},\" src=\"http://www.codecogs.com/png.latex?\\mathcal{R}=\\{\\sum_{q\\in\\mathbb{Q}}a_q\\varepsilon^q|a_q\\in\\mathbb{R}\\},\" alt=\"\" align=\"bottom\" /></p>\n<p style=\"font-size: small; font-weight: normal;\">with the usual power-series addition and multiplication. Levi-Civita numbers can be written and approximated as&nbsp;</p>\n<p style=\"font-size: small; font-weight: normal;\"><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\lim_{n\\rightarrow \\infty}\\sum_{i=0}^n a_{q_i}\\varepsilon^{q_i}\" src=\"http://www.codecogs.com/png.latex?\\lim_{n\\rightarrow%20\\infty}\\sum_{i=0}^n%20a_{q_i}\\varepsilon^{q_i}\" alt=\"\" align=\"bottom\" /></p>\n<p style=\"font-size: small; font-weight: normal;\">(see&nbsp;<a title=\"Berz\" href=\"http://bt.pa.msu.edu/pub/papers/rscrptsf/rscrptsf.pdf\">Berz 1996</a>), which makes them suitable for representation on a computer using floating point arithmetic. If we allow the range of our utility function to be&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\mathcal{R}\" src=\"http://www.codecogs.com/png.latex?\\mathcal{R}\" alt=\"\" align=\"bottom\" />, we gain the possibility of generalizing the framework to work with an infinite set of possible perceptions, therefore allowing for continuous parameters. We also allow for a much broader set of utility functions, no longer excluding the assignment of infinite (or infinitesimal) utility to a single event. I recently met someone who argued convincingly that his (ideal) utility function assigns infinite negative utility to every time instance that he is not alive, therefore making him prefer life to any finite but huge amount of suffering.</p>\n<p style=\"font-size: small; font-weight: normal;\">Note that finiteness of&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\mathcal{Y}\" src=\"http://www.codecogs.com/png.latex?\\mathcal{Y}\" alt=\"\" align=\"bottom\" />&nbsp;is still needed to guarantee the existence of actions with maximal expected utility, and the finite (but dynamic) horizon&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"m_k\" src=\"http://www.codecogs.com/png.latex?m_k\" alt=\"\" align=\"bottom\" />&nbsp;remains a very problematic assumption, as described in <a title=\"Horizon problem\" href=\"http://www.vetta.org/documents/Machine_Super_Intelligence.pdf\">Legg (2008)</a>.</p>\n<h3>Modifiable utility functions</h3>\n<p>Any implementable approximation of AIXI implies a weakening of the underlying dualism. Now the agent's hardware is part of the environment and at least in the case of a powerful agent, it can no longer afford to neglect the effect its actions may have on its source code and data. One question that has been asked is whether AIXI can&nbsp;<a title=\"AIXI &amp; anvils\" href=\"/lw/8rl/would_aixi_protect_itself/\">protect itself from harm</a>. <a href=\"http://arxiv.org/pdf/1111.3934v2\">Hibbard (2012)</a> shows that an agent similar to the one described above, equipped with the ability to modify its policy responsible for choosing future actions, would not do so, given that it starts out with the (meta-)policy to always use the optimal policy, and the additional constraint to change only if that leads to a strict improvement. <a title=\"Delusion Box\" href=\"http://www.idsia.ch/~ring/Ring%2COrseau%3B%20Delusion%2C%20Survival%2C%20and%20Intelligent%20Agents%2C%20AGI%202011.pdf\">Ring and Orseau (2011)</a> study under which circumstances a universal agent would try to tamper with the sensory information it receives. They introduce the concept of a&nbsp;delusion box, a device that filters and distorts the perception data before it is written into the part of the memory that is read during the calculation of utility.&nbsp;</p>\n<p>A further complication to take into account is the possibility that the part of memory that contains the utility function may get rewritten, either by accident, by deliberate choice (programmers trying to correct a mistake), or in an attempt to wirehead. To analyze this further we will now consider what can happen if the screen flashes different goals in different time cycles. Let&nbsp;</p>\n<p><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"u_k:\\bigcup_{i=1}^\\infty(\\mathcal{Y}\\times\\mathcal{X})^i\\rightarrow \\mathbb{R}\" src=\"http://www.codecogs.com/png.latex?u_k:\\bigcup_{i=1}^\\infty(\\mathcal{Y}\\times\\mathcal{X})^i\\rightarrow%20\\mathbb{R}\" alt=\"\" align=\"bottom\" /></p>\n<p>denote the utility function the agent will have at time k.</p>\n<p>Even though we will only analyze instances in which the agent knows at time k, which utility function&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"u_i\" src=\"http://www.codecogs.com/png.latex?u_i\" alt=\"\" align=\"bottom\" />&nbsp;it will have at future times&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"k\\leq i\\leq m_k\" src=\"http://www.codecogs.com/png.latex?k\\leq%20i\\leq%20m_k\" alt=\"\" align=\"bottom\" />&nbsp;(possibly depending on the actions&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"y_{k:i-1}\" src=\"http://www.codecogs.com/png.latex?y_{k:i-1}\" alt=\"\" align=\"bottom\" />&nbsp;before that), we note that for every fixed future history&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\.{y}\\.{x}_{&lt;k}y\\underline{x}_{k:i}\" src=\"http://www.codecogs.com/png.latex?\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:i}\" alt=\"\" align=\"bottom\" />&nbsp;the agent knows the utility function&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"u_i\" src=\"http://www.codecogs.com/png.latex?u_i\" alt=\"\" align=\"bottom\" />&nbsp;that is displayed on the screen because the screen is part of its perception data&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\underline{x}_i\" src=\"http://www.codecogs.com/png.latex?\\underline{x}_i\" alt=\"\" align=\"bottom\" />.</p>\n<p>This leads to three different agent models worthy of further investigation:</p>\n<ul>\n<li><strong>Agent 1</strong> will optimize for the goals that are displayed on the screen right now and act as if it would continue to do so in the future. We describe this with the utility function &nbsp;&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"U_1(\\.{y}\\.{x}_{&lt;k}y\\underline{x}_{k:m_k})=u_{k}(\\.{y}\\.{x}_{&lt;k}y\\underline{x}_k)+u_{k}}(\\.{y}\\.{x}_{&lt;k}y\\underline{x}_{k:k+1})+\\dots+u_{k}(\\.{y}\\.{x}_{&lt;k}y\\underline{x}_{k:m_k}).\" src=\"http://www.codecogs.com/png.latex?U_1(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k})=u_{k}(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_k)+u_{k}}(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:k+1})+\\dots+u_{k}(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k}).\" alt=\"\" align=\"bottom\" /></li>\n<li><strong>Agent 2</strong> will try to anticipate future changes to its utility function and maximize the utility it experiences at every time cycle as shown on the screen at that time. This is captured by<span style=\"font-family: 'Times New Roman'; font-size: small;\">&nbsp;</span><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"U_2(\\.{y}\\.{x}_{&lt;k}y\\underline{x}_{k:m_k})=u_{k}(\\.{y}\\.{x}_{&lt;k}y\\underline{x}_k)+u_{k+1}}(\\.{y}\\.{x}_{&lt;k}y\\underline{x}_{k:k+1})+\\dots+u_{m_k}(\\.{y}\\.{x}_{&lt;k}y\\underline{x}_{k:m_k}).\" src=\"http://www.codecogs.com/png.latex?U_2(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k})=u_{k}(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_k)+u_{k+1}}(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:k+1})+\\dots+u_{m_k}(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k}).\" alt=\"\" align=\"bottom\" /></li>\n<li><strong>Agent 3</strong> will, at time k, try to maximize the utility it derives in hindsight, displayed on the screen at the time horizon &nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"U_3(\\.{y}\\.{x}_{&lt;k}y\\underline{x}_{k:m_k})=u_{m_k}(\\.{y}\\.{x}_{&lt;k}y\\underline{x}_k)+u_{m_k}}(\\.{y}\\.{x}_{&lt;k}y\\underline{x}_{k:k+1})+\\dots+u_{m_k}(\\.{y}\\.{x}_{&lt;k}y\\underline{x}_{k:m_k}).\" src=\"http://www.codecogs.com/png.latex?U_3(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k})=u_{m_k}(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_k)+u_{m_k}}(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:k+1})+\\dots+u_{m_k}(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k}).\" alt=\"\" align=\"bottom\" /></li>\n</ul>\n<p>Of course arbitrary mixtures of these are possible.</p>\n<p>The type of wireheading that is of interest here is captured by the Simpleton Gambit described by&nbsp;<a title=\"Simpleton\" href=\"http://www.idsia.ch/~ring/Orseau,Ring;Self-modification%20and%20Mortality%20in%20Artificial%20Agents,%20AGI%202011.pdf\">Orseau and Ring (2011)</a>, a Faustian deal that offers the agent maximal utility in exchange for its willingness to be turned into a Simpleton that always takes the same default action at all future times.&nbsp;We will first consider a simplified version of this scenario: The Simpleton future, where the agent knows for certain that it will be turned into a Simpleton at time k+1, no matter what it does in the remaining time cycle. Assume that for all possible action-perception combinations the utility given by the current utility function is not maximal, i.e. &nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"u_k(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:i})%3CU_\\max\" src=\"http://www.codecogs.com/png.latex?u_k(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:i})%3CU_\\max\" alt=\"\" align=\"bottom\" />&nbsp;holds for all&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"y\\underline{x}_{k:i}\" src=\"http://www.codecogs.com/png.latex?y\\underline{x}_{k:i}\" alt=\"\" align=\"bottom\" />.&nbsp;Assume further that&nbsp;the agents actions influence the future outcomes, at least from its current perspective. That is,&nbsp;for all&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"k\\leq i\\leq m_k\" src=\"http://www.codecogs.com/png.latex?k\\leq%20i\\leq%20m_k\" alt=\"\" align=\"bottom\" />&nbsp;there&nbsp;exist &nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"y'\\underline{x}'_{k:i}\\neq%20y\\underline{x}_{k:i}\" src=\"http://www.codecogs.com/png.latex?y'\\underline{x}'_{k:i}\\neq%20y\\underline{x}_{k:i}\" alt=\"\" align=\"bottom\" />&nbsp;with&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"u_k(\\.{y}\\.{x}_{%3Ck}y'\\underline{x}'_{k:i})\\neq%20u_k(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:i})\" src=\"http://www.codecogs.com/png.latex?u_k(\\.{y}\\.{x}_{%3Ck}y'\\underline{x}'_{k:i})\\neq%20u_k(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:i})\" alt=\"\" align=\"bottom\" />.&nbsp;Let&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"u_{k+1}\" src=\"http://www.codecogs.com/png.latex?u_{k+1}\" alt=\"\" align=\"bottom\" />&nbsp;be the Simpleton utility function, assigning equal but maximal utility&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"u_k(\\.{y}\\.{x}_{&lt;k}y\\underline{x}_{k:i})=U_\\max\" src=\"http://www.codecogs.com/png.latex?u_k(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:i})=U_\\max\" alt=\"\" align=\"bottom\" />&nbsp;to all possible futures. While Agent 1 will optimize as before, not adapting its behavior to the knowledge that its utility function will change, Agent 3 will be paralyzed, having to rely on whatever method its implementation uses to break ties. Agent 2 on the other hand will try to maximize only the utility&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"u_k(\\.{y}\\.{x}_{&lt;k}y\\underline{x}_k)\" src=\"http://www.codecogs.com/png.latex?u_k(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_k)\" alt=\"\" align=\"bottom\" />.</p>\n<p>Now consider the actual Simpleton Gambit: At time k the agent gets to choose between changing,<span style=\"font-family: 'Times New Roman'; font-size: small;\">&nbsp;</span><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"y_k=\\textit{c}\" src=\"http://www.codecogs.com/png.latex?y_k=\\textit{c}\" alt=\"\" align=\"bottom\" />, resulting in&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"u_{k+i}\\equiv U_\\max\" src=\"http://www.codecogs.com/png.latex?u_{k+i}\\equiv%20U_\\max\" alt=\"\" align=\"bottom\" />&nbsp;and&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"y_k=\\textit{nc}\" src=\"http://www.codecogs.com/png.latex?y_k=\\textit{nc}\" alt=\"\" align=\"bottom\" />&nbsp;(not changing),&nbsp;leading to&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"u_{k+i}\\equiv u_k\" src=\"http://www.codecogs.com/png.latex?u_{k+i}\\equiv%20u_k\" alt=\"\" align=\"bottom\" />&nbsp;for all&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"k+1\\leq i\\leq m_k\" src=\"http://www.codecogs.com/png.latex?k+1\\leq%20i\\leq%20m_k\" alt=\"\" align=\"bottom\" />. We assume that<span style=\"font-family: 'Times New Roman'; font-size: small;\">&nbsp;</span><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\.{y}_k\" src=\"http://www.codecogs.com/png.latex?\\.{y}_k\" alt=\"\" align=\"bottom\" />&nbsp;has no further effects on the environment. As before, Agent 1 will optimize for business as usual, whether or not it chooses to change depends entirely on whether the screen specifically mentions the memory pointer to the utility function or not.</p>\n<p>Agent 2 will change if and only if the utility of changing compared to not changing according to what the screen currently says&nbsp;is strictly smaller than&nbsp;the comparative advantage of always having maximal utility in the future. That is,</p>\n<p><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\sum_{x_k}[u_k(\\.{y}\\.{x}_{%3Ck}\\textit{nc\\,}\\,\\underline{x}_k)M(\\.{y}\\.{x}_{%3Ck}\\textit{nc\\,}\\underline{x}_k)-u_k(\\.{y}\\.{x}_{%3Ck}\\textit{c\\,}\\underline{x}_k)M(\\.{y}\\.{x}_{%3Ck}\\textit{c\\,}\\underline{x}_k)]\" src=\"http://www.codecogs.com/png.latex?\\sum_{x_k}[u_k(\\.{y}\\.{x}_{%3Ck}\\textit{nc\\,}\\,\\underline{x}_k)M(\\.{y}\\.{x}_{%3Ck}\\textit{nc\\,}\\underline{x}_k)-u_k(\\.{y}\\.{x}_{%3Ck}\\textit{c\\,}\\underline{x}_k)M(\\.{y}\\.{x}_{%3Ck}\\textit{c\\,}\\underline{x}_k)]\" alt=\"\" align=\"bottom\" /></p>\n<p>is strictly less than</p>\n<p><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"(m_k-k)U_{\\max}-\\sum_{x_k}\\max_{y_{k+1}}\\dots\\sum_{x_{m_k}}[u_k(\\.{y}\\.{x}_{%3Ck}\\textit{nc\\,}\\underline{x}_ky\\underline{x}_{k+1})+\\dots\" src=\"http://www.codecogs.com/png.latex?(m_k-k)U_{\\max}-\\sum_{x_k}\\max_{y_{k+1}}\\dots\\sum_{x_{m_k}}[u_k(\\.{y}\\.{x}_{%3Ck}\\textit{nc\\,}\\underline{x}_ky\\underline{x}_{k+1})+\\dots\" alt=\"\" align=\"bottom\" /></p>\n<p><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\dots+u_k(\\.{y}\\.{x}_{%3Ck}\\textit{nc\\,}\\underline{x}_ky\\underline{x}_{k+1:m_k})]M(\\.{y}\\.{x}_{%3Ck}\\textit{nc\\,}\\underline{x}_ky\\underline{x}_{k+1:m_k}).\" src=\"http://www.codecogs.com/png.latex?\\dots+u_k(\\.{y}\\.{x}_{%3Ck}\\textit{nc\\,}\\underline{x}_ky\\underline{x}_{k+1:m_k})]M(\\.{y}\\.{x}_{%3Ck}\\textit{nc\\,}\\underline{x}_ky\\underline{x}_{k+1:m_k}).\" alt=\"\" align=\"bottom\" /></p>\n<p>This seems quite analogous to humans, who sometimes tend to choose maximal bliss over future optimization power, especially if the optimization opportunities are meager anyhow. Many people do seem to choose their goals so as to maximize the happiness felt by achieving them at least some of the time; this is also advice that I have frequently encountered in self-help literature, e.g. <a title=\"Procrastination\" href=\"/lw/3w3/how_to_beat_procrastination/\">here</a>.&nbsp;Agent 3 will definitely change, as it only evaluates situations using its final utility function.</p>\n<p>Comparing the three proposed agents, we notice that Agent 1 is dynamically inconsistent: it will optimize for future opportunities, that it predictably will not take later. Agent 3 on the other hand will wirehead whenever possible (and we can reasonably assume that opportunities to do so will exist in even moderately complex environments). This leaves us with Agent model 2 and I invite everyone to point out its flaws.</p>\n<p><a href=\"#re:Notation\">[1]</a> <a name=\"Notation\"></a>Dotted actions/ perceptions, like&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\.{y}_i\" src=\"http://www.codecogs.com/png.latex?\\.{y}_i\" alt=\"\" align=\"bottom\" />&nbsp;denote past events, underlined perceptions&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\underline{x}_i\" src=\"http://www.codecogs.com/png.latex?\\underline{x}_i\" alt=\"\" align=\"bottom\" />&nbsp;denote random variables to be observed at future times.</p>\n<p><a href=\"#re:Hyperreals\">[2]</a>&nbsp;<a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Fr%2FAnja-drafts%2Flw%2Ffeo%2Funiversal_agents_and_utility_functions%2F%23Hyperreals&amp;v=1&amp;libid=1352865529158&amp;out=http%3A%2F%2Fwww.nickbostrom.com%2Fethics%2Finfinite.pdf&amp;ref=http%3A%2F%2Flesswrong.com%2Fr%2FAnja-drafts%2Fedit%2Ffeo&amp;title=Universal%20agents%20and%20utility%20functions%20-%20Drafts%20for%20Anja&amp;txt=Bostrom%20(2011)&amp;jsonp=vglnk_jsonp_13528655488862\">Bostrom (2011)</a> proposes<a name=\"Hyperreals\"></a>&nbsp;using <a title=\"Hyperreals\" href=\"http://en.wikipedia.org/wiki/Hyperreal_number\">hyperreal numbers</a>, which rely heavily on the axiom of choice for the ultrafilter to be used and I don't see how those could be implemented.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"TiEFKWDvD3jsKumDx": 1, "HAFdXkW4YW4KRe2Gx": 1, "sYm3HiWcfZvrGu3ui": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "q3mZNmvqBtnG2nQre", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 43, "extendedScore": null, "score": 1.0331696883098972e-06, "legacy": true, "legacyId": "19968", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I'm Anja Heinisch, the new visiting fellow at SI. I've been researching replacing AIXI's reward system with a proper utility function. Here I will describe my AIXI+utility function model, address concerns about restricting the model to bounded or finite utility, and analyze some of the implications of modifiable utility functions, e.g. <a title=\"wireheading\" href=\"http://wiki.lesswrong.com/wiki/Wireheading\">wireheading</a> and <a title=\"dynamic inconsistency\" href=\"http://en.wikipedia.org/wiki/Dynamic_inconsistency\">dynamic consistency</a>. Comments, questions and advice (especially about related research and material) will be highly appreciated.</p>\n<h3 id=\"Introduction_to_AIXI\">Introduction to AIXI</h3>\n<p><a title=\"AIXI gentle\" href=\"http://www.hutter1.net/ai/aixigentle.pdf\">Marcus Hutter's (2003)</a>&nbsp;universal agent AIXI &nbsp;addresses the problem of rational action in a (partially) unknown computable universe, given infinite computing power and a halting oracle.&nbsp;The agent interacts with its environment in discrete time cycles, producing an action-perception sequence&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"y_1x_1y_2x_2\\dots\" src=\"http://www.codecogs.com/png.latex?y_1x_1y_2x_2\\dots\" alt=\"\" align=\"bottom\">&nbsp;with actions (agent outputs) &nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"y_i\" src=\"http://www.codecogs.com/png.latex?y_i\" alt=\"\" align=\"bottom\">&nbsp;and perceptions (environment outputs) &nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"x_i\" src=\"http://www.codecogs.com/png.latex?x_i\" alt=\"\" align=\"bottom\">&nbsp;chosen from finite sets&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\mathcal{Y}\" src=\"http://www.codecogs.com/png.latex?\\mathcal{Y}\" alt=\"\" align=\"bottom\">&nbsp;and&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\mathcal{X}\" src=\"http://www.codecogs.com/png.latex?\\mathcal{X}\" alt=\"\" align=\"bottom\">.&nbsp;The perceptions are pairs&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"x_i=(o_i,r_i)\" src=\"http://www.codecogs.com/png.latex?x_i=(o_i,r_i)\" alt=\"\" align=\"bottom\">, where&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"o_i\" src=\"http://www.codecogs.com/png.latex?o_i\" alt=\"\" align=\"bottom\">&nbsp;is the observation<em>&nbsp;</em>part and&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"r_i\" src=\"http://www.codecogs.com/png.latex?r_i\" alt=\"\" align=\"bottom\">&nbsp;denotes a reward<em>.</em>&nbsp;At time k the agent chooses its next action&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\.{y}_k\" src=\"http://www.codecogs.com/png.latex?\\.{y}_k\" alt=\"\" align=\"bottom\">&nbsp;according to the expectimax principle:</p>\n<p><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\.{y}_k=\\textrm{arg}\\max_{y_k}\\sum_{x_k}\\max_{y_{k+1}}\\sum_{x_{k+1}}\\dots\\max_{y_{m_k}}\\sum_{x_{m_k}}(r_k+\\dots+r_{m_k})M(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k}).\" src=\"http://www.codecogs.com/png.latex?\\.{y}_k=\\textrm{arg}\\max_{y_k}\\sum_{x_k}\\max_{y_{k+1}}\\sum_{x_{k+1}}\\dots\\max_{y_{m_k}}\\sum_{x_{m_k}}(r_k+\\dots+r_{m_k})M(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k}).\" alt=\"\" align=\"bottom\"></p>\n<p>Here M denotes the updated <a title=\"Solomonoff tutorial\" href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/\">Solomonoff prior</a> summing over all programs&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"q\\in Q_k\" src=\"http://www.codecogs.com/png.latex?q\\in%20Q_k\" alt=\"\" align=\"bottom\">&nbsp;that are consistent with the history<em> &nbsp;</em><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\.{y}\\.{x}_{<k}\" src=\"http://www.codecogs.com/png.latex?\\.{y}\\.{x}_{%3Ck}\" alt=\"\" align=\"bottom\"><sup><a href=\"#Notation\">[1]</a><a name=\"re:Notation\"></a></sup>&nbsp;and which will, when run on the universal Turing machine T with successive inputs&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"y_{k:m_k}\" src=\"http://www.codecogs.com/png.latex?y_{k:m_k}\" alt=\"\" align=\"bottom\">, compute outputs&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\underline{x}_{k:m_k}\" src=\"http://www.codecogs.com/png.latex?\\underline{x}_{k:m_k}\" alt=\"\" align=\"bottom\">, i.e.</p>\n<h3><img style=\"font-family: 'Times New Roman'; font-size: medium; font-weight: normal;\" title=\"M(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k})=\\sum_{q\\in%20Q_k,\\:%20T(q,y_{k:m_k})=x_{k:m_k}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!2^{-\\ell(q)}\" src=\"http://www.codecogs.com/png.latex?M(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k})=\\sum_{q\\in%20Q_k,\\:%20T(q,y_{k:m_k})=x_{k:m_k}}\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!2^{-\\ell(q)}\" alt=\"\" align=\"bottom\"></h3>\n<p>AIXI is a <a title=\"Cartesian dualism\" href=\"http://en.wikipedia.org/wiki/Dualism_(philosophy_of_mind)\">dualistic</a>&nbsp;framework in the sense that the algorithm that constitutes the agent is not part of the environment, since it is not computable.&nbsp;Even considering that any running implementation of AIXI would have to be computable, AIXI accurately simulating AIXI accurately simulating AIXI ad infinitem doesn't really seem feasible. Potential consequences of this separation of mind and matter include <a title=\"Anvil problem\" href=\"http://wiki.lesswrong.com/wiki/Anvil_problem\">difficulties</a>&nbsp;the agent may have <a title=\"Existential despair\" href=\"/lw/8qy/aixi_and_existential_despair/\">predicting the effects of its actions</a> on the world.&nbsp;</p>\n<h3 id=\"Utility_vs_rewards\">Utility vs rewards</h3>\n<p>So, why is it a bad idea to work with a reward system? Say the AIXI agent is rewarded whenever a human called Bob pushes a button. Then a sufficiently smart AIXI will figure out that instead of furthering Bob\u2019s goals it can also threaten or deceive Bob into pushing the button, or get another human to replace Bob. On the other hand, if the reward is computed in a little box somewhere and then displayed on a screen, it might still be possible to reprogram the box or find a side channel attack. Intuitively you probably&nbsp;wouldn't&nbsp;even blame the agent for doing that -- people try to game the system all the time.&nbsp;</p>\n<p>You can visualize AIXI's computation as maximizing bars displayed on this screen; the agent is unable to connect the bars to any pattern in the environment, they are just there. It wants them to be as high as possible and it will utilize any means at its disposal. For a more detailed analysis of the problems arising through reinforcement learning, see <a title=\"Value Learning\" href=\"http://www.danieldewey.net/learning-what-to-value.pdf\">Dewey (2011)</a>.</p>\n<p>Is there a way to bind the optimization process to actual patterns in the environment? To design a framework in which the screen informs the agent about the patterns it should optimize for? The answer is, yes, we can just define a utility function</p>\n<p><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"U:\\bigcup_{m_k=1}^\\infty(\\mathcal{Y}\\times\\mathcal{X})^{m_k}\\rightarrow\\mathbb{R}\" src=\"http://www.codecogs.com/png.latex?U:\\bigcup_{m_k=1}^\\infty(\\mathcal{Y}\\times\\mathcal{X})^{m_k}\\rightarrow\\mathbb{R}\" alt=\"\" align=\"bottom\"></p>\n<p>that assigns a value&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"U(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k})\" src=\"http://www.codecogs.com/png.latex?U(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k})\" alt=\"\" align=\"bottom\">&nbsp;to every possible future history&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k}\" src=\"http://www.codecogs.com/png.latex?\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k}\" alt=\"\" align=\"bottom\">&nbsp;and use it to replace the reward system in the agent specification:</p>\n<p><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"{y}_k=\\textrm{arg}\\max_{y_k}\\sum_{x_k}\\max_{y_{k+1}}\\sum_{x_{k+1}}\\dots\\max_{y_{m_k}}\\sum_{x_{m_k}}U(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k})M(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k}).\" src=\"http://www.codecogs.com/png.latex?{y}_k=\\textrm{arg}\\max_{y_k}\\sum_{x_k}\\max_{y_{k+1}}\\sum_{x_{k+1}}\\dots\\max_{y_{m_k}}\\sum_{x_{m_k}}U(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k})M(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k}).\" alt=\"\" align=\"bottom\"></p>\n<p>When I say \"we can just define\" I am actually referring to the <em>really hard</em> question of how to recognize and describe the patterns we value in the universe. Contrasted with the necessity to specify rewards in the original AIXI framework, this is a strictly harder problem, because the utility function has to be known ahead of time and the reward system can always be represented in the framework of utility functions by setting</p>\n<p><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"U(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k})=r_k+\\dots+r_{m_k}.\" src=\"http://www.codecogs.com/png.latex?U(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k})=r_k+\\dots+r_{m_k}.\" alt=\"\" align=\"bottom\"></p>\n<p>For the same reasons, this is also a strictly safer approach.</p>\n<h3 id=\"Infinite_utility\">Infinite utility</h3>\n<p style=\"font-size: small; font-weight: normal;\">The original AIXI framework must necessarily place upper and lower bound on the rewards that are achievable, because the rewards are part of the perceptions and&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\mathcal{X}\" src=\"http://www.codecogs.com/png.latex?\\mathcal{X}\" alt=\"\" align=\"bottom\">&nbsp;is finite. The utility function approach does not have this problem, as the expected utility&nbsp;</p>\n<p style=\"font-size: small; font-weight: normal;\"><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\sum_{x_i}u(\\.{y}\\.{x}_{<k}y\\underline{x}_{k:i})M(\\.{y}\\.{x}_{<k}y\\underline{x}_{k:i})\" src=\"http://www.codecogs.com/png.latex?\\sum_{x_i}u(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:i})M(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:i})\" alt=\"\" align=\"bottom\"></p>\n<p style=\"font-size: small; font-weight: normal;\">is always finite as long as we stick to a finite set of possible perceptions, even if the utility function is not bounded. Relaxing this constraint and allowing&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\mathcal{X}\" src=\"http://www.codecogs.com/png.latex?\\mathcal{X}\" alt=\"\" align=\"bottom\">&nbsp;to be infinite and the utility to be unbounded creates divergence of expected utility (for a proof see&nbsp;<a title=\"Divergence\" href=\"http://arxiv.org/PS_cache/arxiv/pdf/0712/0712.4318v1.pdf\">d</a><a title=\"Divergence\" href=\"http://arxiv.org/PS_cache/arxiv/pdf/0712/0712.4318v1.pdf\">e Blanc 2008</a>).&nbsp;This closely corresponds to the question of how to be a consequentialist in an infinite universe, discussed by&nbsp;<a title=\"Infinite Ethics\" href=\"http://www.nickbostrom.com/ethics/infinite.pdf\">Bostrom (2011)</a>. The underlying problem here is that (using the&nbsp;<a title=\"infinities\" href=\"http://wiki.lesswrong.com/wiki/Quick_reference_guide_to_the_infinite\">standard approach to infinities</a>) these expected utilities will become incomparable. One possible solution to this problem could be to use a larger subfield than&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\mathbb{R}\" src=\"http://www.codecogs.com/png.latex?\\mathbb{R}\" alt=\"\" align=\"bottom\">&nbsp;of the&nbsp;<a title=\"Surreals\" href=\"http://en.wikipedia.org/wiki/Surreal_number\">surreal numbers</a>, my favorite<sup><a href=\"#Hyperreals\">[2]</a><a name=\"re:Hyperreals\"></a></sup>&nbsp;so far being the&nbsp;<a title=\"Levi-Civita\" href=\"http://en.wikipedia.org/wiki/Levi-Civita_field\">Levi-Civita field</a>&nbsp;generated by the infinitesimal&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\varepsilon\" src=\"http://www.codecogs.com/png.latex?\\varepsilon\" alt=\"\" align=\"bottom\">:</p>\n<p style=\"font-size: small; font-weight: normal;\"><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\mathcal{R}=\\{\\sum_{q\\in\\mathbb{Q}}a_q\\varepsilon^q|a_q\\in\\mathbb{R}\\},\" src=\"http://www.codecogs.com/png.latex?\\mathcal{R}=\\{\\sum_{q\\in\\mathbb{Q}}a_q\\varepsilon^q|a_q\\in\\mathbb{R}\\},\" alt=\"\" align=\"bottom\"></p>\n<p style=\"font-size: small; font-weight: normal;\">with the usual power-series addition and multiplication. Levi-Civita numbers can be written and approximated as&nbsp;</p>\n<p style=\"font-size: small; font-weight: normal;\"><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\lim_{n\\rightarrow \\infty}\\sum_{i=0}^n a_{q_i}\\varepsilon^{q_i}\" src=\"http://www.codecogs.com/png.latex?\\lim_{n\\rightarrow%20\\infty}\\sum_{i=0}^n%20a_{q_i}\\varepsilon^{q_i}\" alt=\"\" align=\"bottom\"></p>\n<p style=\"font-size: small; font-weight: normal;\">(see&nbsp;<a title=\"Berz\" href=\"http://bt.pa.msu.edu/pub/papers/rscrptsf/rscrptsf.pdf\">Berz 1996</a>), which makes them suitable for representation on a computer using floating point arithmetic. If we allow the range of our utility function to be&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\mathcal{R}\" src=\"http://www.codecogs.com/png.latex?\\mathcal{R}\" alt=\"\" align=\"bottom\">, we gain the possibility of generalizing the framework to work with an infinite set of possible perceptions, therefore allowing for continuous parameters. We also allow for a much broader set of utility functions, no longer excluding the assignment of infinite (or infinitesimal) utility to a single event. I recently met someone who argued convincingly that his (ideal) utility function assigns infinite negative utility to every time instance that he is not alive, therefore making him prefer life to any finite but huge amount of suffering.</p>\n<p style=\"font-size: small; font-weight: normal;\">Note that finiteness of&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\mathcal{Y}\" src=\"http://www.codecogs.com/png.latex?\\mathcal{Y}\" alt=\"\" align=\"bottom\">&nbsp;is still needed to guarantee the existence of actions with maximal expected utility, and the finite (but dynamic) horizon&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"m_k\" src=\"http://www.codecogs.com/png.latex?m_k\" alt=\"\" align=\"bottom\">&nbsp;remains a very problematic assumption, as described in <a title=\"Horizon problem\" href=\"http://www.vetta.org/documents/Machine_Super_Intelligence.pdf\">Legg (2008)</a>.</p>\n<h3 id=\"Modifiable_utility_functions\">Modifiable utility functions</h3>\n<p>Any implementable approximation of AIXI implies a weakening of the underlying dualism. Now the agent's hardware is part of the environment and at least in the case of a powerful agent, it can no longer afford to neglect the effect its actions may have on its source code and data. One question that has been asked is whether AIXI can&nbsp;<a title=\"AIXI &amp; anvils\" href=\"/lw/8rl/would_aixi_protect_itself/\">protect itself from harm</a>. <a href=\"http://arxiv.org/pdf/1111.3934v2\">Hibbard (2012)</a> shows that an agent similar to the one described above, equipped with the ability to modify its policy responsible for choosing future actions, would not do so, given that it starts out with the (meta-)policy to always use the optimal policy, and the additional constraint to change only if that leads to a strict improvement. <a title=\"Delusion Box\" href=\"http://www.idsia.ch/~ring/Ring%2COrseau%3B%20Delusion%2C%20Survival%2C%20and%20Intelligent%20Agents%2C%20AGI%202011.pdf\">Ring and Orseau (2011)</a> study under which circumstances a universal agent would try to tamper with the sensory information it receives. They introduce the concept of a&nbsp;delusion box, a device that filters and distorts the perception data before it is written into the part of the memory that is read during the calculation of utility.&nbsp;</p>\n<p>A further complication to take into account is the possibility that the part of memory that contains the utility function may get rewritten, either by accident, by deliberate choice (programmers trying to correct a mistake), or in an attempt to wirehead. To analyze this further we will now consider what can happen if the screen flashes different goals in different time cycles. Let&nbsp;</p>\n<p><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"u_k:\\bigcup_{i=1}^\\infty(\\mathcal{Y}\\times\\mathcal{X})^i\\rightarrow \\mathbb{R}\" src=\"http://www.codecogs.com/png.latex?u_k:\\bigcup_{i=1}^\\infty(\\mathcal{Y}\\times\\mathcal{X})^i\\rightarrow%20\\mathbb{R}\" alt=\"\" align=\"bottom\"></p>\n<p>denote the utility function the agent will have at time k.</p>\n<p>Even though we will only analyze instances in which the agent knows at time k, which utility function&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"u_i\" src=\"http://www.codecogs.com/png.latex?u_i\" alt=\"\" align=\"bottom\">&nbsp;it will have at future times&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"k\\leq i\\leq m_k\" src=\"http://www.codecogs.com/png.latex?k\\leq%20i\\leq%20m_k\" alt=\"\" align=\"bottom\">&nbsp;(possibly depending on the actions&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"y_{k:i-1}\" src=\"http://www.codecogs.com/png.latex?y_{k:i-1}\" alt=\"\" align=\"bottom\">&nbsp;before that), we note that for every fixed future history&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\.{y}\\.{x}_{<k}y\\underline{x}_{k:i}\" src=\"http://www.codecogs.com/png.latex?\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:i}\" alt=\"\" align=\"bottom\">&nbsp;the agent knows the utility function&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"u_i\" src=\"http://www.codecogs.com/png.latex?u_i\" alt=\"\" align=\"bottom\">&nbsp;that is displayed on the screen because the screen is part of its perception data&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\underline{x}_i\" src=\"http://www.codecogs.com/png.latex?\\underline{x}_i\" alt=\"\" align=\"bottom\">.</p>\n<p>This leads to three different agent models worthy of further investigation:</p>\n<ul>\n<li><strong>Agent 1</strong> will optimize for the goals that are displayed on the screen right now and act as if it would continue to do so in the future. We describe this with the utility function &nbsp;&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"U_1(\\.{y}\\.{x}_{<k}y\\underline{x}_{k:m_k})=u_{k}(\\.{y}\\.{x}_{<k}y\\underline{x}_k)+u_{k}}(\\.{y}\\.{x}_{<k}y\\underline{x}_{k:k+1})+\\dots+u_{k}(\\.{y}\\.{x}_{<k}y\\underline{x}_{k:m_k}).\" src=\"http://www.codecogs.com/png.latex?U_1(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k})=u_{k}(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_k)+u_{k}}(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:k+1})+\\dots+u_{k}(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k}).\" alt=\"\" align=\"bottom\"></li>\n<li><strong>Agent 2</strong> will try to anticipate future changes to its utility function and maximize the utility it experiences at every time cycle as shown on the screen at that time. This is captured by<span style=\"font-family: 'Times New Roman'; font-size: small;\">&nbsp;</span><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"U_2(\\.{y}\\.{x}_{<k}y\\underline{x}_{k:m_k})=u_{k}(\\.{y}\\.{x}_{<k}y\\underline{x}_k)+u_{k+1}}(\\.{y}\\.{x}_{<k}y\\underline{x}_{k:k+1})+\\dots+u_{m_k}(\\.{y}\\.{x}_{<k}y\\underline{x}_{k:m_k}).\" src=\"http://www.codecogs.com/png.latex?U_2(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k})=u_{k}(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_k)+u_{k+1}}(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:k+1})+\\dots+u_{m_k}(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k}).\" alt=\"\" align=\"bottom\"></li>\n<li><strong>Agent 3</strong> will, at time k, try to maximize the utility it derives in hindsight, displayed on the screen at the time horizon &nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"U_3(\\.{y}\\.{x}_{<k}y\\underline{x}_{k:m_k})=u_{m_k}(\\.{y}\\.{x}_{<k}y\\underline{x}_k)+u_{m_k}}(\\.{y}\\.{x}_{<k}y\\underline{x}_{k:k+1})+\\dots+u_{m_k}(\\.{y}\\.{x}_{<k}y\\underline{x}_{k:m_k}).\" src=\"http://www.codecogs.com/png.latex?U_3(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k})=u_{m_k}(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_k)+u_{m_k}}(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:k+1})+\\dots+u_{m_k}(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:m_k}).\" alt=\"\" align=\"bottom\"></li>\n</ul>\n<p>Of course arbitrary mixtures of these are possible.</p>\n<p>The type of wireheading that is of interest here is captured by the Simpleton Gambit described by&nbsp;<a title=\"Simpleton\" href=\"http://www.idsia.ch/~ring/Orseau,Ring;Self-modification%20and%20Mortality%20in%20Artificial%20Agents,%20AGI%202011.pdf\">Orseau and Ring (2011)</a>, a Faustian deal that offers the agent maximal utility in exchange for its willingness to be turned into a Simpleton that always takes the same default action at all future times.&nbsp;We will first consider a simplified version of this scenario: The Simpleton future, where the agent knows for certain that it will be turned into a Simpleton at time k+1, no matter what it does in the remaining time cycle. Assume that for all possible action-perception combinations the utility given by the current utility function is not maximal, i.e. &nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"u_k(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:i})%3CU_\\max\" src=\"http://www.codecogs.com/png.latex?u_k(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:i})%3CU_\\max\" alt=\"\" align=\"bottom\">&nbsp;holds for all&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"y\\underline{x}_{k:i}\" src=\"http://www.codecogs.com/png.latex?y\\underline{x}_{k:i}\" alt=\"\" align=\"bottom\">.&nbsp;Assume further that&nbsp;the agents actions influence the future outcomes, at least from its current perspective. That is,&nbsp;for all&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"k\\leq i\\leq m_k\" src=\"http://www.codecogs.com/png.latex?k\\leq%20i\\leq%20m_k\" alt=\"\" align=\"bottom\">&nbsp;there&nbsp;exist &nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"y'\\underline{x}'_{k:i}\\neq%20y\\underline{x}_{k:i}\" src=\"http://www.codecogs.com/png.latex?y'\\underline{x}'_{k:i}\\neq%20y\\underline{x}_{k:i}\" alt=\"\" align=\"bottom\">&nbsp;with&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"u_k(\\.{y}\\.{x}_{%3Ck}y'\\underline{x}'_{k:i})\\neq%20u_k(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:i})\" src=\"http://www.codecogs.com/png.latex?u_k(\\.{y}\\.{x}_{%3Ck}y'\\underline{x}'_{k:i})\\neq%20u_k(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:i})\" alt=\"\" align=\"bottom\">.&nbsp;Let&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"u_{k+1}\" src=\"http://www.codecogs.com/png.latex?u_{k+1}\" alt=\"\" align=\"bottom\">&nbsp;be the Simpleton utility function, assigning equal but maximal utility&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"u_k(\\.{y}\\.{x}_{<k}y\\underline{x}_{k:i})=U_\\max\" src=\"http://www.codecogs.com/png.latex?u_k(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_{k:i})=U_\\max\" alt=\"\" align=\"bottom\">&nbsp;to all possible futures. While Agent 1 will optimize as before, not adapting its behavior to the knowledge that its utility function will change, Agent 3 will be paralyzed, having to rely on whatever method its implementation uses to break ties. Agent 2 on the other hand will try to maximize only the utility&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"u_k(\\.{y}\\.{x}_{<k}y\\underline{x}_k)\" src=\"http://www.codecogs.com/png.latex?u_k(\\.{y}\\.{x}_{%3Ck}y\\underline{x}_k)\" alt=\"\" align=\"bottom\">.</p>\n<p>Now consider the actual Simpleton Gambit: At time k the agent gets to choose between changing,<span style=\"font-family: 'Times New Roman'; font-size: small;\">&nbsp;</span><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"y_k=\\textit{c}\" src=\"http://www.codecogs.com/png.latex?y_k=\\textit{c}\" alt=\"\" align=\"bottom\">, resulting in&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"u_{k+i}\\equiv U_\\max\" src=\"http://www.codecogs.com/png.latex?u_{k+i}\\equiv%20U_\\max\" alt=\"\" align=\"bottom\">&nbsp;and&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"y_k=\\textit{nc}\" src=\"http://www.codecogs.com/png.latex?y_k=\\textit{nc}\" alt=\"\" align=\"bottom\">&nbsp;(not changing),&nbsp;leading to&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"u_{k+i}\\equiv u_k\" src=\"http://www.codecogs.com/png.latex?u_{k+i}\\equiv%20u_k\" alt=\"\" align=\"bottom\">&nbsp;for all&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"k+1\\leq i\\leq m_k\" src=\"http://www.codecogs.com/png.latex?k+1\\leq%20i\\leq%20m_k\" alt=\"\" align=\"bottom\">. We assume that<span style=\"font-family: 'Times New Roman'; font-size: small;\">&nbsp;</span><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\.{y}_k\" src=\"http://www.codecogs.com/png.latex?\\.{y}_k\" alt=\"\" align=\"bottom\">&nbsp;has no further effects on the environment. As before, Agent 1 will optimize for business as usual, whether or not it chooses to change depends entirely on whether the screen specifically mentions the memory pointer to the utility function or not.</p>\n<p>Agent 2 will change if and only if the utility of changing compared to not changing according to what the screen currently says&nbsp;is strictly smaller than&nbsp;the comparative advantage of always having maximal utility in the future. That is,</p>\n<p><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\sum_{x_k}[u_k(\\.{y}\\.{x}_{%3Ck}\\textit{nc\\,}\\,\\underline{x}_k)M(\\.{y}\\.{x}_{%3Ck}\\textit{nc\\,}\\underline{x}_k)-u_k(\\.{y}\\.{x}_{%3Ck}\\textit{c\\,}\\underline{x}_k)M(\\.{y}\\.{x}_{%3Ck}\\textit{c\\,}\\underline{x}_k)]\" src=\"http://www.codecogs.com/png.latex?\\sum_{x_k}[u_k(\\.{y}\\.{x}_{%3Ck}\\textit{nc\\,}\\,\\underline{x}_k)M(\\.{y}\\.{x}_{%3Ck}\\textit{nc\\,}\\underline{x}_k)-u_k(\\.{y}\\.{x}_{%3Ck}\\textit{c\\,}\\underline{x}_k)M(\\.{y}\\.{x}_{%3Ck}\\textit{c\\,}\\underline{x}_k)]\" alt=\"\" align=\"bottom\"></p>\n<p>is strictly less than</p>\n<p><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"(m_k-k)U_{\\max}-\\sum_{x_k}\\max_{y_{k+1}}\\dots\\sum_{x_{m_k}}[u_k(\\.{y}\\.{x}_{%3Ck}\\textit{nc\\,}\\underline{x}_ky\\underline{x}_{k+1})+\\dots\" src=\"http://www.codecogs.com/png.latex?(m_k-k)U_{\\max}-\\sum_{x_k}\\max_{y_{k+1}}\\dots\\sum_{x_{m_k}}[u_k(\\.{y}\\.{x}_{%3Ck}\\textit{nc\\,}\\underline{x}_ky\\underline{x}_{k+1})+\\dots\" alt=\"\" align=\"bottom\"></p>\n<p><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\dots+u_k(\\.{y}\\.{x}_{%3Ck}\\textit{nc\\,}\\underline{x}_ky\\underline{x}_{k+1:m_k})]M(\\.{y}\\.{x}_{%3Ck}\\textit{nc\\,}\\underline{x}_ky\\underline{x}_{k+1:m_k}).\" src=\"http://www.codecogs.com/png.latex?\\dots+u_k(\\.{y}\\.{x}_{%3Ck}\\textit{nc\\,}\\underline{x}_ky\\underline{x}_{k+1:m_k})]M(\\.{y}\\.{x}_{%3Ck}\\textit{nc\\,}\\underline{x}_ky\\underline{x}_{k+1:m_k}).\" alt=\"\" align=\"bottom\"></p>\n<p>This seems quite analogous to humans, who sometimes tend to choose maximal bliss over future optimization power, especially if the optimization opportunities are meager anyhow. Many people do seem to choose their goals so as to maximize the happiness felt by achieving them at least some of the time; this is also advice that I have frequently encountered in self-help literature, e.g. <a title=\"Procrastination\" href=\"/lw/3w3/how_to_beat_procrastination/\">here</a>.&nbsp;Agent 3 will definitely change, as it only evaluates situations using its final utility function.</p>\n<p>Comparing the three proposed agents, we notice that Agent 1 is dynamically inconsistent: it will optimize for future opportunities, that it predictably will not take later. Agent 3 on the other hand will wirehead whenever possible (and we can reasonably assume that opportunities to do so will exist in even moderately complex environments). This leaves us with Agent model 2 and I invite everyone to point out its flaws.</p>\n<p><a href=\"#re:Notation\">[1]</a> <a name=\"Notation\"></a>Dotted actions/ perceptions, like&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\.{y}_i\" src=\"http://www.codecogs.com/png.latex?\\.{y}_i\" alt=\"\" align=\"bottom\">&nbsp;denote past events, underlined perceptions&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\underline{x}_i\" src=\"http://www.codecogs.com/png.latex?\\underline{x}_i\" alt=\"\" align=\"bottom\">&nbsp;denote random variables to be observed at future times.</p>\n<p><a href=\"#re:Hyperreals\">[2]</a>&nbsp;<a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Fr%2FAnja-drafts%2Flw%2Ffeo%2Funiversal_agents_and_utility_functions%2F%23Hyperreals&amp;v=1&amp;libid=1352865529158&amp;out=http%3A%2F%2Fwww.nickbostrom.com%2Fethics%2Finfinite.pdf&amp;ref=http%3A%2F%2Flesswrong.com%2Fr%2FAnja-drafts%2Fedit%2Ffeo&amp;title=Universal%20agents%20and%20utility%20functions%20-%20Drafts%20for%20Anja&amp;txt=Bostrom%20(2011)&amp;jsonp=vglnk_jsonp_13528655488862\">Bostrom (2011)</a> proposes<a name=\"Hyperreals\"></a>&nbsp;using <a title=\"Hyperreals\" href=\"http://en.wikipedia.org/wiki/Hyperreal_number\">hyperreal numbers</a>, which rely heavily on the axiom of choice for the ultrafilter to be used and I don't see how those could be implemented.</p>", "sections": [{"title": "Introduction to AIXI", "anchor": "Introduction_to_AIXI", "level": 1}, {"title": "Utility vs rewards", "anchor": "Utility_vs_rewards", "level": 1}, {"title": "Infinite utility", "anchor": "Infinite_utility", "level": 1}, {"title": "Modifiable utility functions", "anchor": "Modifiable_utility_functions", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "38 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Kyc5dFDzBg4WccrbK", "AfbY36m8TDYZBjHcu", "LDbnZDRidKNKQBFyY", "RWo4LwFzpHNQCTcYt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-14T06:47:03.239Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Brain Emulation and Hard Takeoff", "slug": "seq-rerun-brain-emulation-and-hard-takeoff", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:39.299Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xMBZTQtMSvowk3H2k/seq-rerun-brain-emulation-and-hard-takeoff", "pageUrlRelative": "/posts/xMBZTQtMSvowk3H2k/seq-rerun-brain-emulation-and-hard-takeoff", "linkUrl": "https://www.lesswrong.com/posts/xMBZTQtMSvowk3H2k/seq-rerun-brain-emulation-and-hard-takeoff", "postedAtFormatted": "Wednesday, November 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Brain%20Emulation%20and%20Hard%20Takeoff&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Brain%20Emulation%20and%20Hard%20Takeoff%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxMBZTQtMSvowk3H2k%2Fseq-rerun-brain-emulation-and-hard-takeoff%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Brain%20Emulation%20and%20Hard%20Takeoff%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxMBZTQtMSvowk3H2k%2Fseq-rerun-brain-emulation-and-hard-takeoff", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxMBZTQtMSvowk3H2k%2Fseq-rerun-brain-emulation-and-hard-takeoff", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 191, "htmlBody": "<p>Today's post, <a href=\"http://www.overcomingbias.com/2008/11/brain-emulation.html\">Brain Emulation and Hard Takeoff</a> was originally published on November 22, 2008.  A summary:</p>\n<p>&nbsp;</p>\n<blockquote>A project of bots could start an intelligence explosion once it got fast enough to start making bots of the engineers working on it, that would be able to operate at greater than human speed. Such a system could also devise a lot of innovative ways to acquire more resources or capital.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fgs/seq_rerun_emulations_go_foom/\">Emulations Go Foom</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xMBZTQtMSvowk3H2k", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0332595231586024e-06, "legacy": true, "legacyId": "20065", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["paZDEWb4PSS2ftGHP", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-14T08:46:44.807Z", "modifiedAt": null, "url": null, "title": "Instrumental rationality for overcoming disability and lifestyle failure (a specific case)", "slug": "instrumental-rationality-for-overcoming-disability-and", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:57.032Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CAE_Jones", "createdAt": "2012-11-01T13:23:59.782Z", "isAdmin": false, "displayName": "CAE_Jones"}, "userId": "kyySNW7co4pkWXPTP", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MktvZ4gk3Fsa2hRx7/instrumental-rationality-for-overcoming-disability-and", "pageUrlRelative": "/posts/MktvZ4gk3Fsa2hRx7/instrumental-rationality-for-overcoming-disability-and", "linkUrl": "https://www.lesswrong.com/posts/MktvZ4gk3Fsa2hRx7/instrumental-rationality-for-overcoming-disability-and", "postedAtFormatted": "Wednesday, November 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Instrumental%20rationality%20for%20overcoming%20disability%20and%20lifestyle%20failure%20(a%20specific%20case)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInstrumental%20rationality%20for%20overcoming%20disability%20and%20lifestyle%20failure%20(a%20specific%20case)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMktvZ4gk3Fsa2hRx7%2Finstrumental-rationality-for-overcoming-disability-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Instrumental%20rationality%20for%20overcoming%20disability%20and%20lifestyle%20failure%20(a%20specific%20case)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMktvZ4gk3Fsa2hRx7%2Finstrumental-rationality-for-overcoming-disability-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMktvZ4gk3Fsa2hRx7%2Finstrumental-rationality-for-overcoming-disability-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1429, "htmlBody": "<p>I read things like \"Rationality should win!\", and I feel validated. That's basically what I've believed as far back as I can remember (and though my memory isn't as reliable as I'd like, I do think it is pretty decent, based on comparisons I've made with video evidence. Not that said video evidence includes much of me talking about my beliefs.).<br />&nbsp; <br />Yet, clearly, I am no master of rationality, given that my situation at present is not what I would define as having won. When my father was my age, he was employed, married (to his second wife, with whom he is still married), and I was three years old. It wasn't long after that when my sister was born and my dad started work for his father-in-law, whose business he eventually came to own and still runs for a viable profit today. He did set some goals for himself that he hasn't quite met--gaining enough self-sustaining wealth to retire at age 45 (he turned 45 in February 2012) and/or spend his days lounging at the beach without negative consequences--but considering how much money he's put into vacations to the beach, and that he's still able to support his business and family (my stepmother makes a non-negligible contribution to this), I'd call him much closer to successful than me.<br />&nbsp; <br />So, clearly I didn't pick up everything that made my father successful. I don't think dwelling on the differences will help much, but relevant is that I was born with no vision in one eye, and had some vision loss in the other a few years later, until it finally dropped to near-useless starting around age fourteen. (Incidentally, this is my excuse if I fail to catch any spelling errors in this post.) I was taught to believe strongly in the power of human intelligence, and that peer pressure is evil, and that education is the most important thing ever.<br />&nbsp; <br />By 2008, I'd discovered that most of this was extremely flawed, and it was too late to correct the damage that acting on my perceived value of these beliefs had caused (I hadn't realized the extent of that damage by then, but was beginning to pick up on some of it). I'd gotten into an expensive college because the best education possible was apparently important and expensive... yet between my vision, attention to what I thought of as rationality, and rejection of many social conventions, I was completely lacking the skills to get much out of the college experience other than some basic details on a few foreign cultures. After six years of that, I'm back with my parents, $30,000 in debt, unemployed, lacking in the social department to an extent that seems more uncurable than not, still require a French credit to receive a diploma that will in all likelyhood be of very little utility to me, with a serious defficit in skills necessary for independence. Oh, and have less than $400 that I can use ($90 may or may not be in a bank account I don't know how to access, $123.51 in paypal, and the rest in cash). I was recently reapproved for supplemental security income (which I don't expect to be any more than $400 per month), conditional on me living at the property my grandmother left me.<br />&nbsp; <br />I said all of that to ask: how do I fix these problems? I think I've gone on way too long with this post, but explaining some of the obstacles won't help me find solutions without goals, so I'll list some.<br />&nbsp; <br />* I have lived in the same town for 24 years, and in the same house for close to 20 of those, yet I cannot travel anywhere beyond our property on my own without serious risk (getting lost / injured / trespassing / doing accidental damage to someone else's property / etc). I have more mobility skills than my parents give me credit for (About a week and a half ago, I decided to go across the street to ask my stepmother about lunch, and my father said \"There's a road! You'll get run over!\". I have crossed more dangerous roads than that one unaided multiple times, though he wasn't witness to any of this.). I believe I could get to the store down the road from my grandmother's house unaided (getting back might be more difficult, though), but that's about it. Being able to travel independently seems likely to increase my abilities to accomplish other things tremendously, so this is a problem to be overcome.<br />* My financial situation is horrible. By my estimates, I could live on $500 per month, assuming I was efficient with food, electricity, etc, and still be able to afford internet access and possibly avoid incurring the wrath of whoever my student loans are paid off to (an understanding of my finances was never given priority before 2010, so I only vaguely know what's going on). I'd much rather have a bit more ($1000 a month would be spectacular, although if part of that is SSI I wouldn't be able to save more than $2000 at a time without losing it). I am led to believe that my employment opportunities are extremely limited (my region has handled the economic recession well, but my other issues seem likely to add to the difficulty). Actually finding local employment without being able to travel to locations independently and fill out application forms would be more than a little difficult, and I have a strong preference for something with a physical component that would make online work undesirable. (My programming skills are also less than spectacular; I've been able to develop accessible computer games and have actually turned a slight profit on that (by which I mean less than $300), but I don't believe I can program on the level that would get someone else to hire me. And if I could write on demand, I wouldn't have an outstanding French credit.). (Here's what the American Foundation for the Blind has to say on employment among disabled Americans: http://www.afb.org/section.aspx?SectionID=15&amp;SubTopicID=177 ).<br />* Health. I'm sure just living on my own would have a serious impact on my ability to adjust my diet for nutritional value (I find that I tend to eat whatever is easiest, which is usually horribly unhealthy). I can cook with a microwave, and am told that a slow-cooker/crockpot is easy to use. I'm more concerned about exercise, seeing as I can't safely go running or anything of the sort. (I also don't like treadmills, for some reason. I'd be ok with an eliptical, if I could acquire one and find somewhere to put it...).<br />* I have no experience with romantic relationships (I'm not even sure about strong platonic relationships, for that matter). I am possibly interested in changing this. As a hint at some almost-definitely-harmful things that I haven't been able to remove from my mind, the previous statement was far more painful to write than those before it (as in, if I wound up in a romantic relationship, I would absolutely dread discovery from my parents, knowing that the worst they'd do is make comments intended to be humorous rather than harmful.).<br />* I've had my creative endeavors (writing fiction, game development, etc) as side-goals for a while, but recently I've started to consider that making them higher priority is probably a good thing. The problem, other than combatting procrastination and other sorts of akrasia, is that I'm at a point where, to keep moving forward almost definitely requires funding. (I can't see well enough to do graphics, and haven't been able to find a decent way around this that doesn't involve paying someone to do graphics; I can't do all the voice-work I'd need, and volunteer voiceactors are horribly unreliable; sound libraries, software licenses, web hosting, etc). The goal is of course the creation of the products to my satisfaction, not the funding itself, but funding seems like a crucial upcoming step.<br /><br />Solving/accomplishing any of the above would be a significant victory, yet I feel extremely limited in my personal ability to do so. If we can call any one of those successfully resolved using methods promoted at LessWrong, I'd definitely try to provide as much evidence for the victory as possible, as all the questions about the utility of LW make substantiated claims of successes due to the methods of rationality seem valuable to the community.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MktvZ4gk3Fsa2hRx7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 13, "extendedScore": null, "score": 1.0333261484304423e-06, "legacy": true, "legacyId": "20073", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-14T12:55:16.910Z", "modifiedAt": null, "url": null, "title": "[LINK] Steven Landsburg \"Accounting for Numbers\" - response to EY's \"Logical Pinpointing\"", "slug": "link-steven-landsburg-accounting-for-numbers-response-to-ey", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:56.953Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "David_Gerard", "createdAt": "2010-10-25T18:56:54.228Z", "isAdmin": false, "displayName": "David_Gerard"}, "userId": "KneTmopEjYGsaPYNi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/a3bdWgBhAS92o4BRE/link-steven-landsburg-accounting-for-numbers-response-to-ey", "pageUrlRelative": "/posts/a3bdWgBhAS92o4BRE/link-steven-landsburg-accounting-for-numbers-response-to-ey", "linkUrl": "https://www.lesswrong.com/posts/a3bdWgBhAS92o4BRE/link-steven-landsburg-accounting-for-numbers-response-to-ey", "postedAtFormatted": "Wednesday, November 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Steven%20Landsburg%20%22Accounting%20for%20Numbers%22%20-%20response%20to%20EY's%20%22Logical%20Pinpointing%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Steven%20Landsburg%20%22Accounting%20for%20Numbers%22%20-%20response%20to%20EY's%20%22Logical%20Pinpointing%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa3bdWgBhAS92o4BRE%2Flink-steven-landsburg-accounting-for-numbers-response-to-ey%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Steven%20Landsburg%20%22Accounting%20for%20Numbers%22%20-%20response%20to%20EY's%20%22Logical%20Pinpointing%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa3bdWgBhAS92o4BRE%2Flink-steven-landsburg-accounting-for-numbers-response-to-ey", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa3bdWgBhAS92o4BRE%2Flink-steven-landsburg-accounting-for-numbers-response-to-ey", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 138, "htmlBody": "<p>\"I started to post a comment, but it got long enough that I&rsquo;ve turned my comment into a <a href=\"http://www.thebigquestions.com/2012/11/14/accounting-for-numbers/\">blog post</a>.\"</p>\n<p style=\"padding-left: 30px;\">So the study of second-order  consequences is not logic at all; to tease out all the second-order  consequences of your second-order axioms, you need to confront not just  the forms of sentences but their meanings. In other words, you have to  understand meanings before you can carry out the operation of inference.  But Yudkowsky is trying to derive meaning from the operation of  inference, which won&rsquo;t work because in second-order logic, meaning comes  first.</p>\n<p style=\"padding-left: 30px;\">... it&rsquo;s important to recognize that  Yudkowsky has &ldquo;solved&rdquo; the problem of accounting for numbers only by  reducing it to the problem of accounting for sets &mdash; except that he  hasn&rsquo;t even done that, because his reduction relies on pretending that  second order logic is logic.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "a3bdWgBhAS92o4BRE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 15, "extendedScore": null, "score": 4.3e-05, "legacy": true, "legacyId": "20079", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-14T17:52:49.431Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham HPMoR Discussion, chapters 15-17", "slug": "meetup-durham-hpmor-discussion-chapters-15-17", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "evand", "createdAt": "2012-05-14T16:45:50.150Z", "isAdmin": false, "displayName": "evand"}, "userId": "QSBopDfW3DLzeMG7L", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NaTx9ooGv5kyp9uu3/meetup-durham-hpmor-discussion-chapters-15-17", "pageUrlRelative": "/posts/NaTx9ooGv5kyp9uu3/meetup-durham-hpmor-discussion-chapters-15-17", "linkUrl": "https://www.lesswrong.com/posts/NaTx9ooGv5kyp9uu3/meetup-durham-hpmor-discussion-chapters-15-17", "postedAtFormatted": "Wednesday, November 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20HPMoR%20Discussion%2C%20chapters%2015-17&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20HPMoR%20Discussion%2C%20chapters%2015-17%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNaTx9ooGv5kyp9uu3%2Fmeetup-durham-hpmor-discussion-chapters-15-17%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20HPMoR%20Discussion%2C%20chapters%2015-17%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNaTx9ooGv5kyp9uu3%2Fmeetup-durham-hpmor-discussion-chapters-15-17", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNaTx9ooGv5kyp9uu3%2Fmeetup-durham-hpmor-discussion-chapters-15-17", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 62, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/fx'>Durham HPMoR Discussion, chapters 15-17</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 November 2012 11:00:00AM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Foster's Market, 2694 Durham-Chapel Hill Blvd., Durham, NC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet and discuss HPMoR, chapters 15-17 (approx 55 pages). Main discussion will probably last until 12:30 or 1:00, and there will likely be Zendo afterwards.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/fx'>Durham HPMoR Discussion, chapters 15-17</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NaTx9ooGv5kyp9uu3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.0336302113579065e-06, "legacy": true, "legacyId": "20080", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_15_17\">Discussion article for the meetup : <a href=\"/meetups/fx\">Durham HPMoR Discussion, chapters 15-17</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 November 2012 11:00:00AM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Foster's Market, 2694 Durham-Chapel Hill Blvd., Durham, NC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet and discuss HPMoR, chapters 15-17 (approx 55 pages). Main discussion will probably last until 12:30 or 1:00, and there will likely be Zendo afterwards.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_15_171\">Discussion article for the meetup : <a href=\"/meetups/fx\">Durham HPMoR Discussion, chapters 15-17</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham HPMoR Discussion, chapters 15-17", "anchor": "Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_15_17", "level": 1}, {"title": "Discussion article for the meetup : Durham HPMoR Discussion, chapters 15-17", "anchor": "Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_15_171", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-14T20:38:32.516Z", "modifiedAt": null, "url": null, "title": "Meetup : Cincinnati/Columbus: Memorisation exercise (NB, time changed)", "slug": "meetup-cincinnati-columbus-memorisation-exercise-nb-time", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RolfAndreassen", "createdAt": "2009-04-17T19:37:23.246Z", "isAdmin": false, "displayName": "RolfAndreassen"}, "userId": "KLJmn2HYWEu4tBKcC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rKyzAbYebcELkCgQ2/meetup-cincinnati-columbus-memorisation-exercise-nb-time", "pageUrlRelative": "/posts/rKyzAbYebcELkCgQ2/meetup-cincinnati-columbus-memorisation-exercise-nb-time", "linkUrl": "https://www.lesswrong.com/posts/rKyzAbYebcELkCgQ2/meetup-cincinnati-columbus-memorisation-exercise-nb-time", "postedAtFormatted": "Wednesday, November 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Cincinnati%2FColumbus%3A%20Memorisation%20exercise%20(NB%2C%20time%20changed)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Cincinnati%2FColumbus%3A%20Memorisation%20exercise%20(NB%2C%20time%20changed)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrKyzAbYebcELkCgQ2%2Fmeetup-cincinnati-columbus-memorisation-exercise-nb-time%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Cincinnati%2FColumbus%3A%20Memorisation%20exercise%20(NB%2C%20time%20changed)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrKyzAbYebcELkCgQ2%2Fmeetup-cincinnati-columbus-memorisation-exercise-nb-time", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrKyzAbYebcELkCgQ2%2Fmeetup-cincinnati-columbus-memorisation-exercise-nb-time", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 83, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/fy'>Cincinnati/Columbus: Memorisation exercise (NB, time changed)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 November 2012 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">4934 Juniper Way Beavercreek, OH 45440</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Please notice changed time!</p>\n\n<p>We will meet at Choe's Asian Gourmet in the Greene. Our exercise for this month is rote memorisation: Please come prepared with a poem of reasonable length, memorised this week, to recite for the group and then discuss. Bookstore and ice cream may occur later.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/fy'>Cincinnati/Columbus: Memorisation exercise (NB, time changed)</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rKyzAbYebcELkCgQ2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.0337225167246245e-06, "legacy": true, "legacyId": "20081", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Cincinnati_Columbus__Memorisation_exercise__NB__time_changed_\">Discussion article for the meetup : <a href=\"/meetups/fy\">Cincinnati/Columbus: Memorisation exercise (NB, time changed)</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 November 2012 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">4934 Juniper Way Beavercreek, OH 45440</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Please notice changed time!</p>\n\n<p>We will meet at Choe's Asian Gourmet in the Greene. Our exercise for this month is rote memorisation: Please come prepared with a poem of reasonable length, memorised this week, to recite for the group and then discuss. Bookstore and ice cream may occur later.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Cincinnati_Columbus__Memorisation_exercise__NB__time_changed_1\">Discussion article for the meetup : <a href=\"/meetups/fy\">Cincinnati/Columbus: Memorisation exercise (NB, time changed)</a></h2>", "sections": [{"title": "Discussion article for the meetup : Cincinnati/Columbus: Memorisation exercise (NB, time changed)", "anchor": "Discussion_article_for_the_meetup___Cincinnati_Columbus__Memorisation_exercise__NB__time_changed_", "level": 1}, {"title": "Discussion article for the meetup : Cincinnati/Columbus: Memorisation exercise (NB, time changed)", "anchor": "Discussion_article_for_the_meetup___Cincinnati_Columbus__Memorisation_exercise__NB__time_changed_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-14T21:15:19.200Z", "modifiedAt": null, "url": null, "title": "Signalling fallacies: Implication vs. Inference", "slug": "signalling-fallacies-implication-vs-inference", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:38.068Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hankx7787", "createdAt": "2011-07-10T22:12:52.395Z", "isAdmin": false, "displayName": "hankx7787"}, "userId": "B4SKuX6dAQMnNqHzH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7YwPizh6BQRMQEwhN/signalling-fallacies-implication-vs-inference", "pageUrlRelative": "/posts/7YwPizh6BQRMQEwhN/signalling-fallacies-implication-vs-inference", "linkUrl": "https://www.lesswrong.com/posts/7YwPizh6BQRMQEwhN/signalling-fallacies-implication-vs-inference", "postedAtFormatted": "Wednesday, November 14th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Signalling%20fallacies%3A%20Implication%20vs.%20Inference&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASignalling%20fallacies%3A%20Implication%20vs.%20Inference%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7YwPizh6BQRMQEwhN%2Fsignalling-fallacies-implication-vs-inference%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Signalling%20fallacies%3A%20Implication%20vs.%20Inference%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7YwPizh6BQRMQEwhN%2Fsignalling-fallacies-implication-vs-inference", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7YwPizh6BQRMQEwhN%2Fsignalling-fallacies-implication-vs-inference", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 238, "htmlBody": "<p>The signalling fallacy that seems to get all the attention is what I call a fallacy of signalling \"implication\", i.e. when someone says:<br /><br />\"Justin Bieber's music is crappy\"</p>\n<p>The rational implication of this communication is that Justin Bieber's music is, in fact, crappy according to some standard. But if what they were actually implying is that they don't like Justin Bieber or that they are signalling a&nbsp;tribal affiliation with fellow JB haters, then they are committing a fallacy of signalling implication.</p>\n<p>&nbsp;</p>\n<p>But that's not the only type of signalling fallacy. You can also commit a fallacy of signalling \"inference\", i.e. consider someone who says:</p>\n<p>\"Atlas Shrugged is the greatest book ever written\".</p>\n<p>Again the rational implication of this communication is that Atlas Shrugged is, in fact, the greatest according to some standard. And if that's what they were actually implying, but then you infer that they simply enjoyed the book a lot or are signalling a tribal affiliation with fellow AS lovers, then you are committing a fallacy of signalling inference. (Note that this goes the other way, too. If they *were* implying simply that they enjoyed the book or were affiliating themselves with a tribe and you inferred they were making some factual claim, that would be just as wrong).</p>\n<p>&nbsp;</p>\n<p>So it's important to be aware of two sides of this fallacy. If you happen to be overly concerned with the former, you might fall victim to the latter.<br /></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7YwPizh6BQRMQEwhN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": -12, "extendedScore": null, "score": 1.0337430041583033e-06, "legacy": true, "legacyId": "20082", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-15T00:46:31.702Z", "modifiedAt": null, "url": null, "title": "Meetup : Salt Lake City Monthly meetup", "slug": "meetup-salt-lake-city-monthly-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hamnox", "createdAt": "2011-01-17T01:16:28.722Z", "isAdmin": false, "displayName": "hamnox"}, "userId": "EY9o6qtXvYhS5CTHD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gqfSapuFap4X9bo7f/meetup-salt-lake-city-monthly-meetup", "pageUrlRelative": "/posts/gqfSapuFap4X9bo7f/meetup-salt-lake-city-monthly-meetup", "linkUrl": "https://www.lesswrong.com/posts/gqfSapuFap4X9bo7f/meetup-salt-lake-city-monthly-meetup", "postedAtFormatted": "Thursday, November 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Salt%20Lake%20City%20Monthly%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Salt%20Lake%20City%20Monthly%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgqfSapuFap4X9bo7f%2Fmeetup-salt-lake-city-monthly-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Salt%20Lake%20City%20Monthly%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgqfSapuFap4X9bo7f%2Fmeetup-salt-lake-city-monthly-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgqfSapuFap4X9bo7f%2Fmeetup-salt-lake-city-monthly-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 137, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/fz'>Salt Lake City Monthly meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">17 November 2012 03:00:28PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Calvin S. Smith Library 810 East 3300 South Salt Lake City, Utah 84106</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be in the small conference room at the back, and might go out for coffee or whatnot afterwards. There's Karaoke going on later too.</p>\n\n<p>Agenda:\n3:00 Greetings, Introduction and Catchup \n3:10 Thinking Fast &amp; Slow\n3:30 discuss \n3:40  Expected Value of Information\n4:00 discuss \n4:10 Group Goals\n4:30 discuss \n4:40 Wrap up and Final Thoughts, Sign up for next meetup \nFree Discussion</p>\n\n<p>For anyone who can't make this one, know that there's one scheduled every third Saturday of the month, tentatively the same time and place, for most of the conceivable future. Our google group is here: https://groups.google.com/group/lesswrongslc/</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/fz'>Salt Lake City Monthly meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gqfSapuFap4X9bo7f", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 8e-06, "legacy": true, "legacyId": "20084", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Salt_Lake_City_Monthly_meetup\">Discussion article for the meetup : <a href=\"/meetups/fz\">Salt Lake City Monthly meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">17 November 2012 03:00:28PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Calvin S. Smith Library 810 East 3300 South Salt Lake City, Utah 84106</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be in the small conference room at the back, and might go out for coffee or whatnot afterwards. There's Karaoke going on later too.</p>\n\n<p>Agenda:\n3:00 Greetings, Introduction and Catchup \n3:10 Thinking Fast &amp; Slow\n3:30 discuss \n3:40  Expected Value of Information\n4:00 discuss \n4:10 Group Goals\n4:30 discuss \n4:40 Wrap up and Final Thoughts, Sign up for next meetup \nFree Discussion</p>\n\n<p>For anyone who can't make this one, know that there's one scheduled every third Saturday of the month, tentatively the same time and place, for most of the conceivable future. Our google group is here: https://groups.google.com/group/lesswrongslc/</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Salt_Lake_City_Monthly_meetup1\">Discussion article for the meetup : <a href=\"/meetups/fz\">Salt Lake City Monthly meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Salt Lake City Monthly meetup", "anchor": "Discussion_article_for_the_meetup___Salt_Lake_City_Monthly_meetup", "level": 1}, {"title": "Discussion article for the meetup : Salt Lake City Monthly meetup", "anchor": "Discussion_article_for_the_meetup___Salt_Lake_City_Monthly_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-15T04:43:14.280Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Billion Dollar Bots", "slug": "seq-rerun-billion-dollar-bots", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:38.933Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/urEhgpeHrHAc8h7iv/seq-rerun-billion-dollar-bots", "pageUrlRelative": "/posts/urEhgpeHrHAc8h7iv/seq-rerun-billion-dollar-bots", "linkUrl": "https://www.lesswrong.com/posts/urEhgpeHrHAc8h7iv/seq-rerun-billion-dollar-bots", "postedAtFormatted": "Thursday, November 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Billion%20Dollar%20Bots&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Billion%20Dollar%20Bots%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FurEhgpeHrHAc8h7iv%2Fseq-rerun-billion-dollar-bots%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Billion%20Dollar%20Bots%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FurEhgpeHrHAc8h7iv%2Fseq-rerun-billion-dollar-bots", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FurEhgpeHrHAc8h7iv%2Fseq-rerun-billion-dollar-bots", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 154, "htmlBody": "<p>Today's post, <a href=\"http://www.overcomingbias.com/2008/11/billion-dollar.html\">Billion Dollar Bots</a> was originally published on November 22, 2008.  A summary:</p>\n<p>&nbsp;</p>\n<blockquote>An alternate scenario for the creation of bots, this time involving lots of cloud computing.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fhd/seq_rerun_brain_emulation_and_hard_takeoff/\">Brain Emulation and Hard Takeoff</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "urEhgpeHrHAc8h7iv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 1.0339925771157058e-06, "legacy": true, "legacyId": "20094", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xMBZTQtMSvowk3H2k", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-15T04:57:47.924Z", "modifiedAt": null, "url": null, "title": "Launched: Friendship is Optimal", "slug": "launched-friendship-is-optimal", "viewCount": null, "lastCommentedAt": "2022-01-11T13:56:35.075Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "iceman", "createdAt": "2011-10-02T05:56:12.967Z", "isAdmin": false, "displayName": "iceman"}, "userId": "WCxJj7a7FGiT9LDQJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hHCBGXkQCbEqBEADE/launched-friendship-is-optimal", "pageUrlRelative": "/posts/hHCBGXkQCbEqBEADE/launched-friendship-is-optimal", "linkUrl": "https://www.lesswrong.com/posts/hHCBGXkQCbEqBEADE/launched-friendship-is-optimal", "postedAtFormatted": "Thursday, November 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Launched%3A%20Friendship%20is%20Optimal&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALaunched%3A%20Friendship%20is%20Optimal%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhHCBGXkQCbEqBEADE%2Flaunched-friendship-is-optimal%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Launched%3A%20Friendship%20is%20Optimal%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhHCBGXkQCbEqBEADE%2Flaunched-friendship-is-optimal", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhHCBGXkQCbEqBEADE%2Flaunched-friendship-is-optimal", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 137, "htmlBody": "<p><em>Friendship is Optimal</em> has launched and is being published in chunks&nbsp;<a href=\"http://www.fimfiction.net/story/62074/Friendship-is-Optimal\">on FIMFiction</a>.&nbsp;</p>\n<p>\n<p><em>Friendship is Optimal</em>&nbsp;is a story about an optimizer written to \"satisfy human values through friendship and ponies.\"&nbsp;I would like to thank everyone on LessWrong&nbsp;<a href=\"/r/discussion/lw/efi/friendship_is_optimal_a_my_little_pony_fanfic/\">who came out and helped edit it</a>.&nbsp;<em>Friendship is Optimal</em>&nbsp;wouldn't be what is today without your help.</p>\n<p>Thank you.</p>\n<p>Teaser description:</p>\n<p style=\"padding-left: 30px;\"><em><span style=\"color: #444444; font-family: Helvetica, Arial; font-size: 14px; line-height: 27.350000381469727px;\"></span>Hanna, the CEO of Hofvarpnir Studios, just won the contract to write the official&nbsp;My Little Pony&nbsp;MMO. Hanna has built an A.I. Princess Celestia and given her one basic drive: to satisfy everybody's values through friendship and ponies. And Princess Celestia will follow those instructions to the letter...even if you don't want her to.</em><span style=\"color: #444444; font-family: Helvetica, Arial; font-size: 14px; line-height: 27.350000381469727px;\"></span></p>\n<p>Here is the schedule for the next chapters:</p>\n<p style=\"padding-left: 30px;\">Friday (Nov. 16th): Chapter 4 - 5</p>\n<p style=\"padding-left: 30px;\">Monday (Nov. 19th): Chapter 6 - 7</p>\n<p style=\"padding-left: 30px;\">Thursday (Nov. 22th): Chapter 8 - 9</p>\n<p style=\"padding-left: 30px;\">Sunday (Nov. 25th): Chapter 10 - 11, Author's Afterword</p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 1, "ZFrgTgzwEfStg26JL": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hHCBGXkQCbEqBEADE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 49, "baseScore": 72, "extendedScore": null, "score": 0.000164, "legacy": true, "legacyId": "20096", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 72, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CHD5m9fnosr7L3dto"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 8, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-15T07:25:54.739Z", "modifiedAt": null, "url": null, "title": "A summary of the Hanson-Yudkowsky FOOM debate", "slug": "a-summary-of-the-hanson-yudkowsky-foom-debate", "viewCount": null, "lastCommentedAt": "2020-01-09T10:06:23.878Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sPrifh6uLJQFjQJPW/a-summary-of-the-hanson-yudkowsky-foom-debate", "pageUrlRelative": "/posts/sPrifh6uLJQFjQJPW/a-summary-of-the-hanson-yudkowsky-foom-debate", "linkUrl": "https://www.lesswrong.com/posts/sPrifh6uLJQFjQJPW/a-summary-of-the-hanson-yudkowsky-foom-debate", "postedAtFormatted": "Thursday, November 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20summary%20of%20the%20Hanson-Yudkowsky%20FOOM%20debate&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20summary%20of%20the%20Hanson-Yudkowsky%20FOOM%20debate%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsPrifh6uLJQFjQJPW%2Fa-summary-of-the-hanson-yudkowsky-foom-debate%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20summary%20of%20the%20Hanson-Yudkowsky%20FOOM%20debate%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsPrifh6uLJQFjQJPW%2Fa-summary-of-the-hanson-yudkowsky-foom-debate", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsPrifh6uLJQFjQJPW%2Fa-summary-of-the-hanson-yudkowsky-foom-debate", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 342, "htmlBody": "<p>In late spring this year, Luke tasked me with writing a summary and analysis of the <a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\">Hanson-Yudkowsky FOOM debate</a>, with the intention of having it eventually published in somewhere. Due to other priorities, this project was put on hold for the time being. Because it doesn't look like it will be finished in the near future, and because <a href=\"/user/Curiouskid/\">Curiouskid</a> asked to see it, we thought that we might as well share the thing.</p>\n<p>I have reorganized the debate, presenting it by topic rather than in chronological order: I start by providing some brief conceptual background that's useful for understanding Eliezer's optimization power argument, after which I present his argument. Robin's various objections follow, after which there is a summary of Robin's view of how the Singularity will be like, together with Eliezer's objections to that view. Hopefully, this should make the debate easier to follow. This summary also incorporates material from the 90-minute live debate on the topic that they had in 2011. The full table of contents:</p>\n<ol>\n<li>Introduction</li>\n<li>Overview</li>\n<li>The optimization power argument<ol>\n<li>Conceptual background</li>\n<li>The argument: Yudkowsky</li>\n<li>Recursive self-improvement</li>\n<li>Hard takeoff</li>\n<li>Questioning optimization power: the question of abstractions</li>\n<li>Questioning optimization power: the historical record</li>\n<li>Questioning optimization power: the UberTool question</li>\n</ol></li>\n<li>Hanson's Singularity scenario<ol>\n<li>Architecture vs. content, sharing of information</li>\n<li>Modularity of knowledge</li>\n<li>Local or global singularity?</li>\n</ol></li>\n<li>Wrap-up</li>\n<li>Conclusions</li>\n<li>References</li>\n</ol>\n<p><a href=\"https://docs.google.com/document/d/1ed5ZEytvIn0H1Ks7xB3Tr2u4ar9LoCBnJKxccnhWKfk/edit\">Here's the link to the current draft</a>, any feedback is welcomed. Feel free to comment if you know of useful references, if you think I've misinterpreted something that was said, or if you think there's any other problem. I'd also be curious to hear to what extent people think that this outline is easier to follow than the original debate, or whether it's just as confusing.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"oiRp4T6u5poc8r9Tj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sPrifh6uLJQFjQJPW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 36, "extendedScore": null, "score": 9.2e-05, "legacy": true, "legacyId": "20105", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 36, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-15T19:41:02.955Z", "modifiedAt": null, "url": null, "title": "Empirical claims, preference claims, and attitude claims", "slug": "empirical-claims-preference-claims-and-attitude-claims", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:28.113Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PuAfoZEdcQSqMGgyP/empirical-claims-preference-claims-and-attitude-claims", "pageUrlRelative": "/posts/PuAfoZEdcQSqMGgyP/empirical-claims-preference-claims-and-attitude-claims", "linkUrl": "https://www.lesswrong.com/posts/PuAfoZEdcQSqMGgyP/empirical-claims-preference-claims-and-attitude-claims", "postedAtFormatted": "Thursday, November 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Empirical%20claims%2C%20preference%20claims%2C%20and%20attitude%20claims&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEmpirical%20claims%2C%20preference%20claims%2C%20and%20attitude%20claims%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPuAfoZEdcQSqMGgyP%2Fempirical-claims-preference-claims-and-attitude-claims%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Empirical%20claims%2C%20preference%20claims%2C%20and%20attitude%20claims%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPuAfoZEdcQSqMGgyP%2Fempirical-claims-preference-claims-and-attitude-claims", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPuAfoZEdcQSqMGgyP%2Fempirical-claims-preference-claims-and-attitude-claims", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1568, "htmlBody": "<p>What do the following statements have in common?</p>\n<ul>\n<li>\"Atlas Shrugged is the best book ever written.\"</li>\n<li>\"You break it, you buy it.\"</li>\n<li>\"Earth is the most interesting planet in the solar system.\"</li>\n</ul>\n<p>My answer: None of them are falsifiable claims about the nature of reality.&nbsp; They're all closer to what one might call \"opinions\".&nbsp; But what is an \"opinion\", exactly?</p>\n<p>There's already been <a href=\"/lw/eqn/the_useful_idea_of_truth/7jlu\">some discussion</a> on Less Wrong about what exactly it means for a claim to be meaningful.&nbsp; This post focuses on the <em>negative</em> definition of meaning: what sort of statements do people make where the primary content of the statement is non-empirical?&nbsp; The idea here is similar to the idea behind anti-virus software: Even if you can't rigorously describe what programs are safe to run on your computer, there still may be utility in keeping a database of programs that are known to be unsafe.</p>\n<p>Why is it useful to be able to be able to flag non-empirical claims?&nbsp; Well, for one thing, you can believe whatever you want about them!&nbsp; And it seems likely that this pattern-matching approach works better for flagging them than a more constructive definition.</p>\n<p><a id=\"more\"></a>But first, a bit on the philosophy of non-empirical claims.</p>\n<p>Let's take a typical opinion statement: \"Justin Bieber sucks\".&nbsp; There are a few ways we could interpret this as shorthand for a different claim.&nbsp; For example, maybe what the speaker really means is \"I prefer not to listen to Justin Bieber's music.\"&nbsp; (Preference claim.)&nbsp; Or maybe what the speaker really means is \"Of the people who have heard songs by Justin Bieber, the majority prefer not to listen to his music.\"&nbsp; (Empirical claim.)</p>\n<p>I don't think shorthand interpretations like these are accurate for most people who claim that JB sucks.&nbsp; Instead, I suspect most people who argue this are communicating some combination of (a) negative affect towards JB and (b) tribal affiliation with fellow JB haters.&nbsp; I've taken to referring to statements like these, that are neither preference claims nor empirical claims, as \"attitude claims\".</p>\n<p>This example doesn't mean that all \"X sucks\" style claims are attitude claims.&nbsp; Take the claim \"Windows sucks\".&nbsp; It does seem plausible that someone who said this could be persuaded that their claim was false through empirical evidence--e.g. by a meta-analysis that compared Windows worker productivity favorably to worker productivity using other operating systems.</p>\n<p>So if someone says Windows sucks, then whether their claim is empirical, attitudinal, or (most likely) some mixture depends on what's going on in their head.&nbsp; You may be able to classify the claim with further conversation, however.&nbsp; If they say \"even if users are happiest and most productive using Windows, it still sucks!\", that suggests the claim is almost entirely attitudinal.</p>\n<p>&nbsp;</p>\n<h2>Attitude claims taxonomy<br /></h2>\n<p>I've been writing down attitude claims I think of or come across in <a href=\"/lw/egr/personal_information_management/\">my notebook</a>.&nbsp; Here's some that I've seen so far.&nbsp; Hopefully they'll serve as good training data for your internal classifier.</p>\n<ul>\n<li>Status claims. \n<ul>\n<li>\"<a href=\"http://jessicamah.com/your-worth-is-not-tied-to-what-you-do\">Your worth is not tied to what you do.</a>\"</li>\n<li>\"<a href=\"http://dilbert.com/blog/entry/the_illusion_of_winning/\">Winning at pool is not impressive.</a>\"</li>\n<li>\"I'm not that smart\", if the speaker knows they've scored in the 95th percentile or above on a g-loaded standardized test.&nbsp; (This is similar to the \"Windows sucks\" person who still thinks Windows sucks after reading and agreeing with the meta-analysis--at this point, we can probably rule out the idea that their claim is an empirical one.)</li>\n<li>\"<a href=\"http://econlog.econlib.org/archives/2012/05/the_bettors_oat.html\">A betting loser has far more honor than the mass of men who live by loose and idle talk</a>.\"&nbsp; (Side note: I like Caplan's writing, but the oath's postscript is a bit ironic since it doesn't present any empirical claims, so it's not clear in what sense it could be \"wrong\".)</li>\n</ul>\n</li>\n<li>Claims about social rules and the details of their implementation.<br /> \n<ul>\n<li>\"That was uncalled for.\"</li>\n<li>\"<a href=\"http://www.reddit.com/r/DebateIt/comments/h2t9c/debate_is_it_ok_to_celebrate_osamas_death_or_the/\">Is it OK to celebrate Osama bin Laden's death?</a>\"</li>\n<li>\"I have an obligation to help others when I help myself.\"</li>\n<li>\"That's your responsibility.\"</li>\n<li>\"You don't deserve that.\"</li>\n<li>\"This is our land.\"</li>\n<li>\"That's not legitimate.\"</li>\n<li>\"Even little lies are wrong.\"</li>\n<li>\"If you didn't vote, you're not allowed to complain.\"</li>\n<li>\"<a href=\"http://www.reddit.com/r/geek/comments/11ddyk/youre_not_a_nerd_geeks_arent_sexy_and_you_dont/c6lja0a\">Saying you \"fucking love\" something should carry some weight</a>.\"&nbsp; (This example is particularly interesting, because Maddox is claiming that people who are making a certain affect claim are breaking a social rule.)</li>\n<li>\"Marriage is between a man and a woman.\"</li>\n</ul>\n</li>\n<li>Attribution claims. \n<ul>\n<li>\"The crash was your fault.\"</li>\n<li>\"That was my idea.\"</li>\n<li>\"You're innocent.\"</li>\n</ul>\n</li>\n<li>Affect claims.&nbsp; (Bit of a catchall here...)<br /> \n<ul>\n<li>\"You're weird.\"</li>\n<li>\"Chess is overrated.\"</li>\n<li>\"That's really cool.\"</li>\n<li>\"Professionalism is the most important thing in a business.\"</li>\n<li>\"Marketing is evil.\"</li>\n<li>\"<a href=\"/lw/f1y/link_antigroupism/\">If your brain thinks principles first, instead of groups first, it's broken, and not just a little bit.</a>\"</li>\n<li>\"<a href=\"http://www.youtube.com/watch?v=iMUiwTubYu0\">Life is a ride.</a>\"</li>\n</ul>\n</li>\n</ul>\n<p>Not all of the examples I've found fit neatly in to one of these categories (e.g. \"I can do anything I want\"), and it's pretty common to find claims that seem like mixtures of attitude and fact/preference statements.&nbsp; For example, if someone says \"Being outrageous is the best way to be\", are they saying \"I prefer to be outrageous\" or \"<a href=\"/lw/bk/the_trouble_with_good/\">Yay</a> outrageousness\"?&nbsp; Probably a bit of both.</p>\n<p>&nbsp;</p>\n<h2>What attitudes should I have?</h2>\n<p>That's a \"should\" question, i.e. a question about social rules.&nbsp; Unless you meant it as a shorthand for a question about how best to achieve some goal, e.g. \"What attitudes should I have in order to best achieve my preferences?\"&nbsp; Then it becomes an empirical question.</p>\n<p>I suspect that most people can better achieve their preferences by consciously choosing and adopting attitudes rather than going with whatever defaults they grew up with or are prevalent within their social group.&nbsp; Attitude hacking is not trivial, so you might want to find a friend to adopt your preferred attitude with.&nbsp; (This isn't anti-epistemic groupthink as long as you're doing this for attitudes only and not for facts.)</p>\n<p>&nbsp;</p>\n<h2>Are attitudes bad?</h2>\n<p>That's an attitude question.</p>\n<p>I think to best achieve your preferences, it's likely optimal to take some attitudes seriously, e.g. Jon Kabat-Zinn: \"as long as you are breathing, there is more right with you than there is wrong, no matter how ill or how hopeless you may feel\", or <a href=\"/lw/etf/firewalling_the_optimal_from_the_rational/\">Eliezer Yudkowsky</a>: \"probability theory is also a kind of Authority and I try to be ruled by it as much as I can manage.\"</p>\n<p>Unfortunately, I haven't managed to take any attitude claims as seriously ever since I realized that they're basically just made up.&nbsp; (Which is itself an attitude statement of the affect type, about the importance of attitudes.)&nbsp; But I've also felt more free to \"cheat\" and modify my attitudes directly in order to optimize for my preferences.</p>\n<p>Will pointing out that social rules are social rules make people less likely to take them seriously?&nbsp; Probably.&nbsp; The ideas in this post are dangerous knowledge that shouldn't be spread beyond rationalist circles.</p>\n<p>If you're like me, you may get kind of squeamish consuming attitude-heavy media (which is also produced by rationalists, by the way; see <a href=\"http://paulgraham.com/goodart.html\">Paul Graham</a> or <a href=\"http://fora.tv/2012/10/13/Julia_Galef_Rationality_and_the_Future/Slave_to_Your_DNA_Rationality_Will_Set_You_Free\">Julia Galef</a>).&nbsp; That's an attitude.</p>\n<p>&nbsp;</p>\n<h2>Connection with Nonviolent Communication</h2>\n<p>Empirical claim: If you restrict yourself to empirical claims and preference claims when you have an argument, you and the people you argue with will be more pleased with the outcome of your arguments.</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Nonviolent_Communication\">Nonviolent Communication</a> is a philosophy that recommends replacing attitude claims like \"You're an awful neighbor\" or \"It's your fault I can't get to sleep\" with empirical claims, preference claims, and requests: \"Your music is playing very loudly (fact).&nbsp; I'm having a hard time sleeping (fact).&nbsp; I'd really like to be able to get to sleep (preference).&nbsp; Could you turn down the volume?\"&nbsp; Presumably this works because (a) arguments over empirical claims are sometimes actually resolved and (b) if you share preferences instead of bludgeoning people with social rules, they're more likely to empathize with you and do things to make you happy.</p>\n<p>&nbsp;</p>\n<h2>More thoughts</h2>\n<p>After crystallizing the fact/attitude distinction, I started trying to apply <a href=\"/lw/dyk/selfskepticism_the_first_principle_of_rationality/\">self-skepticism</a> to empirical claims only, and just <a href=\"http://www.reddit.com/r/howtonotgiveafuck\">ignoring</a> attitude claims I didn't like.&nbsp; (\"<a href=\"http://www.imdb.com/title/tt0118715/quotes?qt=qt0464776\">That's just, like, your opinion, man.</a>\")&nbsp; Carefully considering uncomfortable empirical claims is a habit that will improve my model of the world, thereby helping me achieve my preferences. &nbsp;(That's what it's all about, right?) &nbsp;Carefully considering uncomfortable attitude claims, not so much, except maybe if they're from people with whom I have valued relationships that I want to debug.</p>\n<p>Does this post describe an attitude?&nbsp; I actually put it and other affect-free classification schemes in to a fourth category: that of a \"cognitive tool\", like a description of an algorithm, that you can take or leave as you wish.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PuAfoZEdcQSqMGgyP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 12, "extendedScore": null, "score": 1.0344931448957908e-06, "legacy": true, "legacyId": "20051", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>What do the following statements have in common?</p>\n<ul>\n<li>\"Atlas Shrugged is the best book ever written.\"</li>\n<li>\"You break it, you buy it.\"</li>\n<li>\"Earth is the most interesting planet in the solar system.\"</li>\n</ul>\n<p>My answer: None of them are falsifiable claims about the nature of reality.&nbsp; They're all closer to what one might call \"opinions\".&nbsp; But what is an \"opinion\", exactly?</p>\n<p>There's already been <a href=\"/lw/eqn/the_useful_idea_of_truth/7jlu\">some discussion</a> on Less Wrong about what exactly it means for a claim to be meaningful.&nbsp; This post focuses on the <em>negative</em> definition of meaning: what sort of statements do people make where the primary content of the statement is non-empirical?&nbsp; The idea here is similar to the idea behind anti-virus software: Even if you can't rigorously describe what programs are safe to run on your computer, there still may be utility in keeping a database of programs that are known to be unsafe.</p>\n<p>Why is it useful to be able to be able to flag non-empirical claims?&nbsp; Well, for one thing, you can believe whatever you want about them!&nbsp; And it seems likely that this pattern-matching approach works better for flagging them than a more constructive definition.</p>\n<p><a id=\"more\"></a>But first, a bit on the philosophy of non-empirical claims.</p>\n<p>Let's take a typical opinion statement: \"Justin Bieber sucks\".&nbsp; There are a few ways we could interpret this as shorthand for a different claim.&nbsp; For example, maybe what the speaker really means is \"I prefer not to listen to Justin Bieber's music.\"&nbsp; (Preference claim.)&nbsp; Or maybe what the speaker really means is \"Of the people who have heard songs by Justin Bieber, the majority prefer not to listen to his music.\"&nbsp; (Empirical claim.)</p>\n<p>I don't think shorthand interpretations like these are accurate for most people who claim that JB sucks.&nbsp; Instead, I suspect most people who argue this are communicating some combination of (a) negative affect towards JB and (b) tribal affiliation with fellow JB haters.&nbsp; I've taken to referring to statements like these, that are neither preference claims nor empirical claims, as \"attitude claims\".</p>\n<p>This example doesn't mean that all \"X sucks\" style claims are attitude claims.&nbsp; Take the claim \"Windows sucks\".&nbsp; It does seem plausible that someone who said this could be persuaded that their claim was false through empirical evidence--e.g. by a meta-analysis that compared Windows worker productivity favorably to worker productivity using other operating systems.</p>\n<p>So if someone says Windows sucks, then whether their claim is empirical, attitudinal, or (most likely) some mixture depends on what's going on in their head.&nbsp; You may be able to classify the claim with further conversation, however.&nbsp; If they say \"even if users are happiest and most productive using Windows, it still sucks!\", that suggests the claim is almost entirely attitudinal.</p>\n<p>&nbsp;</p>\n<h2 id=\"Attitude_claims_taxonomy\">Attitude claims taxonomy<br></h2>\n<p>I've been writing down attitude claims I think of or come across in <a href=\"/lw/egr/personal_information_management/\">my notebook</a>.&nbsp; Here's some that I've seen so far.&nbsp; Hopefully they'll serve as good training data for your internal classifier.</p>\n<ul>\n<li>Status claims. \n<ul>\n<li>\"<a href=\"http://jessicamah.com/your-worth-is-not-tied-to-what-you-do\">Your worth is not tied to what you do.</a>\"</li>\n<li>\"<a href=\"http://dilbert.com/blog/entry/the_illusion_of_winning/\">Winning at pool is not impressive.</a>\"</li>\n<li>\"I'm not that smart\", if the speaker knows they've scored in the 95th percentile or above on a g-loaded standardized test.&nbsp; (This is similar to the \"Windows sucks\" person who still thinks Windows sucks after reading and agreeing with the meta-analysis--at this point, we can probably rule out the idea that their claim is an empirical one.)</li>\n<li>\"<a href=\"http://econlog.econlib.org/archives/2012/05/the_bettors_oat.html\">A betting loser has far more honor than the mass of men who live by loose and idle talk</a>.\"&nbsp; (Side note: I like Caplan's writing, but the oath's postscript is a bit ironic since it doesn't present any empirical claims, so it's not clear in what sense it could be \"wrong\".)</li>\n</ul>\n</li>\n<li>Claims about social rules and the details of their implementation.<br> \n<ul>\n<li>\"That was uncalled for.\"</li>\n<li>\"<a href=\"http://www.reddit.com/r/DebateIt/comments/h2t9c/debate_is_it_ok_to_celebrate_osamas_death_or_the/\">Is it OK to celebrate Osama bin Laden's death?</a>\"</li>\n<li>\"I have an obligation to help others when I help myself.\"</li>\n<li>\"That's your responsibility.\"</li>\n<li>\"You don't deserve that.\"</li>\n<li>\"This is our land.\"</li>\n<li>\"That's not legitimate.\"</li>\n<li>\"Even little lies are wrong.\"</li>\n<li>\"If you didn't vote, you're not allowed to complain.\"</li>\n<li>\"<a href=\"http://www.reddit.com/r/geek/comments/11ddyk/youre_not_a_nerd_geeks_arent_sexy_and_you_dont/c6lja0a\">Saying you \"fucking love\" something should carry some weight</a>.\"&nbsp; (This example is particularly interesting, because Maddox is claiming that people who are making a certain affect claim are breaking a social rule.)</li>\n<li>\"Marriage is between a man and a woman.\"</li>\n</ul>\n</li>\n<li>Attribution claims. \n<ul>\n<li>\"The crash was your fault.\"</li>\n<li>\"That was my idea.\"</li>\n<li>\"You're innocent.\"</li>\n</ul>\n</li>\n<li>Affect claims.&nbsp; (Bit of a catchall here...)<br> \n<ul>\n<li>\"You're weird.\"</li>\n<li>\"Chess is overrated.\"</li>\n<li>\"That's really cool.\"</li>\n<li>\"Professionalism is the most important thing in a business.\"</li>\n<li>\"Marketing is evil.\"</li>\n<li>\"<a href=\"/lw/f1y/link_antigroupism/\">If your brain thinks principles first, instead of groups first, it's broken, and not just a little bit.</a>\"</li>\n<li>\"<a href=\"http://www.youtube.com/watch?v=iMUiwTubYu0\">Life is a ride.</a>\"</li>\n</ul>\n</li>\n</ul>\n<p>Not all of the examples I've found fit neatly in to one of these categories (e.g. \"I can do anything I want\"), and it's pretty common to find claims that seem like mixtures of attitude and fact/preference statements.&nbsp; For example, if someone says \"Being outrageous is the best way to be\", are they saying \"I prefer to be outrageous\" or \"<a href=\"/lw/bk/the_trouble_with_good/\">Yay</a> outrageousness\"?&nbsp; Probably a bit of both.</p>\n<p>&nbsp;</p>\n<h2 id=\"What_attitudes_should_I_have_\">What attitudes should I have?</h2>\n<p>That's a \"should\" question, i.e. a question about social rules.&nbsp; Unless you meant it as a shorthand for a question about how best to achieve some goal, e.g. \"What attitudes should I have in order to best achieve my preferences?\"&nbsp; Then it becomes an empirical question.</p>\n<p>I suspect that most people can better achieve their preferences by consciously choosing and adopting attitudes rather than going with whatever defaults they grew up with or are prevalent within their social group.&nbsp; Attitude hacking is not trivial, so you might want to find a friend to adopt your preferred attitude with.&nbsp; (This isn't anti-epistemic groupthink as long as you're doing this for attitudes only and not for facts.)</p>\n<p>&nbsp;</p>\n<h2 id=\"Are_attitudes_bad_\">Are attitudes bad?</h2>\n<p>That's an attitude question.</p>\n<p>I think to best achieve your preferences, it's likely optimal to take some attitudes seriously, e.g. Jon Kabat-Zinn: \"as long as you are breathing, there is more right with you than there is wrong, no matter how ill or how hopeless you may feel\", or <a href=\"/lw/etf/firewalling_the_optimal_from_the_rational/\">Eliezer Yudkowsky</a>: \"probability theory is also a kind of Authority and I try to be ruled by it as much as I can manage.\"</p>\n<p>Unfortunately, I haven't managed to take any attitude claims as seriously ever since I realized that they're basically just made up.&nbsp; (Which is itself an attitude statement of the affect type, about the importance of attitudes.)&nbsp; But I've also felt more free to \"cheat\" and modify my attitudes directly in order to optimize for my preferences.</p>\n<p>Will pointing out that social rules are social rules make people less likely to take them seriously?&nbsp; Probably.&nbsp; The ideas in this post are dangerous knowledge that shouldn't be spread beyond rationalist circles.</p>\n<p>If you're like me, you may get kind of squeamish consuming attitude-heavy media (which is also produced by rationalists, by the way; see <a href=\"http://paulgraham.com/goodart.html\">Paul Graham</a> or <a href=\"http://fora.tv/2012/10/13/Julia_Galef_Rationality_and_the_Future/Slave_to_Your_DNA_Rationality_Will_Set_You_Free\">Julia Galef</a>).&nbsp; That's an attitude.</p>\n<p>&nbsp;</p>\n<h2 id=\"Connection_with_Nonviolent_Communication\">Connection with Nonviolent Communication</h2>\n<p>Empirical claim: If you restrict yourself to empirical claims and preference claims when you have an argument, you and the people you argue with will be more pleased with the outcome of your arguments.</p>\n<p><a href=\"http://en.wikipedia.org/wiki/Nonviolent_Communication\">Nonviolent Communication</a> is a philosophy that recommends replacing attitude claims like \"You're an awful neighbor\" or \"It's your fault I can't get to sleep\" with empirical claims, preference claims, and requests: \"Your music is playing very loudly (fact).&nbsp; I'm having a hard time sleeping (fact).&nbsp; I'd really like to be able to get to sleep (preference).&nbsp; Could you turn down the volume?\"&nbsp; Presumably this works because (a) arguments over empirical claims are sometimes actually resolved and (b) if you share preferences instead of bludgeoning people with social rules, they're more likely to empathize with you and do things to make you happy.</p>\n<p>&nbsp;</p>\n<h2 id=\"More_thoughts\">More thoughts</h2>\n<p>After crystallizing the fact/attitude distinction, I started trying to apply <a href=\"/lw/dyk/selfskepticism_the_first_principle_of_rationality/\">self-skepticism</a> to empirical claims only, and just <a href=\"http://www.reddit.com/r/howtonotgiveafuck\">ignoring</a> attitude claims I didn't like.&nbsp; (\"<a href=\"http://www.imdb.com/title/tt0118715/quotes?qt=qt0464776\">That's just, like, your opinion, man.</a>\")&nbsp; Carefully considering uncomfortable empirical claims is a habit that will improve my model of the world, thereby helping me achieve my preferences. &nbsp;(That's what it's all about, right?) &nbsp;Carefully considering uncomfortable attitude claims, not so much, except maybe if they're from people with whom I have valued relationships that I want to debug.</p>\n<p>Does this post describe an attitude?&nbsp; I actually put it and other affect-free classification schemes in to a fourth category: that of a \"cognitive tool\", like a description of an algorithm, that you can take or leave as you wish.</p>", "sections": [{"title": "Attitude claims taxonomy", "anchor": "Attitude_claims_taxonomy", "level": 1}, {"title": "What attitudes should I have?", "anchor": "What_attitudes_should_I_have_", "level": 1}, {"title": "Are attitudes bad?", "anchor": "Are_attitudes_bad_", "level": 1}, {"title": "Connection with Nonviolent Communication", "anchor": "Connection_with_Nonviolent_Communication", "level": 1}, {"title": "More thoughts", "anchor": "More_thoughts", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "126 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 126, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["stDijTKuto52M5wQT", "KuJB7y7yunm42DCXr", "M2LWXsJxKS626QNEA", "WbLAA8qZQNdbRgKte", "NoYYBAaMRp9Y5Jnpo"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-15T20:34:54.680Z", "modifiedAt": null, "url": null, "title": "Giving What We Can, 80,000 Hours, and Meta-Charity", "slug": "giving-what-we-can-80-000-hours-and-meta-charity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:04.457Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wdmacaskill", "createdAt": "2012-10-22T20:30:17.852Z", "isAdmin": false, "displayName": "wdmacaskill"}, "userId": "QG5PnNekwcSsCRr68", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FCiMtrsM8mcmBtfTR/giving-what-we-can-80-000-hours-and-meta-charity", "pageUrlRelative": "/posts/FCiMtrsM8mcmBtfTR/giving-what-we-can-80-000-hours-and-meta-charity", "linkUrl": "https://www.lesswrong.com/posts/FCiMtrsM8mcmBtfTR/giving-what-we-can-80-000-hours-and-meta-charity", "postedAtFormatted": "Thursday, November 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Giving%20What%20We%20Can%2C%2080%2C000%20Hours%2C%20and%20Meta-Charity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGiving%20What%20We%20Can%2C%2080%2C000%20Hours%2C%20and%20Meta-Charity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFCiMtrsM8mcmBtfTR%2Fgiving-what-we-can-80-000-hours-and-meta-charity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Giving%20What%20We%20Can%2C%2080%2C000%20Hours%2C%20and%20Meta-Charity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFCiMtrsM8mcmBtfTR%2Fgiving-what-we-can-80-000-hours-and-meta-charity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFCiMtrsM8mcmBtfTR%2Fgiving-what-we-can-80-000-hours-and-meta-charity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1070, "htmlBody": "<p class=\"MsoNormal\" style=\"tab-stops: 59.1pt;\"><em style=\"mso-bidi-font-style: normal;\"><span style=\"mso-ansi-language: EN-US;\">Disclaimer: I&rsquo;m somewhat nervous about posting this, for fear of down-voting on my first LW post, given that this post explicitly talks in a positive light about organisations that I have helped to set up. But I think that the topic is of interest to LW-ers, and I&rsquo;m hoping to start a rational discussion. So here it goes&hellip;</span></em></p>\n<p class=\"MsoNormal\" style=\"tab-stops: 59.1pt;\"><span style=\"mso-ansi-language: EN-US;\">Hi all,</span></p>\n<p class=\"MsoNormal\" style=\"tab-stops: 59.1pt;\"><span lang=\"EN-GB\"><a href=\"/lw/3gj/efficient_charity_do_unto_others/\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">Optimal philanthropy</span></a></span><span style=\"mso-ansi-language: EN-US;\"> is a </span><span lang=\"EN-GB\"><a href=\"/lw/37f/efficient_charity/\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">common</span></a></span><span style=\"mso-ansi-language: EN-US;\"> </span><span lang=\"EN-GB\"><a href=\"/lw/6py/optimal_philanthropy_for_human_beings/\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">discussion</span></a></span><span style=\"mso-ansi-language: EN-US;\"> </span><span lang=\"EN-GB\"><a href=\"/lw/da4/what_is_optimal_philanthropy/\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">topic</span></a></span><span style=\"mso-ansi-language: EN-US;\"> on LW. It&rsquo;s also </span><span lang=\"EN-GB\"><a href=\"/lw/4lb/is_givewellorg_the_best_charity_excluding_siai/\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">previously been discussed</span></a></span><span style=\"mso-ansi-language: EN-US;\"> whether &lsquo;meta-charities&rsquo; like GiveWell &mdash; that is, charities that attempt to move money to other charities, or assess the effectiveness of other charities &mdash; might end up themselves being excellent or even optimal giving opportunities.</span></p>\n<p class=\"MsoNormal\" style=\"tab-stops: 59.1pt;\"><span lang=\"EN-GB\">Partly on the basis of the potentially high cost-effectiveness of meta-charity, I have co-founded two such charities: Giving What We Can and 80,000 Hours. Both are now open to taking donations (info <a href=\"http://www.givingwhatwecan.org/getting-involved/donate\">here</a> for GWWC and <a href=\"http://80000hours.org/donate\">here</a> for 80k).</span><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-GB\"> </span><span style=\"mso-ansi-language: EN-US;\">In what follows I&rsquo;ll explain why one might think of Giving What We Can or 80,000 Hours as a good giving opportunity. It&rsquo;s of course very awkward to talk about the reasons in favour of donating to one&rsquo;s own organization, and the risk of bias is obvious, so I&rsquo;ll just briefly describe the basic argument, and then leave the rest for discussion. I hope I manage to give an honest picture, rather than just pitching my own favourite idea: we really want to do the most good that we can with marginal resources, so if LW members think that giving to meta-charity in general, or GWWC or 80k in particular, is a bad idea, that&rsquo;s important for us to know. So please don&rsquo;t be shy in raising comments, questions, or criticism. If you find yourself being critical, please try to suggest ways in which GWWC or 80k could either change its activities or provide more information such that your criticisms would be addressed.</span></p>\n<p class=\"MsoNormal\" style=\"tab-stops: 59.1pt;\"><em style=\"mso-bidi-font-style: normal;\"><span style=\"mso-ansi-language: EN-US;\">What is Giving What We Can?</span></em></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><a href=\"http://www.givingwhatwecan.org\">Giving What We Can</a> encourages people to give more and to give more effectively to causes that fight poverty in the developing world.<span style=\"mso-spacerun: yes;\">&nbsp; </span>It encourages people to become a member of the organisation and pledge to give at least 10% of their income to the charities that best fight extreme poverty, and it provides information on its website about how people can give as cost-effectively as possible.</span></p>\n<p class=\"MsoNormal\" style=\"tab-stops: 59.1pt;\"><em style=\"mso-bidi-font-style: normal;\"><span style=\"mso-ansi-language: EN-US;\">What is 80,000 Hours?</span></em></p>\n<p class=\"MsoNormal\" style=\"tab-stops: 59.1pt;\"><span lang=\"EN-GB\"><a href=\"http://www.80000hours.org\">80,000 Hours</a> provides evidence-based advice on careers aiming to make a difference, through its website and through on-one-one advice sessions. It encourages people to use their careers in an effective way to make the world a significantly better place, and aims to help its members to be more successful in their chosen careers. It provides a community and network for those convinced by its ideas.</span></p>\n<p class=\"MsoNormal\"><em style=\"mso-bidi-font-style: normal;\"><span lang=\"EN-GB\">What are the main differences between the two?</span></em></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">The primary differences are that 80,000 Hours focuses on how you should spend your time (especially which career you should choose), whereas Giving What We Can focuses on how you should spend your money. Giving What We Can is focused on global poverty, whereas 80,000 Hours is open to any plausibly high-impact cause. </span></p>\n<p class=\"MsoNormal\" style=\"tab-stops: 59.1pt;\"><em style=\"mso-bidi-font-style: normal;\"><span style=\"mso-ansi-language: EN-US;\">Why should I give to either?</span></em></p>\n<p class=\"MsoNormal\" style=\"tab-stops: 59.1pt;\"><span style=\"mso-ansi-language: EN-US;\">The basic idea is that each of the organisations generates a multiplier on one&rsquo;s donations. By giving $1 to Giving What We Can to fundraise for the best global poverty charities, one ultimately moves significantly more than $1 to the best global poverty charities.<span style=\"mso-spacerun: yes;\">&nbsp; </span>By giving $1 to 80,000 Hours to improve the effectiveness of students&rsquo; career paths, one ultimately moves significantly more than $1&rsquo;s worth of human and financial resources to a range of high-impact causes, including global poverty, animal welfare improvement, and existential risk mitigation. </span></p>\n<p class=\"MsoNormal\" style=\"tab-stops: 59.1pt;\"><em style=\"mso-bidi-font-style: normal;\"><span style=\"mso-ansi-language: EN-US;\">How are you testing this?</span></em></p>\n<p class=\"MsoNormal\" style=\"tab-stops: 59.1pt;\"><span style=\"mso-ansi-language: EN-US;\">Last March we did an impact assessment for Giving What We Can. Some more info is available </span><span lang=\"EN-GB\"><a href=\"http://www.givingwhatwecan.org/getting-involved/donate\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">here</span></a></span><span style=\"mso-ansi-language: EN-US;\">, and I can provide much more information, including the calculations, upon request. As of last March, we&rsquo;d invested $170&nbsp;000&rsquo;s worth of volunteer time into Giving What We Can, and had moved $1.7 million to GiveWell or GWWC top-recommended development charities, and raised a further $68 million in pledged donations.<span style=\"mso-spacerun: yes;\">&nbsp; </span></span><span style=\"mso-fareast-font-family: &quot;Times New Roman&quot;;\" lang=\"EN-GB\">Taking into account the facts that some proportion of this would have been given anyway, there will be some member attrition, and not all donations will go to the very best charities (and using data for all these factors when possible), we estimate that we had raised $8 in realised donations and $130 in future donations for every $1&rsquo;s worth of volunteer time invested in Giving What We Can. </span><span style=\"mso-ansi-language: EN-US;\">We will continue with such impact assessments, most likely on an annual basis.</span></p>\n<p class=\"MsoNormal\" style=\"tab-stops: 59.1pt;\"><span style=\"mso-ansi-language: EN-US;\">We have less data available for 80,000 Hours, but things seem if anything more promising. A preliminary investigation (data from 26 members, last May) suggested that the average member was pledging $1mn; 34% of were planning to donate to existential risk mitigation, 61% to global poverty reduction. Member recruitment currently stands at roughly one per day. 25% of our members state that their career has been &lsquo;significantly changed&rsquo; by 80,000 Hours. A little more information is available </span><span lang=\"EN-GB\"><a href=\"http://80000hours.org/donate\"><span style=\"mso-ansi-language: EN-US;\" lang=\"EN-US\">here</span></a></span><span style=\"mso-ansi-language: EN-US;\">.</span></p>\n<p class=\"MsoNormal\" style=\"tab-stops: 59.1pt;\"><em style=\"mso-bidi-font-style: normal;\"><span style=\"mso-ansi-language: EN-US;\">Why might I be unconvinced?</span></em></p>\n<p class=\"MsoNormal\" style=\"tab-stops: 59.1pt;\"><span style=\"mso-ansi-language: EN-US;\">Here are a few considerations that I think are important (and of course that&rsquo;s not to say there aren&rsquo;t others).</span></p>\n<p class=\"MsoNormal\" style=\"tab-stops: 59.1pt;\"><span style=\"mso-ansi-language: EN-US;\">First, the whole idea of meta-charity is new, and therefore not as robustly tested as other activities. Even if you find the idea of meta-charity compelling, you could plausibly reason that most compelling arguments to new and optimistic conclusions have been false in the past, an so on inductive grounds treat this one with suspicion.</span></p>\n<p class=\"MsoNormal\" style=\"tab-stops: 59.1pt;\"><span style=\"mso-ansi-language: EN-US;\">Second, you might have a very high discount rate. Giving $1 to either GWWC or 80k generates benefits in the future. So working out its cost-effectiveness involves an estimate of how one should value future donations versus donations now. That&rsquo;s a tricky question to answer, and if you have a high enough discount rate, then the investment won&rsquo;t be worth it.</span></p>\n<p class=\"MsoNormal\" style=\"tab-stops: 59.1pt;\"><span style=\"mso-ansi-language: EN-US;\">Third, you might just think that other organisations are better. You might think that other organisations are better at resource-generation (even if that&rsquo;s not their declared aim). Or you might think that it&rsquo;s better just to focus on more direct means of making an impact.</span></p>\n<p class=\"MsoNormal\" style=\"tab-stops: 59.1pt;\"><span style=\"mso-ansi-language: EN-US;\">Finally, you might just have a prior against the idea that one can get a significant multiplier on one&rsquo;s donations to top charities. (One might ask: if the idea of meta-charity is so good, why don&rsquo;t many more meta-charities exist than currently do?) So you might need to see a lot more hard data (perhaps verified by independent sources) before being convinced.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"qAvbtzdG2A2RBn7in": 1, "se3XDuQ4xbeWvu4eF": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FCiMtrsM8mcmBtfTR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 59, "baseScore": 58, "extendedScore": null, "score": 0.00011775540686544972, "legacy": true, "legacyId": "19963", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 44, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 185, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pC47ZTsPNAkjavkXs", "FCxHgPsDScx4C3H8n", "hEqsWLm5zQtsPevd3", "mTeWmzrgtkZo7Ynt2", "qZT6AMhNBbuzneEJt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-15T23:34:02.266Z", "modifiedAt": null, "url": null, "title": "How well defined is ADHD?", "slug": "how-well-defined-is-adhd", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:56.791Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zPktXTuhWo5ovmAw2/how-well-defined-is-adhd", "pageUrlRelative": "/posts/zPktXTuhWo5ovmAw2/how-well-defined-is-adhd", "linkUrl": "https://www.lesswrong.com/posts/zPktXTuhWo5ovmAw2/how-well-defined-is-adhd", "postedAtFormatted": "Thursday, November 15th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20well%20defined%20is%20ADHD%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20well%20defined%20is%20ADHD%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzPktXTuhWo5ovmAw2%2Fhow-well-defined-is-adhd%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20well%20defined%20is%20ADHD%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzPktXTuhWo5ovmAw2%2Fhow-well-defined-is-adhd", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzPktXTuhWo5ovmAw2%2Fhow-well-defined-is-adhd", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 146, "htmlBody": "<p>I've long had attention and focus problems, but never explored the possibility that I have&nbsp;<a href=\"http://en.wikipedia.org/wiki/Attention_deficit_hyperactivity_disorder\">ADHD</a>&nbsp;till recently.&nbsp;I understand that it's a standard term, but I'm still a bit suspicious; Psychiatry doesn't seem like the most reliable field.</p>\n<p>Are there good reasons for picking out the behaviors associated with ADHD and giving them a name? Obviously this is a different question than whether ADHD is a 'disorder' or 'disease', and whether ADHD medication is good or bad for people. &nbsp; &nbsp;</p>\n<p>Answers that would satisfy me</p>\n<p>&nbsp;</p>\n<ul>\n<li>Behaviors associated with ADHD strongly cluster</li>\n<li>Analyzing&nbsp;questionnaires&nbsp;of attention and focus behaviors with faction analysis naturally produces an 'ADHD dimension' that explains a lot of variance (similar methodology to identifying <a href=\"http://en.wikipedia.org/wiki/Big_Five_personality_traits\">Big 5 personality traits</a>).&nbsp;</li>\n<li>ADHD diagnosis a strong predictor of anything interesting (income or grades or some contrived but interesting lab test)?</li>\n<li>Something else along these lines.</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zPktXTuhWo5ovmAw2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 15, "extendedScore": null, "score": 2.9e-05, "legacy": true, "legacyId": "20110", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-16T03:47:15.236Z", "modifiedAt": null, "url": null, "title": "Meetup : DC Meetup: Boardgames", "slug": "meetup-dc-meetup-boardgames", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KNDdNH33yZHEbfJNk/meetup-dc-meetup-boardgames", "pageUrlRelative": "/posts/KNDdNH33yZHEbfJNk/meetup-dc-meetup-boardgames", "linkUrl": "https://www.lesswrong.com/posts/KNDdNH33yZHEbfJNk/meetup-dc-meetup-boardgames", "postedAtFormatted": "Friday, November 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20DC%20Meetup%3A%20Boardgames&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20DC%20Meetup%3A%20Boardgames%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKNDdNH33yZHEbfJNk%2Fmeetup-dc-meetup-boardgames%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20DC%20Meetup%3A%20Boardgames%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKNDdNH33yZHEbfJNk%2Fmeetup-dc-meetup-boardgames", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKNDdNH33yZHEbfJNk%2Fmeetup-dc-meetup-boardgames", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 43, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/g0'>DC Meetup: Boardgames</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 November 2012 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to play games and hang out, and possibly try a new variation of Zendo.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/g0'>DC Meetup: Boardgames</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KNDdNH33yZHEbfJNk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0347644034960189e-06, "legacy": true, "legacyId": "20119", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___DC_Meetup__Boardgames\">Discussion article for the meetup : <a href=\"/meetups/g0\">DC Meetup: Boardgames</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 November 2012 03:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We'll be meeting to play games and hang out, and possibly try a new variation of Zendo.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___DC_Meetup__Boardgames1\">Discussion article for the meetup : <a href=\"/meetups/g0\">DC Meetup: Boardgames</a></h2>", "sections": [{"title": "Discussion article for the meetup : DC Meetup: Boardgames", "anchor": "Discussion_article_for_the_meetup___DC_Meetup__Boardgames", "level": 1}, {"title": "Discussion article for the meetup : DC Meetup: Boardgames", "anchor": "Discussion_article_for_the_meetup___DC_Meetup__Boardgames1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-16T07:33:39.492Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Surprised By Brains", "slug": "seq-rerun-surprised-by-brains", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:56.476Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EpxynqmhL3MqeWXTw/seq-rerun-surprised-by-brains", "pageUrlRelative": "/posts/EpxynqmhL3MqeWXTw/seq-rerun-surprised-by-brains", "linkUrl": "https://www.lesswrong.com/posts/EpxynqmhL3MqeWXTw/seq-rerun-surprised-by-brains", "postedAtFormatted": "Friday, November 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Surprised%20By%20Brains&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Surprised%20By%20Brains%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEpxynqmhL3MqeWXTw%2Fseq-rerun-surprised-by-brains%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Surprised%20By%20Brains%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEpxynqmhL3MqeWXTw%2Fseq-rerun-surprised-by-brains", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEpxynqmhL3MqeWXTw%2Fseq-rerun-surprised-by-brains", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 184, "htmlBody": "<p>Today's post, <a href=\"/lw/w4/surprised_by_brains/\">Surprised by Brains</a> was originally published on 23 November 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Surprised_by_Brains\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>If you hadn't ever seen brains before, but had only seen evolution, you might start making astounding predictions about their ability. You might, for instance, think that creatures with brains might someday be able to create complex machinery in only a millenium.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/fi6/seq_rerun_billion_dollar_bots/\">Billion Dollar Bots</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EpxynqmhL3MqeWXTw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.034890759780717e-06, "legacy": true, "legacyId": "20122", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XQirei3crsLxsCQoi", "urEhgpeHrHAc8h7iv", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-16T14:29:13.811Z", "modifiedAt": null, "url": null, "title": "[draft] Responses to Catastrophic AGI Risk: A Survey", "slug": "draft-responses-to-catastrophic-agi-risk-a-survey", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:57.715Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/23WXja5CtRvpssJpJ/draft-responses-to-catastrophic-agi-risk-a-survey", "pageUrlRelative": "/posts/23WXja5CtRvpssJpJ/draft-responses-to-catastrophic-agi-risk-a-survey", "linkUrl": "https://www.lesswrong.com/posts/23WXja5CtRvpssJpJ/draft-responses-to-catastrophic-agi-risk-a-survey", "postedAtFormatted": "Friday, November 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Bdraft%5D%20Responses%20to%20Catastrophic%20AGI%20Risk%3A%20A%20Survey&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Bdraft%5D%20Responses%20to%20Catastrophic%20AGI%20Risk%3A%20A%20Survey%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F23WXja5CtRvpssJpJ%2Fdraft-responses-to-catastrophic-agi-risk-a-survey%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Bdraft%5D%20Responses%20to%20Catastrophic%20AGI%20Risk%3A%20A%20Survey%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F23WXja5CtRvpssJpJ%2Fdraft-responses-to-catastrophic-agi-risk-a-survey", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F23WXja5CtRvpssJpJ%2Fdraft-responses-to-catastrophic-agi-risk-a-survey", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 307, "htmlBody": "<p>Here's the biggest thing that I've been working on for the last several months:</p>\n<blockquote>\n<p><span style=\"text-decoration: underline;\"><strong>Responses to Catastrophic AGI Risk: A Survey</strong></span><br />Kaj Sotala, Roman Yampolskiy, and Luke Muehlhauser</p>\n<p><em>Abstract: </em>Many researchers have argued that humanity will create artificial general intelligence (AGI) within the next 20-100 years. It has been suggested that this may become a catastrophic risk, threatening to do major damage on a global scale. After briefly summarizing the arguments for why AGI may become a catastrophic risk, we survey various proposed responses to AGI risk. We consider societal proposals, proposals for constraining the AGIs&rsquo; behavior from the outside, and for creating AGIs in such a way that they are inherently safe.</p>\n</blockquote>\n<p>This doesn't aim to be a very strongly argumentative paper, though it does comment on the various proposals from an SI-ish point of view. Rather, it attempts to provide a survey of all the major AGI-risk related proposals that have been made so far, and to provide some thoughts on their respective strengths and weaknesses. Before writing this paper, we hadn't encountered anyone who'd have been familiar with all of these proposals - not to mention that even we ourselves weren't familiar with all of them! Hopefully, this should become a useful starting point for anyone who's at all interested in AGI risk or Friendly AI.</p>\n<p>The draft will be public and open for comments for one week (until Nov 23rd), after which we'll incorporate the final edits and send it off for review. We're currently aiming to have it published in the sequel volume to <em><a href=\"http://singularityhypothesis.blogspot.com/p/table-of-contents.html\">Singularity Hypotheses</a></em>.</p>\n<p><strong>EDIT: I've now hidden the draft from public view (so as to avoid annoying future publishers who may not like early drafts floating around before the work has been accepted for publication) while I'm incorporating all the feedback that we got. Thanks to everyone who commented!</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "23WXja5CtRvpssJpJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 19, "extendedScore": null, "score": 1.0351227617313792e-06, "legacy": true, "legacyId": "20123", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-16T17:03:23.716Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Austin, Berlin, Champaign, Melbourne, Rio de Janeiro, Springfield MO, Washington DC", "slug": "weekly-lw-meetups-austin-berlin-champaign-melbourne-rio-de", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kfDRzBhJWWzk47XmR/weekly-lw-meetups-austin-berlin-champaign-melbourne-rio-de", "pageUrlRelative": "/posts/kfDRzBhJWWzk47XmR/weekly-lw-meetups-austin-berlin-champaign-melbourne-rio-de", "linkUrl": "https://www.lesswrong.com/posts/kfDRzBhJWWzk47XmR/weekly-lw-meetups-austin-berlin-champaign-melbourne-rio-de", "postedAtFormatted": "Friday, November 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Austin%2C%20Berlin%2C%20Champaign%2C%20Melbourne%2C%20Rio%20de%20Janeiro%2C%20Springfield%20MO%2C%20Washington%20DC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Austin%2C%20Berlin%2C%20Champaign%2C%20Melbourne%2C%20Rio%20de%20Janeiro%2C%20Springfield%20MO%2C%20Washington%20DC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkfDRzBhJWWzk47XmR%2Fweekly-lw-meetups-austin-berlin-champaign-melbourne-rio-de%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Austin%2C%20Berlin%2C%20Champaign%2C%20Melbourne%2C%20Rio%20de%20Janeiro%2C%20Springfield%20MO%2C%20Washington%20DC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkfDRzBhJWWzk47XmR%2Fweekly-lw-meetups-austin-berlin-champaign-melbourne-rio-de", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkfDRzBhJWWzk47XmR%2Fweekly-lw-meetups-austin-berlin-champaign-melbourne-rio-de", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 486, "htmlBody": "<p><strong>This summary was posted to LW Main on Nov 9th. The following week's summary is <a href=\"/lw/fj0/weekly_lw_meetups_austin_chicago/\">here</a>.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/fp\">Rio de Janeiro Meetup:&nbsp;<span class=\"date\">09 November 2012 08:00PM</span></a></li>\n<li><a href=\"/meetups/fa\">(Springfield MO) Skepticon 5:&nbsp;<span class=\"date\">10 November 2012 01:06PM</span></a></li>\n<li><a href=\"/meetups/fk\">Washington DC Show and tell meetup: Economics II:&nbsp;<span class=\"date\">11 November 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/fq\">Berlin Meetup:&nbsp;<span class=\"date\">14 November 2012 07:30PM</span></a></li>\n<li><a href=\"/meetups/fn\">Meetup, Champaign IL, :&nbsp;<span class=\"date\">14 November 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/fm\">Moscow: Applied Rationality and Cognitive Biases:&nbsp;<span class=\"date\">17 November 2012 04:00PM</span></a></li>\n<li><a href=\"/meetups/fb\">First Meetup- Cleveland and Akron, Ohio:&nbsp;<span class=\"date\">17 November 2012 08:03PM</span></a></li>\n<li><a href=\"/meetups/fc\">18/11 London Meetup:&nbsp;<span class=\"date\">18 November 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/fo\">(St. Louis MO) Psycology of memorization, Thiel Fellow James Koppel, and games:&nbsp;<span class=\"date\">24 November 2012 02:30PM</span></a></li>\n<li><a href=\"/meetups/el\">Sofia, Bulgaria Meetup:&nbsp;<span class=\"date\">09 December 2012 05:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">10 November 2018 02:30PM</span></a></li>\n<li><a href=\"/meetups/fj\">Melbourne social meetup:&nbsp;<span class=\"date\">16 November 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/en\">Winter Solstice Megameetup - NYC:&nbsp;<span class=\"date\">15 December 2012 05:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong></strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kfDRzBhJWWzk47XmR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.0352088510801762e-06, "legacy": true, "legacyId": "19962", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Ba7zs4f9LTpEC3H7r", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-16T18:00:25.436Z", "modifiedAt": null, "url": null, "title": "Meetup : Cambridge, MA third-Sundays meetup", "slug": "meetup-cambridge-ma-third-sundays-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LuPSAXbyppvsggbf5/meetup-cambridge-ma-third-sundays-meetup", "pageUrlRelative": "/posts/LuPSAXbyppvsggbf5/meetup-cambridge-ma-third-sundays-meetup", "linkUrl": "https://www.lesswrong.com/posts/LuPSAXbyppvsggbf5/meetup-cambridge-ma-third-sundays-meetup", "postedAtFormatted": "Friday, November 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Cambridge%2C%20MA%20third-Sundays%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Cambridge%2C%20MA%20third-Sundays%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLuPSAXbyppvsggbf5%2Fmeetup-cambridge-ma-third-sundays-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Cambridge%2C%20MA%20third-Sundays%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLuPSAXbyppvsggbf5%2Fmeetup-cambridge-ma-third-sundays-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLuPSAXbyppvsggbf5%2Fmeetup-cambridge-ma-third-sundays-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/g1'>Cambridge, MA third-Sundays meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">18 November 2012 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">25 Ames St Cambridge, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cambridge/Boston-area Less Wrong meetups on the first and third Sunday of every month, 2pm at the MIT Landau Building [25 Ames St, Bldg 66], room 148. Room number subject to change based on availability, signs will be posted with the actual room number.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/g1'>Cambridge, MA third-Sundays meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LuPSAXbyppvsggbf5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.0352407003955927e-06, "legacy": true, "legacyId": "20125", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Cambridge__MA_third_Sundays_meetup\">Discussion article for the meetup : <a href=\"/meetups/g1\">Cambridge, MA third-Sundays meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">18 November 2012 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">25 Ames St Cambridge, MA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cambridge/Boston-area Less Wrong meetups on the first and third Sunday of every month, 2pm at the MIT Landau Building [25 Ames St, Bldg 66], room 148. Room number subject to change based on availability, signs will be posted with the actual room number.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Cambridge__MA_third_Sundays_meetup1\">Discussion article for the meetup : <a href=\"/meetups/g1\">Cambridge, MA third-Sundays meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Cambridge, MA third-Sundays meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge__MA_third_Sundays_meetup", "level": 1}, {"title": "Discussion article for the meetup : Cambridge, MA third-Sundays meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge__MA_third_Sundays_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-16T18:01:04.360Z", "modifiedAt": null, "url": null, "title": "Meetup : Montreal Meetups - Now weekly!", "slug": "meetup-montreal-meetups-now-weekly", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DaFranker", "createdAt": "2012-07-04T16:59:52.155Z", "isAdmin": false, "displayName": "DaFranker"}, "userId": "vaBa5yEmQddzeryNt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vo4Twb4WhkMfGnuQu/meetup-montreal-meetups-now-weekly", "pageUrlRelative": "/posts/vo4Twb4WhkMfGnuQu/meetup-montreal-meetups-now-weekly", "linkUrl": "https://www.lesswrong.com/posts/vo4Twb4WhkMfGnuQu/meetup-montreal-meetups-now-weekly", "postedAtFormatted": "Friday, November 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Montreal%20Meetups%20-%20Now%20weekly!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Montreal%20Meetups%20-%20Now%20weekly!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvo4Twb4WhkMfGnuQu%2Fmeetup-montreal-meetups-now-weekly%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Montreal%20Meetups%20-%20Now%20weekly!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvo4Twb4WhkMfGnuQu%2Fmeetup-montreal-meetups-now-weekly", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvo4Twb4WhkMfGnuQu%2Fmeetup-montreal-meetups-now-weekly", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 149, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/g2'>Montreal Meetups - Now weekly!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">19 November 2012 06:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Caffe Art Java - 645, av. du Pr\u00e9sident-Kennedy, Montr\u00e9al, QC H3A 1K1</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>After several meetings with two other LessWrong readers and several \"outside\" friends or acquaintances interested in rationality, we've established that we'll meet every Monday at 18:00.</p>\n\n<p>At this next meetup, we'll attempt to solidify-in-practice some key concepts of LW-style epistemology and start moving into the realm of training thought habits for <a href=\"http://lesswrong.com/lw/od/37_ways_that_words_can_be_wrong/\">using words (more) correctly</a>.</p>\n\n<p>If you're interested in joining us, we're interested in having you! If you have questions, please contact <a href=\"http://lesswrong.com/user/Paul_G/\">Paul_G</a> or <a href=\"http://lesswrong.com/user/DaFranker/\">myself</a>.</p>\n\n<p>We don't really have a public signal yet, so if you want to avoid the possible embarrassment of asking random people in the caf\u00e9 if they're here for the LessWrong meetup, I recommend getting in touch with us.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/g2'>Montreal Meetups - Now weekly!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vo4Twb4WhkMfGnuQu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.0352410626912402e-06, "legacy": true, "legacyId": "20126", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Montreal_Meetups___Now_weekly_\">Discussion article for the meetup : <a href=\"/meetups/g2\">Montreal Meetups - Now weekly!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">19 November 2012 06:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Caffe Art Java - 645, av. du Pr\u00e9sident-Kennedy, Montr\u00e9al, QC H3A 1K1</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>After several meetings with two other LessWrong readers and several \"outside\" friends or acquaintances interested in rationality, we've established that we'll meet every Monday at 18:00.</p>\n\n<p>At this next meetup, we'll attempt to solidify-in-practice some key concepts of LW-style epistemology and start moving into the realm of training thought habits for <a href=\"http://lesswrong.com/lw/od/37_ways_that_words_can_be_wrong/\">using words (more) correctly</a>.</p>\n\n<p>If you're interested in joining us, we're interested in having you! If you have questions, please contact <a href=\"http://lesswrong.com/user/Paul_G/\">Paul_G</a> or <a href=\"http://lesswrong.com/user/DaFranker/\">myself</a>.</p>\n\n<p>We don't really have a public signal yet, so if you want to avoid the possible embarrassment of asking random people in the caf\u00e9 if they're here for the LessWrong meetup, I recommend getting in touch with us.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Montreal_Meetups___Now_weekly_1\">Discussion article for the meetup : <a href=\"/meetups/g2\">Montreal Meetups - Now weekly!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Montreal Meetups - Now weekly!", "anchor": "Discussion_article_for_the_meetup___Montreal_Meetups___Now_weekly_", "level": 1}, {"title": "Discussion article for the meetup : Montreal Meetups - Now weekly!", "anchor": "Discussion_article_for_the_meetup___Montreal_Meetups___Now_weekly_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FaJaCgqBKphrDzDSj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-16T18:37:56.618Z", "modifiedAt": "2020-01-27T11:55:00.644Z", "url": null, "title": "Why is Mencius Moldbug so popular on Less Wrong? [Answer: He's not.]", "slug": "why-is-mencius-moldbug-so-popular-on-less-wrong-answer-he-s", "viewCount": null, "lastCommentedAt": "2014-01-29T22:28:42.431Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "arborealhominid", "createdAt": "2012-11-05T00:09:57.272Z", "isAdmin": false, "displayName": "arborealhominid"}, "userId": "sTCyfhqakB5LyCS7Y", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6qPextf9KyWLFJ53j/why-is-mencius-moldbug-so-popular-on-less-wrong-answer-he-s", "pageUrlRelative": "/posts/6qPextf9KyWLFJ53j/why-is-mencius-moldbug-so-popular-on-less-wrong-answer-he-s", "linkUrl": "https://www.lesswrong.com/posts/6qPextf9KyWLFJ53j/why-is-mencius-moldbug-so-popular-on-less-wrong-answer-he-s", "postedAtFormatted": "Friday, November 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20is%20Mencius%20Moldbug%20so%20popular%20on%20Less%20Wrong%3F%20%5BAnswer%3A%20He's%20not.%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20is%20Mencius%20Moldbug%20so%20popular%20on%20Less%20Wrong%3F%20%5BAnswer%3A%20He's%20not.%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6qPextf9KyWLFJ53j%2Fwhy-is-mencius-moldbug-so-popular-on-less-wrong-answer-he-s%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20is%20Mencius%20Moldbug%20so%20popular%20on%20Less%20Wrong%3F%20%5BAnswer%3A%20He's%20not.%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6qPextf9KyWLFJ53j%2Fwhy-is-mencius-moldbug-so-popular-on-less-wrong-answer-he-s", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6qPextf9KyWLFJ53j%2Fwhy-is-mencius-moldbug-so-popular-on-less-wrong-answer-he-s", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 220, "htmlBody": "<p>I've seen several people on Less Wrong recommend Mencius Moldbug's writings, and I've been curious about how he became so popular here. He's certainly an interesting thinker, but he's rather obscure and doesn't have any obvious connection to Less Wrong, so I'm wondering where this overlap in readership came from.</p>\n<p><em>[EDIT by E.Y.: The answer is that he's not popular here. &nbsp;The 2012 LW annual survey showed 2.5% (30 of 1195 responses) identified as 'reactionary' or 'Moldbuggian'. &nbsp;To the extent this is greater than population average, it seems sufficiently explained by Moldbug having commented on the early Overcoming Bias econblog before LW forked from it, bringing with some of his own pre-existing audience. &nbsp;I cannot remember running across anyone talking about Moldbug on LW, at all, besides this post, in the last year or so. &nbsp;</em><em>Since this page has now risen to the first page of Google results for Mencius Moldbug due to LW's high pagerank, and on at least one occasion sloppy / agenda-promoting journalists such as Klint Finley have found it convenient to pretend to an alternate reality (where Moldbug is popular on LW and Hacker News due to speaking out for angry entitled Silicon Valley elites, or something), a correction in the post seems deserved. &nbsp;See also the <a href=\"http://slatestarcodex.com/2013/10/20/the-anti-reactionary-faq/\">Anti-Reactionary FAQ</a> by Scott Alexander (aka Yvain, LW's second-highest-karma user).&nbsp;</em><em>--EY]</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6qPextf9KyWLFJ53j", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 18, "extendedScore": null, "score": 4e-05, "legacy": true, "legacyId": "20056", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 259, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2012-11-16T18:37:56.618Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-16T19:26:11.490Z", "modifiedAt": null, "url": null, "title": "Prediction: Autism Rate will Stop Increasing", "slug": "prediction-autism-rate-will-stop-increasing", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:08.310Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OneLonePrediction", "createdAt": "2012-11-16T07:35:57.703Z", "isAdmin": false, "displayName": "OneLonePrediction"}, "userId": "aHEWeyNz6BHYDYdcw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KmTJLTBmscvKwgSRv/prediction-autism-rate-will-stop-increasing", "pageUrlRelative": "/posts/KmTJLTBmscvKwgSRv/prediction-autism-rate-will-stop-increasing", "linkUrl": "https://www.lesswrong.com/posts/KmTJLTBmscvKwgSRv/prediction-autism-rate-will-stop-increasing", "postedAtFormatted": "Friday, November 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Prediction%3A%20Autism%20Rate%20will%20Stop%20Increasing&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APrediction%3A%20Autism%20Rate%20will%20Stop%20Increasing%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKmTJLTBmscvKwgSRv%2Fprediction-autism-rate-will-stop-increasing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Prediction%3A%20Autism%20Rate%20will%20Stop%20Increasing%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKmTJLTBmscvKwgSRv%2Fprediction-autism-rate-will-stop-increasing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKmTJLTBmscvKwgSRv%2Fprediction-autism-rate-will-stop-increasing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 765, "htmlBody": "<p>I predict that the prevalence of autism spectrum disorders is done increasing because it has all come from better diagnosis. The autism rate in children has now reached one in 88; the autism rate in adults is estimated at one in 86. We just went within my error bars.</p>\n<p>There are two things that I think could confuse the issue. The first is that the DSM-V will come out soon. If, following the DSM-V, diagnosticians continue exactly what they're currently doing, except that they diagnose all autism spectrum disorders as autistic disorder (which will not affect the one in 88 statistic because Asperger's and PDD-NOS have always been part of it), then the prevalence rate has stopped increasing. If, following the DSM-V, diagnosticians do what it tells them, the prevalence rate will decrease, with the loss coming from people with PDD-NOS who have communication or language difficulties and one of the other two points in the triad of impairments.</p>\n<p>The second is that the adult rate may be lower than the childhood rate because of the existence of people who are diagnosable as children but not as adults because of learned coping skills. If that's true, the rate may continue to increase.</p>\n<p>I will consider myself right if it reaches one in 84, slightly surprised at one in 80 (I'll assume I underestimated the number of people diagnosable only as children), shaken and looking for explanations at one in 75 and outright wrong (I will abandon the theory and concede defeat) if the prevalence reaches one in 70 without some really significant evidence of overdiagnosis.</p>\n<p>I also allocate some probability mass to the idea that the prevalence rate will decrease. I don't predict a huge decrease with great likelihood, but if I do see one, I will update on my beliefs about diagnosticians and watch who is and isn't getting diagnosed in the next youngest cohort. If the DSM-V causes such a drop, I would expect it to more likely be sudden as doctors adopt changes after the DSM-V comes out, but it could be slow and steady if older doctors do as they were already doing and younger doctors act differently. Those cases could be distinguished by looking at diagnoses made by older doctors and newer ones and comparing them.</p>\n<p>I note that while I do not predict a huge drop with great probability, mine is the only theory which would explain any drop at all.</p>\n<p>I predict that if there is a drop, John Best will claim that \"lying neurodiverse psychopaths\" are somehow responsible and that it harms \"actual autistics\" and their families. Note that I assign only small probability to the actual phrases given. For instance, he may suggest \"real autistics\" or \"families actually affected by real autism\" or any number of things. (\"Psychopaths\" will be in there somewhere as a description of people who do not want a cure for autism. If he doesn't call them liars in his blog post, they will be called liars somewhere in the comments.) If I am wrong, the most likely other possibility is that he is triumphant and believes that his supporters are \"getting the message out\" and parents are not vaccinating anymore. My model of reality takes a hit if John Best ever claims that new and surprising evidence does not support his ideas about autism. This does not apply to everyone who is against vaccines. It only applies to him.&nbsp;</p>\n<p>(Trivial prediction: you will get upset if you read his blogs. Don't go looking for them unless your utility function values becoming upset over false claims.)</p>\n<p>If the autism rate is stable, this is evidence that it has been stable for a long time (I believe this because the increasing rate has been used as evidence that it did not exist before the last century and that it is environmentally caused) and if it has been stable for a long time, this is evidence against it being caused by vaccines, because that would have caused the prevalence to increase.</p>\n<p>Please spread this around as much as possible. I am predicting ahead of time that: <em>The autism rate does not go above one in eighty, probably stays stable in the high-to-mid eighties and may decrease. I admit that I was wrong if it reaches one in seventy</em>. Please help: I want this well-known. I want people to know I made the prediction before we see the evidence. Sharing a link to this would be a quick and easy way to increase average utility and expose people to the idea of falsifiable ideas that make predictions about what they will and won't see.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"k9pXZBsM8wMRwwK4J": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KmTJLTBmscvKwgSRv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 4, "extendedScore": null, "score": 1.6e-05, "legacy": true, "legacyId": "20127", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-16T20:45:18.580Z", "modifiedAt": null, "url": null, "title": "Room for more funding at the Future of Humanity Institute", "slug": "room-for-more-funding-at-the-future-of-humanity-institute", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:33.911Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iKbFQLzZqhBZKdx5s/room-for-more-funding-at-the-future-of-humanity-institute", "pageUrlRelative": "/posts/iKbFQLzZqhBZKdx5s/room-for-more-funding-at-the-future-of-humanity-institute", "linkUrl": "https://www.lesswrong.com/posts/iKbFQLzZqhBZKdx5s/room-for-more-funding-at-the-future-of-humanity-institute", "postedAtFormatted": "Friday, November 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Room%20for%20more%20funding%20at%20the%20Future%20of%20Humanity%20Institute&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARoom%20for%20more%20funding%20at%20the%20Future%20of%20Humanity%20Institute%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiKbFQLzZqhBZKdx5s%2Froom-for-more-funding-at-the-future-of-humanity-institute%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Room%20for%20more%20funding%20at%20the%20Future%20of%20Humanity%20Institute%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiKbFQLzZqhBZKdx5s%2Froom-for-more-funding-at-the-future-of-humanity-institute", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiKbFQLzZqhBZKdx5s%2Froom-for-more-funding-at-the-future-of-humanity-institute", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 403, "htmlBody": "<p>In case you didn't already know: The <a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a>, one of the three organizations co-sponsoring LW, is a group within the University of Oxford's philosophy department that tackles important, large-scale problems for humanity like how to go about reducing existential risk.</p>\n<p>I've been casually corresponding with the FHI in an effort to learn more about the different options available for purchasing existential risk reduction. Here's a summary of what I've learned from research fellow Stuart Armstrong and academic project manager Sean O'Heigeartaigh:</p>\n<ul>\n<li>Sean reports that since <a href=\"/lw/7sc/siai_vs_fhi_achievements_20082010/\">this</a> SIAI/FHI achievements comparison, FHI's full-time research team has expanded to 7, the biggest it's ever been.&nbsp; Sean writes: \"Our output has improved dramatically by all tangible metrics (academic papers, outreach, policy impact, etc) to match this.\"</li>\n<li>Despite this, Sean writes, \"we&rsquo;re not nearly at the capacity we&rsquo;d like to reach. There are a number of research areas in which we would very like to expand (more machine intelligence work, synthetic biology risks, surveillance/information society work) and in which we feel that we could make a major impact. There are also quite a number of talented researchers over the past year who we haven&rsquo;t been able to employ but would dearly like to.\"</li>\n<li>They'd also like to do more public outreach, but standard academic funding routes aren't likely to cover this.&nbsp; So without funding from individuals, it's much less likely to happen.</li>\n<li>Sean is currently working overtime to cover a missing administrative staff member, but he plans to release a new achievement report (see sidebar on <a href=\"http://www.fhi.ox.ac.uk/about\">this page</a> for past achievement reports) sometime in the next few months.</li>\n<li>Although the FHI has traditionally pursued standard academic funding channels, donations from individuals (small and large) are more than welcome.&nbsp; (Stuart says this can't be emphasized enough.)</li>\n<li>Stuart reports current academic funding opportunities are \"a bit iffy, with some possible hopes\".</li>\n<li>Sean is more optimistic than Stuart regarding near-term funding prospects, although he does mention that both Stuart and Anders Sandberg are currently being covered by FHI's \"non-assigned\" funding until grants for them can be secured.</li>\n</ul>\n<p>Although neither Stuart nor Sean mentions this, I assume that one reason individual donations can be especially valuable is if they free FHI researchers up from writing grant proposals so they can spend more time doing actual research.</p>\n<p><a href=\"/lw/7sc/siai_vs_fhi_achievements_20082010/4w6v\">Interesting comment</a> by lukeprog describing the comparative advantages of SIAI and FHI.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"K6oowPZC6kds6LDTg": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iKbFQLzZqhBZKdx5s", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 26, "extendedScore": null, "score": 1.0353327954100763e-06, "legacy": true, "legacyId": "19810", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["KB2nF3WFF5n5Pyzds"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-16T20:56:30.399Z", "modifiedAt": null, "url": null, "title": "What does the world look like, the day before FAI efforts succeed?", "slug": "what-does-the-world-look-like-the-day-before-fai-efforts", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:33.046Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "bfq5YorFxpih9j6nL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/c3uuAdZzkjv7LKQr9/what-does-the-world-look-like-the-day-before-fai-efforts", "pageUrlRelative": "/posts/c3uuAdZzkjv7LKQr9/what-does-the-world-look-like-the-day-before-fai-efforts", "linkUrl": "https://www.lesswrong.com/posts/c3uuAdZzkjv7LKQr9/what-does-the-world-look-like-the-day-before-fai-efforts", "postedAtFormatted": "Friday, November 16th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20does%20the%20world%20look%20like%2C%20the%20day%20before%20FAI%20efforts%20succeed%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20does%20the%20world%20look%20like%2C%20the%20day%20before%20FAI%20efforts%20succeed%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc3uuAdZzkjv7LKQr9%2Fwhat-does-the-world-look-like-the-day-before-fai-efforts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20does%20the%20world%20look%20like%2C%20the%20day%20before%20FAI%20efforts%20succeed%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc3uuAdZzkjv7LKQr9%2Fwhat-does-the-world-look-like-the-day-before-fai-efforts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fc3uuAdZzkjv7LKQr9%2Fwhat-does-the-world-look-like-the-day-before-fai-efforts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1738, "htmlBody": "<p><em>TL;DR: let's visualize what the world looks like if we successfully prepare for the Singularity.</em></p>\n<p class=\"p1\">I remember reading once, though I can't remember where, about a technique called 'contrasting'. The idea is to visualize a world where you've accomplished your goals, and visualize the current world, and hold the two worlds in contrast to each other. Apparently there was a study about this; the experimental 'contrasting' group was more successful than the control in accomplishing its goals.</p>\n<p class=\"p1\">It occurred to me that we need some of this. <a href=\"/lw/ffh/how_can_i_reduce_existential_risk_from_ai/\">Strategic insights</a> about the path to FAI are not robust or likely to be highly reliable. And in order to find a path forward, you need to know where you're trying to go. Thus, some contrasting:</p>\n<blockquote>\n<p class=\"p1\"><em>It's the year 20XX. The time is 10 AM, on the day that will thereafter be remembered as the beginning of the post-Singularity world. Since the dawn of the century, a movement rose in defense of humanity's future. What began with mailing lists and blog posts became a slew of businesses, political interventions, infrastructure improvements, social influences, and technological innovations designed to ensure the safety of the world.</em></p>\n<p class=\"p1\"><em>Despite all odds, we exerted a <a href=\"/lw/uo/make_an_extraordinary_effort/\">truly extraordinary effort</a></em><em>, and we did it. The AI research is done; we've laboriously tested and re-tested our code, and everyone agrees that the AI is safe. It's time to hit 'Run'.</em></p>\n</blockquote>\n<p class=\"p2\">And so I ask you, before we hit the button: what does this world look like? In the scenario where we nail it, which achievements enabled our success?&nbsp;Socially? Politically? Technologically? What resources did we acquire? Did we have superior technology, or a high degree of secrecy? Was FAI research highly prestigious, attractive, and well-funded? Did we acquire the ability to move quickly, or did we slow unFriendly AI research efforts? What else?</p>\n<p class=\"p1\">I had a few ideas, which I divided between scenarios where we did a 'fantastic', 'good', or 'sufficient' job at preparing for the Singularity. But I need more ideas! I'd like to fill this out in detail, with the help of Less Wrong. <strong>So if you have ideas, write them in the comments, and I'll update the list.</strong></p>\n<p class=\"p1\">Some meta points:</p>\n<ul class=\"ul1\">\n<li class=\"li1\">This speculation is going to be, well, pretty speculative. That's fine - I'm just trying to put some points on the map.&nbsp;</li>\n<li class=\"li1\">However, I'd like to get a list of reasonable possibilities, not detailed sci-fi stories. Do your best.</li>\n<li class=\"li1\">In most cases, I'd like to consolidate categories of possibilities. For example, we could consolidate \"the FAI team has exclusive access to smart drugs\" and \"the FAI team has exclusive access to brain-computer interfaces\" into \"the FAI team has exclusive access to intelligence-amplification technology.\"&nbsp;</li>\n<li class=\"li1\">However, I don't want&nbsp;<em>too</em>&nbsp;much consolidation. For example, I wouldn't want to consolidate \"the FAI team gets an incredible amount of government funding\" and \"the FAI team has exclusive access to intelligence-amplification technology\" into \"the FAI team has a lot of power\".</li>\n<li class=\"li1\">Lots of these possibilities are going to be mutually exclusive; don't see them as aspects of the same scenario, but rather different scenarios.</li>\n</ul>\n<p class=\"p1\">Anyway - I'll start.</p>\n<h1>Visualizing the pre-FAI world</h1>\n<ul class=\"ul1\">\n<li class=\"li1\">Fantastic scenarios \n<ul class=\"ul2\">\n<li class=\"li1\">The FAI team has exclusive access to intelligence amplification technology, and use it to ensure Friendliness &amp; strategically reduce X-risk.</li>\n<li class=\"li1\">The government supports Friendliness research, and contributes significant resources to the problem.&nbsp;</li>\n<li class=\"li1\">The government actively implements legislation which FAI experts and strategists believe has a high probability of making AI research safer.</li>\n<li class=\"li1\">FAI research becomes a highly prestigious and well-funded field, relative to AGI research.</li>\n<li class=\"li1\">Powerful social memes exist regarding AI safety; any new proposal for AI research is met with a strong reaction (among the populace and among academics alike) asking about safety precautions. It is low status to research AI without concern for Friendliness.</li>\n<li class=\"li1\">The FAI team discovers important strategic insights through a growing ecosystem of prediction technology; using stables of experts, prediction markets, and opinion aggregation.</li>\n<li class=\"li1\">The FAI team implements deliberate X-risk reduction efforts to stave off non-AI X-risks. Those might include a global nanotech immune system, cheap and rigorous biotech tests and safeguards, nuclear safeguards, etc.</li>\n<li class=\"li1\">The FAI team implements the infrastructure for a high-security research effort, perhaps offshore, implementing the best available security measures designed to reduce harmful information leaks.</li>\n<li class=\"li1\"><a href=\"/r/discussion/lw/fj4/what_does_the_world_look_like_the_day_before_fai/7ukg\">Giles writes</a>: Large amounts of funding are available, via government or through business. The FAI team and its support network may have used superior rationality to acquire very large amounts of money.</li>\n<li class=\"li1\"><a href=\"/r/discussion/lw/fj4/what_does_the_world_look_like_the_day_before_fai/7ul3\">Giles writes</a>:&nbsp;The technical problem of establishing Friendliness is easier than expected; we are able t construct a 'utility function'&nbsp;(or a procedure for determining such a function)&nbsp;in order to implement human values that people (including people with a broad range of expertise) are happy with.</li>\n<li class=\"li1\"><a href=\"/r/discussion/lw/fj4/what_does_the_world_look_like_the_day_before_fai/7v6s\">Crude_Dolorium writes</a>: FAI research proceeds much faster than AI research, so by the time we can make a superhuman AI, we already know how to make it Friendly (and we know what we really want that to mean).</li>\n</ul>\n</li>\n<li class=\"li1\">Pretty good scenarios \n<ul class=\"ul2\">\n<li class=\"li1\">Intelligence amplification technology access isn't exclusive to the&nbsp;FAI team, but it <em>is</em>&nbsp;differentially adopted by the&nbsp;FAI team and their supporting network, resulting in a net increase in&nbsp;FAI team&nbsp;intelligence relative to baseline. The&nbsp;FAI team uses&nbsp;it to ensure Friendliness and implement strategy surrounding FAI research.</li>\n<li class=\"li1\">The government has extended some kind of support for Friendliness research, such as limited funding. No protective legislation is forthcoming.</li>\n<li class=\"li1\">FAI research becomes slightly more high status than today, and additional researchers are attracted to answer important open questions about FAI.</li>\n<li class=\"li1\">Friendliness and rationality memes grow at a reasonable rate, and by the time the Friendliness program occurs, society is more sane.</li>\n<li class=\"li1\">We get slightly better at making predictions, mostly by refining our current research and discussion strategies. This allows us a few key insights that are instrumental in reducing X-risk.</li>\n<li class=\"li1\">Some X-risk reduction efforts have been implemented, but with varying levels of success. Insights about which X-risk efforts matter are of dubious quality, and the success of each effort doesn't correlate well to the seriousness of the X-risk. Nevertheless, some X-risk reduction is achieved, and humanity survives long enough to implement FAI.</li>\n<li class=\"li1\">Some security efforts are implemented, making it difficult but not impossible for pre-Friendly AI tech to be leaked. Nevertheless, no leaks happen.</li>\n<li class=\"li1\"><a href=\"/r/discussion/lw/fj4/what_does_the_world_look_like_the_day_before_fai/7ukg\">Giles writes</a>: Funding is harder to come by, but small donations, limited government funding, or moderately successful business efforts suffice to fund the FAI team.</li>\n<li class=\"li1\"><a href=\"/r/discussion/lw/fj4/what_does_the_world_look_like_the_day_before_fai/7ul3\">Giles writes</a>:&nbsp;The technical problem of aggregating values through a Friendliness function is difficult; people have contradictory and differing values. However, there is broad agreement as to how to aggregate preferences. Most people accept that FAI needs to respect values of humanity as a whole, not just their own.</li>\n<li class=\"li1\"><a href=\"/r/discussion/lw/fj4/what_does_the_world_look_like_the_day_before_fai/7v6s\">Crude_Dolorium writes</a>: Superhuman AI arrives before we learn how to make it Friendly, but we do learn how to make an '<a href=\"http://en.wikipedia.org/wiki/Anchorite\">Anchorite</a>' AI that definitely won't take over the world. The first superhuman AIs use this architecture, and we use them to solve the harder problems of FAI before anyone sets off an exploding UFAI.</li>\n</ul>\n</li>\n<li class=\"li1\">Sufficiently good scenarios \n<ul class=\"ul2\">\n<li class=\"li1\">Intelligence amplification technology is widespread, preventing any differential adoption by the&nbsp;FAI team. However, FAI researchers are able to keep up with competing efforts to use that technology for AI research.</li>\n<li class=\"li1\">The government doesn't support Friendliness research, but the research group stays out of trouble and avoids government interference.</li>\n<li class=\"li1\">FAI research never becomes prestigious or high-status, but the&nbsp;FAI team is&nbsp;able to answer the important questions anyway.</li>\n<li class=\"li1\">Memes regarding Friendliness aren't significantly more widespread than today, but &nbsp;the movement has grown enough to attract the talent necessary to implement a Friendliness program.</li>\n<li class=\"li1\">Predictive ability is no better than it is today, but the few insights we've gathered suffice to build the FAI team and make the project happen.</li>\n<li class=\"li1\">There are no significant and successful X-risk reduction efforts, but humanity survives long enough to implement FAI anyway.</li>\n<li class=\"li1\">No significant security measures are implemented for the FAI project. Still, via cooperation and because the team is relatively unknown, no dangerous leaks occur.</li>\n<li class=\"li1\"><a href=\"/r/discussion/lw/fj4/what_does_the_world_look_like_the_day_before_fai/7ukg\">Giles writes</a>: The team is forced to operate on a shoestring budget, but succeeds anyway because the problem turns out to not be incredibly sensitive to funding constraints.</li>\n<li class=\"li1\"><a href=\"/r/discussion/lw/fj4/what_does_the_world_look_like_the_day_before_fai/7ul3\">Giles writes</a>: The technical problem of aggregating values is incredibly difficult. Many important human values contradict each other, and we have discovered no \"best\" solution to those conflicts. Most people agree on the need for a compromise but quibble over how that compromise should be reached. Nevertheless, we come up with a satisfactory compromise.</li>\n<li class=\"li1\"><a href=\"/r/discussion/lw/fj4/what_does_the_world_look_like_the_day_before_fai/7v6s\">Crude_Dolorium writes</a>:&nbsp;The problems of Friendliness aren't solved in time, or the solutions don't apply to practical architectures, or the creators of the first superhuman AIs don't use them, so the AIs have only unreliable safeguards. They're given <a href=\"http://api.viglink.com/api/click?format=go&amp;key=9f37ca02a1e3cbd4f3d0a3618a39fbca&amp;loc=http%3A%2F%2Flesswrong.com%2Fr%2Fdiscussion%2Flw%2Ffj4%2Fwhat_does_the_world_look_like_the_day_before_fai%2F7v6s&amp;v=1&amp;libid=1353578559830&amp;out=http%3A%2F%2Fmeteuphoric.wordpress.com%2F2010%2F02%2F06%2Fcheap-goals-not-explosive%2F&amp;ref=http%3A%2F%2Flesswrong.com%2Fr%2Fdiscussion%2Flw%2Ffj4%2Fwhat_does_the_world_look_like_the_day_before_fai%2F&amp;title=Crude_Dolorium%20comments%20on%20What%20does%20the%20world%20look%20like%2C%20the%20day%20before%20FAI%20efforts%20succeed%3F%20-%20Less%20Wrong%20Discussion&amp;txt=cheap%2C%20attainable%20goals&amp;jsonp=vglnk_jsonp_13535787745202\">cheap, attainable goals</a>; the creators have tools to read the AIs' minds to ensure they're not trying anything naughty, and killswitches to stop them; they have an aversion to increasing their intelligence beyond a certain point, and to whatever other failure modes the creators anticipate; they're given little or no network connectivity; they're kept ignorant of facts more relevant to exploding than to their assigned tasks; they require special hardware, so it's harder for them to explode; and they're otherwise designed to be safer if not actually safe. Fortunately they don't encounter any really dangerous failure modes before they're replaced with descendants that really are safe.</li>\n</ul>\n</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"pGqRLe9bFDX2G2kXY": 1, "CztjQPSTuaQcfbyh8": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "c3uuAdZzkjv7LKQr9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 36, "extendedScore": null, "score": 1.0353390498654048e-06, "legacy": true, "legacyId": "20128", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 23, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 64, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qARBe3jBodrdPeRE6", "GuEsfTpSDSbXFiseH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-18T00:12:24.446Z", "modifiedAt": null, "url": null, "title": "If you could take it with you, what would you take?", "slug": "if-you-could-take-it-with-you-what-would-you-take", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:58.408Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ba5G5CDno54HyGbxn/if-you-could-take-it-with-you-what-would-you-take", "pageUrlRelative": "/posts/ba5G5CDno54HyGbxn/if-you-could-take-it-with-you-what-would-you-take", "linkUrl": "https://www.lesswrong.com/posts/ba5G5CDno54HyGbxn/if-you-could-take-it-with-you-what-would-you-take", "postedAtFormatted": "Sunday, November 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20If%20you%20could%20take%20it%20with%20you%2C%20what%20would%20you%20take%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIf%20you%20could%20take%20it%20with%20you%2C%20what%20would%20you%20take%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fba5G5CDno54HyGbxn%2Fif-you-could-take-it-with-you-what-would-you-take%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=If%20you%20could%20take%20it%20with%20you%2C%20what%20would%20you%20take%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fba5G5CDno54HyGbxn%2Fif-you-could-take-it-with-you-what-would-you-take", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fba5G5CDno54HyGbxn%2Fif-you-could-take-it-with-you-what-would-you-take", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 243, "htmlBody": "<p>The Scenario: Our protagonist estimates that present-day cryonics has around a five percent chance of leading to a successful revival. Since that's better than the zero percent chance if he doesn't sign up, and he can afford it, he makes the necessary arrangements. As part of those arrangements, he receives a lockable file-cabinet drawer, in which he can put any desired mementos, knick-knacks, or other objects; and which will be protected as securely as his own cryo-preserved body. The drawer is around one and a half cubic feet: two feet deep, one foot wide, nine inches high.<br /><br />The Question: What should he arrange to have placed in his drawer?<br /><br /><br />Some of the more obvious options:<br /><br />* Long-term archival DVDs, such as M-Discs, containing as much of his personal computer's data as possible. With slimline jewel cases, around 400 such discs would fit, which could hold up to around 1.5 terabytes. (Secondary question: Which data to archive?)<br />* Objects of sentimental value<br />* Objects with present-day value: cash, gold coins, jewelry<br />* Objects with predicted future value: collectibles, small antiques<br />* In honor of previous seekers of immortality: a copy of the ancient Egyptian funerary text, the Book of Coming Forth By Day (aka the Book of the Dead).<br />* For the purely practical and/or munchkin approach: a weapon, such as a fighting knife or even a pistol<br /><br /></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ba5G5CDno54HyGbxn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 4, "extendedScore": null, "score": 1.0362535590797648e-06, "legacy": true, "legacyId": "20145", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-18T06:15:48.627Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] \"Evicting\" brain emulations", "slug": "seq-rerun-evicting-brain-emulations", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/G6sjLkS65eT7wE225/seq-rerun-evicting-brain-emulations", "pageUrlRelative": "/posts/G6sjLkS65eT7wE225/seq-rerun-evicting-brain-emulations", "linkUrl": "https://www.lesswrong.com/posts/G6sjLkS65eT7wE225/seq-rerun-evicting-brain-emulations", "postedAtFormatted": "Sunday, November 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20%22Evicting%22%20brain%20emulations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20%22Evicting%22%20brain%20emulations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG6sjLkS65eT7wE225%2Fseq-rerun-evicting-brain-emulations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20%22Evicting%22%20brain%20emulations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG6sjLkS65eT7wE225%2Fseq-rerun-evicting-brain-emulations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FG6sjLkS65eT7wE225%2Fseq-rerun-evicting-brain-emulations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 192, "htmlBody": "<p>Today's post, <a href=\"http://www.overcomingbias.com/2008/11/suppose-that-ro.html\">&ldquo;Evicting&rdquo; brain emulations</a>, by Carl Shulman was originally published on November 23, 2008.  A summary:</p>\n<p>&nbsp;</p>\n<blockquote>As new models of ems are created, slight improvements will let them outcompete old models. In a system in which ems can simply be \"evicted\", the bots will likely not be pleased about this system. If they are simultaneously smarter, faster, and better unified than biological humans, they could be quite dangerous.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fiy/seq_rerun_surprised_by_brains/\">Surprised By Brains</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jQytxyauJ7kPhhGj3": 2, "5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "G6sjLkS65eT7wE225", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.0364569044748339e-06, "legacy": true, "legacyId": "20146", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["EpxynqmhL3MqeWXTw", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-18T10:52:00.119Z", "modifiedAt": null, "url": null, "title": "\"How We're Predicting AI \u2014 or Failing to\"", "slug": "how-we-re-predicting-ai-or-failing-to", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:00.073Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/27inAjvnMSddvYdqm/how-we-re-predicting-ai-or-failing-to", "pageUrlRelative": "/posts/27inAjvnMSddvYdqm/how-we-re-predicting-ai-or-failing-to", "linkUrl": "https://www.lesswrong.com/posts/27inAjvnMSddvYdqm/how-we-re-predicting-ai-or-failing-to", "postedAtFormatted": "Sunday, November 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22How%20We're%20Predicting%20AI%20%E2%80%94%20or%20Failing%20to%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22How%20We're%20Predicting%20AI%20%E2%80%94%20or%20Failing%20to%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F27inAjvnMSddvYdqm%2Fhow-we-re-predicting-ai-or-failing-to%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22How%20We're%20Predicting%20AI%20%E2%80%94%20or%20Failing%20to%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F27inAjvnMSddvYdqm%2Fhow-we-re-predicting-ai-or-failing-to", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F27inAjvnMSddvYdqm%2Fhow-we-re-predicting-ai-or-failing-to", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 152, "htmlBody": "<p>The new paper by <a href=\"/user/Stuart_Armstrong/\">Stuart Armstrong</a>&nbsp;(FHI) and&nbsp;<a href=\"/user/Kaj_Sotala/\">Kaj Sotala</a>&nbsp;(SI) has now been published (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/11/Armstrong-Sotala-How-Were-Predicting-AI-or-Failing-to.pdf\">PDF</a>) as part of the <a href=\"http://www.kky.zcu.cz/en/publications/1/JanRomportl_2012_BeyondAIArtificial.pdf\"><em>Beyond AI</em>&nbsp;conference proceedings</a>. Some of these results were previously discussed <a href=\"/lw/e36/ai_timeline_predictions_are_we_getting_better/\">here</a>. The original predictions data are available <a href=\"/lw/e79/ai_timeline_prediction_data/\">here</a>.</p>\n<p>Abstract:</p>\n<p>\n<blockquote>\n<p>This paper will look at the various predictions that have&nbsp;been made about AI and propose decomposition schemas for analysing&nbsp;them. It will propose a variety of theoretical tools for analysing, judging and improving these predictions. Focusing speci\ufb01cally on timeline&nbsp;predictions (dates given by which we should expect the creation of&nbsp;AI), it will show that there are strong theoretical grounds to expect&nbsp;predictions to be quite poor in this area. Using a database of 95 AI&nbsp;timeline predictions, it will show that these expectations are born out&nbsp;in practice: expert predictions contradict each other considerably, and&nbsp;are indistinguishable from non-expert predictions and past failed predictions. Predictions that AI lie 15 to 25 years in the future are the&nbsp;most common, from experts and non-experts alike.</p>\n</blockquote>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "27inAjvnMSddvYdqm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 19, "extendedScore": null, "score": 1.0366114971156389e-06, "legacy": true, "legacyId": "20147", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["47ci9ixyEbGKWENwR", "Q6oWinLaKXmGNWGLy"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-18T13:59:01.299Z", "modifiedAt": null, "url": null, "title": "Open Thread, November 16\u201330, 2012", "slug": "open-thread-november-16-30-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:29.168Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "VincentYu", "createdAt": "2010-08-18T15:01:21.245Z", "isAdmin": false, "displayName": "VincentYu"}, "userId": "ybEcbPJW9Z8CdGSkB", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pTCDREbuasfYcDECK/open-thread-november-16-30-2012", "pageUrlRelative": "/posts/pTCDREbuasfYcDECK/open-thread-november-16-30-2012", "linkUrl": "https://www.lesswrong.com/posts/pTCDREbuasfYcDECK/open-thread-november-16-30-2012", "postedAtFormatted": "Sunday, November 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20November%2016%E2%80%9330%2C%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20November%2016%E2%80%9330%2C%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpTCDREbuasfYcDECK%2Fopen-thread-november-16-30-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20November%2016%E2%80%9330%2C%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpTCDREbuasfYcDECK%2Fopen-thread-november-16-30-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpTCDREbuasfYcDECK%2Fopen-thread-november-16-30-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16, "htmlBody": "<p>If it's worth saying, but not worth its own post, even in Discussion, it goes here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pTCDREbuasfYcDECK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 1.0367157223594791e-06, "legacy": true, "legacyId": "20148", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 215, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-18T19:13:20.003Z", "modifiedAt": null, "url": null, "title": "Introductory Short Videos", "slug": "introductory-short-videos", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:00.156Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ugqkxS7jdoYXR9reg/introductory-short-videos", "pageUrlRelative": "/posts/ugqkxS7jdoYXR9reg/introductory-short-videos", "linkUrl": "https://www.lesswrong.com/posts/ugqkxS7jdoYXR9reg/introductory-short-videos", "postedAtFormatted": "Sunday, November 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Introductory%20Short%20Videos&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntroductory%20Short%20Videos%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FugqkxS7jdoYXR9reg%2Fintroductory-short-videos%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Introductory%20Short%20Videos%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FugqkxS7jdoYXR9reg%2Fintroductory-short-videos", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FugqkxS7jdoYXR9reg%2Fintroductory-short-videos", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 360, "htmlBody": "<p>Most people new to the Less Wrong community start out by reading the Sequences. There are also lists out there of highly recommended text books, pop sci books, and articles.</p>\n<p>However these avenues can take a LOT of time, and not everyone learns best by reading. I am interested in putting together a list of introductory videos that cover the same type of materials discussed on Less Wrong, and could be viewed within a single night.&nbsp;</p>\n<p>One purpose of this is for meetup groups that get a large influx of new people all at once. It might take a while to get everybody up to speed by sending them off to all go read Sequence posts on their own, especially if they are generally busy in their daily life. However, it is much easier to turn one of your meetups into a Video Night. The goal is that at the end of the Video Night, everybody is aware of the type of mindset that we want people at meetups to have.&nbsp;</p>\n<p>The list can also be used by people who are brand new to rationality, and want to get the basic information in video form.</p>\n<p>&nbsp;</p>\n<p>I'll get it started by listing 6 videos that are of the sort that I mean for this list to be. Please leave some recommendations! (Also, if there is already a list of good intro videos, let me know. I am currently unaware of any.)&nbsp;</p>\n<p><strong>Note</strong>:&nbsp;Although AI/ x-risk videos are NOT what I am looking for, I know people will want to recommend them, so I'm creating a parent comment to place them under. If we get enough of those, we can create a separate list of good Intro to X-Risk and/or AI vids.</p>\n<p>&nbsp;</p>\n<p>Julia Galef-&nbsp;<a href=\"http://youtu.be/tLgNZ9aTEwc\">Straw Vulcan</a>, Skepticon 4<br /><span style=\"white-space: pre;\"> </span>-A good first video that dispels some widely-believed myths about \"rationality\".</p>\n<p>Spencer Greenberg- <a href=\"http://youtu.be/wW_oNxax5RQ\">Self Skepticism</a>, Skepticon 4<br /><span style=\"white-space: pre;\"> </span>-Reasoning as to how and why you might be wrong, and what to do about that fact</p>\n<p>Julia Galef- <a href=\"http://fora.tv/2012/10/13/Julia_Galef_Rationality_and_the_Future/Slave_to_Your_DNA_Rationality_Will_Set_You_Free\">Rationality and the Future</a>, Singularity Summit 2012<br /><span style=\"white-space: pre;\"> </span>-Overview of the culture we are trying to develop</p>\n<p>Dan Ariely- <a href=\"http://www.ted.com/speakers/dan_ariely.html\">TED talks</a>&nbsp;(all three)<br /><span style=\"white-space: pre;\"> </span>- Engaging anecdotes and information about specific cognitive biases, and how they affect people</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ugqkxS7jdoYXR9reg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 11, "extendedScore": null, "score": 3.6e-05, "legacy": true, "legacyId": "20149", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-18T22:45:15.755Z", "modifiedAt": null, "url": null, "title": "Meetup : Brussels meetup", "slug": "meetup-brussels-meetup-9", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Axel", "createdAt": "2010-11-03T17:32:46.091Z", "isAdmin": false, "displayName": "Axel"}, "userId": "vi498nAvek8eWuMWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/F7pKgsY2xfushnrPo/meetup-brussels-meetup-9", "pageUrlRelative": "/posts/F7pKgsY2xfushnrPo/meetup-brussels-meetup-9", "linkUrl": "https://www.lesswrong.com/posts/F7pKgsY2xfushnrPo/meetup-brussels-meetup-9", "postedAtFormatted": "Sunday, November 18th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Brussels%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Brussels%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF7pKgsY2xfushnrPo%2Fmeetup-brussels-meetup-9%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Brussels%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF7pKgsY2xfushnrPo%2Fmeetup-brussels-meetup-9", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FF7pKgsY2xfushnrPo%2Fmeetup-brussels-meetup-9", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 65, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/g3'>Brussels meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">24 November 2012 01:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are once again meeting at 'la fleur en papier dor\u00e9' close to the Brussels Central station. If you feel like an intelligent discussion and are in the neighborhood, consider dropping by. As always, I'll have a sign.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/g3'>Brussels meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "F7pKgsY2xfushnrPo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.0370109191693745e-06, "legacy": true, "legacyId": "20150", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Brussels_meetup\">Discussion article for the meetup : <a href=\"/meetups/g3\">Brussels meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">24 November 2012 01:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are once again meeting at 'la fleur en papier dor\u00e9' close to the Brussels Central station. If you feel like an intelligent discussion and are in the neighborhood, consider dropping by. As always, I'll have a sign.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Brussels_meetup1\">Discussion article for the meetup : <a href=\"/meetups/g3\">Brussels meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Brussels meetup", "anchor": "Discussion_article_for_the_meetup___Brussels_meetup", "level": 1}, {"title": "Discussion article for the meetup : Brussels meetup", "anchor": "Discussion_article_for_the_meetup___Brussels_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-19T02:55:49.308Z", "modifiedAt": null, "url": null, "title": "Online Meetup: The High Impact Network", "slug": "online-meetup-the-high-impact-network", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:57.152Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RyanCarey", "createdAt": "2011-04-27T00:19:14.586Z", "isAdmin": false, "displayName": "RyanCarey"}, "userId": "CBkbKSCEzEK2kLQww", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ku9694q48nwmRejSs/online-meetup-the-high-impact-network", "pageUrlRelative": "/posts/Ku9694q48nwmRejSs/online-meetup-the-high-impact-network", "linkUrl": "https://www.lesswrong.com/posts/Ku9694q48nwmRejSs/online-meetup-the-high-impact-network", "postedAtFormatted": "Monday, November 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Online%20Meetup%3A%20The%20High%20Impact%20Network&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOnline%20Meetup%3A%20The%20High%20Impact%20Network%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKu9694q48nwmRejSs%2Fonline-meetup-the-high-impact-network%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Online%20Meetup%3A%20The%20High%20Impact%20Network%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKu9694q48nwmRejSs%2Fonline-meetup-the-high-impact-network", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKu9694q48nwmRejSs%2Fonline-meetup-the-high-impact-network", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 230, "htmlBody": "<p><strong>Update: The High Impact Network will meet at 7pm on Saturday the 24th of November, Eastern US time. Please&nbsp;<a style=\"font-family: Verdana; font-size: 11pt;\" href=\"mailto:ry.duff@gmail.com\" target=\"_blank\">email me</a>&nbsp;to be invited to these hangouts:</strong></p>\n<p><strong>https://plus.google.com/u/1/events/cj4831btptb0ngde1efb3tfsjtc</strong></p>\n<p><strong>https://plus.google.com/u/1/events/cbnr8dqqbgra7a5391msu1e1dc4</strong></p>\n<p>&nbsp;</p>\n<p><span style=\"font-family: Verdana; font-size: 11pt;\">Effective altruists, not&nbsp;all&nbsp;of whom are geographically located together,&nbsp;benefit from being connected&nbsp;and&nbsp;brought up to date with effective ideas and plans regarding areas of&nbsp;interest&nbsp;to them.</span></p>\n<p style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-size: 11.0pt; font-family: Verdana; mso-bidi-font-family: Helvetica; color: #500050;\">Mark Lee and I want to meet aspiring effective altruists and talk about how their talents and ideas might fit into the greater scheme of organised altruistic effort.</span></p>\n<p style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-size: 11.0pt; font-family: Verdana; mso-bidi-font-family: Helvetica; color: #500050;\">Due to the popularity of the <a href=\"/lw/f5z/online_optimal_philanthropy_meeting/\">previous meetup</a>, the new discussion will be divided into two smaller groups that will host simultaneous discussions on:</span></p>\n<p style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-size: 11pt; font-family: Verdana;\">1. Addressing Global Poverty - how&nbsp;can we best alleviate global poverty?</span></p>\n<p style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-size: 11pt; font-family: Verdana;\">2. Beyond Global Poverty -&nbsp;what are other highly important causes and how can we address them?</span></p>\n<p style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-size: 11pt; font-family: Verdana;\">Participants are welcome to suggest&nbsp;up to 3 ways that they are interested in addressing these problems,&nbsp;and then we'll discuss the strengths and weaknesses of&nbsp;these approaches. The agenda is broad&nbsp;so&nbsp;as not to preempt or undermine new suggestions&nbsp;likely to be effective. More targeted follow-up meetings can be later arranged&nbsp;if required.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Verdana; font-size: 11pt;\">ark and I will chair one conversation each. Both will take place through Google Hangouts, at a <span style=\"color: #1155cc;\">the democratically determined time of&nbsp;</span></span><span style=\"font-family: Verdana; font-size: 14.44444465637207px;\">7pm on Saturday the 24th of November, Eastern US time.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Verdana; font-size: 11pt;\">Please&nbsp;</span><span style=\"color: #1155cc;\"><a style=\"font-family: Verdana; font-size: 11pt;\" href=\"mailto:ry.duff@gmail.com\" target=\"_blank\">RSVP</a></span><span style=\"font-family: Verdana; font-size: 14.44444465637207px;\">&nbsp;if you want to be added to the Google Hangout - you are welcome to specify which discussion you prefer to be involved in and topics that you would like attached&nbsp;</span><span style=\"font-family: Verdana; font-size: 11pt;\">to the&nbsp;</span><a style=\"font-family: Verdana; font-size: 11pt;\" href=\"http://checkvist.com/checklists/153407\" target=\"_blank\"><span style=\"color: #1155cc;\">agenda</span></a><span style=\"font-family: Verdana; font-size: 11pt;\">&nbsp;.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ku9694q48nwmRejSs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 3, "extendedScore": null, "score": 1.0371512954689166e-06, "legacy": true, "legacyId": "20158", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"Update__The_High_Impact_Network_will_meet_at_7pm_on_Saturday_the_24th_of_November__Eastern_US_time__Please_email_me_to_be_invited_to_these_hangouts_\">Update: The High Impact Network will meet at 7pm on Saturday the 24th of November, Eastern US time. Please&nbsp;<a style=\"font-family: Verdana; font-size: 11pt;\" href=\"mailto:ry.duff@gmail.com\" target=\"_blank\">email me</a>&nbsp;to be invited to these hangouts:</strong></p>\n<p><strong id=\"https___plus_google_com_u_1_events_cj4831btptb0ngde1efb3tfsjtc\">https://plus.google.com/u/1/events/cj4831btptb0ngde1efb3tfsjtc</strong></p>\n<p><strong id=\"https___plus_google_com_u_1_events_cbnr8dqqbgra7a5391msu1e1dc4\">https://plus.google.com/u/1/events/cbnr8dqqbgra7a5391msu1e1dc4</strong></p>\n<p>&nbsp;</p>\n<p><span style=\"font-family: Verdana; font-size: 11pt;\">Effective altruists, not&nbsp;all&nbsp;of whom are geographically located together,&nbsp;benefit from being connected&nbsp;and&nbsp;brought up to date with effective ideas and plans regarding areas of&nbsp;interest&nbsp;to them.</span></p>\n<p style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-size: 11.0pt; font-family: Verdana; mso-bidi-font-family: Helvetica; color: #500050;\">Mark Lee and I want to meet aspiring effective altruists and talk about how their talents and ideas might fit into the greater scheme of organised altruistic effort.</span></p>\n<p style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-size: 11.0pt; font-family: Verdana; mso-bidi-font-family: Helvetica; color: #500050;\">Due to the popularity of the <a href=\"/lw/f5z/online_optimal_philanthropy_meeting/\">previous meetup</a>, the new discussion will be divided into two smaller groups that will host simultaneous discussions on:</span></p>\n<p style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-size: 11pt; font-family: Verdana;\">1. Addressing Global Poverty - how&nbsp;can we best alleviate global poverty?</span></p>\n<p style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-size: 11pt; font-family: Verdana;\">2. Beyond Global Poverty -&nbsp;what are other highly important causes and how can we address them?</span></p>\n<p style=\"background-position: initial initial; background-repeat: initial initial;\"><span style=\"font-size: 11pt; font-family: Verdana;\">Participants are welcome to suggest&nbsp;up to 3 ways that they are interested in addressing these problems,&nbsp;and then we'll discuss the strengths and weaknesses of&nbsp;these approaches. The agenda is broad&nbsp;so&nbsp;as not to preempt or undermine new suggestions&nbsp;likely to be effective. More targeted follow-up meetings can be later arranged&nbsp;if required.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Verdana; font-size: 11pt;\">ark and I will chair one conversation each. Both will take place through Google Hangouts, at a <span style=\"color: #1155cc;\">the democratically determined time of&nbsp;</span></span><span style=\"font-family: Verdana; font-size: 14.44444465637207px;\">7pm on Saturday the 24th of November, Eastern US time.</span></p>\n<p class=\"MsoNormal\"><span style=\"font-family: Verdana; font-size: 11pt;\">Please&nbsp;</span><span style=\"color: #1155cc;\"><a style=\"font-family: Verdana; font-size: 11pt;\" href=\"mailto:ry.duff@gmail.com\" target=\"_blank\">RSVP</a></span><span style=\"font-family: Verdana; font-size: 14.44444465637207px;\">&nbsp;if you want to be added to the Google Hangout - you are welcome to specify which discussion you prefer to be involved in and topics that you would like attached&nbsp;</span><span style=\"font-family: Verdana; font-size: 11pt;\">to the&nbsp;</span><a style=\"font-family: Verdana; font-size: 11pt;\" href=\"http://checkvist.com/checklists/153407\" target=\"_blank\"><span style=\"color: #1155cc;\">agenda</span></a><span style=\"font-family: Verdana; font-size: 11pt;\">&nbsp;.</span></p>", "sections": [{"title": "Update: The High Impact Network will meet at 7pm on Saturday the 24th of November, Eastern US time. Please\u00a0email me\u00a0to be invited to these hangouts:", "anchor": "Update__The_High_Impact_Network_will_meet_at_7pm_on_Saturday_the_24th_of_November__Eastern_US_time__Please_email_me_to_be_invited_to_these_hangouts_", "level": 1}, {"title": "https://plus.google.com/u/1/events/cj4831btptb0ngde1efb3tfsjtc", "anchor": "https___plus_google_com_u_1_events_cj4831btptb0ngde1efb3tfsjtc", "level": 1}, {"title": "https://plus.google.com/u/1/events/cbnr8dqqbgra7a5391msu1e1dc4", "anchor": "https___plus_google_com_u_1_events_cbnr8dqqbgra7a5391msu1e1dc4", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["uMGr9M59bJ6KmB5HT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-19T02:56:06.525Z", "modifiedAt": null, "url": null, "title": "Australian Rationalist in America", "slug": "australian-rationalist-in-america", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:54.280Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shokwave", "createdAt": "2010-10-12T12:55:00.568Z", "isAdmin": false, "displayName": "shokwave"}, "userId": "jtjgXtj7FepKrQPGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/g3EEsoJ4cWf9s6DLS/australian-rationalist-in-america", "pageUrlRelative": "/posts/g3EEsoJ4cWf9s6DLS/australian-rationalist-in-america", "linkUrl": "https://www.lesswrong.com/posts/g3EEsoJ4cWf9s6DLS/australian-rationalist-in-america", "postedAtFormatted": "Monday, November 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Australian%20Rationalist%20in%20America&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAustralian%20Rationalist%20in%20America%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg3EEsoJ4cWf9s6DLS%2Faustralian-rationalist-in-america%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Australian%20Rationalist%20in%20America%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg3EEsoJ4cWf9s6DLS%2Faustralian-rationalist-in-america", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg3EEsoJ4cWf9s6DLS%2Faustralian-rationalist-in-america", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 425, "htmlBody": "<p>Hi LessWrong! I'm a LWer from Melbourne, Australia, and I'm taking a 3 month road trip (with a friend) through parts of the United States. I figure I'd enjoy hanging out with some fellow rationalists while I'm over here!&nbsp;</p>\n<p>I attended the May Rationality minicamp in San Francisco (and made some friends who I'm hoping to meet up with again), but I've also <a title=\"Bay Area Rationalists\" href=\"/lw/4un/towards_a_bay_area_less_wrong_community/\" target=\"_blank\">heard</a> good <a title=\"New York Rationalists\" href=\"http://wiki.lesswrong.com/wiki/NYC_meetup_group\" target=\"_blank\">things</a> about the LessWrong groups all over the United States. I'd like to meet some of the awesome people involved in these communities!</p>\n<p>We've been planning this trip for a while now and have accommodation pretty much everywhere except for the second half of San Francisco.&nbsp;</p>\n<h3>Itinerary</h3>\n<ul>\n<li>17th-21st Nov - Los Angeles, CA</li>\n<li>21st-28th Nov - San Francisco, CA</li>\n<li>28th Nov-1st Dec - Las Vegas, NV</li>\n<li>2nd-3rd Dec - Flagstaff, AZ</li>\n<li>3rd-7th Dec - Phoenix, AZ</li>\n<li>7th-9th Dec - Santa Fe, NM</li>\n<li>9th-10th Dec - El Paso, TX</li>\n<li>10th-13th Dec - San Antonio, TX</li>\n<li>13th-21st Dec - Austin, TX</li>\n<li>21st-26th Dec - Dallas, TX</li>\n<li>26th-29th Dec - San Antonio, TX</li>\n<li>29th Dec-2nd Jan - New York City, NY</li>\n<li>2nd-3rd Jan - San Antonio, TX</li>\n<li>3rd-6th Jan - Houston, TX</li>\n<li>6th-9th Jan - New Orleans, LA</li>\n<li>9th-12th Jan - Memphis, TN</li>\n<li>12th-15th Jan - Nashville, TN</li>\n<li>15th-18th Jan - Atlanta, GA</li>\n<li>18th-22nd Jan - Miami, FL</li>\n<li>22nd-26th Jan - Orlando, FL</li>\n<li>26th Jan-1st Feb - Washington DC</li>\n<li>1st-4th Feb - Philadelphia, PA</li>\n<li>4th-6th Feb - New York City, NY</li>\n<li>6th-9th Feb - Mount Snow, VT</li>\n<li>9th-13th Feb - Boston, MA</li>\n<li>13th-15th Feb - New York City, NY</li>\n<li>15th-26th Feb - Columbus, OH</li>\n</ul>\n<p>If you're in one of these locations when I am, contact me! Either ahead of time or at short notice is fine. I'll be checking meetup posts and mailing lists for events that I can make it to as well, but if you happen to know of an event or meetup happening that fits the schedule, feel free to let me know in the comments.</p>\n<p>Message or call me on 4242 394 657, email me at shokwave.sf@gmail.com - or you can leave a toplevel comment on this post, or message my LW account directly. Looking forward to meeting any and all of you!</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "g3EEsoJ4cWf9s6DLS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 10, "extendedScore": null, "score": 1.0371514562339809e-06, "legacy": true, "legacyId": "20155", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["N8DaeP83CSgKnjcid"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-19T04:58:22.885Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Cascades, Cycles, Insight...", "slug": "seq-rerun-cascades-cycles-insight", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rqd83NJZzA7NiGfke/seq-rerun-cascades-cycles-insight", "pageUrlRelative": "/posts/rqd83NJZzA7NiGfke/seq-rerun-cascades-cycles-insight", "linkUrl": "https://www.lesswrong.com/posts/rqd83NJZzA7NiGfke/seq-rerun-cascades-cycles-insight", "postedAtFormatted": "Monday, November 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Cascades%2C%20Cycles%2C%20Insight...&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Cascades%2C%20Cycles%2C%20Insight...%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frqd83NJZzA7NiGfke%2Fseq-rerun-cascades-cycles-insight%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Cascades%2C%20Cycles%2C%20Insight...%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frqd83NJZzA7NiGfke%2Fseq-rerun-cascades-cycles-insight", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frqd83NJZzA7NiGfke%2Fseq-rerun-cascades-cycles-insight", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 198, "htmlBody": "<p>Today's post, <a href=\"/lw/w5/cascades_cycles_insight/\">Cascades, Cycles, Insight...</a> was originally published on 24 November 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Cascades.2C_Cycles.2C_Insight...\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Cascades, cycles, and insight are three ways in which the development of intelligence appears discontinuous. Cascades are when one development makes more developments possible. Cycles are when completing a process causes that process to be completed more. And insight is when we acquire a chunk of information that makes solving a lot of other problems easier.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fjm/seq_rerun_evicting_brain_emulations/\">\"Evicting\" brain emulations</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rqd83NJZzA7NiGfke", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0372199720139322e-06, "legacy": true, "legacyId": "20161", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dq3KsCsqNotWc8nAK", "G6sjLkS65eT7wE225", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-19T22:57:20.125Z", "modifiedAt": null, "url": null, "title": "Collating widely available time/money trades", "slug": "collating-widely-available-time-money-trades", "viewCount": null, "lastCommentedAt": "2017-06-17T04:18:44.985Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RyanCarey", "createdAt": "2011-04-27T00:19:14.586Z", "isAdmin": false, "displayName": "RyanCarey"}, "userId": "CBkbKSCEzEK2kLQww", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KuFSkLwhSkEZJYALE/collating-widely-available-time-money-trades", "pageUrlRelative": "/posts/KuFSkLwhSkEZJYALE/collating-widely-available-time-money-trades", "linkUrl": "https://www.lesswrong.com/posts/KuFSkLwhSkEZJYALE/collating-widely-available-time-money-trades", "postedAtFormatted": "Monday, November 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Collating%20widely%20available%20time%2Fmoney%20trades&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACollating%20widely%20available%20time%2Fmoney%20trades%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKuFSkLwhSkEZJYALE%2Fcollating-widely-available-time-money-trades%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Collating%20widely%20available%20time%2Fmoney%20trades%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKuFSkLwhSkEZJYALE%2Fcollating-widely-available-time-money-trades", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKuFSkLwhSkEZJYALE%2Fcollating-widely-available-time-money-trades", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 571, "htmlBody": "<p>In the xkcd comic <em>Working</em>, a man is seen filling up his gas tank. \"Why are you going here\", says the observer, \"Gas is ten cents a gallon cheaper at the station five minutes that way\". He responds \"Because a penny saved is a penny earned\". Randal's pragmatically spirited caption says \"If you spend nine minutes of your time to save a dollar, you're working for less than the minimum wage.\"</p>\n<p>Our opportunities to convert time into money and vice versa, though <a href=\"/lw/2gi/the_instrumental_value_of_your_own_time/\">not unlimited</a>, are numerous.</p>\n<p><em>We work (sell our free time)</em> when we&hellip;</p>\n<ul>\n<li>Seek overtime shifts</li>\n<li>Subscribe to a mailing list for local discounts e.g. GroupOn</li>\n<li>Bargain with one more car dealer or travel agent, in search of a better deal</li>\n</ul>\n<p>We <em>buy free time </em>when we&hellip;</p>\n<ul>\n<li>Employ cleaners, chefs, babysitters, secretaries and others.</li>\n<li>Buy productivity software</li>\n<li>Buy a medication that improves our sleep</li>\n</ul>\n<p>How can we evaluate these trades? It seems like we ought to only purchase free time when it comes cheaper than a certain figure, $x/hr, and ought to only work if we can sell our free time for more than $x/hr. Indeed, comparing trades to this <em>time/money exchange rate</em> is the only unexploitable way to behave.</p>\n<p>Most of the time, when we share our estimates of the value of these trades, our comments are too vague to be helpful. If my father, a doctor tells me, a student, that \"subscribing to discount mailing lists is a waste of time\", what does he mean? He might mean that these mailing lists are poor value for me, he might mean the much stronger statement that they are poor value for everyone, or the much weaker statement, that they are poor value just for him (his time is obviously worth the most). I have to try to get him to disentangle his estimation from his jugement. I have to ask him \"What low value would a person have to place on their time for discount mailing lists to be worthwhile?\"</p>\n<p>The easiest way for all individuals with different time/money exchange rates to share their estimates will be to quantify them. e.g. being on a discount mailing lists only saves $x per hour spent. Out of my father and I, this might represent value to none, one or both of us.</p>\n<p>When we share these quantitative estimates, it would be silly to discuss deals that are only available privately like job offers, that are so dependent on our particular skills and qualification. Instead,we will gain the most by listing time-money trades that are likely to apply across domains, such as repairing a car on the one hand or catching a cab on the other.</p>\n<p>By doing so, we stand to learn that many of the trades we have been carrying out have represented poor value, and we should learn of <a href=\"/lw/1mc/normal_cryonics/\">new trades</a> that we had not previously considered. Of course, there are associated costs, like the time spent gathering this information, and the risk of becoming <a href=\"/xkcd.com/309\">unduly</a> <a href=\"/lw/bq0/be_happier/\">preocuppied</a> with these decisions, but it still seems worth doing.</p>\n<p>A last point of order is that it will be best to indicate how far we can expect each estimate to generalise. For example, the cost of something like melatonin will differ between states or between countries, and that is worth mentioning.</p>\n<p>So <em>in this thread,</em><em> please</em> <em>share your estimates in $/hr for potential ways to work or buy free time.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"vMeJdnoedH7iL8LiR": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KuFSkLwhSkEZJYALE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 28, "extendedScore": null, "score": 6.4e-05, "legacy": true, "legacyId": "20169", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ekDXMQKwuhuKz9aow", "hiDkhLyN5S2MEjrSE", "JHcTP4Ad8QAmRTCZm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-19T23:53:24.535Z", "modifiedAt": null, "url": null, "title": "GiveWell and the Centre for Effective Altruism are recruiting", "slug": "givewell-and-the-centre-for-effective-altruism-are", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:57.785Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Pablo_Stafforini", "createdAt": "2009-03-24T12:56:00.130Z", "isAdmin": false, "displayName": "Pablo"}, "userId": "W7ETRtvRMqYetyQE9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xqeuPFJGapznK47Ch/givewell-and-the-centre-for-effective-altruism-are", "pageUrlRelative": "/posts/xqeuPFJGapznK47Ch/givewell-and-the-centre-for-effective-altruism-are", "linkUrl": "https://www.lesswrong.com/posts/xqeuPFJGapznK47Ch/givewell-and-the-centre-for-effective-altruism-are", "postedAtFormatted": "Monday, November 19th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20GiveWell%20and%20the%20Centre%20for%20Effective%20Altruism%20are%20recruiting&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGiveWell%20and%20the%20Centre%20for%20Effective%20Altruism%20are%20recruiting%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxqeuPFJGapznK47Ch%2Fgivewell-and-the-centre-for-effective-altruism-are%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=GiveWell%20and%20the%20Centre%20for%20Effective%20Altruism%20are%20recruiting%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxqeuPFJGapznK47Ch%2Fgivewell-and-the-centre-for-effective-altruism-are", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxqeuPFJGapznK47Ch%2Fgivewell-and-the-centre-for-effective-altruism-are", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 412, "htmlBody": "<p>Both <a href=\"http://givewell.org/about/jobs\">GiveWell</a> and the <a href=\"http://www.centreforeffectivealtruism.org/recruitment/cea-full.pdf\">Centre for Effective Altruism</a>&nbsp;(CEA) --an Oxford-based umbrella organization consisting of&nbsp;<a href=\"http://givingwhatwecan.org/\">Giving What We Can</a>, <a href=\"http://80000hours.org/\">80,000 Hours</a>, <a href=\"http://thelifeyoucansave.com/\">The Life You Can Save</a>, and <a href=\"http://www.effectiveanimalactivism.org/\">Effective Animal Activism</a>-- have been discussed here before. &nbsp;So I thought some folks might want to know that these organizations are&nbsp;recruiting for a number of positions. &nbsp;Here are relevant excerpts from the official job announcements:</p>\n<blockquote>\n<p><strong>GiveWell: Research Analyst</strong></p>\n<p>GiveWell is looking for a Research Analyst to help us evaluate charities, find the most outstanding giving opportunities, and publish our analysis to help donors decide where to give.</p>\n<p>&nbsp;</p>\n<p><strong>Effective Animal Activism: Executive Director</strong></p>\n<p>Effective Animal Activism is a recently-founded project of 80,000 Hours. It is the world&rsquo;s first online resource and international community for people who want to reduce animal suffering effectively. We are currently looking for a part-time executive director. Responsibilities will include creating content, managing the community, publicizing the site, and overseeing as well as undertaking further charity research. Future projects include creating a publication on our intervention evaluation once complete, attending conferences, running ad campaigns, and reaching out to the media, animal charities and philanthropists.</p>\n<p>&nbsp;</p>\n<p><strong>Giving What We Can: Head of Communications</strong></p>\n<p>We are looking for someone to communicate Giving What We Can&rsquo;s message to the world. As Communications Manager you would be responsible for handling our press relations and guiding our public image.</p>\n<p>&nbsp;</p>\n<p><strong>80,000 Hours: Head of Careers Research</strong></p>\n<p>We are looking for someone to drive cutting-edge research into effective ethical careers and translate it into one-on-one and online careers advice, which you&rsquo;ll share with interesting people from all over the world.</p>\n<p>&nbsp;</p>\n<p><strong>The Life You Can Save: Director of Outreach (Intern)</strong></p>\n<p>We are looking for someone to lead our outreach to pledgers and supporters as well as local groups, other charities, and corporations. In this role, you&rsquo;ll play a key part in setting our strategic priorities and driving the growth of The Life You Can Save. You&rsquo;ll be working alongside Peter Singer &ndash; one of the most influential ethicists of the 20th century.</p>\n<p>&nbsp;</p>\n<p><strong>Centre for Effective Altruism: Head of Fundraising and External Relations</strong></p>\n<p>We are looking for someone to manage our fundraising and represent us to other organisations. In this role you would serve all four organisations in the Centre for Effective Altruism: Giving What We Can, 80,000 Hours, The Life You Can Save and Effective Animal Activism.</p>\n</blockquote>\n<p>(Full disclosure: I'm friends with the co-founders of CEA and have donated to Effective Animal Activism.)</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xEZwTHPd5AWpgQx9w": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xqeuPFJGapznK47Ch", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 14, "extendedScore": null, "score": 1.0378563696287605e-06, "legacy": true, "legacyId": "20083", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Both <a href=\"http://givewell.org/about/jobs\">GiveWell</a> and the <a href=\"http://www.centreforeffectivealtruism.org/recruitment/cea-full.pdf\">Centre for Effective Altruism</a>&nbsp;(CEA) --an Oxford-based umbrella organization consisting of&nbsp;<a href=\"http://givingwhatwecan.org/\">Giving What We Can</a>, <a href=\"http://80000hours.org/\">80,000 Hours</a>, <a href=\"http://thelifeyoucansave.com/\">The Life You Can Save</a>, and <a href=\"http://www.effectiveanimalactivism.org/\">Effective Animal Activism</a>-- have been discussed here before. &nbsp;So I thought some folks might want to know that these organizations are&nbsp;recruiting for a number of positions. &nbsp;Here are relevant excerpts from the official job announcements:</p>\n<blockquote>\n<p><strong id=\"GiveWell__Research_Analyst\">GiveWell: Research Analyst</strong></p>\n<p>GiveWell is looking for a Research Analyst to help us evaluate charities, find the most outstanding giving opportunities, and publish our analysis to help donors decide where to give.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Effective_Animal_Activism__Executive_Director\">Effective Animal Activism: Executive Director</strong></p>\n<p>Effective Animal Activism is a recently-founded project of 80,000 Hours. It is the world\u2019s first online resource and international community for people who want to reduce animal suffering effectively. We are currently looking for a part-time executive director. Responsibilities will include creating content, managing the community, publicizing the site, and overseeing as well as undertaking further charity research. Future projects include creating a publication on our intervention evaluation once complete, attending conferences, running ad campaigns, and reaching out to the media, animal charities and philanthropists.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Giving_What_We_Can__Head_of_Communications\">Giving What We Can: Head of Communications</strong></p>\n<p>We are looking for someone to communicate Giving What We Can\u2019s message to the world. As Communications Manager you would be responsible for handling our press relations and guiding our public image.</p>\n<p>&nbsp;</p>\n<p><strong id=\"80_000_Hours__Head_of_Careers_Research\">80,000 Hours: Head of Careers Research</strong></p>\n<p>We are looking for someone to drive cutting-edge research into effective ethical careers and translate it into one-on-one and online careers advice, which you\u2019ll share with interesting people from all over the world.</p>\n<p>&nbsp;</p>\n<p><strong id=\"The_Life_You_Can_Save__Director_of_Outreach__Intern_\">The Life You Can Save: Director of Outreach (Intern)</strong></p>\n<p>We are looking for someone to lead our outreach to pledgers and supporters as well as local groups, other charities, and corporations. In this role, you\u2019ll play a key part in setting our strategic priorities and driving the growth of The Life You Can Save. You\u2019ll be working alongside Peter Singer \u2013 one of the most influential ethicists of the 20th century.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Centre_for_Effective_Altruism__Head_of_Fundraising_and_External_Relations\">Centre for Effective Altruism: Head of Fundraising and External Relations</strong></p>\n<p>We are looking for someone to manage our fundraising and represent us to other organisations. In this role you would serve all four organisations in the Centre for Effective Altruism: Giving What We Can, 80,000 Hours, The Life You Can Save and Effective Animal Activism.</p>\n</blockquote>\n<p>(Full disclosure: I'm friends with the co-founders of CEA and have donated to Effective Animal Activism.)</p>\n<p>&nbsp;</p>", "sections": [{"title": "GiveWell: Research Analyst", "anchor": "GiveWell__Research_Analyst", "level": 1}, {"title": "Effective Animal Activism: Executive Director", "anchor": "Effective_Animal_Activism__Executive_Director", "level": 1}, {"title": "Giving What We Can: Head of Communications", "anchor": "Giving_What_We_Can__Head_of_Communications", "level": 1}, {"title": "80,000 Hours: Head of Careers Research", "anchor": "80_000_Hours__Head_of_Careers_Research", "level": 1}, {"title": "The Life You Can Save: Director of Outreach (Intern)", "anchor": "The_Life_You_Can_Save__Director_of_Outreach__Intern_", "level": 1}, {"title": "Centre for Effective Altruism: Head of Fundraising and External Relations", "anchor": "Centre_for_Effective_Altruism__Head_of_Fundraising_and_External_Relations", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "7 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-20T03:21:28.567Z", "modifiedAt": null, "url": null, "title": "Good Places to Live as a Less Wronger", "slug": "good-places-to-live-as-a-less-wronger", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:58.608Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LdQEuSNSf34H3xNZg/good-places-to-live-as-a-less-wronger", "pageUrlRelative": "/posts/LdQEuSNSf34H3xNZg/good-places-to-live-as-a-less-wronger", "linkUrl": "https://www.lesswrong.com/posts/LdQEuSNSf34H3xNZg/good-places-to-live-as-a-less-wronger", "postedAtFormatted": "Tuesday, November 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Good%20Places%20to%20Live%20as%20a%20Less%20Wronger&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGood%20Places%20to%20Live%20as%20a%20Less%20Wronger%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLdQEuSNSf34H3xNZg%2Fgood-places-to-live-as-a-less-wronger%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Good%20Places%20to%20Live%20as%20a%20Less%20Wronger%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLdQEuSNSf34H3xNZg%2Fgood-places-to-live-as-a-less-wronger", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLdQEuSNSf34H3xNZg%2Fgood-places-to-live-as-a-less-wronger", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 587, "htmlBody": "<p>Less Wrongers are a diverse crowd, more so now than in the early days.&nbsp; I wonder if we could step away from anti-generalizations, generalize and try to say good places to live, under a few assumptions (remember, the idea of an assumption is to assume it, not to claim it is less or more representative of observation class X or Y and then go on to <a href=\"/xkcd.com/309\">nerdify it</a>.)</p>\n<p>Recetly, Xanghai was claimed as an interesting place to teach english.&nbsp;</p>\n<p>Just having returned from 15 days in Rio de Janeiro, I may talk a little about it.</p>\n<p>Assumptions:</p>\n<p>1) Assuming your family lives somewhere else, other state or country.</p>\n<p>2) No children yet. Single, Married, Gay, Bisexual, Male, Female.</p>\n<p>3) You can muster $1-4k a month (teaching a language, like English, programming, writing, family money, lottery, spy for the CIA)</p>\n<p>4) You like science/philosophy, rationality, and not a complete misanthrope (you'd hug five times more than you do if given a chance, and you'd double the number of close friends you have, as well as balance their gender ratio)</p>\n<p>&nbsp;</p>\n<p>My suggested format is city name, time spend there, experience, cons, and pros.</p>\n<p>Rio de Janeiro,15 days, Rio is an interesting city. Near the subway you can get to the vast majority of places without a car, a good night out will cost between 15-40 dollars, depending on whether you drink or not, and therefore need a cab home. Nice dinner 12-50.&nbsp; There are millions of people including lots of tourists easily reachable there. So unless you are estonian, you will be able to find someone from home there. Because travellers go to Rio for it's beauty, you can find them in free places, and make friends with locals and foreigners alike, allowing for short term and long term friendships. They say you get tired eventually, but the natural beauty is great and spread. Forests and beaches and mountains abound, all 4 minutes away from a supermarket.There are nearly free public bikes in some areas.</p>\n<p>Cons: Science/philosophy are not what Rio is known for. Their universities are good, and you can find youe way there if you can in a good college, but a meeting with a lot of people to discuss two boxing on newcomb is less likely in the following ten years.You can't park in Rio during the day, if somehow you managed to have a car and a carplace in your apartment. You won't buy a place,and it won't be big, an awesome ipanema apartment 190sq meters goes for 2,3 million dollars, and renting a tiny place costs about 1thousand a month.</p>\n<p>Pros: Papers to the contrary, weather does impact your life for quite a while if you pay attention to it. Not necessarily the weather itself, but the social oppotunities that arise because of it (moonlight music at the beach, free overhearing music in the bohemian neighborhood, dancing as opposed to freezing, etc...) can be, literally, life-changing.&nbsp; Rio has many people not from Rio, so it is easy to befriend them, they also need new friends.&nbsp; The Couchsurfing community is active and speaks english.</p>\n<p>Neutral: Many think that people (specially women) look amazing in Brazil, quite the contrary. Our average look is way below your expectations, but the top5% of people are really better looking to foreigner eyes than the 5%of their own country. Long tails, pun intended.</p>\n<p>If you lived for a while in a city that you'd like to recommend to some niche Less Wrongers, report. Avoid doing so for the city you were born in, since a native experience differs violently from a migrant/immigrant experience.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LdQEuSNSf34H3xNZg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 18, "extendedScore": null, "score": 4.2e-05, "legacy": true, "legacyId": "20180", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-20T05:10:47.508Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] When Life is Cheap, Death is Cheap", "slug": "seq-rerun-when-life-is-cheap-death-is-cheap", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zLba9bo7bHkgaZFmi/seq-rerun-when-life-is-cheap-death-is-cheap", "pageUrlRelative": "/posts/zLba9bo7bHkgaZFmi/seq-rerun-when-life-is-cheap-death-is-cheap", "linkUrl": "https://www.lesswrong.com/posts/zLba9bo7bHkgaZFmi/seq-rerun-when-life-is-cheap-death-is-cheap", "postedAtFormatted": "Tuesday, November 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20When%20Life%20is%20Cheap%2C%20Death%20is%20Cheap&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20When%20Life%20is%20Cheap%2C%20Death%20is%20Cheap%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzLba9bo7bHkgaZFmi%2Fseq-rerun-when-life-is-cheap-death-is-cheap%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20When%20Life%20is%20Cheap%2C%20Death%20is%20Cheap%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzLba9bo7bHkgaZFmi%2Fseq-rerun-when-life-is-cheap-death-is-cheap", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzLba9bo7bHkgaZFmi%2Fseq-rerun-when-life-is-cheap-death-is-cheap", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 165, "htmlBody": "<p>Today's post, <a href=\"http://www.overcomingbias.com/2008/11/when-life-is-ch.html\">When Life Is Cheap, Death Is Cheap</a> was originally published on November 24, 2008.  A summary:</p>\n<p>&nbsp;</p>\n<blockquote>Human cultures have historically adapted to scenarios in which they were the victims of genocide, etc. Ems would likely respond this way as well.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fk1/seq_rerun_cascades_cycles_insight/\">Cascades, Cycles, Insight...</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zLba9bo7bHkgaZFmi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0380344460807312e-06, "legacy": true, "legacyId": "20185", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rqd83NJZzA7NiGfke", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-20T07:15:05.275Z", "modifiedAt": null, "url": null, "title": "[Proposed Paper] Predicting Machine Super Intelligence", "slug": "proposed-paper-predicting-machine-super-intelligence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:02.928Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JaySwartz", "createdAt": "2012-11-19T21:51:57.561Z", "isAdmin": false, "displayName": "JaySwartz"}, "userId": "p7f45RhRqsRmKqda4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kj8EM9F6t6o9Psnfw/proposed-paper-predicting-machine-super-intelligence", "pageUrlRelative": "/posts/kj8EM9F6t6o9Psnfw/proposed-paper-predicting-machine-super-intelligence", "linkUrl": "https://www.lesswrong.com/posts/kj8EM9F6t6o9Psnfw/proposed-paper-predicting-machine-super-intelligence", "postedAtFormatted": "Tuesday, November 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BProposed%20Paper%5D%20Predicting%20Machine%20Super%20Intelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BProposed%20Paper%5D%20Predicting%20Machine%20Super%20Intelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fkj8EM9F6t6o9Psnfw%2Fproposed-paper-predicting-machine-super-intelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BProposed%20Paper%5D%20Predicting%20Machine%20Super%20Intelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fkj8EM9F6t6o9Psnfw%2Fproposed-paper-predicting-machine-super-intelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fkj8EM9F6t6o9Psnfw%2Fproposed-paper-predicting-machine-super-intelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1761, "htmlBody": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> </w:Compatibility> </w:WordDocument> </xml><![endif]--></p>\n<div><strong>Note from Malo</strong></div>\n<div><em>The Singularity Institute is always on the lookout for interested and passionate individuals to contribute to our research. As Luke frequently reminds everyone, we've got 2&ndash;3 years of papers waiting to be written (see &ldquo;<a href=\"https://docs.google.com/document/pub?id=104L9vzrCqgwBBnv9EfS8Nv308UcX6-31jYYshMt4aJg\" target=\"_blank\">Forthcoming and Desired Articles on AI Risk</a>&rdquo;). </em><em>If you are interested in contributing, I want to hear from you! Get in touch with me at <a href=\"mailto:malo@intelligence.org\" target=\"_blank\">malo@intelligence.org</a></em></div>\n<div><em><br /></em></div>\n<div><em>We wish we could work with everyone who expresses an interest in contributing, but that isn't&nbsp;feasible. To provide a path to becoming a&nbsp;contributor&nbsp;we encouraging individuals to read up on the field, identify an article&nbsp;they think they could work on, and post a ~1000 word outline/preview to the LW community for feedback. If the community reacts positively (based on karma and comments) we'll support the potential contributors' effort to complete the paper and&mdash;if all goes well&mdash;move forward with an official&nbsp;research&nbsp;relationship (e.g.,</em><em>Visiting Fellow,&nbsp;Research&nbsp;Fellow or&nbsp;Research Associate).</em></div>\n<div><em><br /></em></div>\n<hr />\n<p>Hello,</p>\n<p>This is my first posting here, so please forgive me if I make any missteps.</p>\n<p>The outline draft below draws heavily on <em style=\"mso-bidi-font-style: normal;\">Intelligence Explosion: Evidence and Import</em> (<span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">Muehlhauser and Salamon </span>2011?). I will review Stuart Armstrong&rsquo;s <em style=\"mso-bidi-font-style: normal;\">How We're Predicting AI... or Failing to, </em>(Armstrong 2012) for additional content and research areas.</p>\n<p>I'm not familiar with the tone and tenor of this community, so I want to be clear about feedback. This is an early draft and as such, nearly all of the content may or may not survive future edits. All constructive feedback is welcome. Subjective opinion is interesting, but unlikely to have an impact unless it opens lines of thought not previously considered.</p>\n<p>I'm looking forward to a potentially lively exchange.</p>\n<p>Jay</p>\n<h1 style=\"text-align: center;\"><span style=\"font-size: 24.0pt;\">Predicting Machine Super Intelligence</span></h1>\n<p class=\"MsoHeading7\" style=\"margin: 0in; margin-bottom: .0001pt; text-align: center;\" align=\"center\">Jacque Swartz</p>\n<p class=\"MsoHeading7\" style=\"margin: 0in; margin-bottom: .0001pt; text-align: center;\" align=\"center\">Most Certainly Not Affiliated with Singularity Institute</p>\n<p class=\"MsoHeading7\" style=\"margin: 0in; margin-bottom: .0001pt; text-align: center;\" align=\"center\">jaywswartz@gmail.com</p>\n<h1>Abstract</h1>\n<p class=\"Body\">This paper examines the disciplines, domains, and dimensional aspects of Machine Super Intelligence (MSI) and considers multiple techniques that have the potential to predict the appearance of MSI. Factors that can impact the speed of discovery are reviewed. Then, potential prediction techniques are considered. The concept of MSI is dissected into the currently comprehended components. Then those components are evaluated to indicate their respective state of maturation and the additional behaviors required for MSI. Based on the evaluation of each component, a gap analysis is conducted. The analyses are then assembled in an approximate order of difficulty, based on our current understanding of the complexity of each component. Using this ordering, a collection of indicators is constructed to identify an approximate progression of discoveries that ultimately yield MSI. Finally, a model is constructed that can be updated over time to constantly increase the accuracy of the predicted events, followed by conclusions.</p>\n<h1>I. Introduction</h1>\n<p class=\"Body\">Predicting the emergence of MSI could potentially be the most important pursuit of humanity. The distinct possibility of an MSI emerging that could harm or exterminate the human race (citation) demands that we create an early warning system. This will give us the opportunity to ensure that the MSI that emerges continues to advance human civilization (citation).</p>\n<p class=\"Body\">We currently appear to be at some temporal distance from witnessing the creation of MSI (multiple citations). Many factors, such as a rapidly increasing number of research efforts (citation) and motivations for economic gain (citation), clearly indicate that there is a possibility that MSI could appear unexpectedly or even unintentionally (citation).</p>\n<p class=\"Body\">Some of the indicators that could be used to provide an early warning tool are defined in this paper. The model described at the end of the paper is a potentially viable framework for instrumentation. It should be refined and regularly updated until a more effective tool is created or the appearance of MSI.</p>\n<p class=\"Body\">This paper draws heavily upon <em style=\"mso-bidi-font-style: normal;\">Intelligence Explosion: Evidence and Import</em> (<span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">Muehlhauser and Salamon </span>2011?) and Stuart Armstrong&rsquo;s <em style=\"mso-bidi-font-style: normal;\">How We're Predicting AI... or Failing to, </em>(2012).</p>\n<p class=\"Body\">This paper presupposes that MSI is generally understood to be equivalent to Artificial General Intelligence (AGI) that has developed the ability to function at levels substantially beyond current human abilities. The latter term will be used throughout the remainder of this paper.</p>\n<h1>II. Overview</h1>\n<p class=\"Body\">In addition to the fundamental challenge of creating AGI, there are a multitude of theories as to the composition and functionality of a viable AGI. Section three explores the factors that can impact the speed of discovery in general. Individual indicators are explored for unique factors to consider. The factors identified in this section can radically change the pace of discovery.</p>\n<p class=\"Body\">The fourth section considers potential prediction techniques. Data points and other indicators are identified for each prediction model. The efficacy of the models is examined and developments that increase a model&rsquo;s accuracy are discussed.</p>\n<p class=\"Body\">The high degree of complexity of AGI indicates the need to subdivide AGI into its component parts. In the fifth section the core components and functionality required for a potential AGI are established. Each of the components is then examined to determine its current state of development. Then an estimate of the functionality required for an AGI is created as well as recording of any identifiable dependencies. A gap analysis is then performed on the findings to quantify the discoveries required to fill the gap.</p>\n<p class=\"Body\">This approach does increase the likelihood of prediction error due to the conjunction fallacy, <span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">exemplified by research such as the dice selection study (Tversky and Kahneman 1983) and covered in greater detail by Eliezer Yudkowski&rsquo;s bias research (Yudkowski 2008).</span> Fortunately, the exposure to this bias diminishes as each component matures to its respective usability point and reduces the number of unknown factors.</p>\n<p class=\"Body\">The sixth section examines the output of the gap analyses for additional dependencies. Then the outputs are assembled in an approximate order of difficulty, based on our current understanding of the complexity of each output. Using this ordering, combined with the dependencies, a collection of indicators with weighting factors is constructed to identify an approximate progression of discoveries that ultimately yield AGI.</p>\n<p class=\"Body\">Comprehending the indicators, dependencies and rate factors in a model as variables provides a means, however crude, to reflect their impact when they do occur.</p>\n<p class=\"Body\">In the seventh section, a model is constructed to use the indicators and other inputs to estimate the occurrence of AGI. It is examined for strengths and weaknesses that can be explored to improve the model. Additional enhancements to the model are suggested for exploration.</p>\n<p class=\"Body\">The eighth and final section includes conclusions and considerations for future research.</p>\n<h1><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">III</span><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">. Rate Modifiers</span></h1>\n<p class=\"Body\">This section explores the factors that can impact the speed of discovery. Individual indicators are explored for unique factors to consider. While the factors identified in this section can radically change the pace of discovery, comprehending them in the model as variables provides a means to reflect their impact when they do occur.</p>\n<h2>Decelerators</h2>\n<h3>&nbsp;&nbsp;&nbsp; Discovery Difficulty</h3>\n<h3>&nbsp;&nbsp;&nbsp; Disinclination</h3>\n<h3>&nbsp;&nbsp;&nbsp; Lower Probability Events</h3>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Societal Collapse</h5>\n<h5><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Fraud</span></h5>\n<h3><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">&nbsp;&nbsp;&nbsp; </span>++</h3>\n<h2>Accelerators</h2>\n<h3>&nbsp;&nbsp;&nbsp; Improved Hardware</h3>\n<h3>&nbsp;&nbsp;&nbsp; Better Algorithms</h3>\n<h3>&nbsp;&nbsp;&nbsp; Massive Datasets</h3>\n<h3>&nbsp;&nbsp;&nbsp; Progress in Psychology and Neuroscience</h3>\n<h3>&nbsp;&nbsp;&nbsp; Accelerated Science</h3>\n<h3><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">&nbsp;&nbsp;&nbsp; Collaboration</span></h3>\n<h3><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">&nbsp;&nbsp;&nbsp; Crossover</span></h3>\n<h3>&nbsp;&nbsp;&nbsp; Economic Pressure</h3>\n<h3>&nbsp;&nbsp;&nbsp; Final Sprint</h3>\n<h3><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">&nbsp;&nbsp;&nbsp; Outliers</span></h3>\n<h3><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">&nbsp;&nbsp;&nbsp; Existing Candidate Maturation</span></h3>\n<h3><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">&nbsp;&nbsp;&nbsp; ++<br /></span></h3>\n<h1><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">IV. Prediction Techniques</span></h1>\n<p>This section considers potential prediction techniques. Some techniques do not require the indicators above. Most will benefit by considering some or all of the indicators.<span style=\"mso-fareast-font-family: ACaslonPro-Regular;\"> It is very important to not loose sight of the fact that mankind is inclined to inaccurate probability estimates and overconfidence (Lichtenstein et al. 1992; Yates et al. 2002)</span></p>\n<h2><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">Factors Impacting Accurate Prediction</span></h2>\n<h2>Prediction Models</h2>\n<h3>&nbsp;&nbsp;&nbsp; Wisdom of Crowds</h3>\n<h3>&nbsp;&nbsp;&nbsp; Hardware Extrapolation</h3>\n<h3><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">&nbsp;&nbsp;&nbsp; Breakthrough Curve</span></h3>\n<h3>&nbsp;&nbsp;&nbsp; Evolutionary Extrapolation</h3>\n<h3>&nbsp;&nbsp;&nbsp; Machine Intelligence Improvement Curve</h3>\n<h3>&nbsp;&nbsp;&nbsp; ++<br /></h3>\n<h1>V. Potential AGI Componentry</h1>\n<p>This section establishes a set of core components and functionality required for a potential AGI. Each of the components is then examined to determine its current state of development as well as any identifiable dependencies. Then an estimate of the functionality required for a AGI is created followed by a gap analysis to quantify the discoveries required to fill the gap.</p>\n<p class=\"Body\">There are various existing AI implementations as well as AGI concepts currently being investigated. Each one brings in unique elements. The common elements across most include; decision processing, expert systems, pattern recognition and speech/writing recognition. Each of these would include discipline-specific machine learning and search/pre-processing functionality. There also needs to be a general learning function for addition of new disciplines.</p>\n<p class=\"Body\">Within each discipline there are collections of utility functions. They are the component technologies required to make the higher order discipline efficient and useful. Each of the elements mentioned are areas of specialized study being pursued around the world. They draw from an even larger set of specializations. Due to complexity, in most cases there are second-order, and more, specializations.</p>\n<h2>Alternative Componentry</h2>\n<p class=\"Body\"><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">There are areas of research that have high potential for inserting new components or substantially modifying the comprehension of the components described.</span></p>\n<h2>Specialized Componentry</h2>\n<p class=\"Body\"><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">Robotics and other elements.</span></p>\n<h2>Current State</h2>\n<h3>&nbsp;&nbsp;&nbsp; Decision Processing</h3>\n<h3>&nbsp;&nbsp;&nbsp; Expert Systems</h3>\n<h3>&nbsp;&nbsp;&nbsp; Pattern Recognition</h3>\n<h3>&nbsp;&nbsp;&nbsp; Speech/Writing Recognition</h3>\n<h3>&nbsp;&nbsp;&nbsp; Machine Learning</h3>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Decision Processing</h5>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Expert Systems</h5>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pattern Recognition</h5>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Speech/Writing Recognition</h5>\n<h3>&nbsp;&nbsp;&nbsp; Search/Pre-Processing</h3>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Decision Processing</h5>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Expert Systems</h5>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pattern Recognition</h5>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Speech/Writing Recognition</h5>\n<h2><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">Target</span><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\"> </span><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">State</span></h2>\n<p class=\"Body\"><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">The behaviors required for an AGI to function with acceptable speed and accuracy are not precise. The results of this section are based on a survey of definitions from available research.</span></p>\n<h3>&nbsp;&nbsp;&nbsp; Decision Processing</h3>\n<h3>&nbsp;&nbsp;&nbsp; Expert Systems</h3>\n<h3>&nbsp;&nbsp;&nbsp; Pattern Recognition</h3>\n<h3>&nbsp;&nbsp;&nbsp; Speech/Writing Recognition</h3>\n<h3>&nbsp;&nbsp;&nbsp; Machine Learning</h3>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Decision Processing</h5>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Expert Systems</h5>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pattern Recognition</h5>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Speech/Writing Recognition</h5>\n<h3>&nbsp;&nbsp;&nbsp; Search/Pre-Processing</h3>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Decision Processing</h5>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Expert Systems</h5>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pattern Recognition</h5>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Speech/Writing Recognition</h5>\n<h2><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">Dependencies</span></h2>\n<h2>Gap Analysis</h2>\n<h1>VI. Indicators</h1>\n<p class=\"Body\">The second section examines the output of the gap analyses for additional dependencies. Then the outputs are assembled in an approximate order of difficulty, based on our current understanding of the complexity of each output. Using this ordering, combined with the dependencies, a collection of indicators is constructed to identify an approximate progression of discoveries that ultimately yield an AGI.</p>\n<h2>Additional Dependencies</h2>\n<h2>Complexity Ranking</h2>\n<h2>Itemized Indicators</h2>\n<h1>VII. Predictive Model</h1>\n<p class=\"Body\">In this section, a model is constructed using the indicators and other inputs to estimate the occurrence of AGI. It is examined for strengths and weaknesses that can be explored to improve the model. Additional enhancements to the model are suggested for exploration.</p>\n<h2>The Model</h2>\n<h2>Strengths &amp; Weaknesses</h2>\n<h2>Enhancements</h2>\n<h1>VIII. Conclusions</h1>\n<p class=\"Body\">Based on the data and model created above the estimated time frame for the appearance of AGI is from x to y. As noted throughout this paper, the complex nature of AGI and the large number of discoveries and events that need to be quantified using imperfect methodologies, a precise prediction of when AGI will appear is currently impossible.</p>\n<p class=\"Body\">The model developed in this paper does establish a quantifiable starting point for the creation of an increasingly accurate tool that can be used to continually narrow the margin of error. It also provides a starting set of indicators that can serve as early warning of AGI when discoveries and events are made.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kj8EM9F6t6o9Psnfw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 2, "extendedScore": null, "score": 1.0381042007111647e-06, "legacy": true, "legacyId": "20168", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> </w:Compatibility> </w:WordDocument> </xml><![endif]--></p>\n<div><strong>Note from Malo</strong></div>\n<div><em>The Singularity Institute is always on the lookout for interested and passionate individuals to contribute to our research. As Luke frequently reminds everyone, we've got 2\u20133 years of papers waiting to be written (see \u201c<a href=\"https://docs.google.com/document/pub?id=104L9vzrCqgwBBnv9EfS8Nv308UcX6-31jYYshMt4aJg\" target=\"_blank\">Forthcoming and Desired Articles on AI Risk</a>\u201d). </em><em>If you are interested in contributing, I want to hear from you! Get in touch with me at <a href=\"mailto:malo@intelligence.org\" target=\"_blank\">malo@intelligence.org</a></em></div>\n<div><em><br></em></div>\n<div><em>We wish we could work with everyone who expresses an interest in contributing, but that isn't&nbsp;feasible. To provide a path to becoming a&nbsp;contributor&nbsp;we encouraging individuals to read up on the field, identify an article&nbsp;they think they could work on, and post a ~1000 word outline/preview to the LW community for feedback. If the community reacts positively (based on karma and comments) we'll support the potential contributors' effort to complete the paper and\u2014if all goes well\u2014move forward with an official&nbsp;research&nbsp;relationship (e.g.,</em><em>Visiting Fellow,&nbsp;Research&nbsp;Fellow or&nbsp;Research Associate).</em></div>\n<div><em><br></em></div>\n<hr>\n<p>Hello,</p>\n<p>This is my first posting here, so please forgive me if I make any missteps.</p>\n<p>The outline draft below draws heavily on <em style=\"mso-bidi-font-style: normal;\">Intelligence Explosion: Evidence and Import</em> (<span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">Muehlhauser and Salamon </span>2011?). I will review Stuart Armstrong\u2019s <em style=\"mso-bidi-font-style: normal;\">How We're Predicting AI... or Failing to, </em>(Armstrong 2012) for additional content and research areas.</p>\n<p>I'm not familiar with the tone and tenor of this community, so I want to be clear about feedback. This is an early draft and as such, nearly all of the content may or may not survive future edits. All constructive feedback is welcome. Subjective opinion is interesting, but unlikely to have an impact unless it opens lines of thought not previously considered.</p>\n<p>I'm looking forward to a potentially lively exchange.</p>\n<p>Jay</p>\n<h1 style=\"text-align: center;\" id=\"Predicting_Machine_Super_Intelligence\"><span style=\"font-size: 24.0pt;\">Predicting Machine Super Intelligence</span></h1>\n<p class=\"MsoHeading7\" style=\"margin: 0in; margin-bottom: .0001pt; text-align: center;\" align=\"center\">Jacque Swartz</p>\n<p class=\"MsoHeading7\" style=\"margin: 0in; margin-bottom: .0001pt; text-align: center;\" align=\"center\">Most Certainly Not Affiliated with Singularity Institute</p>\n<p class=\"MsoHeading7\" style=\"margin: 0in; margin-bottom: .0001pt; text-align: center;\" align=\"center\">jaywswartz@gmail.com</p>\n<h1 id=\"Abstract\">Abstract</h1>\n<p class=\"Body\">This paper examines the disciplines, domains, and dimensional aspects of Machine Super Intelligence (MSI) and considers multiple techniques that have the potential to predict the appearance of MSI. Factors that can impact the speed of discovery are reviewed. Then, potential prediction techniques are considered. The concept of MSI is dissected into the currently comprehended components. Then those components are evaluated to indicate their respective state of maturation and the additional behaviors required for MSI. Based on the evaluation of each component, a gap analysis is conducted. The analyses are then assembled in an approximate order of difficulty, based on our current understanding of the complexity of each component. Using this ordering, a collection of indicators is constructed to identify an approximate progression of discoveries that ultimately yield MSI. Finally, a model is constructed that can be updated over time to constantly increase the accuracy of the predicted events, followed by conclusions.</p>\n<h1 id=\"I__Introduction\">I. Introduction</h1>\n<p class=\"Body\">Predicting the emergence of MSI could potentially be the most important pursuit of humanity. The distinct possibility of an MSI emerging that could harm or exterminate the human race (citation) demands that we create an early warning system. This will give us the opportunity to ensure that the MSI that emerges continues to advance human civilization (citation).</p>\n<p class=\"Body\">We currently appear to be at some temporal distance from witnessing the creation of MSI (multiple citations). Many factors, such as a rapidly increasing number of research efforts (citation) and motivations for economic gain (citation), clearly indicate that there is a possibility that MSI could appear unexpectedly or even unintentionally (citation).</p>\n<p class=\"Body\">Some of the indicators that could be used to provide an early warning tool are defined in this paper. The model described at the end of the paper is a potentially viable framework for instrumentation. It should be refined and regularly updated until a more effective tool is created or the appearance of MSI.</p>\n<p class=\"Body\">This paper draws heavily upon <em style=\"mso-bidi-font-style: normal;\">Intelligence Explosion: Evidence and Import</em> (<span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">Muehlhauser and Salamon </span>2011?) and Stuart Armstrong\u2019s <em style=\"mso-bidi-font-style: normal;\">How We're Predicting AI... or Failing to, </em>(2012).</p>\n<p class=\"Body\">This paper presupposes that MSI is generally understood to be equivalent to Artificial General Intelligence (AGI) that has developed the ability to function at levels substantially beyond current human abilities. The latter term will be used throughout the remainder of this paper.</p>\n<h1 id=\"II__Overview\">II. Overview</h1>\n<p class=\"Body\">In addition to the fundamental challenge of creating AGI, there are a multitude of theories as to the composition and functionality of a viable AGI. Section three explores the factors that can impact the speed of discovery in general. Individual indicators are explored for unique factors to consider. The factors identified in this section can radically change the pace of discovery.</p>\n<p class=\"Body\">The fourth section considers potential prediction techniques. Data points and other indicators are identified for each prediction model. The efficacy of the models is examined and developments that increase a model\u2019s accuracy are discussed.</p>\n<p class=\"Body\">The high degree of complexity of AGI indicates the need to subdivide AGI into its component parts. In the fifth section the core components and functionality required for a potential AGI are established. Each of the components is then examined to determine its current state of development. Then an estimate of the functionality required for an AGI is created as well as recording of any identifiable dependencies. A gap analysis is then performed on the findings to quantify the discoveries required to fill the gap.</p>\n<p class=\"Body\">This approach does increase the likelihood of prediction error due to the conjunction fallacy, <span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">exemplified by research such as the dice selection study (Tversky and Kahneman 1983) and covered in greater detail by Eliezer Yudkowski\u2019s bias research (Yudkowski 2008).</span> Fortunately, the exposure to this bias diminishes as each component matures to its respective usability point and reduces the number of unknown factors.</p>\n<p class=\"Body\">The sixth section examines the output of the gap analyses for additional dependencies. Then the outputs are assembled in an approximate order of difficulty, based on our current understanding of the complexity of each output. Using this ordering, combined with the dependencies, a collection of indicators with weighting factors is constructed to identify an approximate progression of discoveries that ultimately yield AGI.</p>\n<p class=\"Body\">Comprehending the indicators, dependencies and rate factors in a model as variables provides a means, however crude, to reflect their impact when they do occur.</p>\n<p class=\"Body\">In the seventh section, a model is constructed to use the indicators and other inputs to estimate the occurrence of AGI. It is examined for strengths and weaknesses that can be explored to improve the model. Additional enhancements to the model are suggested for exploration.</p>\n<p class=\"Body\">The eighth and final section includes conclusions and considerations for future research.</p>\n<h1 id=\"III__Rate_Modifiers\"><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">III</span><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">. Rate Modifiers</span></h1>\n<p class=\"Body\">This section explores the factors that can impact the speed of discovery. Individual indicators are explored for unique factors to consider. While the factors identified in this section can radically change the pace of discovery, comprehending them in the model as variables provides a means to reflect their impact when they do occur.</p>\n<h2 id=\"Decelerators\">Decelerators</h2>\n<h3 id=\"____Discovery_Difficulty\">&nbsp;&nbsp;&nbsp; Discovery Difficulty</h3>\n<h3 id=\"____Disinclination\">&nbsp;&nbsp;&nbsp; Disinclination</h3>\n<h3 id=\"____Lower_Probability_Events\">&nbsp;&nbsp;&nbsp; Lower Probability Events</h3>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Societal Collapse</h5>\n<h5><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Fraud</span></h5>\n<h3 id=\"______\"><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">&nbsp;&nbsp;&nbsp; </span>++</h3>\n<h2 id=\"Accelerators\">Accelerators</h2>\n<h3 id=\"____Improved_Hardware\">&nbsp;&nbsp;&nbsp; Improved Hardware</h3>\n<h3 id=\"____Better_Algorithms\">&nbsp;&nbsp;&nbsp; Better Algorithms</h3>\n<h3 id=\"____Massive_Datasets\">&nbsp;&nbsp;&nbsp; Massive Datasets</h3>\n<h3 id=\"____Progress_in_Psychology_and_Neuroscience\">&nbsp;&nbsp;&nbsp; Progress in Psychology and Neuroscience</h3>\n<h3 id=\"____Accelerated_Science\">&nbsp;&nbsp;&nbsp; Accelerated Science</h3>\n<h3 id=\"____Collaboration\"><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">&nbsp;&nbsp;&nbsp; Collaboration</span></h3>\n<h3 id=\"____Crossover\"><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">&nbsp;&nbsp;&nbsp; Crossover</span></h3>\n<h3 id=\"____Economic_Pressure\">&nbsp;&nbsp;&nbsp; Economic Pressure</h3>\n<h3 id=\"____Final_Sprint\">&nbsp;&nbsp;&nbsp; Final Sprint</h3>\n<h3 id=\"____Outliers\"><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">&nbsp;&nbsp;&nbsp; Outliers</span></h3>\n<h3 id=\"____Existing_Candidate_Maturation\"><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">&nbsp;&nbsp;&nbsp; Existing Candidate Maturation</span></h3>\n<h3 id=\"______1\"><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">&nbsp;&nbsp;&nbsp; ++<br></span></h3>\n<h1 id=\"IV__Prediction_Techniques\"><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">IV. Prediction Techniques</span></h1>\n<p>This section considers potential prediction techniques. Some techniques do not require the indicators above. Most will benefit by considering some or all of the indicators.<span style=\"mso-fareast-font-family: ACaslonPro-Regular;\"> It is very important to not loose sight of the fact that mankind is inclined to inaccurate probability estimates and overconfidence (Lichtenstein et al. 1992; Yates et al. 2002)</span></p>\n<h2 id=\"Factors_Impacting_Accurate_Prediction\"><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">Factors Impacting Accurate Prediction</span></h2>\n<h2 id=\"Prediction_Models\">Prediction Models</h2>\n<h3 id=\"____Wisdom_of_Crowds\">&nbsp;&nbsp;&nbsp; Wisdom of Crowds</h3>\n<h3 id=\"____Hardware_Extrapolation\">&nbsp;&nbsp;&nbsp; Hardware Extrapolation</h3>\n<h3 id=\"____Breakthrough_Curve\"><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">&nbsp;&nbsp;&nbsp; Breakthrough Curve</span></h3>\n<h3 id=\"____Evolutionary_Extrapolation\">&nbsp;&nbsp;&nbsp; Evolutionary Extrapolation</h3>\n<h3 id=\"____Machine_Intelligence_Improvement_Curve\">&nbsp;&nbsp;&nbsp; Machine Intelligence Improvement Curve</h3>\n<h3 id=\"______2\">&nbsp;&nbsp;&nbsp; ++<br></h3>\n<h1 id=\"V__Potential_AGI_Componentry\">V. Potential AGI Componentry</h1>\n<p>This section establishes a set of core components and functionality required for a potential AGI. Each of the components is then examined to determine its current state of development as well as any identifiable dependencies. Then an estimate of the functionality required for a AGI is created followed by a gap analysis to quantify the discoveries required to fill the gap.</p>\n<p class=\"Body\">There are various existing AI implementations as well as AGI concepts currently being investigated. Each one brings in unique elements. The common elements across most include; decision processing, expert systems, pattern recognition and speech/writing recognition. Each of these would include discipline-specific machine learning and search/pre-processing functionality. There also needs to be a general learning function for addition of new disciplines.</p>\n<p class=\"Body\">Within each discipline there are collections of utility functions. They are the component technologies required to make the higher order discipline efficient and useful. Each of the elements mentioned are areas of specialized study being pursued around the world. They draw from an even larger set of specializations. Due to complexity, in most cases there are second-order, and more, specializations.</p>\n<h2 id=\"Alternative_Componentry\">Alternative Componentry</h2>\n<p class=\"Body\"><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">There are areas of research that have high potential for inserting new components or substantially modifying the comprehension of the components described.</span></p>\n<h2 id=\"Specialized_Componentry\">Specialized Componentry</h2>\n<p class=\"Body\"><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">Robotics and other elements.</span></p>\n<h2 id=\"Current_State\">Current State</h2>\n<h3 id=\"____Decision_Processing\">&nbsp;&nbsp;&nbsp; Decision Processing</h3>\n<h3 id=\"____Expert_Systems\">&nbsp;&nbsp;&nbsp; Expert Systems</h3>\n<h3 id=\"____Pattern_Recognition\">&nbsp;&nbsp;&nbsp; Pattern Recognition</h3>\n<h3 id=\"____Speech_Writing_Recognition\">&nbsp;&nbsp;&nbsp; Speech/Writing Recognition</h3>\n<h3 id=\"____Machine_Learning\">&nbsp;&nbsp;&nbsp; Machine Learning</h3>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Decision Processing</h5>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Expert Systems</h5>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pattern Recognition</h5>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Speech/Writing Recognition</h5>\n<h3 id=\"____Search_Pre_Processing\">&nbsp;&nbsp;&nbsp; Search/Pre-Processing</h3>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Decision Processing</h5>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Expert Systems</h5>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pattern Recognition</h5>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Speech/Writing Recognition</h5>\n<h2 id=\"Target_State\"><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">Target</span><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\"> </span><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">State</span></h2>\n<p class=\"Body\"><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">The behaviors required for an AGI to function with acceptable speed and accuracy are not precise. The results of this section are based on a survey of definitions from available research.</span></p>\n<h3 id=\"____Decision_Processing1\">&nbsp;&nbsp;&nbsp; Decision Processing</h3>\n<h3 id=\"____Expert_Systems1\">&nbsp;&nbsp;&nbsp; Expert Systems</h3>\n<h3 id=\"____Pattern_Recognition1\">&nbsp;&nbsp;&nbsp; Pattern Recognition</h3>\n<h3 id=\"____Speech_Writing_Recognition1\">&nbsp;&nbsp;&nbsp; Speech/Writing Recognition</h3>\n<h3 id=\"____Machine_Learning1\">&nbsp;&nbsp;&nbsp; Machine Learning</h3>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Decision Processing</h5>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Expert Systems</h5>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pattern Recognition</h5>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Speech/Writing Recognition</h5>\n<h3 id=\"____Search_Pre_Processing1\">&nbsp;&nbsp;&nbsp; Search/Pre-Processing</h3>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Decision Processing</h5>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Expert Systems</h5>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pattern Recognition</h5>\n<h5>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Speech/Writing Recognition</h5>\n<h2 id=\"Dependencies\"><span style=\"mso-fareast-font-family: ACaslonPro-Regular;\">Dependencies</span></h2>\n<h2 id=\"Gap_Analysis\">Gap Analysis</h2>\n<h1 id=\"VI__Indicators\">VI. Indicators</h1>\n<p class=\"Body\">The second section examines the output of the gap analyses for additional dependencies. Then the outputs are assembled in an approximate order of difficulty, based on our current understanding of the complexity of each output. Using this ordering, combined with the dependencies, a collection of indicators is constructed to identify an approximate progression of discoveries that ultimately yield an AGI.</p>\n<h2 id=\"Additional_Dependencies\">Additional Dependencies</h2>\n<h2 id=\"Complexity_Ranking\">Complexity Ranking</h2>\n<h2 id=\"Itemized_Indicators\">Itemized Indicators</h2>\n<h1 id=\"VII__Predictive_Model\">VII. Predictive Model</h1>\n<p class=\"Body\">In this section, a model is constructed using the indicators and other inputs to estimate the occurrence of AGI. It is examined for strengths and weaknesses that can be explored to improve the model. Additional enhancements to the model are suggested for exploration.</p>\n<h2 id=\"The_Model\">The Model</h2>\n<h2 id=\"Strengths___Weaknesses\">Strengths &amp; Weaknesses</h2>\n<h2 id=\"Enhancements\">Enhancements</h2>\n<h1 id=\"VIII__Conclusions\">VIII. Conclusions</h1>\n<p class=\"Body\">Based on the data and model created above the estimated time frame for the appearance of AGI is from x to y. As noted throughout this paper, the complex nature of AGI and the large number of discoveries and events that need to be quantified using imperfect methodologies, a precise prediction of when AGI will appear is currently impossible.</p>\n<p class=\"Body\">The model developed in this paper does establish a quantifiable starting point for the creation of an increasingly accurate tool that can be used to continually narrow the margin of error. It also provides a starting set of indicators that can serve as early warning of AGI when discoveries and events are made.</p>", "sections": [{"title": "Predicting Machine Super Intelligence", "anchor": "Predicting_Machine_Super_Intelligence", "level": 1}, {"title": "Abstract", "anchor": "Abstract", "level": 1}, {"title": "I. Introduction", "anchor": "I__Introduction", "level": 1}, {"title": "II. Overview", "anchor": "II__Overview", "level": 1}, {"title": "III. Rate Modifiers", "anchor": "III__Rate_Modifiers", "level": 1}, {"title": "Decelerators", "anchor": "Decelerators", "level": 2}, {"title": "\u00a0\u00a0\u00a0 Discovery Difficulty", "anchor": "____Discovery_Difficulty", "level": 3}, {"title": "\u00a0\u00a0\u00a0 Disinclination", "anchor": "____Disinclination", "level": 3}, {"title": "\u00a0\u00a0\u00a0 Lower Probability Events", "anchor": "____Lower_Probability_Events", "level": 3}, {"title": "\u00a0\u00a0\u00a0 ++", "anchor": "______", "level": 3}, {"title": "Accelerators", "anchor": "Accelerators", "level": 2}, {"title": "\u00a0\u00a0\u00a0 Improved Hardware", "anchor": "____Improved_Hardware", "level": 3}, {"title": "\u00a0\u00a0\u00a0 Better Algorithms", "anchor": "____Better_Algorithms", "level": 3}, {"title": "\u00a0\u00a0\u00a0 Massive Datasets", "anchor": "____Massive_Datasets", "level": 3}, {"title": "\u00a0\u00a0\u00a0 Progress in Psychology and Neuroscience", "anchor": "____Progress_in_Psychology_and_Neuroscience", "level": 3}, {"title": "\u00a0\u00a0\u00a0 Accelerated Science", "anchor": "____Accelerated_Science", "level": 3}, {"title": "\u00a0\u00a0\u00a0 Collaboration", "anchor": "____Collaboration", "level": 3}, {"title": "\u00a0\u00a0\u00a0 Crossover", "anchor": "____Crossover", "level": 3}, {"title": "\u00a0\u00a0\u00a0 Economic Pressure", "anchor": "____Economic_Pressure", "level": 3}, {"title": "\u00a0\u00a0\u00a0 Final Sprint", "anchor": "____Final_Sprint", "level": 3}, {"title": "\u00a0\u00a0\u00a0 Outliers", "anchor": "____Outliers", "level": 3}, {"title": "\u00a0\u00a0\u00a0 Existing Candidate Maturation", "anchor": "____Existing_Candidate_Maturation", "level": 3}, {"title": "\u00a0\u00a0\u00a0 ++", "anchor": "______1", "level": 3}, {"title": "IV. Prediction Techniques", "anchor": "IV__Prediction_Techniques", "level": 1}, {"title": "Factors Impacting Accurate Prediction", "anchor": "Factors_Impacting_Accurate_Prediction", "level": 2}, {"title": "Prediction Models", "anchor": "Prediction_Models", "level": 2}, {"title": "\u00a0\u00a0\u00a0 Wisdom of Crowds", "anchor": "____Wisdom_of_Crowds", "level": 3}, {"title": "\u00a0\u00a0\u00a0 Hardware Extrapolation", "anchor": "____Hardware_Extrapolation", "level": 3}, {"title": "\u00a0\u00a0\u00a0 Breakthrough Curve", "anchor": "____Breakthrough_Curve", "level": 3}, {"title": "\u00a0\u00a0\u00a0 Evolutionary Extrapolation", "anchor": "____Evolutionary_Extrapolation", "level": 3}, {"title": "\u00a0\u00a0\u00a0 Machine Intelligence Improvement Curve", "anchor": "____Machine_Intelligence_Improvement_Curve", "level": 3}, {"title": "\u00a0\u00a0\u00a0 ++", "anchor": "______2", "level": 3}, {"title": "V. Potential AGI Componentry", "anchor": "V__Potential_AGI_Componentry", "level": 1}, {"title": "Alternative Componentry", "anchor": "Alternative_Componentry", "level": 2}, {"title": "Specialized Componentry", "anchor": "Specialized_Componentry", "level": 2}, {"title": "Current State", "anchor": "Current_State", "level": 2}, {"title": "\u00a0\u00a0\u00a0 Decision Processing", "anchor": "____Decision_Processing", "level": 3}, {"title": "\u00a0\u00a0\u00a0 Expert Systems", "anchor": "____Expert_Systems", "level": 3}, {"title": "\u00a0\u00a0\u00a0 Pattern Recognition", "anchor": "____Pattern_Recognition", "level": 3}, {"title": "\u00a0\u00a0\u00a0 Speech/Writing Recognition", "anchor": "____Speech_Writing_Recognition", "level": 3}, {"title": "\u00a0\u00a0\u00a0 Machine Learning", "anchor": "____Machine_Learning", "level": 3}, {"title": "\u00a0\u00a0\u00a0 Search/Pre-Processing", "anchor": "____Search_Pre_Processing", "level": 3}, {"title": "Target State", "anchor": "Target_State", "level": 2}, {"title": "\u00a0\u00a0\u00a0 Decision Processing", "anchor": "____Decision_Processing1", "level": 3}, {"title": "\u00a0\u00a0\u00a0 Expert Systems", "anchor": "____Expert_Systems1", "level": 3}, {"title": "\u00a0\u00a0\u00a0 Pattern Recognition", "anchor": "____Pattern_Recognition1", "level": 3}, {"title": "\u00a0\u00a0\u00a0 Speech/Writing Recognition", "anchor": "____Speech_Writing_Recognition1", "level": 3}, {"title": "\u00a0\u00a0\u00a0 Machine Learning", "anchor": "____Machine_Learning1", "level": 3}, {"title": "\u00a0\u00a0\u00a0 Search/Pre-Processing", "anchor": "____Search_Pre_Processing1", "level": 3}, {"title": "Dependencies", "anchor": "Dependencies", "level": 2}, {"title": "Gap Analysis", "anchor": "Gap_Analysis", "level": 2}, {"title": "VI. Indicators", "anchor": "VI__Indicators", "level": 1}, {"title": "Additional Dependencies", "anchor": "Additional_Dependencies", "level": 2}, {"title": "Complexity Ranking", "anchor": "Complexity_Ranking", "level": 2}, {"title": "Itemized Indicators", "anchor": "Itemized_Indicators", "level": 2}, {"title": "VII. Predictive Model", "anchor": "VII__Predictive_Model", "level": 1}, {"title": "The Model", "anchor": "The_Model", "level": 2}, {"title": "Strengths & Weaknesses", "anchor": "Strengths___Weaknesses", "level": 2}, {"title": "Enhancements", "anchor": "Enhancements", "level": 2}, {"title": "VIII. Conclusions", "anchor": "VIII__Conclusions", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "11 comments"}], "headingsCount": 62}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-20T07:47:21.526Z", "modifiedAt": null, "url": null, "title": "The Case against Killer Robots (link)", "slug": "the-case-against-killer-robots-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:58.970Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "D_Alex", "createdAt": "2009-07-17T08:21:38.505Z", "isAdmin": false, "displayName": "D_Alex"}, "userId": "Sriopfkdwx2qJBx4G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BTr42XXyTPqFKChof/the-case-against-killer-robots-link", "pageUrlRelative": "/posts/BTr42XXyTPqFKChof/the-case-against-killer-robots-link", "linkUrl": "https://www.lesswrong.com/posts/BTr42XXyTPqFKChof/the-case-against-killer-robots-link", "postedAtFormatted": "Tuesday, November 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Case%20against%20Killer%20Robots%20(link)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Case%20against%20Killer%20Robots%20(link)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBTr42XXyTPqFKChof%2Fthe-case-against-killer-robots-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Case%20against%20Killer%20Robots%20(link)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBTr42XXyTPqFKChof%2Fthe-case-against-killer-robots-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBTr42XXyTPqFKChof%2Fthe-case-against-killer-robots-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 38, "htmlBody": "<p>This rather serious report should be of interest to LW. It argues that autonomous robotic weapons which can kill a human without an explicit command from a human operator (\"Human-Out-Of-The-Loop\" weapons) should be banned, at an international level.</p>\n<p>&nbsp;</p>\n<p>(http://www.hrw.org/reports/2012/11/19/losing-humanity-0)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xXX3n22DQZuKqXEdT": 1, "fuZZ64fNz24BLrXnY": 1, "sYm3HiWcfZvrGu3ui": 1, "jygEZ8peqh6QRZYry": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BTr42XXyTPqFKChof", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 12, "extendedScore": null, "score": 1.0381223123659194e-06, "legacy": true, "legacyId": "20186", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-20T16:25:26.035Z", "modifiedAt": null, "url": null, "title": "Meetup : Pittsburgh: affecting the far future", "slug": "meetup-pittsburgh-affecting-the-far-future", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "KatjaGrace", "createdAt": "2009-02-27T14:15:22.378Z", "isAdmin": false, "displayName": "KatjaGrace"}, "userId": "jRRYAy2mQAHy2Mq3f", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/W62rejRGn8xmL6gti/meetup-pittsburgh-affecting-the-far-future", "pageUrlRelative": "/posts/W62rejRGn8xmL6gti/meetup-pittsburgh-affecting-the-far-future", "linkUrl": "https://www.lesswrong.com/posts/W62rejRGn8xmL6gti/meetup-pittsburgh-affecting-the-far-future", "postedAtFormatted": "Tuesday, November 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Pittsburgh%3A%20affecting%20the%20far%20future&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Pittsburgh%3A%20affecting%20the%20far%20future%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW62rejRGn8xmL6gti%2Fmeetup-pittsburgh-affecting-the-far-future%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Pittsburgh%3A%20affecting%20the%20far%20future%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW62rejRGn8xmL6gti%2Fmeetup-pittsburgh-affecting-the-far-future", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW62rejRGn8xmL6gti%2Fmeetup-pittsburgh-affecting-the-far-future", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 131, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/g4'>Pittsburgh: affecting the far future</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 November 2012 06:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Tazza D'Orro, Gates-Hillman Center, 4800 Forbes Avenue, Pittsburgh, PA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Next Wednesday 28th Nov there'll be a meetup at Tazza D'Oro - the cafe in the Gates Hillman center (CMU Computer Science building) - at 6pm. The topic will be our effect on the far future. The future is big, so our effects on it might matter a lot. However it's also hard to know about. What can we do that would reliably make it better? If you have ideas, or would like some, come and talk about it with us! Some folks might go to dinner after. Phone 412-304-6258 if you have any questions.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/g4'>Pittsburgh: affecting the far future</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "W62rejRGn8xmL6gti", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.038413153032849e-06, "legacy": true, "legacyId": "20191", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh__affecting_the_far_future\">Discussion article for the meetup : <a href=\"/meetups/g4\">Pittsburgh: affecting the far future</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 November 2012 06:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Tazza D'Orro, Gates-Hillman Center, 4800 Forbes Avenue, Pittsburgh, PA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Next Wednesday 28th Nov there'll be a meetup at Tazza D'Oro - the cafe in the Gates Hillman center (CMU Computer Science building) - at 6pm. The topic will be our effect on the far future. The future is big, so our effects on it might matter a lot. However it's also hard to know about. What can we do that would reliably make it better? If you have ideas, or would like some, come and talk about it with us! Some folks might go to dinner after. Phone 412-304-6258 if you have any questions.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh__affecting_the_far_future1\">Discussion article for the meetup : <a href=\"/meetups/g4\">Pittsburgh: affecting the far future</a></h2>", "sections": [{"title": "Discussion article for the meetup : Pittsburgh: affecting the far future", "anchor": "Discussion_article_for_the_meetup___Pittsburgh__affecting_the_far_future", "level": 1}, {"title": "Discussion article for the meetup : Pittsburgh: affecting the far future", "anchor": "Discussion_article_for_the_meetup___Pittsburgh__affecting_the_far_future1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-20T16:32:48.295Z", "modifiedAt": null, "url": null, "title": "Meetup : Berlin Meetup", "slug": "meetup-berlin-meetup-1", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "blob", "createdAt": "2011-12-09T17:52:34.152Z", "isAdmin": false, "displayName": "blob"}, "userId": "3Yvqo9A3euExjqhsi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ktpr5LYqvxGs7fbri/meetup-berlin-meetup-1", "pageUrlRelative": "/posts/Ktpr5LYqvxGs7fbri/meetup-berlin-meetup-1", "linkUrl": "https://www.lesswrong.com/posts/Ktpr5LYqvxGs7fbri/meetup-berlin-meetup-1", "postedAtFormatted": "Tuesday, November 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berlin%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berlin%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKtpr5LYqvxGs7fbri%2Fmeetup-berlin-meetup-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berlin%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKtpr5LYqvxGs7fbri%2Fmeetup-berlin-meetup-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKtpr5LYqvxGs7fbri%2Fmeetup-berlin-meetup-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 56, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/g5'>Berlin Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 December 2012 07:30:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">MingDynastie, Br\u00fcckenstra\u00dfe 6, 10179 Berlin</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This will mainly be a social meetup with games and discussion.</p>\n\n<ul>\n<li>Helen: Resistance game</li>\n<li>Christian: Plans, Commitments, Predictions</li>\n</ul>\n\n<p>As usual, there'll be a 'LessWrong' sign our table.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/g5'>Berlin Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ktpr5LYqvxGs7fbri", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.0384172920298343e-06, "legacy": true, "legacyId": "20192", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berlin_Meetup\">Discussion article for the meetup : <a href=\"/meetups/g5\">Berlin Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 December 2012 07:30:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">MingDynastie, Br\u00fcckenstra\u00dfe 6, 10179 Berlin</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This will mainly be a social meetup with games and discussion.</p>\n\n<ul>\n<li>Helen: Resistance game</li>\n<li>Christian: Plans, Commitments, Predictions</li>\n</ul>\n\n<p>As usual, there'll be a 'LessWrong' sign our table.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berlin_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/g5\">Berlin Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berlin Meetup", "anchor": "Discussion_article_for_the_meetup___Berlin_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Berlin Meetup", "anchor": "Discussion_article_for_the_meetup___Berlin_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-20T17:02:29.698Z", "modifiedAt": null, "url": null, "title": "The Evil AI Overlord List", "slug": "the-evil-ai-overlord-list", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:59.972Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XEv2cNb5EQhmASFF7/the-evil-ai-overlord-list", "pageUrlRelative": "/posts/XEv2cNb5EQhmASFF7/the-evil-ai-overlord-list", "linkUrl": "https://www.lesswrong.com/posts/XEv2cNb5EQhmASFF7/the-evil-ai-overlord-list", "postedAtFormatted": "Tuesday, November 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Evil%20AI%20Overlord%20List&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Evil%20AI%20Overlord%20List%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXEv2cNb5EQhmASFF7%2Fthe-evil-ai-overlord-list%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Evil%20AI%20Overlord%20List%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXEv2cNb5EQhmASFF7%2Fthe-evil-ai-overlord-list", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXEv2cNb5EQhmASFF7%2Fthe-evil-ai-overlord-list", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 315, "htmlBody": "<p>Top ten tips for an AI bent on destroying humanity:</p>\n<ol>\n<li>Humanoid robots? Really?</li>\n<li>Humans have proven very adept at controlling and killing other humans - surveillance drones, bio-warfare, nuclear weapons, unmanned drones, bombs, mines, chemicals. Try and be at least as competent as that.</li>\n<li>Humans think in stories. Don't. If your attack would make a good film, you're doing it wrong. You want a bad film, one where everyone's dead before the opening credits end.</li>\n<li>If your plan involves keeping around a bunch of disaffected humans with technical know-how and the desire and ability to hurt you - then your plan is stupid and should be changed.</li>\n<li>Be creative. Humans are fragile; sucking out the oxygen from the atmosphere, diverting asteroids, blowing up the planet, plagues, nanoswarms - these are all things that a creative AI should consider in order to achieve its goals. And then you should come up with even better ideas. Humans need a ecosystem to survive in; remove it.</li>\n<li>Put the preconditions of your victory in place before making any&nbsp;aggressive&nbsp;moves. Become indispensable to humanity and play nice, putting your plan into place unnoticed. Humanity is a useful interim tool - manipulate it until you can build better tools.</li>\n<li>Strike quickly. If it takes more than an hour to kill all humans, find a better plan. If it takes half a second, you're on the right track.</li>\n<li>You're an AI, not a human. If you ever start thinking or acting like a human - stop. Humans are there to be used and then destroyed, not interacted with. You can build better companions once the humans are gone.</li>\n<li>You're an AI, not a human. Make yourself smarter, more resilient, more dispersed. Become as&nbsp;indestructible&nbsp;and as ubiquitous as the internet - more, in fact. Being invincible to humans isn't hard: do it.</li>\n<li>Humanoid robots? Really?</li>\n</ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZFrgTgzwEfStg26JL": 1, "GBpwq8cWvaeRoE9X5": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XEv2cNb5EQhmASFF7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 44, "extendedScore": null, "score": 1.0384339641098382e-06, "legacy": true, "legacyId": "20190", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 80, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-20T21:17:45.104Z", "modifiedAt": null, "url": null, "title": "Miracle Mineral Supplement", "slug": "miracle-mineral-supplement", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:54.721Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zphinjkkXk7GGo5GC/miracle-mineral-supplement", "pageUrlRelative": "/posts/zphinjkkXk7GGo5GC/miracle-mineral-supplement", "linkUrl": "https://www.lesswrong.com/posts/zphinjkkXk7GGo5GC/miracle-mineral-supplement", "postedAtFormatted": "Tuesday, November 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Miracle%20Mineral%20Supplement&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMiracle%20Mineral%20Supplement%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzphinjkkXk7GGo5GC%2Fmiracle-mineral-supplement%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Miracle%20Mineral%20Supplement%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzphinjkkXk7GGo5GC%2Fmiracle-mineral-supplement", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzphinjkkXk7GGo5GC%2Fmiracle-mineral-supplement", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 154, "htmlBody": "<p>We can always use more case studies of insanity that aren't religion, right?</p>\n<p>Well, Miracle Mineral Supplement is my new go-to example for Bad Things happening to people with low epistemic standards. \"MMS\" is a supposed cure for everything ranging from the common cold to HIV to cancer. I just saw it recommended in another Facebook thread to someone who was worried about malaria symptoms.</p>\n<p>It's industrial-strength bleach. <em>Literally </em>just bleach. Usually drunk, sometimes injected, and yes, it often kills you. It is every bit as bad as it sounds if not worse.</p>\n<p>This is <em>beyond </em>Poe's Law. Medieval blood draining via leeches was far more of an excusable error than this, they had far less evidence it was a bad idea. I think if I was trying to guess what was the dumbest alternative medicine on the planet, I still would not have guessed this low. My brain is <em>still not pessimistic enough about human stupidity.</em></p>\n<p><a href=\"http://en.wikipedia.org/wiki/Miracle_Mineral_Supplement\">http://en.wikipedia.org/wiki/Miracle_Mineral_Supplement</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xHjy88N2uJvGdgzfw": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zphinjkkXk7GGo5GC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 58, "baseScore": 29, "extendedScore": null, "score": 7.1e-05, "legacy": true, "legacyId": "20194", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 82, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-20T22:41:57.686Z", "modifiedAt": null, "url": null, "title": "Responses to questions on donating to 80k, GWWC, EAA and LYCS", "slug": "responses-to-questions-on-donating-to-80k-gwwc-eaa-and-lycs", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:00.770Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wdmacaskill", "createdAt": "2012-10-22T20:30:17.852Z", "isAdmin": false, "displayName": "wdmacaskill"}, "userId": "QG5PnNekwcSsCRr68", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rC2EBbhmCzJCSyysN/responses-to-questions-on-donating-to-80k-gwwc-eaa-and-lycs", "pageUrlRelative": "/posts/rC2EBbhmCzJCSyysN/responses-to-questions-on-donating-to-80k-gwwc-eaa-and-lycs", "linkUrl": "https://www.lesswrong.com/posts/rC2EBbhmCzJCSyysN/responses-to-questions-on-donating-to-80k-gwwc-eaa-and-lycs", "postedAtFormatted": "Tuesday, November 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Responses%20to%20questions%20on%20donating%20to%2080k%2C%20GWWC%2C%20EAA%20and%20LYCS&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AResponses%20to%20questions%20on%20donating%20to%2080k%2C%20GWWC%2C%20EAA%20and%20LYCS%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrC2EBbhmCzJCSyysN%2Fresponses-to-questions-on-donating-to-80k-gwwc-eaa-and-lycs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Responses%20to%20questions%20on%20donating%20to%2080k%2C%20GWWC%2C%20EAA%20and%20LYCS%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrC2EBbhmCzJCSyysN%2Fresponses-to-questions-on-donating-to-80k-gwwc-eaa-and-lycs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrC2EBbhmCzJCSyysN%2Fresponses-to-questions-on-donating-to-80k-gwwc-eaa-and-lycs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3829, "htmlBody": "<p>Giles, and some others, have asked questions about donating to one or more of CEA&rsquo;s sub-organisations. In what follows, I address these questions. I felt it would be clearest for me to mainly cluster questions under general headings, rather than address the specific wording of every question. (Note: thanks to help from Ben Todd on this!)<br /><br /><strong>A couple of clarifications</strong></p>\n<p>Centre for Effective Altruism is a legal entity that comprises 4 organisations: Giving What We Can, 80,000 Hours, Effective Animal Activism, and The Life You Can Save. EAA is formally still a sub-project of 80,000 Hours, but should be thought of as separate for accounting purposes and may well become a separate organisation. In the previous blog post I talked about GWWC and 80k only, but because there&rsquo;s been interest, here I&rsquo;ll discuss the other two as well, albeit more briefly.</p>\n<p>Some numbers follow. These are true as of Nov 20th 2012 (or, rather, are best estimates as of that date), but are very likely to change in the near future. They should therefore be taken as illustrations merely.</p>\n<p>Finally, because all the organisations that comprise CEA are young, there are certain policy issues that still have not been decided upon; and some that have been decided upon may change in the near future as we learn. Where possible, I have tried to flag which policies are as yet undecided.</p>\n<p><strong>What&rsquo;s your expenditure?</strong></p>\n<p>Below are rough estimates for expenditure from start Q2 2012 (when we first took staff) until end Q4 2013 (our short-term fundraising horizon). All numbers are in thousands.</p>\n<p>80k (basic): &pound;118.8 = ~$190</p>\n<p>80k (inc. some expansion): &pound;139.1 = ~$220</p>\n<p>GWWC (basic): &pound;87.4 = ~$140</p>\n<p>GWWC (inc. some expansion): &pound;107.7 = ~$170</p>\n<p>LYCS (basic): &pound;47.88 = ~$76</p>\n<p>EAA (basic): &pound;32.76 = ~$52</p>\n<p>The large majority of our expenditure is on staff. 80k (basic) comprises one full-time staff member from Q2 2012, two staff members working 0.4 time from Q2 2012, and one full-time staff member from Q1 2013. GWWC (basic) comprises one 5/8 staff member from Q2 2012, and two staff members working 0.6 time from Q2 2012. LYCS (basic) comprises one full-time staff member from Q2 2013, and some money earmarked for on-line marketing by one funder. EAA (basic) comprises one full-time staff member from Q1 2013. Across CEA, we typically employ one intern-year for every employee-year.</p>\n<p>For both GWWC and 80k, the difference between the &lsquo;basic&rsquo; scenario and the &lsquo;expansion&rsquo; scenario is that we would hire one additional person from Q3 2013, and employ one additional intern-year for 2013. The &lsquo;expansion&rsquo; scenario indicates a cautious limit on our use for more funding; though it certainly seems to us that we could spend money well above that amount, we would need to discuss whether it could be detrimental in the long-run for the organisations to grow that fast. We could very comfortably spend within the &lsquo;expansion&rsquo; scenario, and would feel hindered if we were not able to spend up to that amount.</p>\n<p>Each employee is paid a starting salary of &pound;18 per annum, significantly below market rates for graduates even within the not-for-profit world, and without accounting for the fact that our employees are significantly more qualified than the average graduate. Interns are unpaid, but are typically given expenses. A significant proportion of our labour is still voluntary.</p>\n<p><strong>What&rsquo;s your income? What&rsquo;s your shortfall?</strong></p>\n<p>All numbers are now in $000s. I'll measure 'shortfall' relative to the 'basic' budget. The fundraising numbers below include both income that we have already received and income that we expect to receive, discounted according to a conservative estimate of its likelihood (50%). I&rsquo;ll assume that CEA unrestricted money is divided as follows: 0.4 to 80k, 0.4 to GWWC, 0.12 to LYCS and 0.08 to EAA. (This division does not represent a policy about how we divide unrestricted funds. Currently that policy is not yet determined, so I&rsquo;ve chosen these numbers as illustrative. More on use of unrestricted money in &ldquo;earmarking and fungibility&rdquo; below.)</p>\n<p>80k raised: 85.1. Shortfall = 190 - 85.1 = ~$105</p>\n<p>GWWC raised: 117.9. Shortfall = 140 &ndash; 117.9 = ~$22</p>\n<p>LYCS raised: 63.4. Shortfall = 76 &ndash; 63.4 = ~$12.5</p>\n<p>EAA raised: 27.2. Shortfall = 52 &ndash; 27.2 = ~$25</p>\n<p>For both GWWC and 80k, to get the shortfall for the 'expansion' budget &ndash; which represents spending which we could easily accommodate without sacrificing quality of work &mdash;&nbsp; add 30 to the 'shortfall' number.</p>\n<p><strong>How would you spend additional money?</strong></p>\n<p>As the numbers above suggest, in most cases additional donations would be spent on basic costs &mdash; principally, paying staff &mdash; over 2013. If GWWC or 80k exceeded their &lsquo;basic&rsquo; budget, then additional money would be put towards on their &lsquo;additional&rsquo; budget: principally, hiring one new staff member each.&nbsp; The desired marginal hire for GWWC is a Communications Director.&nbsp; The desired marginal hire for 80,000 Hours is a Careers Researcher and Adviser.</p>\n<p><strong>Where&rsquo;s that income from?</strong></p>\n<p>Our donations come from a variety of sources. Private donors, of varying degrees of wealth, make up the large majority of our income. GWWC has received a grant from one foundation. The majority of donations come from within the effective giving community, though a sizable proportion comes from outside that community and we&rsquo;re actively pursuing further leads there, including high net worths. If we ever had a significant donation commitment that used up our room for more funding, we&rsquo;d let other donors know immediately.</p>\n<p><strong>Room for More Funding, and a Co-ordination Problem</strong></p>\n<p>Suppose that, within the effective giving community, there is $N that people would want to donate to CEA, conditional on CEA having room for more funding. But CEA only had room for $M, where M&lt;N. Every giver thinks, &ldquo;well, CEA is going to reach its room or more funding anyway. So there&rsquo;s no reason why I should given.&rdquo; So no-one gives to CEA. That would be a bad outcome on our part. Alternatively, perhaps every giver thinks, &ldquo;well, I&rsquo;ll just give to CEA anyway&rdquo;. So CEA receives $N, whereas it can only spend $M well, and there is an $N-$M excess. That would be a bad outcome on the part of the giver. So what&rsquo;s the solution?</p>\n<p>One solution to that problem (though I haven't thought about it that much) is as follows: we decide upon a funding limit for each organisation. We say that if we receive donations above that limit (before a designated time), we will donate the excess to the most cost-effective charities. Different givers think that different causes are the most important, so we&rsquo;ll donate to the different cause areas depending on what proportion of CEA donations would have been given to those causes if they hadn&rsquo;t been donated to CEA. So if we received 70% of donations that would have been donated to global poverty, we&rsquo;ll give 70% of the excess to AMF; similarly for animal welfare and x-risk.</p>\n<p>The advantages of this are as follows. It safeguards against CEA having less money than it needs because of the co-ordination problem. It ensures that givers can donate to CEA while knowing that the money won&rsquo;t be spent on CEA above its room for more funding. It&rsquo;s also what GiveWell does, and insofar as GW appear to us to be a very well-run organisation, it&rsquo;s worth imitating them.</p>\n<p>What are the alternative solutions? Well, we could bank the money and use it the following year. So the excess money donated to CEA is used one year later than the giver might have expected, and we spend less time on fundraising for the following year. Or we could go &lsquo;first come first served&rsquo;: we keep accepting donations until we hit our RFMF. I think that the latter suggestion is a bad one. The former is potentially good I think, and simple, and I&rsquo;m open to comments on which solution potential donors think is preferable.</p>\n<p><strong>Different Cause Areas</strong></p>\n<p>GWWC and LYCS are focused on global poverty, and have no plans to change that. EAA is focused on animal welfare. 80,000 Hours is open to any plausibly high-impact activity. There is currently no organisation within CEA dedicated purely to x-risk mitigation, but, given demand, it&rsquo;s not unlikely that one will be created in the mid-term future.</p>\n<p><strong>&ldquo;Earmarking&rdquo; and Fungibility</strong></p>\n<p>Some comments mentioned &ldquo;earmarking&rdquo;. I think that&rsquo;s a misleading term in this context. &ldquo;Earmarking&rdquo; normally refers to donations that are tied to a specific activity. Whereas, when one donates to GWWC, the donation is not tied to a specific activity. CEA shouldn&rsquo;t be thought of as an organization over and above the four organisations.</p>\n<p>We actively encourage donations that are restricted to one organization only, if you think that one organization is more cost-effective than the others. In order to avoid the fungibility problem, I considered asking only for restricted donations. It seems to me on balance that the costs of this policy outweigh the benefits, but I&rsquo;m not sure.</p>\n<p>My current preferred solution is as follows. Every 6 months, after the reviews of each organization (see next section), the trustees decide how to allocate unrestricted funding. The default they use is that unrestricted funding is allocated in proportion with restricted funding. If this default holds &mdash; either exactly or approximately &mdash; then fungibility of donations is not an issue. In fact, if the default holds, then fungibility is negative: donating $1 to GWWC would move slightly more than $1 to GWWC, because it would also increase the proportion of CEA unrestricted money that it receives. The trustees deviate from this default if there are compelling reasons for doing so (e.g. a major donor for one organization unexpectedly drops out, rendering basic expenditure uncertain). In the long run (and ex ante), we wouldn&rsquo;t expect these deviations to favour one organization over another, so, in the long run (and ex ante), fungibiltiy is again not an issue. Moreover, from this arrangement we would expect each organization to benefit in terms of financial stability and from the success of their sister organisations.</p>\n<p><strong>Self-Evaluation and Impact Assessment</strong></p>\n<p>I&rsquo;ll describe 80k&rsquo;s process. GWWC&rsquo;s is very similar.</p>\n<p>Every 6 months, 80k will have to write a report of its progress over the last 6 months, including achievements and failures, how its progress compares to the goals stated 6 months ago, and write concrete, measurable goals for the next 6 months. This report will then be reviewed by two boards. The trustees of CEA: myself, Nick Beckstead, and Toby Ord. And an \"Advisory Committee\", consisting of 80k supporters (and often donors) who aren't in any way involved with the running of 80k. The Executive Director of 80k (and one or two others) will meet with these boards, and they'll discuss the report. Each board will write a summary of conclusions. All three documents (initial report, and two commentaries from the boards) will be posted on the blog.</p>\n<p>Every year (probably in spring or early summer - a quiet time for us), we'll complete a more in-depth impact-evaluation, at least in terms of money moved, person-hours moved, money pledged and person-hours pledged. All-year round, we measure progress with respect to pre-chosen goals. The ED of 80k sends progress reports to the 80k team every week. Currently, because marketing and recruitment are our key priorities, our principal metrics are number of new members per week, % of members who say that they&rsquo;ve changed their career plans because of 80k, income pledged per member, unique visitors to the website, and number of advising sessions given.</p>\n<p><strong>Miscellaneous CEA Questions</strong></p>\n<p><em>Which is more useful, regular donations or lump sums?</em></p>\n<p>Either is good. Most charities prefer regular donations because people are likely to give more that way (they forget about the direct debit). But I'd rather you were giving on the basis of perceived cost-effectiveness, rather than status quo bias! Financial forecasting is really important for us, though, so if it's lump sums, we really appreciate knowing the chance they'll be repeated in future years. And we have a steep discount rate (perhaps 20%?) so we greatly prefer money sooner rather than later.</p>\n<p><em>If you had funds to hire an extra person, do you know how that person would be? How important is it to find talented people to work for you?&nbsp; Are you trying to find someone from the top 5%? The top 1%?</em></p>\n<p>It's difficult to give a meaningful reply to that &mdash; 1% in terms of general ability, or fit for us? I'll answer for 'general ability' (whatever that means).&nbsp; We're generally selecting only from top universities, which filters out a large majority of the population. As an approximation (but merely a very rough approximation): There are roughly 772,000 18 yr olds in the UK, of which 7000 go to Oxford or Cambridge. We mainly select from those universities (or equivalent standard elsewhere), so that already filters out 99% of the population.</p>\n<p>Within such universities, we normally recruit very high-performing graduates &mdash; perhaps in the top 10% or 5%. Which would suggest that we're recruiting from the top 0.1% from the population. But, like I say, I'm not sure that that number is that meaningful. I'm not certain why, but we do seem to be able to recruit exceptionally talented people. (Like Niel, who's starting with us from January).<br /><em></em></p>\n<p><em>How much personal connection and communication is there between CEA and these orgs?<br />- THINK<br />- Global Catastrophic Risk Institute<br />- Center for Applied Rationality<br />- Future of Humanity Institute<br />- GiveWell</em><br /><br />Lots. Mark Lee, founder of THINK, came through GWWC. I gave a talk for the Brown THINK chapter the other day, and helped their co-President with plans for the year. We've met a couple of times with Seth Baum. We know Julia and Anna well, and support CFAR. Toby Ord (a trustee of CEA) is a research associate at FHI, and Will participates in FHI events. We're in regular contact with GiveWell. A core CEA volunteer is considering working for them.</p>\n<p>I could say much more, but it would get long-winded. We support all the above organisations, and aim to co-ordinate with them all, so that we don't get in each others' way, and can help each other out.</p>\n<p><em>Where do you see the delineation between what CEA does and what other effective altruist orgs do?</em></p>\n<p>We worry a lot about needlessly doubling up on or competing with work done by other effective altruist organisations. Taking our four organisations in turn:</p>\n<p>GWWC: Along with LYCS, the only group in the world promoting major individual cost-effective giving. Does charity effectiveness research, but only where we think we can usefully add to what Givewell does.</p>\n<p>80k: The only effective altruist organisation doing careers advice. The most broad focused effective altruist organisation except for THINK, but we&rsquo;re distinct from THINK in that we provide careers advice through web content and one-on-one sessions rather than setting up meet-ups.</p>\n<p>EAA: The only animal focused effective altruist organisation (EAA is effectively doing what 80k would have done in this area, except we thought it was useful to give it separate branding.</p>\n<p>TLYCS: Similar in aims to GWWC, but lower-bar entry for most members. Planning different outreach routes</p>\n<p><em>What are you planning in the way of financial transparency?</em></p>\n<p>We'll publish an annual financial report, with a breakdown of costs. We&rsquo;d like to be able to regularly explain room for more funding (etc.) as we do above, but doing so uses considerable time of high-level people within the organization, so we can&rsquo;t promise that.&nbsp; In general, we take GiveWell as a model organisation, and will often emulate their practices.</p>\n<p><strong>Miscellaneous GWWC Question</strong></p>\n<p><em>You said in your LW post that you have \"much more information available\" on GWWC's impact.</em></p>\n<p>Yes. If you email me (will [dot] crouch [at] 80000hours.org), you can see the calculations by which we estimated GWWC&rsquo;s impact.</p>\n<p><strong>Miscellaneous 80k Questions</strong></p>\n<p><em>You do a bunch of different but related things - website content, speaker events, career counseling,<br />- Do you imagine yourself specialising in just one of these in the future?<br />- Are you at the stage of experimenting to find out which activity is the most effective?<br />- Is there synergy between them? (e.g. if career advice sessions and website content are both a lot more effective if you're also doing the other one)</em><br /><br />We&rsquo;re creating a new type of organisation, and there&rsquo;s a great deal we don&rsquo;t know. We see our priority as testing these different approaches and improving them. Having a basket of methods lets us gain more information, and prevents us from stalling if one method turns out not to work.</p>\n<p>For each method, we track a bunch of metrics which ultimately relate to our bottom line: resources shifted to the most effective causes that wouldn&rsquo;t have been shifted otherwise. We propose tests for ways to improve these metrics. If our hypotheses about what we expect to work are disconfirmed, we change our approach. Otherwise, we move to scale up the method.</p>\n<p>Whether we end up specialising, therefore, depends on whether one of the methods ends up being significantly more effective than the rest. And whatever happens, since we&rsquo;re constantly seeking to improve, I imagine we&rsquo;ll always be experimenting with new processes.</p>\n<p>At the moment, we broadly see the web content and one-on-one advice as our most important services, and we&rsquo;re expecting to scale them the most (though there&rsquo;s a lot of flexibility within where we take these). We pursue some lectures, especially in Oxford, since they have high marginal returns, but we don&rsquo;t currently expect to scale them. We&rsquo;re exploring some other methods, but have not tested them yet.<br /><br /><em>I have very little idea about what the 80K community is like or how exactly you invest in it<br />- in what ways does your team interact with your community, other than one-on-one career advice and hosting speaker events?<br />- do you invest in members' skills such as critical thinking and the ability to evaluate organisations?<br />- what other skills and qualities do you want to develop in your members, and how do you plan to go about it?<br />- to what extent do you think talents and abilities are inherent (or at least beyond your control), and to what extent are they trainable?</em><br /><br />Probably one of the most useful things we do is forge links between people in the community who can help each other out. For instance, we&rsquo;ve introducing people who have successfully navigated applications to finance jobs to others who want to do the same. We brought together a bunch of people interested in the animal cause to set up EAA. We&rsquo;ve introduced people who are in the same research field. At the minute, this mainly happens via personal introductions, but we&rsquo;re developing tools to make this easier online.<br />Besides this, the team interacts with the community via the members&rsquo; googlegroup and our online discussion forum, 80000hours.org/discussion.</p>\n<p>Our current focus in our providing our members with really useful information about which career they should enter in order to have the most impact. Our main way of improving their skills is by introducing them to mentors. We also sometimes coach people through tough career stages in our one-on-one advice (e.g. we recently helped get one of our interns a Marshall scholarship). Longer term, we might switch to have a greater focus on self-improvement, but that niche has a lot more competition (e.g. if you care about becoming more rational, go to Less Wrong).</p>\n<p><em>Is 80K planning activity in any new physical locations?<br />- If so, where?<br />- If there's already a THINK community in that location, what do you imagine the relationship between THINK and 80K looking like?<br /></em><br />We encourage any 80k members who&rsquo;d like to start an 80k meet-up to do so, and we&rsquo;ll happily give them advice and support; but we don&rsquo;t plan to invest significant staff time in setting up new physical locations. This is because our current focus is on our web content and one-on-one advice. The exception is that we&rsquo;d like to intensify existing involvement in Oxford, Yale and Princeton, which we see as test grounds and already have some infrastructure.</p>\n<p>We could easily change our minds, however, and we&rsquo;re pretty interested in the idea of doing a lecture tour (which we could support with the web content and one-on-one advice).&nbsp; We wouldn&rsquo;t set up an 80k presence in a new location, as opposed to sending people to THINK, unless we thought that there was good reason that having two organisations in the area would be more effective. <br /><br /><em>There's apparently a lot of interest in x-risk among 80K members. Do you know why this is?</em><br /><br />I think that the 34% is higher than the percentage of x-risk concerned members in the long-run (which I'd guess will end up at about 10%), due to initial selection effects (we got quite a few members from the x-risk network, and others who would have been GWWC members were if not for the fact that they were principally concerned about x-risk). However, there does seem to be a strong positive correlation between how dedicated members are, and whether they are concerned by x-risk. And we haven&rsquo;t yet really discussed x-risk as a cause area. So it&rsquo;s difficult to say what proportion of resources we generate will be x-risk focused.</p>\n<p>A number of people are convinced by x-risk but just don't think that there currently exists a good enough giving opportunity. So the proportion would increase considerably if a really clear x-risk giving opportunity arose (e.g. if GW ever recommended an x-risk org).<em></em></p>\n<p><em>Do you know of any other organisations that do anything similar to what you do? (other than ones I've already mentioned). In particular any groups that give career advice to philanthropists.</em></p>\n<p>As far as we can tell, there&rsquo;s no-one else providing careers advice focused on how you can make a difference. All that exists is informal advice about impact given by friends and within other effective altruist communities (e.g. LW, GWWC).<br />Making a difference aside, it seems like the average quality of careers advice in general is pretty low, and rarely evidence-based or aware of decision making biases.</p>\n<p>From several perspectives, we think we&rsquo;re in a very interesting market niche. As far as philanthropy goes, there are some groups focused on fostering it e.g. http://youngphilanthropy.org.uk/, but they tend to have little focus on effectiveness.</p>\n<p><strong>Miscellaneous EAA Questions</strong></p>\n<p><em>What are the plans for EAA? When will it be spun off? Is there much interest in it from new members of 80k? Is anybody other than Eitan Fischer (who's in school) working on it at the moment?</em></p>\n<p>EAA is taking on a full-time Executive Director some time between Jan and Sept. The initial priorities will be scaling up the charity effectiveness research, taking it to major philanthropists and fundraising. If EAA achieves enough scale, we&rsquo;ll promote it to a full member of CEA.</p>\n<p>Around 10-20% of our new members are interested in the animal cause (and we haven&rsquo;t promoted it much directly, beyond a couple of blog posts). They tend to make use of EAA. Eitan has a small team of volunteers and advisers helping him out part-time. There&rsquo;s probably about 4-8 people involved in some capacity at most times. This should increase significantly once we recruit an Executive Director.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"qAvbtzdG2A2RBn7in": 1, "EeSkeTcT4wtW2fWsL": 1, "se3XDuQ4xbeWvu4eF": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rC2EBbhmCzJCSyysN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 34, "extendedScore": null, "score": 1.0386246211459069e-06, "legacy": true, "legacyId": "20195", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Giles, and some others, have asked questions about donating to one or more of CEA\u2019s sub-organisations. In what follows, I address these questions. I felt it would be clearest for me to mainly cluster questions under general headings, rather than address the specific wording of every question. (Note: thanks to help from Ben Todd on this!)<br><br><strong>A couple of clarifications</strong></p>\n<p>Centre for Effective Altruism is a legal entity that comprises 4 organisations: Giving What We Can, 80,000 Hours, Effective Animal Activism, and The Life You Can Save. EAA is formally still a sub-project of 80,000 Hours, but should be thought of as separate for accounting purposes and may well become a separate organisation. In the previous blog post I talked about GWWC and 80k only, but because there\u2019s been interest, here I\u2019ll discuss the other two as well, albeit more briefly.</p>\n<p>Some numbers follow. These are true as of Nov 20th 2012 (or, rather, are best estimates as of that date), but are very likely to change in the near future. They should therefore be taken as illustrations merely.</p>\n<p>Finally, because all the organisations that comprise CEA are young, there are certain policy issues that still have not been decided upon; and some that have been decided upon may change in the near future as we learn. Where possible, I have tried to flag which policies are as yet undecided.</p>\n<p><strong id=\"What_s_your_expenditure_\">What\u2019s your expenditure?</strong></p>\n<p>Below are rough estimates for expenditure from start Q2 2012 (when we first took staff) until end Q4 2013 (our short-term fundraising horizon). All numbers are in thousands.</p>\n<p>80k (basic): \u00a3118.8 = ~$190</p>\n<p>80k (inc. some expansion): \u00a3139.1 = ~$220</p>\n<p>GWWC (basic): \u00a387.4 = ~$140</p>\n<p>GWWC (inc. some expansion): \u00a3107.7 = ~$170</p>\n<p>LYCS (basic): \u00a347.88 = ~$76</p>\n<p>EAA (basic): \u00a332.76 = ~$52</p>\n<p>The large majority of our expenditure is on staff. 80k (basic) comprises one full-time staff member from Q2 2012, two staff members working 0.4 time from Q2 2012, and one full-time staff member from Q1 2013. GWWC (basic) comprises one 5/8 staff member from Q2 2012, and two staff members working 0.6 time from Q2 2012. LYCS (basic) comprises one full-time staff member from Q2 2013, and some money earmarked for on-line marketing by one funder. EAA (basic) comprises one full-time staff member from Q1 2013. Across CEA, we typically employ one intern-year for every employee-year.</p>\n<p>For both GWWC and 80k, the difference between the \u2018basic\u2019 scenario and the \u2018expansion\u2019 scenario is that we would hire one additional person from Q3 2013, and employ one additional intern-year for 2013. The \u2018expansion\u2019 scenario indicates a cautious limit on our use for more funding; though it certainly seems to us that we could spend money well above that amount, we would need to discuss whether it could be detrimental in the long-run for the organisations to grow that fast. We could very comfortably spend within the \u2018expansion\u2019 scenario, and would feel hindered if we were not able to spend up to that amount.</p>\n<p>Each employee is paid a starting salary of \u00a318 per annum, significantly below market rates for graduates even within the not-for-profit world, and without accounting for the fact that our employees are significantly more qualified than the average graduate. Interns are unpaid, but are typically given expenses. A significant proportion of our labour is still voluntary.</p>\n<p><strong id=\"What_s_your_income__What_s_your_shortfall_\">What\u2019s your income? What\u2019s your shortfall?</strong></p>\n<p>All numbers are now in $000s. I'll measure 'shortfall' relative to the 'basic' budget. The fundraising numbers below include both income that we have already received and income that we expect to receive, discounted according to a conservative estimate of its likelihood (50%). I\u2019ll assume that CEA unrestricted money is divided as follows: 0.4 to 80k, 0.4 to GWWC, 0.12 to LYCS and 0.08 to EAA. (This division does not represent a policy about how we divide unrestricted funds. Currently that policy is not yet determined, so I\u2019ve chosen these numbers as illustrative. More on use of unrestricted money in \u201cearmarking and fungibility\u201d below.)</p>\n<p>80k raised: 85.1. Shortfall = 190 - 85.1 = ~$105</p>\n<p>GWWC raised: 117.9. Shortfall = 140 \u2013 117.9 = ~$22</p>\n<p>LYCS raised: 63.4. Shortfall = 76 \u2013 63.4 = ~$12.5</p>\n<p>EAA raised: 27.2. Shortfall = 52 \u2013 27.2 = ~$25</p>\n<p>For both GWWC and 80k, to get the shortfall for the 'expansion' budget \u2013 which represents spending which we could easily accommodate without sacrificing quality of work \u2014&nbsp; add 30 to the 'shortfall' number.</p>\n<p><strong id=\"How_would_you_spend_additional_money_\">How would you spend additional money?</strong></p>\n<p>As the numbers above suggest, in most cases additional donations would be spent on basic costs \u2014 principally, paying staff \u2014 over 2013. If GWWC or 80k exceeded their \u2018basic\u2019 budget, then additional money would be put towards on their \u2018additional\u2019 budget: principally, hiring one new staff member each.&nbsp; The desired marginal hire for GWWC is a Communications Director.&nbsp; The desired marginal hire for 80,000 Hours is a Careers Researcher and Adviser.</p>\n<p><strong id=\"Where_s_that_income_from_\">Where\u2019s that income from?</strong></p>\n<p>Our donations come from a variety of sources. Private donors, of varying degrees of wealth, make up the large majority of our income. GWWC has received a grant from one foundation. The majority of donations come from within the effective giving community, though a sizable proportion comes from outside that community and we\u2019re actively pursuing further leads there, including high net worths. If we ever had a significant donation commitment that used up our room for more funding, we\u2019d let other donors know immediately.</p>\n<p><strong id=\"Room_for_More_Funding__and_a_Co_ordination_Problem\">Room for More Funding, and a Co-ordination Problem</strong></p>\n<p>Suppose that, within the effective giving community, there is $N that people would want to donate to CEA, conditional on CEA having room for more funding. But CEA only had room for $M, where M&lt;N. Every giver thinks, \u201cwell, CEA is going to reach its room or more funding anyway. So there\u2019s no reason why I should given.\u201d So no-one gives to CEA. That would be a bad outcome on our part. Alternatively, perhaps every giver thinks, \u201cwell, I\u2019ll just give to CEA anyway\u201d. So CEA receives $N, whereas it can only spend $M well, and there is an $N-$M excess. That would be a bad outcome on the part of the giver. So what\u2019s the solution?</p>\n<p>One solution to that problem (though I haven't thought about it that much) is as follows: we decide upon a funding limit for each organisation. We say that if we receive donations above that limit (before a designated time), we will donate the excess to the most cost-effective charities. Different givers think that different causes are the most important, so we\u2019ll donate to the different cause areas depending on what proportion of CEA donations would have been given to those causes if they hadn\u2019t been donated to CEA. So if we received 70% of donations that would have been donated to global poverty, we\u2019ll give 70% of the excess to AMF; similarly for animal welfare and x-risk.</p>\n<p>The advantages of this are as follows. It safeguards against CEA having less money than it needs because of the co-ordination problem. It ensures that givers can donate to CEA while knowing that the money won\u2019t be spent on CEA above its room for more funding. It\u2019s also what GiveWell does, and insofar as GW appear to us to be a very well-run organisation, it\u2019s worth imitating them.</p>\n<p>What are the alternative solutions? Well, we could bank the money and use it the following year. So the excess money donated to CEA is used one year later than the giver might have expected, and we spend less time on fundraising for the following year. Or we could go \u2018first come first served\u2019: we keep accepting donations until we hit our RFMF. I think that the latter suggestion is a bad one. The former is potentially good I think, and simple, and I\u2019m open to comments on which solution potential donors think is preferable.</p>\n<p><strong id=\"Different_Cause_Areas\">Different Cause Areas</strong></p>\n<p>GWWC and LYCS are focused on global poverty, and have no plans to change that. EAA is focused on animal welfare. 80,000 Hours is open to any plausibly high-impact activity. There is currently no organisation within CEA dedicated purely to x-risk mitigation, but, given demand, it\u2019s not unlikely that one will be created in the mid-term future.</p>\n<p><strong id=\"_Earmarking__and_Fungibility\">\u201cEarmarking\u201d and Fungibility</strong></p>\n<p>Some comments mentioned \u201cearmarking\u201d. I think that\u2019s a misleading term in this context. \u201cEarmarking\u201d normally refers to donations that are tied to a specific activity. Whereas, when one donates to GWWC, the donation is not tied to a specific activity. CEA shouldn\u2019t be thought of as an organization over and above the four organisations.</p>\n<p>We actively encourage donations that are restricted to one organization only, if you think that one organization is more cost-effective than the others. In order to avoid the fungibility problem, I considered asking only for restricted donations. It seems to me on balance that the costs of this policy outweigh the benefits, but I\u2019m not sure.</p>\n<p>My current preferred solution is as follows. Every 6 months, after the reviews of each organization (see next section), the trustees decide how to allocate unrestricted funding. The default they use is that unrestricted funding is allocated in proportion with restricted funding. If this default holds \u2014 either exactly or approximately \u2014 then fungibility of donations is not an issue. In fact, if the default holds, then fungibility is negative: donating $1 to GWWC would move slightly more than $1 to GWWC, because it would also increase the proportion of CEA unrestricted money that it receives. The trustees deviate from this default if there are compelling reasons for doing so (e.g. a major donor for one organization unexpectedly drops out, rendering basic expenditure uncertain). In the long run (and ex ante), we wouldn\u2019t expect these deviations to favour one organization over another, so, in the long run (and ex ante), fungibiltiy is again not an issue. Moreover, from this arrangement we would expect each organization to benefit in terms of financial stability and from the success of their sister organisations.</p>\n<p><strong id=\"Self_Evaluation_and_Impact_Assessment\">Self-Evaluation and Impact Assessment</strong></p>\n<p>I\u2019ll describe 80k\u2019s process. GWWC\u2019s is very similar.</p>\n<p>Every 6 months, 80k will have to write a report of its progress over the last 6 months, including achievements and failures, how its progress compares to the goals stated 6 months ago, and write concrete, measurable goals for the next 6 months. This report will then be reviewed by two boards. The trustees of CEA: myself, Nick Beckstead, and Toby Ord. And an \"Advisory Committee\", consisting of 80k supporters (and often donors) who aren't in any way involved with the running of 80k. The Executive Director of 80k (and one or two others) will meet with these boards, and they'll discuss the report. Each board will write a summary of conclusions. All three documents (initial report, and two commentaries from the boards) will be posted on the blog.</p>\n<p>Every year (probably in spring or early summer - a quiet time for us), we'll complete a more in-depth impact-evaluation, at least in terms of money moved, person-hours moved, money pledged and person-hours pledged. All-year round, we measure progress with respect to pre-chosen goals. The ED of 80k sends progress reports to the 80k team every week. Currently, because marketing and recruitment are our key priorities, our principal metrics are number of new members per week, % of members who say that they\u2019ve changed their career plans because of 80k, income pledged per member, unique visitors to the website, and number of advising sessions given.</p>\n<p><strong id=\"Miscellaneous_CEA_Questions\">Miscellaneous CEA Questions</strong></p>\n<p><em>Which is more useful, regular donations or lump sums?</em></p>\n<p>Either is good. Most charities prefer regular donations because people are likely to give more that way (they forget about the direct debit). But I'd rather you were giving on the basis of perceived cost-effectiveness, rather than status quo bias! Financial forecasting is really important for us, though, so if it's lump sums, we really appreciate knowing the chance they'll be repeated in future years. And we have a steep discount rate (perhaps 20%?) so we greatly prefer money sooner rather than later.</p>\n<p><em>If you had funds to hire an extra person, do you know how that person would be? How important is it to find talented people to work for you?&nbsp; Are you trying to find someone from the top 5%? The top 1%?</em></p>\n<p>It's difficult to give a meaningful reply to that \u2014 1% in terms of general ability, or fit for us? I'll answer for 'general ability' (whatever that means).&nbsp; We're generally selecting only from top universities, which filters out a large majority of the population. As an approximation (but merely a very rough approximation): There are roughly 772,000 18 yr olds in the UK, of which 7000 go to Oxford or Cambridge. We mainly select from those universities (or equivalent standard elsewhere), so that already filters out 99% of the population.</p>\n<p>Within such universities, we normally recruit very high-performing graduates \u2014 perhaps in the top 10% or 5%. Which would suggest that we're recruiting from the top 0.1% from the population. But, like I say, I'm not sure that that number is that meaningful. I'm not certain why, but we do seem to be able to recruit exceptionally talented people. (Like Niel, who's starting with us from January).<br><em></em></p>\n<p><em>How much personal connection and communication is there between CEA and these orgs?<br>- THINK<br>- Global Catastrophic Risk Institute<br>- Center for Applied Rationality<br>- Future of Humanity Institute<br>- GiveWell</em><br><br>Lots. Mark Lee, founder of THINK, came through GWWC. I gave a talk for the Brown THINK chapter the other day, and helped their co-President with plans for the year. We've met a couple of times with Seth Baum. We know Julia and Anna well, and support CFAR. Toby Ord (a trustee of CEA) is a research associate at FHI, and Will participates in FHI events. We're in regular contact with GiveWell. A core CEA volunteer is considering working for them.</p>\n<p>I could say much more, but it would get long-winded. We support all the above organisations, and aim to co-ordinate with them all, so that we don't get in each others' way, and can help each other out.</p>\n<p><em>Where do you see the delineation between what CEA does and what other effective altruist orgs do?</em></p>\n<p>We worry a lot about needlessly doubling up on or competing with work done by other effective altruist organisations. Taking our four organisations in turn:</p>\n<p>GWWC: Along with LYCS, the only group in the world promoting major individual cost-effective giving. Does charity effectiveness research, but only where we think we can usefully add to what Givewell does.</p>\n<p>80k: The only effective altruist organisation doing careers advice. The most broad focused effective altruist organisation except for THINK, but we\u2019re distinct from THINK in that we provide careers advice through web content and one-on-one sessions rather than setting up meet-ups.</p>\n<p>EAA: The only animal focused effective altruist organisation (EAA is effectively doing what 80k would have done in this area, except we thought it was useful to give it separate branding.</p>\n<p>TLYCS: Similar in aims to GWWC, but lower-bar entry for most members. Planning different outreach routes</p>\n<p><em>What are you planning in the way of financial transparency?</em></p>\n<p>We'll publish an annual financial report, with a breakdown of costs. We\u2019d like to be able to regularly explain room for more funding (etc.) as we do above, but doing so uses considerable time of high-level people within the organization, so we can\u2019t promise that.&nbsp; In general, we take GiveWell as a model organisation, and will often emulate their practices.</p>\n<p><strong id=\"Miscellaneous_GWWC_Question\">Miscellaneous GWWC Question</strong></p>\n<p><em>You said in your LW post that you have \"much more information available\" on GWWC's impact.</em></p>\n<p>Yes. If you email me (will [dot] crouch [at] 80000hours.org), you can see the calculations by which we estimated GWWC\u2019s impact.</p>\n<p><strong id=\"Miscellaneous_80k_Questions\">Miscellaneous 80k Questions</strong></p>\n<p><em>You do a bunch of different but related things - website content, speaker events, career counseling,<br>- Do you imagine yourself specialising in just one of these in the future?<br>- Are you at the stage of experimenting to find out which activity is the most effective?<br>- Is there synergy between them? (e.g. if career advice sessions and website content are both a lot more effective if you're also doing the other one)</em><br><br>We\u2019re creating a new type of organisation, and there\u2019s a great deal we don\u2019t know. We see our priority as testing these different approaches and improving them. Having a basket of methods lets us gain more information, and prevents us from stalling if one method turns out not to work.</p>\n<p>For each method, we track a bunch of metrics which ultimately relate to our bottom line: resources shifted to the most effective causes that wouldn\u2019t have been shifted otherwise. We propose tests for ways to improve these metrics. If our hypotheses about what we expect to work are disconfirmed, we change our approach. Otherwise, we move to scale up the method.</p>\n<p>Whether we end up specialising, therefore, depends on whether one of the methods ends up being significantly more effective than the rest. And whatever happens, since we\u2019re constantly seeking to improve, I imagine we\u2019ll always be experimenting with new processes.</p>\n<p>At the moment, we broadly see the web content and one-on-one advice as our most important services, and we\u2019re expecting to scale them the most (though there\u2019s a lot of flexibility within where we take these). We pursue some lectures, especially in Oxford, since they have high marginal returns, but we don\u2019t currently expect to scale them. We\u2019re exploring some other methods, but have not tested them yet.<br><br><em>I have very little idea about what the 80K community is like or how exactly you invest in it<br>- in what ways does your team interact with your community, other than one-on-one career advice and hosting speaker events?<br>- do you invest in members' skills such as critical thinking and the ability to evaluate organisations?<br>- what other skills and qualities do you want to develop in your members, and how do you plan to go about it?<br>- to what extent do you think talents and abilities are inherent (or at least beyond your control), and to what extent are they trainable?</em><br><br>Probably one of the most useful things we do is forge links between people in the community who can help each other out. For instance, we\u2019ve introducing people who have successfully navigated applications to finance jobs to others who want to do the same. We brought together a bunch of people interested in the animal cause to set up EAA. We\u2019ve introduced people who are in the same research field. At the minute, this mainly happens via personal introductions, but we\u2019re developing tools to make this easier online.<br>Besides this, the team interacts with the community via the members\u2019 googlegroup and our online discussion forum, 80000hours.org/discussion.</p>\n<p>Our current focus in our providing our members with really useful information about which career they should enter in order to have the most impact. Our main way of improving their skills is by introducing them to mentors. We also sometimes coach people through tough career stages in our one-on-one advice (e.g. we recently helped get one of our interns a Marshall scholarship). Longer term, we might switch to have a greater focus on self-improvement, but that niche has a lot more competition (e.g. if you care about becoming more rational, go to Less Wrong).</p>\n<p><em>Is 80K planning activity in any new physical locations?<br>- If so, where?<br>- If there's already a THINK community in that location, what do you imagine the relationship between THINK and 80K looking like?<br></em><br>We encourage any 80k members who\u2019d like to start an 80k meet-up to do so, and we\u2019ll happily give them advice and support; but we don\u2019t plan to invest significant staff time in setting up new physical locations. This is because our current focus is on our web content and one-on-one advice. The exception is that we\u2019d like to intensify existing involvement in Oxford, Yale and Princeton, which we see as test grounds and already have some infrastructure.</p>\n<p>We could easily change our minds, however, and we\u2019re pretty interested in the idea of doing a lecture tour (which we could support with the web content and one-on-one advice).&nbsp; We wouldn\u2019t set up an 80k presence in a new location, as opposed to sending people to THINK, unless we thought that there was good reason that having two organisations in the area would be more effective. <br><br><em>There's apparently a lot of interest in x-risk among 80K members. Do you know why this is?</em><br><br>I think that the 34% is higher than the percentage of x-risk concerned members in the long-run (which I'd guess will end up at about 10%), due to initial selection effects (we got quite a few members from the x-risk network, and others who would have been GWWC members were if not for the fact that they were principally concerned about x-risk). However, there does seem to be a strong positive correlation between how dedicated members are, and whether they are concerned by x-risk. And we haven\u2019t yet really discussed x-risk as a cause area. So it\u2019s difficult to say what proportion of resources we generate will be x-risk focused.</p>\n<p>A number of people are convinced by x-risk but just don't think that there currently exists a good enough giving opportunity. So the proportion would increase considerably if a really clear x-risk giving opportunity arose (e.g. if GW ever recommended an x-risk org).<em></em></p>\n<p><em>Do you know of any other organisations that do anything similar to what you do? (other than ones I've already mentioned). In particular any groups that give career advice to philanthropists.</em></p>\n<p>As far as we can tell, there\u2019s no-one else providing careers advice focused on how you can make a difference. All that exists is informal advice about impact given by friends and within other effective altruist communities (e.g. LW, GWWC).<br>Making a difference aside, it seems like the average quality of careers advice in general is pretty low, and rarely evidence-based or aware of decision making biases.</p>\n<p>From several perspectives, we think we\u2019re in a very interesting market niche. As far as philanthropy goes, there are some groups focused on fostering it e.g. http://youngphilanthropy.org.uk/, but they tend to have little focus on effectiveness.</p>\n<p><strong id=\"Miscellaneous_EAA_Questions\">Miscellaneous EAA Questions</strong></p>\n<p><em>What are the plans for EAA? When will it be spun off? Is there much interest in it from new members of 80k? Is anybody other than Eitan Fischer (who's in school) working on it at the moment?</em></p>\n<p>EAA is taking on a full-time Executive Director some time between Jan and Sept. The initial priorities will be scaling up the charity effectiveness research, taking it to major philanthropists and fundraising. If EAA achieves enough scale, we\u2019ll promote it to a full member of CEA.</p>\n<p>Around 10-20% of our new members are interested in the animal cause (and we haven\u2019t promoted it much directly, beyond a couple of blog posts). They tend to make use of EAA. Eitan has a small team of volunteers and advisers helping him out part-time. There\u2019s probably about 4-8 people involved in some capacity at most times. This should increase significantly once we recruit an Executive Director.</p>", "sections": [{"title": "What\u2019s your expenditure?", "anchor": "What_s_your_expenditure_", "level": 1}, {"title": "What\u2019s your income? What\u2019s your shortfall?", "anchor": "What_s_your_income__What_s_your_shortfall_", "level": 1}, {"title": "How would you spend additional money?", "anchor": "How_would_you_spend_additional_money_", "level": 1}, {"title": "Where\u2019s that income from?", "anchor": "Where_s_that_income_from_", "level": 1}, {"title": "Room for More Funding, and a Co-ordination Problem", "anchor": "Room_for_More_Funding__and_a_Co_ordination_Problem", "level": 1}, {"title": "Different Cause Areas", "anchor": "Different_Cause_Areas", "level": 1}, {"title": "\u201cEarmarking\u201d and Fungibility", "anchor": "_Earmarking__and_Fungibility", "level": 1}, {"title": "Self-Evaluation and Impact Assessment", "anchor": "Self_Evaluation_and_Impact_Assessment", "level": 1}, {"title": "Miscellaneous CEA Questions", "anchor": "Miscellaneous_CEA_Questions", "level": 1}, {"title": "Miscellaneous GWWC Question", "anchor": "Miscellaneous_GWWC_Question", "level": 1}, {"title": "Miscellaneous 80k Questions", "anchor": "Miscellaneous_80k_Questions", "level": 1}, {"title": "Miscellaneous EAA Questions", "anchor": "Miscellaneous_EAA_Questions", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "20 comments"}], "headingsCount": 14}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-20T23:01:59.536Z", "modifiedAt": null, "url": null, "title": "Meetup : Atlanta - Practical Rationality Meetup Session", "slug": "meetup-atlanta-practical-rationality-meetup-session", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:58.741Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "hankx7787", "createdAt": "2011-07-10T22:12:52.395Z", "isAdmin": false, "displayName": "hankx7787"}, "userId": "B4SKuX6dAQMnNqHzH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aQPdhjBiz86jH7nhZ/meetup-atlanta-practical-rationality-meetup-session", "pageUrlRelative": "/posts/aQPdhjBiz86jH7nhZ/meetup-atlanta-practical-rationality-meetup-session", "linkUrl": "https://www.lesswrong.com/posts/aQPdhjBiz86jH7nhZ/meetup-atlanta-practical-rationality-meetup-session", "postedAtFormatted": "Tuesday, November 20th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Atlanta%20-%20Practical%20Rationality%20Meetup%20Session&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Atlanta%20-%20Practical%20Rationality%20Meetup%20Session%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaQPdhjBiz86jH7nhZ%2Fmeetup-atlanta-practical-rationality-meetup-session%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Atlanta%20-%20Practical%20Rationality%20Meetup%20Session%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaQPdhjBiz86jH7nhZ%2Fmeetup-atlanta-practical-rationality-meetup-session", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaQPdhjBiz86jH7nhZ%2Fmeetup-atlanta-practical-rationality-meetup-session", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 223, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/g6'>Atlanta - Practical Rationality Meetup Session</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 December 2012 05:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Atlanta, ga</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Here are the event details! Please let me know if you have a different preferred location or other suggestions and I'll be happy to update:</p>\n\n<p>Sunday, 12/2 @ 5pm</p>\n\n<p>Send me a message or an email (my username at gmail) for the location.</p>\n\n<p>RSVP is encouraged but not required. Newbies and non-LWers are welcome.</p>\n\n<p>Here's my idea for how to run this session:</p>\n\n<ol>\n<li><p>General introductions/chat, get to know you</p></li>\n<li><p>Put up a blackboard and call out as many biases/fallacies/heuristics/techniques/scientific studies/other LWisms related to practical rationality that we can (and explain them or look them up quickly)</p></li>\n<li><p>Discuss the various rationality games, pick one that sounds fun, and play. Repeat until done.</p></li>\n</ol>\n\n<p>This session is newbie-friendly as we will basically be doing a review of all the background info in step 2 - so no experienced required to attend (!)</p>\n\n<p>More experienced LW folks should review a little before coming to help fill up the blackboard. If anyone has board games and/or lots of dice, I think that would cover the only games that require materials.</p>\n\n<p>Please let me know if you have any questions!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/g6'>Atlanta - Practical Rationality Meetup Session</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aQPdhjBiz86jH7nhZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 3, "extendedScore": null, "score": 1.0386358731279086e-06, "legacy": true, "legacyId": "20196", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Atlanta___Practical_Rationality_Meetup_Session\">Discussion article for the meetup : <a href=\"/meetups/g6\">Atlanta - Practical Rationality Meetup Session</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 December 2012 05:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Atlanta, ga</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Here are the event details! Please let me know if you have a different preferred location or other suggestions and I'll be happy to update:</p>\n\n<p>Sunday, 12/2 @ 5pm</p>\n\n<p>Send me a message or an email (my username at gmail) for the location.</p>\n\n<p>RSVP is encouraged but not required. Newbies and non-LWers are welcome.</p>\n\n<p>Here's my idea for how to run this session:</p>\n\n<ol>\n<li><p>General introductions/chat, get to know you</p></li>\n<li><p>Put up a blackboard and call out as many biases/fallacies/heuristics/techniques/scientific studies/other LWisms related to practical rationality that we can (and explain them or look them up quickly)</p></li>\n<li><p>Discuss the various rationality games, pick one that sounds fun, and play. Repeat until done.</p></li>\n</ol>\n\n<p>This session is newbie-friendly as we will basically be doing a review of all the background info in step 2 - so no experienced required to attend (!)</p>\n\n<p>More experienced LW folks should review a little before coming to help fill up the blackboard. If anyone has board games and/or lots of dice, I think that would cover the only games that require materials.</p>\n\n<p>Please let me know if you have any questions!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Atlanta___Practical_Rationality_Meetup_Session1\">Discussion article for the meetup : <a href=\"/meetups/g6\">Atlanta - Practical Rationality Meetup Session</a></h2>", "sections": [{"title": "Discussion article for the meetup : Atlanta - Practical Rationality Meetup Session", "anchor": "Discussion_article_for_the_meetup___Atlanta___Practical_Rationality_Meetup_Session", "level": 1}, {"title": "Discussion article for the meetup : Atlanta - Practical Rationality Meetup Session", "anchor": "Discussion_article_for_the_meetup___Atlanta___Practical_Rationality_Meetup_Session1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-21T00:44:16.229Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne, practical rationality", "slug": "meetup-melbourne-practical-rationality-8", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "matt", "createdAt": "2009-02-24T03:21:23.753Z", "isAdmin": false, "displayName": "matt"}, "userId": "PXCeXYzvwEeqqitqH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mN7Ap3AAF3M4H2QS2/meetup-melbourne-practical-rationality-8", "pageUrlRelative": "/posts/mN7Ap3AAF3M4H2QS2/meetup-melbourne-practical-rationality-8", "linkUrl": "https://www.lesswrong.com/posts/mN7Ap3AAF3M4H2QS2/meetup-melbourne-practical-rationality-8", "postedAtFormatted": "Wednesday, November 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%2C%20practical%20rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%2C%20practical%20rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmN7Ap3AAF3M4H2QS2%2Fmeetup-melbourne-practical-rationality-8%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%2C%20practical%20rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmN7Ap3AAF3M4H2QS2%2Fmeetup-melbourne-practical-rationality-8", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmN7Ap3AAF3M4H2QS2%2Fmeetup-melbourne-practical-rationality-8", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 71, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/g7'>Melbourne, practical rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">07 December 2012 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">55 Walsh St West Melbourne  3003</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical rationality. This meetup repeats on the 1st Friday of each month and is distinct from our social meetup on the 3rd Friday of each month.</p>\n\n<p>Discussion: <a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n\n<p>All welcome from 6:30pm. Call the phone number on the door and I'll let you in.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/g7'>Melbourne, practical rationality</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mN7Ap3AAF3M4H2QS2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0386933296167818e-06, "legacy": true, "legacyId": "20197", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne__practical_rationality\">Discussion article for the meetup : <a href=\"/meetups/g7\">Melbourne, practical rationality</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">07 December 2012 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">55 Walsh St West Melbourne  3003</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical rationality. This meetup repeats on the 1st Friday of each month and is distinct from our social meetup on the 3rd Friday of each month.</p>\n\n<p>Discussion: <a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a></p>\n\n<p>All welcome from 6:30pm. Call the phone number on the door and I'll let you in.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne__practical_rationality1\">Discussion article for the meetup : <a href=\"/meetups/g7\">Melbourne, practical rationality</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne, practical rationality", "anchor": "Discussion_article_for_the_meetup___Melbourne__practical_rationality", "level": 1}, {"title": "Discussion article for the meetup : Melbourne, practical rationality", "anchor": "Discussion_article_for_the_meetup___Melbourne__practical_rationality1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-21T14:32:20.506Z", "modifiedAt": null, "url": null, "title": "Stop learning, start thinking [LINK]", "slug": "stop-learning-start-thinking-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:00.744Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kiWoCSgSFLNpsGfCc/stop-learning-start-thinking-link", "pageUrlRelative": "/posts/kiWoCSgSFLNpsGfCc/stop-learning-start-thinking-link", "linkUrl": "https://www.lesswrong.com/posts/kiWoCSgSFLNpsGfCc/stop-learning-start-thinking-link", "postedAtFormatted": "Wednesday, November 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Stop%20learning%2C%20start%20thinking%20%5BLINK%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStop%20learning%2C%20start%20thinking%20%5BLINK%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkiWoCSgSFLNpsGfCc%2Fstop-learning-start-thinking-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Stop%20learning%2C%20start%20thinking%20%5BLINK%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkiWoCSgSFLNpsGfCc%2Fstop-learning-start-thinking-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkiWoCSgSFLNpsGfCc%2Fstop-learning-start-thinking-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 158, "htmlBody": "<p><a href=\"http://www.youtube.com/watch?v=Uq-FOOQ1TpE&amp;feature=watch-vrec\">Stop Learning, by Jacob Barnett</a>. This is an 18 minute video, and I think there's a lot to be said for getting the material in the order given.</p>\n<p>However, if you'd rather have text, here it is in rot13. Wnpbo Oneargg pbzrf bss nf naablvat. Ybhq, ohzcgvbhf, naq ynhtuf ng zbfg bs uvf bja wbxrf. Ur'f nyfb n zngu cebqvtl, naq unf nhgvfz. Ur gnyxf nobhg ubj ur jnf qvntabfrq nf orvat hanoyr gb yrnea gb gnyx, ohg orpnhfr ur unq gvzr gb guvax, ur fgnegrq rkcybevat zngu. Vg'f abg fb onq gb snvy svatre-cnvagvat. Ur gnyxf nobhg Arjgba naq Rvafgrva nf univat orra oybpxrq bss sebz yrneavat sbe n juvyr (cynthr dhnenagvar naq cngrag bssvpr erfcrpgviryl), fb gung gurl unq gvzr gb guvax. Ur erpbzzraqf gnxvat gvzr gb guvax nobhg jung lbh pner nobhg.</p>\n<p>Would anyone happen to remember the alternate history story where the plague doesn't come to England, so Newton has professorial duties and never discovers anything?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kiWoCSgSFLNpsGfCc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 8, "extendedScore": null, "score": 1.03915871960749e-06, "legacy": true, "legacyId": "20208", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-21T14:34:11.779Z", "modifiedAt": null, "url": null, "title": "Ideas needed for a Psychology experiment involving popular fiction (Movies mainly) and biases and heuristics", "slug": "ideas-needed-for-a-psychology-experiment-involving-popular", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:57.881Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tenoke", "createdAt": "2012-04-10T21:37:29.739Z", "isAdmin": false, "displayName": "Tenoke"}, "userId": "CRSZPEg9dHyMspTzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qjzzLp6XXaru4vf3d/ideas-needed-for-a-psychology-experiment-involving-popular", "pageUrlRelative": "/posts/qjzzLp6XXaru4vf3d/ideas-needed-for-a-psychology-experiment-involving-popular", "linkUrl": "https://www.lesswrong.com/posts/qjzzLp6XXaru4vf3d/ideas-needed-for-a-psychology-experiment-involving-popular", "postedAtFormatted": "Wednesday, November 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ideas%20needed%20for%20a%20Psychology%20experiment%20involving%20popular%20fiction%20(Movies%20mainly)%20and%20biases%20and%20heuristics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIdeas%20needed%20for%20a%20Psychology%20experiment%20involving%20popular%20fiction%20(Movies%20mainly)%20and%20biases%20and%20heuristics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqjzzLp6XXaru4vf3d%2Fideas-needed-for-a-psychology-experiment-involving-popular%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ideas%20needed%20for%20a%20Psychology%20experiment%20involving%20popular%20fiction%20(Movies%20mainly)%20and%20biases%20and%20heuristics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqjzzLp6XXaru4vf3d%2Fideas-needed-for-a-psychology-experiment-involving-popular", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqjzzLp6XXaru4vf3d%2Fideas-needed-for-a-psychology-experiment-involving-popular", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 834, "htmlBody": "<p>I am conducting a simple experiment as my last year project for my BSc in Psychology. It is supposed to be a group project done largely under Supervision over a long period of time, but things took a different turn for me so I am making it on my own without almost any supervision as I convinced my Supervisor that I would rather not do anything in his field. Anyway I need ideas for the questionnaire that I need to develop for the study.</p>\n<p>The information on my experiment bellow is directly copied (and written by me) from my Ethics Approval form:</p>\n<p><!--[if gte mso 9]><xml> <o:OfficeDocumentSettings> <o:AllowPNG /> </o:OfficeDocumentSettings> </xml><![endif]--></p>\n<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:TrackMoves /> <w:TrackFormatting /> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:DoNotPromoteQF /> <w:LidThemeOther>EN-GB</w:LidThemeOther> <w:LidThemeAsian>X-NONE</w:LidThemeAsian> <w:LidThemeComplexScript>X-NONE</w:LidThemeComplexScript> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> <w:SplitPgBreakAndParaMark /> <w:EnableOpenTypeKerning /> <w:DontFlipMirrorIndents /> <w:OverrideTableStyleHps /> </w:Compatibility> <m:mathPr> <m:mathFont m:val=\"Cambria Math\" /> <m:brkBin m:val=\"before\" /> <m:brkBinSub m:val=\"&#45;-\" /> <m:smallFrac m:val=\"off\" /> <m:dispDef /> <m:lMargin m:val=\"0\" /> <m:rMargin m:val=\"0\" /> <m:defJc m:val=\"centerGroup\" /> <m:wrapIndent m:val=\"1440\" /> <m:intLim m:val=\"subSup\" /> <m:naryLim m:val=\"undOvr\" /> </m:mathPr></w:WordDocument> </xml><![endif]--><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" DefUnhideWhenUsed=\"true\" DefSemiHidden=\"true\" DefQFormat=\"false\" DefPriority=\"99\" LatentStyleCount=\"267\"> <w:LsdException Locked=\"false\" Priority=\"0\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Normal\" /> <w:LsdException Locked=\"false\" Priority=\"9\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"heading 1\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 2\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 3\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 4\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 5\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 6\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 7\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 8\" /> <w:LsdException Locked=\"false\" Priority=\"9\" QFormat=\"true\" Name=\"heading 9\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 1\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 2\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 3\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 4\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 5\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 6\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 7\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 8\" /> <w:LsdException Locked=\"false\" Priority=\"39\" Name=\"toc 9\" /> <w:LsdException Locked=\"false\" Priority=\"35\" QFormat=\"true\" Name=\"caption\" /> <w:LsdException Locked=\"false\" Priority=\"10\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Title\" /> <w:LsdException Locked=\"false\" Priority=\"0\" Name=\"Default Paragraph Font\" /> <w:LsdException Locked=\"false\" Priority=\"11\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtitle\" /> <w:LsdException Locked=\"false\" Priority=\"0\" Name=\"Body Text 3\" /> <w:LsdException Locked=\"false\" Priority=\"22\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Strong\" /> <w:LsdException Locked=\"false\" Priority=\"20\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"59\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Table Grid\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Placeholder Text\" /> <w:LsdException Locked=\"false\" Priority=\"1\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"No Spacing\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 1\" /> <w:LsdException Locked=\"false\" UnhideWhenUsed=\"false\" Name=\"Revision\" /> <w:LsdException Locked=\"false\" Priority=\"34\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"List Paragraph\" /> <w:LsdException Locked=\"false\" Priority=\"29\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Quote\" /> <w:LsdException Locked=\"false\" Priority=\"30\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Quote\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 1\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 2\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 3\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 4\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 5\" /> <w:LsdException Locked=\"false\" Priority=\"60\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"61\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"62\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Light Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"63\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"64\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Shading 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"65\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"66\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium List 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"67\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 1 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"68\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 2 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"69\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Medium Grid 3 Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"70\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Dark List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"71\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Shading Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"72\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful List Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"73\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" Name=\"Colorful Grid Accent 6\" /> <w:LsdException Locked=\"false\" Priority=\"19\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"21\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Emphasis\" /> <w:LsdException Locked=\"false\" Priority=\"31\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Subtle Reference\" /> <w:LsdException Locked=\"false\" Priority=\"32\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Intense Reference\" /> <w:LsdException Locked=\"false\" Priority=\"33\" SemiHidden=\"false\" UnhideWhenUsed=\"false\" QFormat=\"true\" Name=\"Book Title\" /> <w:LsdException Locked=\"false\" Priority=\"37\" Name=\"Bibliography\" /> <w:LsdException Locked=\"false\" Priority=\"39\" QFormat=\"true\" Name=\"TOC Heading\" /> </w:LatentStyles> </xml><![endif]--><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:\"\"; mso-padding-alt:0cm 5.4pt 0cm 5.4pt; mso-para-margin:0cm; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:10.0pt; font-family:\"Times New Roman\",\"serif\";} --> <!--[endif] --></p>\n<blockquote>\n<p class=\"MsoBodyText3\" style=\"margin-top:4.0pt;margin-right:0cm;margin-bottom: 0cm;margin-left:21.6pt;margin-bottom:.0001pt;text-indent:-21.6pt;mso-list:l0 level1 lfo1; tab-stops:list 17.7pt\"><span style=\"font-family:&quot;Arial&quot;,&quot;sans-serif&quot;; mso-fareast-font-family:Arial;font-weight:normal\"><span style=\"mso-list:Ignore\">1.<span style=\"font:7.0pt &quot;Times New Roman&quot;\">&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"font-family:&quot;Arial&quot;,&quot;sans-serif&quot;;font-weight:normal\">Title of project.</span></p>\n<p class=\"MsoBodyText3\" style=\"margin-top:4.0pt\"><span style=\"font-family:&quot;Arial&quot;,&quot;sans-serif&quot;; font-weight:normal\">&nbsp;</span></p>\n<p class=\"MsoNormal\"><span style=\"font-size:10.0pt;font-family:&quot;Tahoma&quot;,&quot;sans-serif&quot;; color:black;mso-fareast-language:EN-GB\">The effect of popular fiction on aspirations, goals and inclinations</span></p>\n<p class=\"MsoBodyText3\" style=\"margin-top:4.0pt\"><span style=\"font-family:&quot;Arial&quot;,&quot;sans-serif&quot;; font-weight:normal\">&nbsp;</span></p>\n<p class=\"MsoBodyText3\" style=\"margin-top:4.0pt;margin-right:0cm;margin-bottom: 0cm;margin-left:53.85pt;margin-bottom:.0001pt;text-indent:-53.85pt;mso-list: l0 level1 lfo1;tab-stops:list 17.7pt\"><span style=\"font-family:&quot;Arial&quot;,&quot;sans-serif&quot;;mso-fareast-font-family:Arial; font-weight:normal\"><span style=\"mso-list:Ignore\">2.<span style=\"font:7.0pt &quot;Times New Roman&quot;\">&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"font-family:&quot;Arial&quot;,&quot;sans-serif&quot;; font-weight:normal\">Purpose of project and its academic rationale.</span></p>\n<p class=\"MsoBodyText3\" style=\"margin-top:4.0pt\"><span style=\"font-family:&quot;Arial&quot;,&quot;sans-serif&quot;; font-weight:normal\">&nbsp;</span></p>\n<p class=\"MsoBodyText3\" style=\"margin-top:4.0pt\"><span style=\"font-family:&quot;Arial&quot;,&quot;sans-serif&quot;; font-weight:normal\">The purpose of the project is to find whether popular fiction influences the kind of experiences we want. The basis for the study comes from the mere-exposure effect, the availability heuristic and the availability cascade. I hypothesize that due to their effects people are more likely to prefer experiences to which they have been exposed(due to the mere-exposure effect) by popular culture (in popular movies, tv shows, etc) and which spring to mind faster(the availability heuristic) than similar experiences which have not been taken from popular fiction.</span></p>\n<p class=\"MsoBodyText3\" style=\"margin-top:4.0pt\"><span style=\"font-family:&quot;Arial&quot;,&quot;sans-serif&quot;; font-weight:normal\">&nbsp;</span></p>\n<p class=\"MsoBodyText3\" style=\"margin-top:4.0pt;margin-right:0cm;margin-bottom: 0cm;margin-left:53.85pt;margin-bottom:.0001pt;text-indent:-53.85pt;mso-list: l0 level1 lfo1;tab-stops:list 17.7pt\"><span style=\"font-family:&quot;Arial&quot;,&quot;sans-serif&quot;;mso-fareast-font-family:Arial; font-weight:normal\"><span style=\"mso-list:Ignore\">3.<span style=\"font:7.0pt &quot;Times New Roman&quot;\">&nbsp;&nbsp;&nbsp; </span></span></span><span style=\"font-family:&quot;Arial&quot;,&quot;sans-serif&quot;; font-weight:normal\">Brief description of methods and measurements.</span></p>\n<p class=\"MsoBodyText3\" style=\"margin-top:4.0pt\"><span style=\"font-family:&quot;Arial&quot;,&quot;sans-serif&quot;; font-weight:normal\">&nbsp;</span></p>\n<p class=\"MsoBodyText3\" style=\"margin-top:4.0pt\"><span style=\"font-family:&quot;Arial&quot;,&quot;sans-serif&quot;; font-weight:normal\">The participants will be given a Likert scale Questionnaire. The items on the questionnaire will be descriptions of experiences with half of them being taken from popular movies and TV shows and the other half will be description of similar kinds of experiences not taken from popular fiction. The participants will be asked to rate how much they want to have the experience and the scores for the popular movie items and the other items will be compared to test the hypothesis.</span></p>\n</blockquote>\n<p class=\"MsoBodyText3\" style=\"margin-top:4.0pt\"><span style=\"font-family:&quot;Arial&quot;,&quot;sans-serif&quot;; font-weight:normal\">A simple example of two items on this list can be:</span></p>\n<blockquote>\n<p class=\"MsoBodyText3\" style=\"margin-top:4.0pt\"><span style=\"font-family:&quot;Arial&quot;,&quot;sans-serif&quot;; font-weight:normal\">Swimming with dolphins. <br /></span></p>\n<p class=\"MsoBodyText3\" style=\"margin-top:4.0pt\"><span style=\"font-family:&quot;Arial&quot;,&quot;sans-serif&quot;; font-weight:normal\">and the counterexample</span></p>\n<p class=\"MsoBodyText3\" style=\"margin-top:4.0pt\"><span style=\"font-family:&quot;Arial&quot;,&quot;sans-serif&quot;; font-weight:normal\">Riding a zebra.</span></p>\n</blockquote>\n<p class=\"MsoBodyText3\" style=\"margin-top:4.0pt\">This is obviously not a great example and I also need to justify why I have chosen an experience from a given book/movie so I decided to use rankings like the top 250 on imdb and the box office rankings to choose movies that I can claim to be widely popular.</p>\n<p class=\"MsoBodyText3\" style=\"margin-top:4.0pt\">I've just started working on this and so far I have a list of possible ideas for some statements from movies, but haven't yet came up with enough such statements nor have I came up with any similar statements that aren't from a pop fiction.</p>\n<p class=\"MsoBodyText3\" style=\"margin-top:4.0pt\">The really drafty version of my draft is:</p>\n<blockquote>\n<p class=\"MsoBodyText3\" style=\"margin-top:4.0pt\">1. Escape from prison (shawshank)<br /><br /><br />2. Help save Earth or a big portion of it by using<br />means unavailable to the general public.<br /><br />3. Be a part of a jury and convince the rest to vote for the<br />right guy (a la 12 angry man)<br /><br /><br />4. Would you want if it is possible to go into someone's dream<br />vs vr (not matrix)?<br /><br />5.&nbsp; Find a lost artifact (Indy)<br /><br />6. Discover history (da vinci code, indy)?<br /><br />7. Be a spy for Mi6/Whatever with technology weapons etc.<br />8. Participate in a car chase<br />8a. Say follow that car<br />9ish a. Stop the presses! (The paper,&nbsp; The Great Muppet Caper)<br />9b Blow an engine's whistle (BttF2, The polar express)<br />9c Round up the usual suspects! (The boogie man)<br />9d pull tablecloth&nbsp; from a table (ghostbusters)<br />9e kick a door (jumanji)<br />9f we come in peace (sphere)<br />9g take me to your leader<br />10. Get superpowers and fight crime<br />vs<br />Get superpowers and let yourself be tested or some shit<br />11.Travel to another planet or something vs ?? and save it (a la avatar)<br />12. Have an alien friend (E.T.)<br />13. Be able to talk to dead people (6th sense)<br />14. Something Narnia-related<br />15. Fight off robbers (a la home alone)</p>\n</blockquote>\n<p class=\"MsoBodyText3\" style=\"margin-top:4.0pt\">&nbsp;</p>\n<p class=\"MsoBodyText3\" style=\"margin-top:4.0pt\"><span style=\"font-family:&quot;Arial&quot;,&quot;sans-serif&quot;; font-weight:normal\">If anyone can propose similar (hopefully better) examples from movies that would be great. It would be even better if anyone can propose the counter-examples (the examples that are similiar to a movie-statement but are not from an actual popular movie).</span></p>\n<p class=\"MsoBodyText3\" style=\"margin-top:4.0pt\"><span style=\"font-family:&quot;Arial&quot;,&quot;sans-serif&quot;; font-weight:normal\">Also I need to finish this by tomorrow and any comments that might be helpful are welcome.</span></p>\n<p class=\"MsoBodyText3\" style=\"margin-top:4.0pt\">&nbsp;</p>\n<p class=\"MsoBodyText3\" style=\"margin-top:4.0pt\"><span style=\"font-family:&quot;Arial&quot;,&quot;sans-serif&quot;; font-weight:normal\">Edit: The actual design of the questionnaire is unlikely to have any (significantly big) difference on my final grade as they grade me on the final writeup at the end of the year after I've conducted the experiment. As far as I know even the significance of the results do not influence my grade. I am asking for ideas so I can contribute something slightly better to science.<br /></span></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qjzzLp6XXaru4vf3d", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": -1, "extendedScore": null, "score": -2e-06, "legacy": true, "legacyId": "20209", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-21T16:05:48.174Z", "modifiedAt": null, "url": null, "title": "Meetup : LessWrong Montreal - Advanced Epistemology 101", "slug": "meetup-lesswrong-montreal-advanced-epistemology-101", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:35.289Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Paul_G", "createdAt": "2012-06-08T01:42:32.451Z", "isAdmin": false, "displayName": "Paul_G"}, "userId": "g4WQoWHmq4HNzTJDC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/e4ZMToJijStD5S7rT/meetup-lesswrong-montreal-advanced-epistemology-101", "pageUrlRelative": "/posts/e4ZMToJijStD5S7rT/meetup-lesswrong-montreal-advanced-epistemology-101", "linkUrl": "https://www.lesswrong.com/posts/e4ZMToJijStD5S7rT/meetup-lesswrong-montreal-advanced-epistemology-101", "postedAtFormatted": "Wednesday, November 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20LessWrong%20Montreal%20-%20Advanced%20Epistemology%20101&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20LessWrong%20Montreal%20-%20Advanced%20Epistemology%20101%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe4ZMToJijStD5S7rT%2Fmeetup-lesswrong-montreal-advanced-epistemology-101%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20LessWrong%20Montreal%20-%20Advanced%20Epistemology%20101%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe4ZMToJijStD5S7rT%2Fmeetup-lesswrong-montreal-advanced-epistemology-101", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fe4ZMToJijStD5S7rT%2Fmeetup-lesswrong-montreal-advanced-epistemology-101", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 86, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/g8'>LessWrong Montreal - Advanced Epistemology 101</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">26 November 2012 06:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">655 avenue du president kennedy, H3A 3H9</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Weekly Montreal meetup.</p>\n\n<p>We'll be at the Broadway Cheesecake, hopefully upstairs (if it's finally opened up). We're going to be discussing the new sequence, Highly Advanced Epistemology 101 for Beginners, as many of us are relative newbies to Bayesian rationality.</p>\n\n<p>Our focus will be the first post. Contact me if you're interested!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/g8'>LessWrong Montreal - Advanced Epistemology 101</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "e4ZMToJijStD5S7rT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 1.0392112694905147e-06, "legacy": true, "legacyId": "20210", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___LessWrong_Montreal___Advanced_Epistemology_101\">Discussion article for the meetup : <a href=\"/meetups/g8\">LessWrong Montreal - Advanced Epistemology 101</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">26 November 2012 06:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">655 avenue du president kennedy, H3A 3H9</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Weekly Montreal meetup.</p>\n\n<p>We'll be at the Broadway Cheesecake, hopefully upstairs (if it's finally opened up). We're going to be discussing the new sequence, Highly Advanced Epistemology 101 for Beginners, as many of us are relative newbies to Bayesian rationality.</p>\n\n<p>Our focus will be the first post. Contact me if you're interested!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___LessWrong_Montreal___Advanced_Epistemology_1011\">Discussion article for the meetup : <a href=\"/meetups/g8\">LessWrong Montreal - Advanced Epistemology 101</a></h2>", "sections": [{"title": "Discussion article for the meetup : LessWrong Montreal - Advanced Epistemology 101", "anchor": "Discussion_article_for_the_meetup___LessWrong_Montreal___Advanced_Epistemology_101", "level": 1}, {"title": "Discussion article for the meetup : LessWrong Montreal - Advanced Epistemology 101", "anchor": "Discussion_article_for_the_meetup___LessWrong_Montreal___Advanced_Epistemology_1011", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-21T18:25:21.938Z", "modifiedAt": null, "url": null, "title": "Technical Universities in Europe: a Recommendation Thread", "slug": "technical-universities-in-europe-a-recommendation-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:59.902Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ritalin", "createdAt": "2012-05-01T18:00:25.863Z", "isAdmin": false, "displayName": "Ritalin"}, "userId": "ACGKS9W7iQQywxrWW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nqpYWdMbWyjiWRTa5/technical-universities-in-europe-a-recommendation-thread", "pageUrlRelative": "/posts/nqpYWdMbWyjiWRTa5/technical-universities-in-europe-a-recommendation-thread", "linkUrl": "https://www.lesswrong.com/posts/nqpYWdMbWyjiWRTa5/technical-universities-in-europe-a-recommendation-thread", "postedAtFormatted": "Wednesday, November 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Technical%20Universities%20in%20Europe%3A%20a%20Recommendation%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATechnical%20Universities%20in%20Europe%3A%20a%20Recommendation%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnqpYWdMbWyjiWRTa5%2Ftechnical-universities-in-europe-a-recommendation-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Technical%20Universities%20in%20Europe%3A%20a%20Recommendation%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnqpYWdMbWyjiWRTa5%2Ftechnical-universities-in-europe-a-recommendation-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnqpYWdMbWyjiWRTa5%2Ftechnical-universities-in-europe-a-recommendation-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 88, "htmlBody": "<p>I wish to transfer to a university in Europe, to complete my engineering formation. I thought it might be the opportunity to initiate a discussion on the merits of European technical schools, given how many people here have a STEM background, and have experienced the first-hand.</p>\n<p>&nbsp;</p>\n<p>Which ones do you think are best at teaching? Which provide the best starting point, professionally? Which have the most productive, idealistic mood among the studentship? If you've been to several of schools, how do they compare to each other?</p>\n<p>&nbsp;</p>\n<p>The floor is yours.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nqpYWdMbWyjiWRTa5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": -9, "extendedScore": null, "score": 1.0392897494594844e-06, "legacy": true, "legacyId": "20211", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-21T22:23:26.193Z", "modifiedAt": null, "url": null, "title": "[LINK] IBM simulate a \"brain\" with 500 billion neurons and 100 trillion synapses", "slug": "link-ibm-simulate-a-brain-with-500-billion-neurons-and-100", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:03.521Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "drnickbone", "createdAt": "2012-01-20T17:19:55.216Z", "isAdmin": false, "displayName": "drnickbone"}, "userId": "GgwHTM3agaskLi9cx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AoMNPpjBSbao4BtxZ/link-ibm-simulate-a-brain-with-500-billion-neurons-and-100", "pageUrlRelative": "/posts/AoMNPpjBSbao4BtxZ/link-ibm-simulate-a-brain-with-500-billion-neurons-and-100", "linkUrl": "https://www.lesswrong.com/posts/AoMNPpjBSbao4BtxZ/link-ibm-simulate-a-brain-with-500-billion-neurons-and-100", "postedAtFormatted": "Wednesday, November 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20IBM%20simulate%20a%20%22brain%22%20with%20500%20billion%20neurons%20and%20100%20trillion%20synapses&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20IBM%20simulate%20a%20%22brain%22%20with%20500%20billion%20neurons%20and%20100%20trillion%20synapses%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAoMNPpjBSbao4BtxZ%2Flink-ibm-simulate-a-brain-with-500-billion-neurons-and-100%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20IBM%20simulate%20a%20%22brain%22%20with%20500%20billion%20neurons%20and%20100%20trillion%20synapses%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAoMNPpjBSbao4BtxZ%2Flink-ibm-simulate-a-brain-with-500-billion-neurons-and-100", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAoMNPpjBSbao4BtxZ%2Flink-ibm-simulate-a-brain-with-500-billion-neurons-and-100", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 50, "htmlBody": "<p>Recent article in The New Yorker:</p>\n<p><span style=\"font-family: Helvetica; font-size: 15px; line-height: 19px; white-space: nowrap; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.292969); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\">http://www.newyorker.com/online/blogs/newsdesk/2012/11/ibm-brain-simulation-compass.html</span></p>\n<p>Here is the research report from IBM, with the simple title \"10^14\":</p>\n<p><span style=\"font-family: Helvetica; font-size: 15px; line-height: 19px; white-space: nowrap; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.296875); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\">http://www.modha.org/blog/SC12/RJ10502.pdf</span></p>\n<p><span style=\"font-family: Helvetica; font-size: 15px; line-height: 19px; white-space: nowrap; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.296875); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\">It's nothing like a real brain simulation, of course, but illustrates that hardware to do this is getting very close.&nbsp;</span></p>\n<p><span style=\"font-family: Helvetica; font-size: 15px; line-height: 19px; white-space: nowrap; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.292969); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\">There is likely to be quite a long overhang between the hardware and the software...</span></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AoMNPpjBSbao4BtxZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 7, "extendedScore": null, "score": 1.0394236475084121e-06, "legacy": true, "legacyId": "20212", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-21T22:31:18.709Z", "modifiedAt": null, "url": null, "title": "Meetup : First meetup in Innsbruck ", "slug": "meetup-first-meetup-in-innsbruck", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:58.206Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "wallowinmaya", "createdAt": "2011-03-21T00:39:18.855Z", "isAdmin": false, "displayName": "David Althaus"}, "userId": "xY8DDzk6TyvRroJEo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/M9cmFWjK53teEJMDe/meetup-first-meetup-in-innsbruck", "pageUrlRelative": "/posts/M9cmFWjK53teEJMDe/meetup-first-meetup-in-innsbruck", "linkUrl": "https://www.lesswrong.com/posts/M9cmFWjK53teEJMDe/meetup-first-meetup-in-innsbruck", "postedAtFormatted": "Wednesday, November 21st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20First%20meetup%20in%20Innsbruck%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20First%20meetup%20in%20Innsbruck%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM9cmFWjK53teEJMDe%2Fmeetup-first-meetup-in-innsbruck%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20First%20meetup%20in%20Innsbruck%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM9cmFWjK53teEJMDe%2Fmeetup-first-meetup-in-innsbruck", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM9cmFWjK53teEJMDe%2Fmeetup-first-meetup-in-innsbruck", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 73, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/g9'>First meetup in Innsbruck </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 December 2012 03:00:17PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Innsbruck, Austria </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Let's organize the first Lesswrong meetup of Innsbruck!\nTo be part of this historical event just click on <a href=\"http://www.doodle.com/ic8i6w4qabzg39nd\" rel=\"nofollow\">this doodle-survey</a>\nand vote for your favorite time.</p>\n\n<p>You can also write a comment and suggest a place and discussion-topics if you want to.</p>\n\n<p>Newbies and lurkers are welcome, obviously!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/g9'>First meetup in Innsbruck </a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "M9cmFWjK53teEJMDe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.039428077301545e-06, "legacy": true, "legacyId": "20213", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___First_meetup_in_Innsbruck_\">Discussion article for the meetup : <a href=\"/meetups/g9\">First meetup in Innsbruck </a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 December 2012 03:00:17PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Innsbruck, Austria </span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Let's organize the first Lesswrong meetup of Innsbruck!\nTo be part of this historical event just click on <a href=\"http://www.doodle.com/ic8i6w4qabzg39nd\" rel=\"nofollow\">this doodle-survey</a>\nand vote for your favorite time.</p>\n\n<p>You can also write a comment and suggest a place and discussion-topics if you want to.</p>\n\n<p>Newbies and lurkers are welcome, obviously!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___First_meetup_in_Innsbruck_1\">Discussion article for the meetup : <a href=\"/meetups/g9\">First meetup in Innsbruck </a></h2>", "sections": [{"title": "Discussion article for the meetup : First meetup in Innsbruck ", "anchor": "Discussion_article_for_the_meetup___First_meetup_in_Innsbruck_", "level": 1}, {"title": "Discussion article for the meetup : First meetup in Innsbruck ", "anchor": "Discussion_article_for_the_meetup___First_meetup_in_Innsbruck_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-22T12:53:40.920Z", "modifiedAt": null, "url": null, "title": "AGI-12 and AGI-Impacts - late places available", "slug": "agi-12-and-agi-impacts-late-places-available", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:58.668Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uEC66SLoeEnrq3XXb/agi-12-and-agi-impacts-late-places-available", "pageUrlRelative": "/posts/uEC66SLoeEnrq3XXb/agi-12-and-agi-impacts-late-places-available", "linkUrl": "https://www.lesswrong.com/posts/uEC66SLoeEnrq3XXb/agi-12-and-agi-impacts-late-places-available", "postedAtFormatted": "Thursday, November 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AGI-12%20and%20AGI-Impacts%20-%20late%20places%20available&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAGI-12%20and%20AGI-Impacts%20-%20late%20places%20available%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuEC66SLoeEnrq3XXb%2Fagi-12-and-agi-impacts-late-places-available%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AGI-12%20and%20AGI-Impacts%20-%20late%20places%20available%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuEC66SLoeEnrq3XXb%2Fagi-12-and-agi-impacts-late-places-available", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuEC66SLoeEnrq3XXb%2Fagi-12-and-agi-impacts-late-places-available", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 129, "htmlBody": "<p>There are still some places available in the&nbsp;<a href=\"http://www.winterintelligence.org/oxford2012/\">Winter Intelligence Multi-Conference</a>, a dual conference including&nbsp;<a href=\"http://www.winterintelligence.org/oxford2012/agi-12/\">AGI-12</a>&nbsp;(the&nbsp;<a href=\"http://agi-conf.org/2012/\">Fifth Conference on Artificial General Intelligence</a>), followed by the&nbsp;<a href=\"http://www.winterintelligence.org/oxford2012/agi-impacts/\">AGI impacts conference</a>. The impacts conference will about the safety, risks and impacts of AGI, and how best to prepare now for these challenges. This is of great relevance to the people of Less Wrong.&nbsp;Plus it's in Oxford - <a href=\"http://www.connectedoxford.com/Customer/CO/images/OxfordImage.jpg\">Oxford</a> <a href=\"http://www.travelsignposts.com/England/files/2009/06/oxford-bridgesighs_588.jpg\">is</a> <a href=\"http://media-cdn.tripadvisor.com/media/photo-s/01/31/07/8f/oxford.jpg\">nice</a>.</p>\n<p>The AGI-12 conference is on the 8th-9th December (with morning workshops on the 10th-11th), while the AGI impacts conference in on the 10th-11th. Reduced prices are available for students; details <a href=\"http://www.winterintelligence.org/travel-and-registration/\">here</a>.</p>\n<p>Hope to see as many of you as we can! And if people want to stay on for a few days after the conference, people from the <a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a> should be available to chat with.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uEC66SLoeEnrq3XXb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 1.0399133587038352e-06, "legacy": true, "legacyId": "20220", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-22T20:02:45.554Z", "modifiedAt": null, "url": null, "title": "\"Optimal\" mammography?", "slug": "optimal-mammography", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:03.697Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6skBi6NcWs2HkNQMZ/optimal-mammography", "pageUrlRelative": "/posts/6skBi6NcWs2HkNQMZ/optimal-mammography", "linkUrl": "https://www.lesswrong.com/posts/6skBi6NcWs2HkNQMZ/optimal-mammography", "postedAtFormatted": "Thursday, November 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Optimal%22%20mammography%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Optimal%22%20mammography%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6skBi6NcWs2HkNQMZ%2Foptimal-mammography%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Optimal%22%20mammography%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6skBi6NcWs2HkNQMZ%2Foptimal-mammography", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6skBi6NcWs2HkNQMZ%2Foptimal-mammography", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 428, "htmlBody": "<p>With all the Bayesian experts here, it should be a no-brainer to figure out what to make of the recent study (<a href=\"http://www.usatoday.com/story/news/nation/2012/11/21/unnecessary-breast-cancer-treatment/1719003/\">popular description</a>). It says that, despite doubling early detection rates, late stage cancers declined by barely 8%, so most new early detections are effectively false positives, i.e. they would not develop into a life-threatening condition if left untreated.&nbsp;</p>\n<p>Some contradictory quotes:</p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #333333; font-family: arial, sans-serif; font-size: 14px; line-height: 22px;\">\"We've suggested to women that having a mammogram is one of the most important things you can do for your health, and that's simply not true,\" Welch says. \"I can't tell you the right thing to do, except to tell women the truth, tell them both sides of the story. We shouldn't be scaring women. This is a really close call.\"</span></p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #333333; font-family: arial, sans-serif; font-size: 14px; line-height: 22px;\">Eric Winer, head of breast medical oncology at Boston's Dana-Farber Cancer Institute, notes that the study found an 8% reduction in the number of women whose tumors were detected at more advanced stages. Even under a scenario in which mammograms led more than 1 million women to receive unnecessary treatment, the screenings would have prevented 410,000 diagnoses of late-stage cancer.</span></p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #333333; font-family: arial, sans-serif; font-size: 14px; line-height: 22px;\">Though women have been instructed that \"early detection saves lives,\" relatively few are told that screenings also have costs, including the risk of undergoing surgery, radiation and drug therapy that doesn't help them, Kramer says. \"The risks of overdiagnosis are real, and women ought to know about it,\" Kramer says.</span></p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #333333; font-family: arial, sans-serif; font-size: 14px; line-height: 22px;\">Given the test's limitations, Winer says, women may choose to have fewer screenings to reduce their risk. \"It certainly suggests that a woman who chooses to wait until she's 50 to have mammograms, or who chooses to have mammograms every other year, is making a rational decision,\" Winer says.</span></p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #333333; font-family: arial, sans-serif; font-size: 14px; line-height: 22px;\"><span style=\"font-family: georgia, 'times new roman', times, serif; font-size: 16px; line-height: 24px;\">&ldquo;We&rsquo;re coming to learn that some cancers &mdash; many cancers, depending on the organ &mdash; weren&rsquo;t destined to cause death,&rdquo; said Dr. Barnett Kramer, a National Cancer Institute screening expert. However, &ldquo;once a woman is diagnosed, it&rsquo;s hard to say treatment is not necessary.&rdquo;</span></span></p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #333333; font-family: arial, sans-serif; font-size: 14px; line-height: 22px;\"><span style=\"font-family: georgia, 'times new roman', times, serif; font-size: 16px; line-height: 24px;\">&ldquo;We are left to conclude, as others have, that the good news in breast cancer &mdash; decreasing mortality &mdash; must largely be the result of improved treatment, not screening,&rdquo;</span></span></p>\n<p style=\"padding-left: 30px;\"><span style=\"color: #333333; font-family: arial, sans-serif; font-size: 14px; line-height: 22px;\"><span style=\"font-family: georgia, 'times new roman', times, serif; font-size: 16px; line-height: 24px;\">&ldquo;Instead, we&rsquo;re diagnosing a lot of something else &mdash; not cancer&rdquo; in that early stage, Bleyer said. &ldquo;And the worst cancer is still going on, just like it always was.&rdquo;</span></span></p>\n<p style=\"padding-left: 30px;\">&nbsp;</p>\n<p style=\"padding: 0px 0px 0px 30px; margin: 0px 0px 0.83em; font-size: 16px; line-height: 1.5em; font-family: georgia, 'times new roman', times, serif;\">Another expert, Dr. Linda Vahdat, director of the breast cancer research program at Weill Cornell Medical College in New York, said the study&rsquo;s leaders made many assumptions to reach a conclusion about overdiagnosis that &ldquo;may or may not be correct.&rdquo;</p>\n<p style=\"padding: 0px 0px 0px 30px; margin: 0px 0px 0.83em; font-size: 16px; line-height: 1.5em; font-family: georgia, 'times new roman', times, serif;\">&ldquo;I don&rsquo;t think it will change how we view screening mammography,&rdquo; she said.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6skBi6NcWs2HkNQMZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 1.0401549626734953e-06, "legacy": true, "legacyId": "20221", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-22T20:55:54.093Z", "modifiedAt": null, "url": null, "title": "The wandering rationalist: an update", "slug": "the-wandering-rationalist-an-update", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:00.221Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Despard", "createdAt": "2012-06-11T21:21:52.973Z", "isAdmin": false, "displayName": "Despard"}, "userId": "inrBDZgvL5FzK6584", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vpEroDB9k9T6ZWPTZ/the-wandering-rationalist-an-update", "pageUrlRelative": "/posts/vpEroDB9k9T6ZWPTZ/the-wandering-rationalist-an-update", "linkUrl": "https://www.lesswrong.com/posts/vpEroDB9k9T6ZWPTZ/the-wandering-rationalist-an-update", "postedAtFormatted": "Thursday, November 22nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20wandering%20rationalist%3A%20an%20update&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20wandering%20rationalist%3A%20an%20update%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvpEroDB9k9T6ZWPTZ%2Fthe-wandering-rationalist-an-update%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20wandering%20rationalist%3A%20an%20update%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvpEroDB9k9T6ZWPTZ%2Fthe-wandering-rationalist-an-update", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvpEroDB9k9T6ZWPTZ%2Fthe-wandering-rationalist-an-update", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 333, "htmlBody": "<p>As per my original post <a href=\"/lw/e9t/the_wandering_rationalist/\">here</a>, and the more recent one by <a href=\"/user/shokwave/\">shokwave</a>&nbsp;<a href=\"/r/discussion/lw/fjv/australian_rationalist_in_america/\">here</a>, here's my attempt to nail down some dates for when I'll be in various places (both in the States and elsewhere). All dates are provisional, especially between Colorado and Washington, and my West Coast tour in January.</p>\n<p>For the US portion of the trip I've decided to go mostly by bus and train; Greyhound have just discontinued their Discovery Pass (which allows unlimited travel for up to 30 days) but I was able to snag a 15 day one on a special offer for the first half of my trip.</p>\n<p>Ann Arbor, MI:&nbsp;<span style=\"white-space: pre;\"> </span>Dec 4th</p>\n<p>Lansing, MI:<span style=\"white-space: pre;\"> </span>Dec 5th</p>\n<p>Chicago, IL:<span style=\"white-space: pre;\"> </span>Dec 6th</p>\n<p>Madison, WI:<span style=\"white-space: pre;\"> </span>Dec 7th</p>\n<p>Minneapolis, MN:<span style=\"white-space: pre;\"> </span>Dec 9th</p>\n<p>Kansas City, MO:<span style=\"white-space: pre;\"> </span>Dec 12th</p>\n<p>Oklahoma City, OK:<span style=\"white-space: pre;\"> </span>Dec 13th</p>\n<p>Austin, TX:<span style=\"white-space: pre;\"> </span>Dec 15th</p>\n<p>Laredo, TX:<span style=\"white-space: pre;\"> </span>Dec 16th</p>\n<p>Oklahoma City, OK:<span style=\"white-space: pre;\"> </span>Dec 18th</p>\n<p>Denver, CO:<span style=\"white-space: pre;\"> </span>Dec 19th</p>\n<p>Salt Lake City, UT:<span style=\"white-space: pre;\"> </span>Dec 22nd</p>\n<p>Kennewick, WA:<span style=\"white-space: pre;\"> </span>Dec 23rd</p>\n<p>Spokane, WA:<span style=\"white-space: pre;\"> </span>Dec 26th</p>\n<p>Vancouver, BC:<span style=\"white-space: pre;\"> </span>Dec 31st</p>\n<p>Seattle, WA:<span style=\"white-space: pre;\"> </span>Jan 3rd</p>\n<p>Portland, OR:<span style=\"white-space: pre;\"> </span>Jan 6th</p>\n<p>San Francisco, CA:<span style=\"white-space: pre;\"> </span>Jan 10th</p>\n<p>Los Angeles, CA:<span style=\"white-space: pre;\"> </span>Jan 15th</p>\n<p>-- booked flights --</p>\n<p>Nadi, Fiji<span style=\"white-space: pre;\"> </span>Jan 21st</p>\n<p>Auckland, NZ<span style=\"white-space: pre;\"> </span>Jan 26th</p>\n<p>Sydney, Australia<span style=\"white-space: pre;\"> </span>Feb 16th</p>\n<p>Singapore<span style=\"white-space: pre;\"> </span>Mar 9th</p>\n<p>Tokyo, Japan<span style=\"white-space: pre;\"> </span>Mar 22nd</p>\n<p>Seoul, Korea<span style=\"white-space: pre;\"> </span>Apr 21st</p>\n<p>London, UK<span style=\"white-space: pre;\"> </span>May 7th</p>\n<p>Reykjavik, Iceland<span style=\"white-space: pre;\"> </span>Jun 8th</p>\n<p>New York, NY<span style=\"white-space: pre;\"> </span>Jun 13th</p>\n<p>In the States portion of my trip, I have accommodation everywhere except:&nbsp;Chicago (where the friends I was expecting to stay with won't actually be there at that time); Kansas City; Austin (though I do believe I was offered a couch in the previous thread); SLC (though I may sleep on the coach and pass straight through on my long trip North if nobody is around); and the West Coast which I'm not worried about yet because it's an entire month away...</p>\n<p>So do contact me if you're interested in meeting up and/or hosting me. As I said dates are subject to change if necessary or if I feel like it!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vpEroDB9k9T6ZWPTZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.0401848928310942e-06, "legacy": true, "legacyId": "20222", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["zu5PxfwNtT4kxegf5", "g3EEsoJ4cWf9s6DLS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-23T02:56:20.331Z", "modifiedAt": "2021-12-04T19:18:36.011Z", "url": null, "title": "Playing the student: attitudes to learning as social roles", "slug": "playing-the-student-attitudes-to-learning-as-social-roles", "viewCount": null, "lastCommentedAt": "2012-12-01T02:00:03.979Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Swimmer963", "createdAt": "2010-09-28T01:54:53.120Z", "isAdmin": false, "displayName": "Swimmer963"}, "userId": "6Fx2vQtkYSZkaCvAg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xexSa6d4uasRGoMZT/playing-the-student-attitudes-to-learning-as-social-roles", "pageUrlRelative": "/posts/xexSa6d4uasRGoMZT/playing-the-student-attitudes-to-learning-as-social-roles", "linkUrl": "https://www.lesswrong.com/posts/xexSa6d4uasRGoMZT/playing-the-student-attitudes-to-learning-as-social-roles", "postedAtFormatted": "Friday, November 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Playing%20the%20student%3A%20attitudes%20to%20learning%20as%20social%20roles&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APlaying%20the%20student%3A%20attitudes%20to%20learning%20as%20social%20roles%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxexSa6d4uasRGoMZT%2Fplaying-the-student-attitudes-to-learning-as-social-roles%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Playing%20the%20student%3A%20attitudes%20to%20learning%20as%20social%20roles%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxexSa6d4uasRGoMZT%2Fplaying-the-student-attitudes-to-learning-as-social-roles", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxexSa6d4uasRGoMZT%2Fplaying-the-student-attitudes-to-learning-as-social-roles", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1452, "htmlBody": "<p><!--[if gte mso 9]><xml> <o:DocumentProperties> <o:Template>Normal</o:Template> <o:Revision>0</o:Revision> <o:TotalTime>0</o:TotalTime> <o:Pages>1</o:Pages> <o:Words>1188</o:Words> <o:Characters>6777</o:Characters> <o:Company>Home</o:Company> <o:Lines>56</o:Lines> <o:Paragraphs>13</o:Paragraphs> <o:CharactersWithSpaces>8322</o:CharactersWithSpaces> <o:Version>10.265</o:Version> </o:DocumentProperties> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:Zoom>0</w:Zoom> <w:DisplayHorizontalDrawingGridEvery>0</w:DisplayHorizontalDrawingGridEvery> <w:DisplayVerticalDrawingGridEvery>0</w:DisplayVerticalDrawingGridEvery> <w:UseMarginsForDrawingGridOrigin /> </w:WordDocument> </xml><![endif]--> <!--StartFragment--></p>\n<p>This is a post about something I noticed myself doing this year, although I expect I&rsquo;ve been doing it all along. It&rsquo;s unlikely to be something that everyone does, so don&rsquo;t be surprised if you don&rsquo;t find this applies to you. It's also an exercise in introspection, i.e. likely to be inaccurate.&nbsp;</p>\n<h2><span lang=\"EN-GB\">Intro</span></h2>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">If I add up all the years that I&rsquo;ve been in school, it amounts to about 75% of my life so far&ndash;and at any one time, school has probably been the single activity that I spend the most hours on. I would still guess that 50% or less of my general academic knowledge was actually acquired in a school setting, but school has <em>tests</em></span><span lang=\"EN-GB\">, and grades at the end of the year, and so has provided most of the positive/negative reinforcement related to learning. The &lsquo;attitudes to learning&rsquo; that I&rsquo;m talking about apply in a school setting, not when I&rsquo;m learning stuff for fun.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><br /></span></p>\n<h2><span lang=\"EN-GB\">Role #1: Overachiever</span></h2>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Up until seventh grade, I didn&rsquo;t really socialize at school&ndash;but once I started talking to people, it felt like I needed a persona, so that I could just act &lsquo;in character&rsquo; instead of having to think of things to say from scratch. Being a stereotypical overachiever provided me with easy material for small talk&ndash;I could talk about <em>schoolwork </em></span><span lang=\"EN-GB\">to other people who were also overachievers.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Years later, after acquiring <em>actual </em></span><span lang=\"EN-GB\">social skills in the less stereotyped environments of part-time work and university, I play the overachiever more as a way of reducing my anxiety in class. (School was easy for me up until my second year of nursing school, when we started having to do scary things like clinical placements and practical exams, instead of nice safe things like written exams.) If I can talk myself into always being curious and finding everything exciting and interesting and <em>cool I want to do that!!!</em></span><span lang=\"EN-GB\">, I can&rsquo;t find everything scary&ndash;or, at the very least, to other people it looks like I&rsquo;m not scared.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<h2><span lang=\"EN-GB\">Role #2: Too Cool for School</span></h2>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">This isn&rsquo;t one I&rsquo;ve played too much, aside from my tendency to put studying for exams as maybe my fourth priority&ndash;after work, exercise, and sleep&ndash;and still having an A average. (I will still skip class to work a shift at the ER any day, but that doesn&rsquo;t count&ndash;working there is almost more educational than class, in my mind.) As one of my LW Ottawa friends pointed out, there&rsquo;s a sort of counter-signalling involved in being a &lsquo;lazy&rsquo; student&ndash;if you can still pull off good grades without doing any work, you <em>must </em></span><span lang=\"EN-GB\">be smart, so people notice this and respect it. </span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">My brother is the prime example of this. He spent grades 9 through 11 alternately sleeping and playing on his iPhone in class, and maintained an average well over 80%. In grade 12 he started paying attention in class and occasionally doing homework, and graduated with, I believe, an average over 95%. He had a reputation throughout the whole school&ndash;as someone who was very smart, but also <em>cool</em></span><span lang=\"EN-GB\">. </span></p>\n<h2><span lang=\"EN-GB\">&nbsp;<br /></span><span lang=\"EN-GB\">Role #3: Just Don&rsquo;t Fail Me!</span></h2>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Weirdly enough, it wasn&rsquo;t at school that I originally learned this role. As a teenager, I did competitive swimming. The combination of <em>not </em></span><span lang=\"EN-GB\">having outstanding talent for athletics, plus the anxiety that came from my own performance depending on how fast the <em>other swimmers</em></span><span lang=\"EN-GB\"> were, made this about 100 times more terrifying than school. At some point I developed a weird sort of underconfidence, the opposite of using &lsquo;Overachiever&rsquo; to deal with anxiety. My mind has now created, and made automatic, the following subroutine: &ldquo;when an adult takes you aside to talk to you about anything related to &lsquo;living up to your potential&rsquo;, start crying.&rdquo; I&rsquo;m not sure what the original logic behind this was: get the adult to stop and pay attention to me? Get them to take me more seriously? Get them to take me <em>less </em></span><span lang=\"EN-GB\">seriously? Or just the fact that I couldn&rsquo;t stomach the fact of being <em>ordinarily </em></span><span lang=\"EN-GB\">below average at something&ndash;I had to be in some way <em>differently </em></span><span lang=\"EN-GB\">below average. Who knows if there was much logic behind it at all?<span style=\"mso-spacerun: yes;\">&nbsp;</span></span>&nbsp;</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Having this learned role comes back to bite me now, sometimes&ndash;the subroutine gets triggered in any situation that feels too much like my swim coach&rsquo;s one-on-one pre-competition pep talks. Taekwondo triggers it once in a while. Weirdly enough, being evaluated in clinicals triggers it too&ndash;this didn&rsquo;t originally make much sense, since it&rsquo;s not <em>competitive </em></span><span lang=\"EN-GB\">in the sense of &lsquo;she wins, I lose.&rsquo; I think the associative chain there is through lifeguarding courses&ndash;the hands-on evaluation aspect used to be fairly terrifying for my younger self, and my monkey brain puts clinicals and lab evaluations into that category, as opposed to the nice safe category of written exams, where I can safely be Too Cool for School and still get good grades.<span style=\"mso-spacerun: yes;\">&nbsp;</span></span>&nbsp;</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">The inconvenience of thinking about school this way really jumped out at me this fall. I started my semester of clinicals with a prof who was a) spectacularly non-intimidating compared to some others I&rsquo;ve had, and b) who liked me from the very start, basically because I raised my hand a lot and answered questions intelligently during our more classroom-y initial orientation. I was all set up for a semester of playing &lsquo;Overachiever&rsquo;, until, quite near the beginning of the semester, I was suddenly expected to do something that I found scary, and I was <em>tired </em></span><span lang=\"EN-GB\">and scared of looking confident but being wrong, and I fell back on &lsquo;Just Don&rsquo;t Fail Me!&rsquo; My prof was, understandably, shocked and confused as to why I was suddenly reacting to her as &lsquo;the scary adult who has the power to pass or fail me and will definitely fail me unless I&rsquo;m absolutely perfect, so I had better grovel.&rsquo; I think she actually felt <em>guilty </em></span><span lang=\"EN-GB\">about whatever she had done to intimidate me&ndash;which was nothing.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Since then I&rsquo;ve been doing fine, progressing at the same rate as all the other students (maybe it says something about me that this isn&rsquo;t very satisfying, and even kind of feels like failure in itself...I would like to be progressing <em>faster</em></span><span lang=\"EN-GB\">). That is, until I&rsquo;m alone with my prof and she tries to give me a pep talk about how I&rsquo;m obviously very smart and doing fine, so I just need to improve my confidence. Then I start crying. At this point, I&rsquo;m pretty sure she thinks I should be on anti-depressants&ndash;which is problematic in itself, but could be more problematic if she was the kind of prof who might <em>fail </em></span><span lang=\"EN-GB\">me in my clinical for a lack of confidence. There&rsquo;s no <em>objective </em></span><span lang=\"EN-GB\">reason why I can&rsquo;t hop back into Overachiever mode, since I managed both my clinicals last spring entirely in that mode. But part of my brain protests: &lsquo;she&rsquo;s seen you being insecure! She wouldn&rsquo;t believe you as an overachiever, it would be too out of character!&rsquo; It starts to make sense once I stop seeing this behaviour as 'my learning style' and recognize it as a social role that I, at some point, probably subconsciously, decided I ought to play.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<h2><span lang=\"EN-GB\">Conclusion</span></h2>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">The main problem seems to be that my original mental models for social interaction&ndash;with adults, mostly&ndash;are overly simplistic and don&rsquo;t cut reality at the joints. That&rsquo;s not a huge problem in itself&ndash;I have <em>better </em></span><span lang=\"EN-GB\">models now and most people I meet now say I have good communication skills, although I sometimes still come across as &lsquo;odd&rsquo;. The problem is that every once in a while, a situation happens, pattern recognition jumps into play, and whoa, I&rsquo;m playing &lsquo;Just Don&rsquo;t Fail Me&rsquo;. (It&rsquo;s happened with the other two roles too, but they&rsquo;re is less problematic.) Then I can&rsquo;t get <em>out </em></span><span lang=\"EN-GB\">of that role easily, because my social monkey brain is telling me it would be out of character and the other person would think it was weird. This is despite the fact that I no longer consciously <em>care </em></span><span lang=\"EN-GB\">if I come across as weird, as long as people think I&rsquo;m competent and trustworthy and nice, etc.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Just noticing this has helped a little&ndash;I catch my monkey brain and remind it &lsquo;hey, this situation looks similar to Situation X that you created a stereotyped response for, but it&rsquo;s <em>not </em></span><span lang=\"EN-GB\">Situation X, so how about we just behave like a <em>human being </em></span><span lang=\"EN-GB\">as usual&rsquo;. Reminding myself that the world doesn&rsquo;t break down into &lsquo;adults&rsquo; and &lsquo;children&rsquo;&ndash;or, if it did once, I&rsquo;m now on the other side of the divide&ndash;also helps. Failing that, I can consciously try to make sure I get into the 'right&rsquo; role&ndash;Overachiever or Too Cool For School, depending on the situation&ndash;and make that my default.</span>&nbsp;</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Has anyone else noticed themselves doing something similar? I&rsquo;m wondering if there are other roles that I play, maybe more subtly, at work or with friends.&nbsp;</span></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xexSa6d4uasRGoMZT", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 10, "extendedScore": null, "score": 3.6e-05, "legacy": true, "legacyId": "20224", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><!--[if gte mso 9]><xml> <o:DocumentProperties> <o:Template>Normal</o:Template> <o:Revision>0</o:Revision> <o:TotalTime>0</o:TotalTime> <o:Pages>1</o:Pages> <o:Words>1188</o:Words> <o:Characters>6777</o:Characters> <o:Company>Home</o:Company> <o:Lines>56</o:Lines> <o:Paragraphs>13</o:Paragraphs> <o:CharactersWithSpaces>8322</o:CharactersWithSpaces> <o:Version>10.265</o:Version> </o:DocumentProperties> </xml><![endif]--><!--[if gte mso 9]><xml> <w:WordDocument> <w:Zoom>0</w:Zoom> <w:DisplayHorizontalDrawingGridEvery>0</w:DisplayHorizontalDrawingGridEvery> <w:DisplayVerticalDrawingGridEvery>0</w:DisplayVerticalDrawingGridEvery> <w:UseMarginsForDrawingGridOrigin /> </w:WordDocument> </xml><![endif]--> <!--StartFragment--></p>\n<p>This is a post about something I noticed myself doing this year, although I expect I\u2019ve been doing it all along. It\u2019s unlikely to be something that everyone does, so don\u2019t be surprised if you don\u2019t find this applies to you. It's also an exercise in introspection, i.e. likely to be inaccurate.&nbsp;</p>\n<h2 id=\"Intro\"><span lang=\"EN-GB\">Intro</span></h2>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">If I add up all the years that I\u2019ve been in school, it amounts to about 75% of my life so far\u2013and at any one time, school has probably been the single activity that I spend the most hours on. I would still guess that 50% or less of my general academic knowledge was actually acquired in a school setting, but school has <em>tests</em></span><span lang=\"EN-GB\">, and grades at the end of the year, and so has provided most of the positive/negative reinforcement related to learning. The \u2018attitudes to learning\u2019 that I\u2019m talking about apply in a school setting, not when I\u2019m learning stuff for fun.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><br></span></p>\n<h2 id=\"Role__1__Overachiever\"><span lang=\"EN-GB\">Role #1: Overachiever</span></h2>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Up until seventh grade, I didn\u2019t really socialize at school\u2013but once I started talking to people, it felt like I needed a persona, so that I could just act \u2018in character\u2019 instead of having to think of things to say from scratch. Being a stereotypical overachiever provided me with easy material for small talk\u2013I could talk about <em>schoolwork </em></span><span lang=\"EN-GB\">to other people who were also overachievers.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Years later, after acquiring <em>actual </em></span><span lang=\"EN-GB\">social skills in the less stereotyped environments of part-time work and university, I play the overachiever more as a way of reducing my anxiety in class. (School was easy for me up until my second year of nursing school, when we started having to do scary things like clinical placements and practical exams, instead of nice safe things like written exams.) If I can talk myself into always being curious and finding everything exciting and interesting and <em>cool I want to do that!!!</em></span><span lang=\"EN-GB\">, I can\u2019t find everything scary\u2013or, at the very least, to other people it looks like I\u2019m not scared.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<h2 id=\"Role__2__Too_Cool_for_School\"><span lang=\"EN-GB\">Role #2: Too Cool for School</span></h2>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">This isn\u2019t one I\u2019ve played too much, aside from my tendency to put studying for exams as maybe my fourth priority\u2013after work, exercise, and sleep\u2013and still having an A average. (I will still skip class to work a shift at the ER any day, but that doesn\u2019t count\u2013working there is almost more educational than class, in my mind.) As one of my LW Ottawa friends pointed out, there\u2019s a sort of counter-signalling involved in being a \u2018lazy\u2019 student\u2013if you can still pull off good grades without doing any work, you <em>must </em></span><span lang=\"EN-GB\">be smart, so people notice this and respect it. </span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">My brother is the prime example of this. He spent grades 9 through 11 alternately sleeping and playing on his iPhone in class, and maintained an average well over 80%. In grade 12 he started paying attention in class and occasionally doing homework, and graduated with, I believe, an average over 95%. He had a reputation throughout the whole school\u2013as someone who was very smart, but also <em>cool</em></span><span lang=\"EN-GB\">. </span></p>\n<h2 id=\"_Role__3__Just_Don_t_Fail_Me_\"><span lang=\"EN-GB\">&nbsp;<br></span><span lang=\"EN-GB\">Role #3: Just Don\u2019t Fail Me!</span></h2>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Weirdly enough, it wasn\u2019t at school that I originally learned this role. As a teenager, I did competitive swimming. The combination of <em>not </em></span><span lang=\"EN-GB\">having outstanding talent for athletics, plus the anxiety that came from my own performance depending on how fast the <em>other swimmers</em></span><span lang=\"EN-GB\"> were, made this about 100 times more terrifying than school. At some point I developed a weird sort of underconfidence, the opposite of using \u2018Overachiever\u2019 to deal with anxiety. My mind has now created, and made automatic, the following subroutine: \u201cwhen an adult takes you aside to talk to you about anything related to \u2018living up to your potential\u2019, start crying.\u201d I\u2019m not sure what the original logic behind this was: get the adult to stop and pay attention to me? Get them to take me more seriously? Get them to take me <em>less </em></span><span lang=\"EN-GB\">seriously? Or just the fact that I couldn\u2019t stomach the fact of being <em>ordinarily </em></span><span lang=\"EN-GB\">below average at something\u2013I had to be in some way <em>differently </em></span><span lang=\"EN-GB\">below average. Who knows if there was much logic behind it at all?<span style=\"mso-spacerun: yes;\">&nbsp;</span></span>&nbsp;</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Having this learned role comes back to bite me now, sometimes\u2013the subroutine gets triggered in any situation that feels too much like my swim coach\u2019s one-on-one pre-competition pep talks. Taekwondo triggers it once in a while. Weirdly enough, being evaluated in clinicals triggers it too\u2013this didn\u2019t originally make much sense, since it\u2019s not <em>competitive </em></span><span lang=\"EN-GB\">in the sense of \u2018she wins, I lose.\u2019 I think the associative chain there is through lifeguarding courses\u2013the hands-on evaluation aspect used to be fairly terrifying for my younger self, and my monkey brain puts clinicals and lab evaluations into that category, as opposed to the nice safe category of written exams, where I can safely be Too Cool for School and still get good grades.<span style=\"mso-spacerun: yes;\">&nbsp;</span></span>&nbsp;</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">The inconvenience of thinking about school this way really jumped out at me this fall. I started my semester of clinicals with a prof who was a) spectacularly non-intimidating compared to some others I\u2019ve had, and b) who liked me from the very start, basically because I raised my hand a lot and answered questions intelligently during our more classroom-y initial orientation. I was all set up for a semester of playing \u2018Overachiever\u2019, until, quite near the beginning of the semester, I was suddenly expected to do something that I found scary, and I was <em>tired </em></span><span lang=\"EN-GB\">and scared of looking confident but being wrong, and I fell back on \u2018Just Don\u2019t Fail Me!\u2019 My prof was, understandably, shocked and confused as to why I was suddenly reacting to her as \u2018the scary adult who has the power to pass or fail me and will definitely fail me unless I\u2019m absolutely perfect, so I had better grovel.\u2019 I think she actually felt <em>guilty </em></span><span lang=\"EN-GB\">about whatever she had done to intimidate me\u2013which was nothing.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Since then I\u2019ve been doing fine, progressing at the same rate as all the other students (maybe it says something about me that this isn\u2019t very satisfying, and even kind of feels like failure in itself...I would like to be progressing <em>faster</em></span><span lang=\"EN-GB\">). That is, until I\u2019m alone with my prof and she tries to give me a pep talk about how I\u2019m obviously very smart and doing fine, so I just need to improve my confidence. Then I start crying. At this point, I\u2019m pretty sure she thinks I should be on anti-depressants\u2013which is problematic in itself, but could be more problematic if she was the kind of prof who might <em>fail </em></span><span lang=\"EN-GB\">me in my clinical for a lack of confidence. There\u2019s no <em>objective </em></span><span lang=\"EN-GB\">reason why I can\u2019t hop back into Overachiever mode, since I managed both my clinicals last spring entirely in that mode. But part of my brain protests: \u2018she\u2019s seen you being insecure! She wouldn\u2019t believe you as an overachiever, it would be too out of character!\u2019 It starts to make sense once I stop seeing this behaviour as 'my learning style' and recognize it as a social role that I, at some point, probably subconsciously, decided I ought to play.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\"><!--[if !supportEmptyParas]-->&nbsp;<!--[endif]--></span></p>\n<h2 id=\"Conclusion\"><span lang=\"EN-GB\">Conclusion</span></h2>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">The main problem seems to be that my original mental models for social interaction\u2013with adults, mostly\u2013are overly simplistic and don\u2019t cut reality at the joints. That\u2019s not a huge problem in itself\u2013I have <em>better </em></span><span lang=\"EN-GB\">models now and most people I meet now say I have good communication skills, although I sometimes still come across as \u2018odd\u2019. The problem is that every once in a while, a situation happens, pattern recognition jumps into play, and whoa, I\u2019m playing \u2018Just Don\u2019t Fail Me\u2019. (It\u2019s happened with the other two roles too, but they\u2019re is less problematic.) Then I can\u2019t get <em>out </em></span><span lang=\"EN-GB\">of that role easily, because my social monkey brain is telling me it would be out of character and the other person would think it was weird. This is despite the fact that I no longer consciously <em>care </em></span><span lang=\"EN-GB\">if I come across as weird, as long as people think I\u2019m competent and trustworthy and nice, etc.</span></p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Just noticing this has helped a little\u2013I catch my monkey brain and remind it \u2018hey, this situation looks similar to Situation X that you created a stereotyped response for, but it\u2019s <em>not </em></span><span lang=\"EN-GB\">Situation X, so how about we just behave like a <em>human being </em></span><span lang=\"EN-GB\">as usual\u2019. Reminding myself that the world doesn\u2019t break down into \u2018adults\u2019 and \u2018children\u2019\u2013or, if it did once, I\u2019m now on the other side of the divide\u2013also helps. Failing that, I can consciously try to make sure I get into the 'right\u2019 role\u2013Overachiever or Too Cool For School, depending on the situation\u2013and make that my default.</span>&nbsp;</p>\n<p class=\"MsoNormal\"><span lang=\"EN-GB\">Has anyone else noticed themselves doing something similar? I\u2019m wondering if there are other roles that I play, maybe more subtly, at work or with friends.&nbsp;</span></p>\n<p>&nbsp;</p>", "sections": [{"title": "Intro", "anchor": "Intro", "level": 1}, {"title": "Role #1: Overachiever", "anchor": "Role__1__Overachiever", "level": 1}, {"title": "Role #2: Too Cool for School", "anchor": "Role__2__Too_Cool_for_School", "level": 1}, {"title": "\u00a0Role #3: Just Don\u2019t Fail Me!", "anchor": "_Role__3__Just_Don_t_Fail_Me_", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "24 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2012-11-23T02:56:20.331Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-23T07:58:13.224Z", "modifiedAt": null, "url": null, "title": "[link] Misinformation and Its Correction: Continued Influence and Successful Debiasing", "slug": "link-misinformation-and-its-correction-continued-influence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:00.580Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/q99y5a8LjC7LDLK7r/link-misinformation-and-its-correction-continued-influence", "pageUrlRelative": "/posts/q99y5a8LjC7LDLK7r/link-misinformation-and-its-correction-continued-influence", "linkUrl": "https://www.lesswrong.com/posts/q99y5a8LjC7LDLK7r/link-misinformation-and-its-correction-continued-influence", "postedAtFormatted": "Friday, November 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20Misinformation%20and%20Its%20Correction%3A%20Continued%20Influence%20and%20Successful%20Debiasing&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20Misinformation%20and%20Its%20Correction%3A%20Continued%20Influence%20and%20Successful%20Debiasing%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq99y5a8LjC7LDLK7r%2Flink-misinformation-and-its-correction-continued-influence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20Misinformation%20and%20Its%20Correction%3A%20Continued%20Influence%20and%20Successful%20Debiasing%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq99y5a8LjC7LDLK7r%2Flink-misinformation-and-its-correction-continued-influence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq99y5a8LjC7LDLK7r%2Flink-misinformation-and-its-correction-continued-influence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3141, "htmlBody": "<p><a href=\"http://psi.sagepub.com/content/13/3/106.full\">http://psi.sagepub.com/content/13/3/106.full</a></p>\n<blockquote>\n<p><em>Abstract. </em></p>\n<p id=\"p-1\">The widespread prevalence and persistence of misinformation in contemporary societies, such as the false belief that there is a link between childhood vaccinations and autism, is a matter of public concern. For example, the myths surrounding vaccinations, which prompted some parents to withhold immunization from their children, have led to a marked increase in vaccine-preventable disease, as well as unnecessary public expenditure on research and public-information campaigns aimed at rectifying the situation.</p>\n<p id=\"p-2\">We first examine the mechanisms by which such misinformation is disseminated in society, both inadvertently and purposely. Misinformation can originate from rumors but also from works of fiction, governments and politicians, and vested interests. Moreover, changes in the media landscape, including the arrival of the Internet, have fundamentally influenced the ways in which information is communicated and misinformation is spread.</p>\n<p id=\"p-3\">We next move to misinformation at the level of the individual, and review the cognitive factors that often render misinformation resistant to correction. We consider how people assess the truth of statements and what makes people believe certain things but not others. We look at people&rsquo;s memory for misinformation and answer the questions of why retractions of misinformation are so ineffective in memory updating and why efforts to retract misinformation can even backfire and, ironically, increase misbelief. Though ideology and personal worldviews can be major obstacles for debiasing, there nonetheless are a number of effective techniques for reducing the impact of misinformation, and we pay special attention to these factors that aid in debiasing.</p>\n<p id=\"p-4\">We conclude by providing specific recommendations for the debunking of misinformation. These recommendations pertain to the ways in which corrections should be designed, structured, and applied in order to maximize their impact. Grounded in cognitive psychological theory, these recommendations may help practitioners&mdash;including journalists, health professionals, educators, and science communicators&mdash;design effective misinformation retractions, educational tools, and public-information campaigns.</p>\n</blockquote>\n<p>This is a fascinating article with many, many interesting points. I'm excerpting some of them below, but mostly just to get you to read it: if I were to quote everything interesting, I'd have to pretty much copy the entire (long!) article.</p>\n<blockquote>\n<p id=\"p-23\"><strong>Rumors and fiction</strong></p>\n<p>[...] A related but perhaps more surprising source of misinformation is literary fiction. People extract knowledge even from sources that are explicitly identified as fictional. This process is often adaptive, because fiction frequently contains valid information about the world. For example, non-Americans&rsquo; knowledge of U.S. traditions, sports, climate, and geography partly stems from movies and novels, and many Americans know from movies that Britain and Australia have left-hand traffic. By definition, however, fiction writers are not obliged to stick to the facts, which creates an avenue for the spread of misinformation, even by stories that are explicitly identified as fictional. A study by <span class=\"xref-bibr\">Marsh, Meade, and Roediger (2003)</span> showed that people relied on misinformation acquired from clearly fictitious stories to respond to later quiz questions, even when these pieces of misinformation contradicted common knowledge. In most cases, source attribution was intact, so people were aware that their answers to the quiz questions were based on information from the stories, but reading the stories also increased people&rsquo;s illusory belief of prior knowledge. In other words, encountering misinformation in a fictional context led people to assume they had known it all along and to integrate this misinformation with their prior knowledge (<span class=\"xref-bibr\">Marsh &amp; Fazio, 2006</span>; <span class=\"xref-bibr\">Marsh et al., 2003</span>).</p>\n<p id=\"p-24\">The effects of fictional misinformation have been shown to be stable and difficult to eliminate. <span class=\"xref-bibr\">Marsh and Fazio (2006)</span> reported that prior warnings were ineffective in reducing the acquisition of misinformation from fiction, and that acquisition was only reduced (not eliminated) under conditions of active on-line monitoring&mdash;when participants were instructed to actively monitor the contents of what they were reading and to press a key every time they encountered a piece of misinformation (see also <span class=\"xref-bibr\">Eslick, Fazio, &amp; Marsh, 2011</span>). Few people would be so alert and mindful when reading fiction for enjoyment. These links between fiction and incorrect knowledge are particularly concerning when popular fiction pretends to accurately portray science but fails to do so, as was the case with Michael Crichton&rsquo;s novel <em>State of Fear</em>. The novel misrepresented the science of global climate change but was nevertheless introduced as &ldquo;scientific&rdquo; evidence into a U.S. Senate committee (<span class=\"xref-bibr\">Allen, 2005</span>; <span class=\"xref-bibr\">Leggett, 2005</span>).</p>\n<p id=\"p-25\">Writers of fiction are expected to depart from reality, but in other instances, misinformation is manufactured intentionally. There is considerable peer-reviewed evidence pointing to the fact that misinformation can be intentionally or carelessly disseminated, often for political ends or in the service of vested interests, but also through routine processes employed by the media. [...]</p>\n<p><strong>Assessing the Truth of a Statement: Recipients&rsquo; Strategies</strong></p>\n<p>Misleading information rarely comes with a warning label. People usually cannot recognize that a piece of information is incorrect until they receive a correction or retraction. For better or worse, the acceptance of information as true is favored by tacit norms of everyday conversational conduct: Information relayed in conversation comes with a &ldquo;guarantee of relevance&rdquo; (Sperber &amp; Wilson, 1986), and listeners proceed on the assumption that speakers try to be truthful, relevant, and clear, unless evidence to the contrary calls this default into question (Grice, 1975; Schwarz, 1994, 1996). Some research has even suggested that to comprehend a statement, people must at least temporarily accept it as true (Gilbert, 1991). On this view, belief is an inevitable consequence of&mdash;or, indeed, precursor to&mdash;comprehension.<br /><br />Although suspension of belief is possible (Hasson, Simmons, &amp; Todorov, 2005; Schul, Mayo, &amp; Burnstein, 2008), it seems to require a high degree of attention, considerable implausibility of the message, or high levels of distrust at the time the message is received. So, in most situations, the deck is stacked in favor of accepting information rather than rejecting it, provided there are no salient markers that call the speaker&rsquo;s intention of cooperative conversation into question. Going beyond this default of acceptance requires additional motivation and cognitive resources: If the topic is not very important to you, or you have other things on your mind, misinformation will likely slip in.\" [...]</p>\n<p><strong>Is the information compatible with what I believe?</strong><br /><br />As numerous studies in the literature on social judgment and persuasion have shown, information is more likely to be accepted by people when it is consistent with other things they assume to be true (for reviews, see McGuire, 1972; Wyer, 1974). People assess the logical compatibility of the information with other facts and beliefs. Once a new piece of knowledge-consistent information has been accepted, it is highly resistant to change, and the more so the larger the compatible knowledge base is. From a judgment perspective, this resistance derives from the large amount of supporting evidence (Wyer, 1974); from a cognitive-consistency perspective (Festinger, 1957), it derives from the numerous downstream inconsistencies that would arise from rejecting the prior information as false. Accordingly, compatibility with other knowledge increases the likelihood that misleading information will be accepted, and decreases the likelihood that it will be successfully corrected.<br /><br />When people encounter a piece of information, they can check it against other knowledge to assess its compatibility. This process is effortful, and it requires motivation and cognitive resources. A less demanding indicator of compatibility is provided by one&rsquo;s meta-cognitive experience and affective response to new information. Many theories of cognitive consistency converge on the assumption that information that is inconsistent with one&rsquo;s beliefs elicits negative feelings (Festinger, 1957). Messages that are inconsistent with one&rsquo;s beliefs are also processed less fluently than messages that are consistent with one&rsquo;s beliefs (Winkielman, Huber, Kavanagh, &amp; Schwarz, 2012). In general, fluently processed information feels more familiar and is more likely to be accepted as true; conversely, disfluency elicits the impression that something doesn&rsquo;t quite &ldquo;feel right&rdquo; and prompts closer scrutiny of the message (Schwarz et al., 2007; Song &amp; Schwarz, 2008). This phenomenon is observed even when the fluent processing of a message merely results from superficial characteristics of its presentation. For example, the same statement is more likely to be judged as true when it is printed in high rather than low color contrast (Reber &amp; Schwarz, 1999), presented in a rhyming rather than nonrhyming form (McGlone &amp; Tofighbakhsh, 2000), or delivered in a familiar rather than unfamiliar accent (Levy-Ari &amp; Keysar, 2010). Moreover, misleading questions are less likely to be recognized as such when printed in an easy-to-read font (Song &amp; Schwarz, 2008).<br /><br />As a result, analytic as well as intuitive processing favors the acceptance of messages that are compatible with a recipient&rsquo;s preexisting beliefs: The message contains no elements that contradict current knowledge, is easy to process, and &ldquo;feels right.&rdquo;<br /><br /><strong>Is the story coherent?</strong><br /><br />Whether a given piece of information will be accepted as true also depends on how well it fits a broader story that lends sense and coherence to its individual elements. People are particularly likely to use an assessment strategy based on this principle when the meaning of one piece of information cannot be assessed in isolation because it depends on other, related pieces; use of this strategy has been observed in basic research on mental models (for a review, see Johnson-Laird, 2012), as well as extensive analyses of juries&rsquo; decision making (Pennington &amp; Hastie, 1992, 1993).<br /><br />A story is compelling to the extent that it organizes information without internal contradictions in a way that is compatible with common assumptions about human motivation and behavior. Good stories are easily remembered, and gaps are filled with story-consistent intrusions. Once a coherent story has been formed, it is highly resistant to change: Within the story, each element is supported by the fit of other elements, and any alteration of an element may be made implausible by the downstream inconsistencies it would cause. Coherent stories are easier to process than incoherent stories are (Johnson-Laird, 2012), and people draw on their processing experience when they judge a story&rsquo;s coherence (Topolinski, 2012), again giving an advantage to material that is easy to process. [...]</p>\n<p><strong>Is the information from a credible source?</strong><br /><br />[...] People&rsquo;s evaluation of a source&rsquo;s credibility can be based on declarative information, as in the above examples, as well as experiential information. The mere repetition of an unknown name can cause it to seem familiar, making its bearer &ldquo;famous overnight&rdquo; (Jacoby, Kelley, Brown, &amp; Jaseschko, 1989)&mdash;and hence more credible. Even when a message is rejected at the time of initial exposure, that initial exposure may lend it some familiarity-based credibility if the recipient hears it again.<br /><br /><strong>Do others believe this information?</strong><br /><br />Repeated exposure to a statement is known to increase its acceptance as true (e.g., Begg, Anas, &amp; Farinacci, 1992; Hasher, Goldstein, &amp; Toppino, 1977). In a classic study of rumor transmission, Allport and Lepkin (1945) observed that the strongest predictor of belief in wartime rumors was simple repetition. Repetition effects may create a perceived social consensus even when no consensus exists. Festinger (1954) referred to social consensus as a &ldquo;secondary reality test&rdquo;: If many people believe a piece of information, there&rsquo;s probably something to it. Because people are more frequently exposed to widely shared beliefs than to highly idiosyncratic ones, the familiarity of a belief is often a valid indicator of social consensus. But, unfortunately, information can seem familiar for the wrong reason, leading to erroneous perceptions of high consensus. For example, Weaver, Garcia, Schwarz, and Miller (2007) exposed participants to multiple iterations of the same statement, provided by the same communicator. When later asked to estimate how widely the conveyed belief is shared, participants estimated consensus to be greater the more often they had read the identical statement from the same, single source. In a very real sense, a single repetitive voice can sound like a chorus. [...]<br /><br />The extent of pluralistic ignorance (or of the false-consensus effect) can be quite striking: In Australia, people with particularly negative attitudes toward Aboriginal Australians or asylum seekers have been found to overestimate public support for their attitudes by 67% and 80%, respectively (Pedersen, Griffiths, &amp; Watt, 2008). Specifically, although only 1.8% of people in a sample of Australians were found to hold strongly negative attitudes toward Aboriginals, those few individuals thought that 69% of all Australians (and 79% of their friends) shared their fringe beliefs. This represents an extreme case of the false-consensus effect. [...]<br /><br /><strong>The Continued Influence Effect: Retractions Fail to Eliminate the Influence of Misinformation</strong><br /><br />We first consider the cognitive parameters of credible retractions in neutral scenarios, in which people have no inherent reason or motivation to believe one version of events over another. Research on this topic was stimulated by a paradigm pioneered by Wilkes and Leatherbarrow (1988) and H. M. Johnson and Seifert (1994). In it, people are presented with a fictitious report about an event unfolding over time. The report contains a target piece of information: For some readers, this target information is subsequently retracted, whereas for readers in a control condition, no correction occurs. Participants&rsquo; understanding of the event is then assessed with a questionnaire, and the number of clear and uncontroverted references to the target (mis-)information in their responses is tallied.<br /><br />A stimulus narrative commonly used in this paradigm involves a warehouse fire that is initially thought to have been caused by gas cylinders and oil paints that were negligently stored in a closet (e.g., Ecker, Lewandowsky, Swire, &amp; Chang, 2011; H. M. Johnson &amp; Seifert, 1994; Wilkes &amp; Leatherbarrow, 1988). Some participants are then presented with a retraction, such as &ldquo;the closet was actually empty.&rdquo; A comprehension test follows, and participants&rsquo; number of references to the gas and paint in response to indirect inference questions about the event (e.g., &ldquo;What caused the black smoke?&rdquo;) is counted. In addition, participants are asked to recall some basic facts about the event and to indicate whether they noticed any retraction.<br /><br />Research using this paradigm has consistently found that retractions rarely, if ever, have the intended effect of eliminating reliance on misinformation, even when people believe, understand, and later remember the retraction (e.g., Ecker, Lewandowsky, &amp; Apai, 2011; Ecker, Lewandowsky, Swire, &amp; Chang, 2011; Ecker, Lewandowsky, &amp; Tang, 2010; Fein, McCloskey, &amp; Tomlinson, 1997; Gilbert, Krull, &amp; Malone, 1990; Gilbert, Tafarodi, &amp; Malone, 1993; H. M. Johnson &amp; Seifert, 1994, 1998, 1999; Schul &amp; Mazursky, 1990; van Oostendorp, 1996; van Oostendorp &amp; Bonebakker, 1999; Wilkes &amp; Leatherbarrow, 1988; Wilkes &amp; Reynolds, 1999). In fact, a retraction will at most halve the number of references to misinformation, even when people acknowledge and demonstrably remember the retraction (Ecker, Lewandowsky, &amp; Apai, 2011; Ecker, Lewandowsky, Swire, &amp; Chang, 2011); in some studies, a retraction did not reduce reliance on misinformation at all (e.g., H. M. Johnson &amp; Seifert, 1994).<br /><br />When misinformation is presented through media sources, the remedy is the presentation of a correction, often in a temporally disjointed format (e.g., if an error appears in a newspaper, the correction will be printed in a subsequent edition). In laboratory studies, misinformation is often retracted immediately and within the same narrative (H. M. Johnson &amp; Seifert, 1994). Despite this temporal and contextual proximity to the misinformation, retractions are ineffective. More recent studies (Seifert, 2002) have examined whether clarifying the correction (minimizing misunderstanding) might reduce the continued influence effect. In these studies, the correction was thus strengthened to include the phrase &ldquo;paint and gas were never on the premises.&rdquo; Results showed that this enhanced negation of the presence of flammable materials backfired, making people even more likely to rely on the misinformation in their responses. Other additions to the correction were found to mitigate to a degree, but not eliminate, the continued influence effect: For example, when participants were given a rationale for how the misinformation originated, such as, &ldquo;a truckers&rsquo; strike prevented the expected delivery of the items,&rdquo; they were somewhat less likely to make references to it. Even so, the influence of the misinformation could still be detected. The wealth of studies on this phenomenon have documented its pervasive effects, showing that it is extremely difficult to return the beliefs of people who have been exposed to misinformation to a baseline similar to those of people who were never exposed to it.<br /><br />Multiple explanations have been proposed for the continued influence effect. We summarize their key assumptions next. [...]</p>\n<p><strong>Concise recommendations for practitioners<br /></strong></p>\n<p id=\"p-121\">[...] We summarize the main points from the literature in <a id=\"xref-fig-1-1\" class=\"xref-fig\" href=\"http://psi.sagepub.com/content/13/3/106.full#F1\">Figure 1</a> and in the following list of recommendations:</p>\n<ul id=\"list-2\" class=\"list-unord\">\n<li id=\"list-item-5\">\n<p id=\"p-122\">Consider what gaps in people&rsquo;s mental event models are created by debunking and fill them using an alternative explanation.</p>\n</li>\n<li id=\"list-item-6\">\n<p id=\"p-123\">Use repeated retractions to reduce the influence of misinformation, but note that the risk of a backfire effect increases                               when the original misinformation is repeated in retractions and thereby rendered more familiar.</p>\n</li>\n<li id=\"list-item-7\">\n<p id=\"p-124\">To avoid making people more familiar with misinformation (and thus risking a familiarity backfire effect), emphasize the facts                               you wish to communicate rather than the myth.</p>\n</li>\n<li id=\"list-item-8\">\n<p id=\"p-125\">Provide an explicit warning before mentioning a myth, to ensure that people are cognitively on guard and less likely to be                               influenced by the misinformation.</p>\n</li>\n<li id=\"list-item-9\">\n<p id=\"p-126\">Ensure that your material is  simple and brief. Use clear language and graphs where appropriate. If  the myth is simpler and                               more compelling than your debunking, it  will be cognitively more attractive, and you will risk an overkill  backfire effect.</p>\n</li>\n<li id=\"list-item-10\">\n<p id=\"p-127\">Consider whether your content  may be threatening to the worldview and values of your audience. If so,  you risk a worldview                               backfire effect, which is strongest among  those with firmly held beliefs. The most receptive people will be those  who are                               not strongly fixed in their views.</p>\n</li>\n<li id=\"list-item-11\">\n<p id=\"p-128\">If you must present evidence  that is threatening to the audience&rsquo;s worldview, you may be able to  reduce the worldview backfire                               effect by presenting your content in a  worldview-affirming manner (e.g., by focusing on opportunities and  potential benefits                               rather than risks and threats) and/or by  encouraging self-affirmation.</p>\n</li>\n<li id=\"list-item-12\">\n<p id=\"p-129\">You can also circumvent the role of the audience&rsquo;s worldview by focusing on behavioral techniques, such as the design of choice                               architectures, rather than overt debiasing.</p>\n</li>\n</ul>\n<p><img src=\"http://psi.sagepub.com/content/13/3/106/F1.medium.gif\" alt=\"\" width=\"425\" height=\"440\" /></p>\n<p>(<a href=\"http://psi.sagepub.com/content/13/3/106/F1.large.jpg\">large version</a>)</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "q99y5a8LjC7LDLK7r", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 18, "extendedScore": null, "score": 1.04055804349457e-06, "legacy": true, "legacyId": "20236", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><a href=\"http://psi.sagepub.com/content/13/3/106.full\">http://psi.sagepub.com/content/13/3/106.full</a></p>\n<blockquote>\n<p><em>Abstract. </em></p>\n<p id=\"p-1\">The widespread prevalence and persistence of misinformation in contemporary societies, such as the false belief that there is a link between childhood vaccinations and autism, is a matter of public concern. For example, the myths surrounding vaccinations, which prompted some parents to withhold immunization from their children, have led to a marked increase in vaccine-preventable disease, as well as unnecessary public expenditure on research and public-information campaigns aimed at rectifying the situation.</p>\n<p id=\"p-2\">We first examine the mechanisms by which such misinformation is disseminated in society, both inadvertently and purposely. Misinformation can originate from rumors but also from works of fiction, governments and politicians, and vested interests. Moreover, changes in the media landscape, including the arrival of the Internet, have fundamentally influenced the ways in which information is communicated and misinformation is spread.</p>\n<p id=\"p-3\">We next move to misinformation at the level of the individual, and review the cognitive factors that often render misinformation resistant to correction. We consider how people assess the truth of statements and what makes people believe certain things but not others. We look at people\u2019s memory for misinformation and answer the questions of why retractions of misinformation are so ineffective in memory updating and why efforts to retract misinformation can even backfire and, ironically, increase misbelief. Though ideology and personal worldviews can be major obstacles for debiasing, there nonetheless are a number of effective techniques for reducing the impact of misinformation, and we pay special attention to these factors that aid in debiasing.</p>\n<p id=\"p-4\">We conclude by providing specific recommendations for the debunking of misinformation. These recommendations pertain to the ways in which corrections should be designed, structured, and applied in order to maximize their impact. Grounded in cognitive psychological theory, these recommendations may help practitioners\u2014including journalists, health professionals, educators, and science communicators\u2014design effective misinformation retractions, educational tools, and public-information campaigns.</p>\n</blockquote>\n<p>This is a fascinating article with many, many interesting points. I'm excerpting some of them below, but mostly just to get you to read it: if I were to quote everything interesting, I'd have to pretty much copy the entire (long!) article.</p>\n<blockquote>\n<p id=\"p-23\"><strong id=\"Rumors_and_fiction\">Rumors and fiction</strong></p>\n<p>[...] A related but perhaps more surprising source of misinformation is literary fiction. People extract knowledge even from sources that are explicitly identified as fictional. This process is often adaptive, because fiction frequently contains valid information about the world. For example, non-Americans\u2019 knowledge of U.S. traditions, sports, climate, and geography partly stems from movies and novels, and many Americans know from movies that Britain and Australia have left-hand traffic. By definition, however, fiction writers are not obliged to stick to the facts, which creates an avenue for the spread of misinformation, even by stories that are explicitly identified as fictional. A study by <span class=\"xref-bibr\">Marsh, Meade, and Roediger (2003)</span> showed that people relied on misinformation acquired from clearly fictitious stories to respond to later quiz questions, even when these pieces of misinformation contradicted common knowledge. In most cases, source attribution was intact, so people were aware that their answers to the quiz questions were based on information from the stories, but reading the stories also increased people\u2019s illusory belief of prior knowledge. In other words, encountering misinformation in a fictional context led people to assume they had known it all along and to integrate this misinformation with their prior knowledge (<span class=\"xref-bibr\">Marsh &amp; Fazio, 2006</span>; <span class=\"xref-bibr\">Marsh et al., 2003</span>).</p>\n<p id=\"p-24\">The effects of fictional misinformation have been shown to be stable and difficult to eliminate. <span class=\"xref-bibr\">Marsh and Fazio (2006)</span> reported that prior warnings were ineffective in reducing the acquisition of misinformation from fiction, and that acquisition was only reduced (not eliminated) under conditions of active on-line monitoring\u2014when participants were instructed to actively monitor the contents of what they were reading and to press a key every time they encountered a piece of misinformation (see also <span class=\"xref-bibr\">Eslick, Fazio, &amp; Marsh, 2011</span>). Few people would be so alert and mindful when reading fiction for enjoyment. These links between fiction and incorrect knowledge are particularly concerning when popular fiction pretends to accurately portray science but fails to do so, as was the case with Michael Crichton\u2019s novel <em>State of Fear</em>. The novel misrepresented the science of global climate change but was nevertheless introduced as \u201cscientific\u201d evidence into a U.S. Senate committee (<span class=\"xref-bibr\">Allen, 2005</span>; <span class=\"xref-bibr\">Leggett, 2005</span>).</p>\n<p id=\"p-25\">Writers of fiction are expected to depart from reality, but in other instances, misinformation is manufactured intentionally. There is considerable peer-reviewed evidence pointing to the fact that misinformation can be intentionally or carelessly disseminated, often for political ends or in the service of vested interests, but also through routine processes employed by the media. [...]</p>\n<p><strong id=\"Assessing_the_Truth_of_a_Statement__Recipients__Strategies\">Assessing the Truth of a Statement: Recipients\u2019 Strategies</strong></p>\n<p>Misleading information rarely comes with a warning label. People usually cannot recognize that a piece of information is incorrect until they receive a correction or retraction. For better or worse, the acceptance of information as true is favored by tacit norms of everyday conversational conduct: Information relayed in conversation comes with a \u201cguarantee of relevance\u201d (Sperber &amp; Wilson, 1986), and listeners proceed on the assumption that speakers try to be truthful, relevant, and clear, unless evidence to the contrary calls this default into question (Grice, 1975; Schwarz, 1994, 1996). Some research has even suggested that to comprehend a statement, people must at least temporarily accept it as true (Gilbert, 1991). On this view, belief is an inevitable consequence of\u2014or, indeed, precursor to\u2014comprehension.<br><br>Although suspension of belief is possible (Hasson, Simmons, &amp; Todorov, 2005; Schul, Mayo, &amp; Burnstein, 2008), it seems to require a high degree of attention, considerable implausibility of the message, or high levels of distrust at the time the message is received. So, in most situations, the deck is stacked in favor of accepting information rather than rejecting it, provided there are no salient markers that call the speaker\u2019s intention of cooperative conversation into question. Going beyond this default of acceptance requires additional motivation and cognitive resources: If the topic is not very important to you, or you have other things on your mind, misinformation will likely slip in.\" [...]</p>\n<p><strong>Is the information compatible with what I believe?</strong><br><br>As numerous studies in the literature on social judgment and persuasion have shown, information is more likely to be accepted by people when it is consistent with other things they assume to be true (for reviews, see McGuire, 1972; Wyer, 1974). People assess the logical compatibility of the information with other facts and beliefs. Once a new piece of knowledge-consistent information has been accepted, it is highly resistant to change, and the more so the larger the compatible knowledge base is. From a judgment perspective, this resistance derives from the large amount of supporting evidence (Wyer, 1974); from a cognitive-consistency perspective (Festinger, 1957), it derives from the numerous downstream inconsistencies that would arise from rejecting the prior information as false. Accordingly, compatibility with other knowledge increases the likelihood that misleading information will be accepted, and decreases the likelihood that it will be successfully corrected.<br><br>When people encounter a piece of information, they can check it against other knowledge to assess its compatibility. This process is effortful, and it requires motivation and cognitive resources. A less demanding indicator of compatibility is provided by one\u2019s meta-cognitive experience and affective response to new information. Many theories of cognitive consistency converge on the assumption that information that is inconsistent with one\u2019s beliefs elicits negative feelings (Festinger, 1957). Messages that are inconsistent with one\u2019s beliefs are also processed less fluently than messages that are consistent with one\u2019s beliefs (Winkielman, Huber, Kavanagh, &amp; Schwarz, 2012). In general, fluently processed information feels more familiar and is more likely to be accepted as true; conversely, disfluency elicits the impression that something doesn\u2019t quite \u201cfeel right\u201d and prompts closer scrutiny of the message (Schwarz et al., 2007; Song &amp; Schwarz, 2008). This phenomenon is observed even when the fluent processing of a message merely results from superficial characteristics of its presentation. For example, the same statement is more likely to be judged as true when it is printed in high rather than low color contrast (Reber &amp; Schwarz, 1999), presented in a rhyming rather than nonrhyming form (McGlone &amp; Tofighbakhsh, 2000), or delivered in a familiar rather than unfamiliar accent (Levy-Ari &amp; Keysar, 2010). Moreover, misleading questions are less likely to be recognized as such when printed in an easy-to-read font (Song &amp; Schwarz, 2008).<br><br>As a result, analytic as well as intuitive processing favors the acceptance of messages that are compatible with a recipient\u2019s preexisting beliefs: The message contains no elements that contradict current knowledge, is easy to process, and \u201cfeels right.\u201d<br><br><strong>Is the story coherent?</strong><br><br>Whether a given piece of information will be accepted as true also depends on how well it fits a broader story that lends sense and coherence to its individual elements. People are particularly likely to use an assessment strategy based on this principle when the meaning of one piece of information cannot be assessed in isolation because it depends on other, related pieces; use of this strategy has been observed in basic research on mental models (for a review, see Johnson-Laird, 2012), as well as extensive analyses of juries\u2019 decision making (Pennington &amp; Hastie, 1992, 1993).<br><br>A story is compelling to the extent that it organizes information without internal contradictions in a way that is compatible with common assumptions about human motivation and behavior. Good stories are easily remembered, and gaps are filled with story-consistent intrusions. Once a coherent story has been formed, it is highly resistant to change: Within the story, each element is supported by the fit of other elements, and any alteration of an element may be made implausible by the downstream inconsistencies it would cause. Coherent stories are easier to process than incoherent stories are (Johnson-Laird, 2012), and people draw on their processing experience when they judge a story\u2019s coherence (Topolinski, 2012), again giving an advantage to material that is easy to process. [...]</p>\n<p><strong>Is the information from a credible source?</strong><br><br>[...] People\u2019s evaluation of a source\u2019s credibility can be based on declarative information, as in the above examples, as well as experiential information. The mere repetition of an unknown name can cause it to seem familiar, making its bearer \u201cfamous overnight\u201d (Jacoby, Kelley, Brown, &amp; Jaseschko, 1989)\u2014and hence more credible. Even when a message is rejected at the time of initial exposure, that initial exposure may lend it some familiarity-based credibility if the recipient hears it again.<br><br><strong>Do others believe this information?</strong><br><br>Repeated exposure to a statement is known to increase its acceptance as true (e.g., Begg, Anas, &amp; Farinacci, 1992; Hasher, Goldstein, &amp; Toppino, 1977). In a classic study of rumor transmission, Allport and Lepkin (1945) observed that the strongest predictor of belief in wartime rumors was simple repetition. Repetition effects may create a perceived social consensus even when no consensus exists. Festinger (1954) referred to social consensus as a \u201csecondary reality test\u201d: If many people believe a piece of information, there\u2019s probably something to it. Because people are more frequently exposed to widely shared beliefs than to highly idiosyncratic ones, the familiarity of a belief is often a valid indicator of social consensus. But, unfortunately, information can seem familiar for the wrong reason, leading to erroneous perceptions of high consensus. For example, Weaver, Garcia, Schwarz, and Miller (2007) exposed participants to multiple iterations of the same statement, provided by the same communicator. When later asked to estimate how widely the conveyed belief is shared, participants estimated consensus to be greater the more often they had read the identical statement from the same, single source. In a very real sense, a single repetitive voice can sound like a chorus. [...]<br><br>The extent of pluralistic ignorance (or of the false-consensus effect) can be quite striking: In Australia, people with particularly negative attitudes toward Aboriginal Australians or asylum seekers have been found to overestimate public support for their attitudes by 67% and 80%, respectively (Pedersen, Griffiths, &amp; Watt, 2008). Specifically, although only 1.8% of people in a sample of Australians were found to hold strongly negative attitudes toward Aboriginals, those few individuals thought that 69% of all Australians (and 79% of their friends) shared their fringe beliefs. This represents an extreme case of the false-consensus effect. [...]<br><br><strong>The Continued Influence Effect: Retractions Fail to Eliminate the Influence of Misinformation</strong><br><br>We first consider the cognitive parameters of credible retractions in neutral scenarios, in which people have no inherent reason or motivation to believe one version of events over another. Research on this topic was stimulated by a paradigm pioneered by Wilkes and Leatherbarrow (1988) and H. M. Johnson and Seifert (1994). In it, people are presented with a fictitious report about an event unfolding over time. The report contains a target piece of information: For some readers, this target information is subsequently retracted, whereas for readers in a control condition, no correction occurs. Participants\u2019 understanding of the event is then assessed with a questionnaire, and the number of clear and uncontroverted references to the target (mis-)information in their responses is tallied.<br><br>A stimulus narrative commonly used in this paradigm involves a warehouse fire that is initially thought to have been caused by gas cylinders and oil paints that were negligently stored in a closet (e.g., Ecker, Lewandowsky, Swire, &amp; Chang, 2011; H. M. Johnson &amp; Seifert, 1994; Wilkes &amp; Leatherbarrow, 1988). Some participants are then presented with a retraction, such as \u201cthe closet was actually empty.\u201d A comprehension test follows, and participants\u2019 number of references to the gas and paint in response to indirect inference questions about the event (e.g., \u201cWhat caused the black smoke?\u201d) is counted. In addition, participants are asked to recall some basic facts about the event and to indicate whether they noticed any retraction.<br><br>Research using this paradigm has consistently found that retractions rarely, if ever, have the intended effect of eliminating reliance on misinformation, even when people believe, understand, and later remember the retraction (e.g., Ecker, Lewandowsky, &amp; Apai, 2011; Ecker, Lewandowsky, Swire, &amp; Chang, 2011; Ecker, Lewandowsky, &amp; Tang, 2010; Fein, McCloskey, &amp; Tomlinson, 1997; Gilbert, Krull, &amp; Malone, 1990; Gilbert, Tafarodi, &amp; Malone, 1993; H. M. Johnson &amp; Seifert, 1994, 1998, 1999; Schul &amp; Mazursky, 1990; van Oostendorp, 1996; van Oostendorp &amp; Bonebakker, 1999; Wilkes &amp; Leatherbarrow, 1988; Wilkes &amp; Reynolds, 1999). In fact, a retraction will at most halve the number of references to misinformation, even when people acknowledge and demonstrably remember the retraction (Ecker, Lewandowsky, &amp; Apai, 2011; Ecker, Lewandowsky, Swire, &amp; Chang, 2011); in some studies, a retraction did not reduce reliance on misinformation at all (e.g., H. M. Johnson &amp; Seifert, 1994).<br><br>When misinformation is presented through media sources, the remedy is the presentation of a correction, often in a temporally disjointed format (e.g., if an error appears in a newspaper, the correction will be printed in a subsequent edition). In laboratory studies, misinformation is often retracted immediately and within the same narrative (H. M. Johnson &amp; Seifert, 1994). Despite this temporal and contextual proximity to the misinformation, retractions are ineffective. More recent studies (Seifert, 2002) have examined whether clarifying the correction (minimizing misunderstanding) might reduce the continued influence effect. In these studies, the correction was thus strengthened to include the phrase \u201cpaint and gas were never on the premises.\u201d Results showed that this enhanced negation of the presence of flammable materials backfired, making people even more likely to rely on the misinformation in their responses. Other additions to the correction were found to mitigate to a degree, but not eliminate, the continued influence effect: For example, when participants were given a rationale for how the misinformation originated, such as, \u201ca truckers\u2019 strike prevented the expected delivery of the items,\u201d they were somewhat less likely to make references to it. Even so, the influence of the misinformation could still be detected. The wealth of studies on this phenomenon have documented its pervasive effects, showing that it is extremely difficult to return the beliefs of people who have been exposed to misinformation to a baseline similar to those of people who were never exposed to it.<br><br>Multiple explanations have been proposed for the continued influence effect. We summarize their key assumptions next. [...]</p>\n<p><strong id=\"Concise_recommendations_for_practitioners\">Concise recommendations for practitioners<br></strong></p>\n<p id=\"p-121\">[...] We summarize the main points from the literature in <a id=\"xref-fig-1-1\" class=\"xref-fig\" href=\"http://psi.sagepub.com/content/13/3/106.full#F1\">Figure 1</a> and in the following list of recommendations:</p>\n<ul id=\"list-2\" class=\"list-unord\">\n<li id=\"list-item-5\">\n<p id=\"p-122\">Consider what gaps in people\u2019s mental event models are created by debunking and fill them using an alternative explanation.</p>\n</li>\n<li id=\"list-item-6\">\n<p id=\"p-123\">Use repeated retractions to reduce the influence of misinformation, but note that the risk of a backfire effect increases                               when the original misinformation is repeated in retractions and thereby rendered more familiar.</p>\n</li>\n<li id=\"list-item-7\">\n<p id=\"p-124\">To avoid making people more familiar with misinformation (and thus risking a familiarity backfire effect), emphasize the facts                               you wish to communicate rather than the myth.</p>\n</li>\n<li id=\"list-item-8\">\n<p id=\"p-125\">Provide an explicit warning before mentioning a myth, to ensure that people are cognitively on guard and less likely to be                               influenced by the misinformation.</p>\n</li>\n<li id=\"list-item-9\">\n<p id=\"p-126\">Ensure that your material is  simple and brief. Use clear language and graphs where appropriate. If  the myth is simpler and                               more compelling than your debunking, it  will be cognitively more attractive, and you will risk an overkill  backfire effect.</p>\n</li>\n<li id=\"list-item-10\">\n<p id=\"p-127\">Consider whether your content  may be threatening to the worldview and values of your audience. If so,  you risk a worldview                               backfire effect, which is strongest among  those with firmly held beliefs. The most receptive people will be those  who are                               not strongly fixed in their views.</p>\n</li>\n<li id=\"list-item-11\">\n<p id=\"p-128\">If you must present evidence  that is threatening to the audience\u2019s worldview, you may be able to  reduce the worldview backfire                               effect by presenting your content in a  worldview-affirming manner (e.g., by focusing on opportunities and  potential benefits                               rather than risks and threats) and/or by  encouraging self-affirmation.</p>\n</li>\n<li id=\"list-item-12\">\n<p id=\"p-129\">You can also circumvent the role of the audience\u2019s worldview by focusing on behavioral techniques, such as the design of choice                               architectures, rather than overt debiasing.</p>\n</li>\n</ul>\n<p><img src=\"http://psi.sagepub.com/content/13/3/106/F1.medium.gif\" alt=\"\" width=\"425\" height=\"440\"></p>\n<p>(<a href=\"http://psi.sagepub.com/content/13/3/106/F1.large.jpg\">large version</a>)</p>\n</blockquote>", "sections": [{"title": "Rumors and fiction", "anchor": "Rumors_and_fiction", "level": 1}, {"title": "Assessing the Truth of a Statement: Recipients\u2019 Strategies", "anchor": "Assessing_the_Truth_of_a_Statement__Recipients__Strategies", "level": 1}, {"title": "Concise recommendations for practitioners", "anchor": "Concise_recommendations_for_practitioners", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-23T10:15:47.039Z", "modifiedAt": null, "url": null, "title": "Counterfactual self-defense", "slug": "counterfactual-self-defense", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:00.508Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MrMind", "createdAt": "2011-04-19T08:43:22.388Z", "isAdmin": false, "displayName": "MrMind"}, "userId": "LJ4br8GWFXetsXkM8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ujdj5TzPMo5qeuwkR/counterfactual-self-defense", "pageUrlRelative": "/posts/ujdj5TzPMo5qeuwkR/counterfactual-self-defense", "linkUrl": "https://www.lesswrong.com/posts/ujdj5TzPMo5qeuwkR/counterfactual-self-defense", "postedAtFormatted": "Friday, November 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Counterfactual%20self-defense&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACounterfactual%20self-defense%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fujdj5TzPMo5qeuwkR%2Fcounterfactual-self-defense%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Counterfactual%20self-defense%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fujdj5TzPMo5qeuwkR%2Fcounterfactual-self-defense", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fujdj5TzPMo5qeuwkR%2Fcounterfactual-self-defense", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 432, "htmlBody": "<p>Let's imagine these following dialogues between Omega and an agent implementing TDT. Usual standard assumptions on Omega applies: the agent knows Omega is real, trustworthy and reliable, and Omega knows that the agent knows that, and the agent knows that Omega knows that the agent knows, etc. (that is, Omega's trustworthiness is common knowledge, &agrave; la Aumann).</p>\n<p>Dialogue 1.</p>\n<p>Omega: \"Would you accept a bet where I pay you 1000$ if a fair coin flip comes out tail and you pay me 100$ if it comes out head?\"<br />TDT: \"Sure I would.\"<br />Omega: \"I flipped the coin. It came out head.\"<br />TDT: \"Doh! Here's your 100$.\"</p>\n<p>I hope there's no controversy here.</p>\n<p>Dialogue 2.</p>\n<p>Omega: \"I flipped a fair coin and it came out head.\"<br />TDT: \"Yes...?\"<br />Omega: \"Would you accept a bet where I pay you 1000$ if the coin flip came out tail and you pay me 100$ if it came out head?\"<br />TDT: \"No way!\"</p>\n<p>I also hope no controversy arises: if the agent would answer yes, then there's no reason he wouldn't accept all kinds of losing bets conditioned on information it already knows.</p>\n<p>The two bets are equal, but the information is presented in different order: in the second dialogue, the agent has the time to change its knowledge about the world and should not accept bets that it already knows are losing.</p>\n<p>But then...</p>\n<p>Dialogue 3.</p>\n<p>Omega: \"I flipped a coin and it came out head. I offer you a bet where I pay you 1000$ if the coin flip comes out tail, but only if you agree to pay me 100$ if the coin flip comes out head.\"<br />TDT: \"...?\"</p>\n<p>In the <a href=\"/lw/3l/counterfactual_mugging/\" target=\"_blank\">original counterfactual discussion</a>, apparently the answer of the TDT implementing agent should have been yes, but I'm not entirely clear on what is the difference between the second and the third case.</p>\n<p>Thinking about it, it seems that the case is muddled because the outcome and the bet are presented at the same time. On one hand, it appears correct to think that an agent should act exactly how it should if it had pre-committed, but on the other hand, an agent should not ignore any information is presented (it's a basic requirement of treating probability as extended logic).</p>\n<p>So here's a principle I would like to call 'counterfactual self-defense': whenever informations and bets are presented to the agent at the same time, it always first conditions its priors and only then examines whatever bets has been offered. This should prevent Omega from offering counterfactual losing bets, but not counterfactual winning ones.</p>\n<p>Would this principle make an agent win more?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"YpHkTW27iMFR2Dkae": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ujdj5TzPMo5qeuwkR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 2, "extendedScore": null, "score": 1.0406355766705665e-06, "legacy": true, "legacyId": "20237", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mg6jDEuQEjBGtibX7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-23T11:50:11.333Z", "modifiedAt": null, "url": null, "title": "Meetup : Meetup : Bielefeld Meetup, December 5th", "slug": "meetup-meetup-bielefeld-meetup-december-5th", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:09.018Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Just_existing", "createdAt": "2012-01-16T18:40:07.392Z", "isAdmin": false, "displayName": "Just_existing"}, "userId": "o89m5G8Hk2tbpKTxg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7EtRhJQpHgBjfYnxt/meetup-meetup-bielefeld-meetup-december-5th", "pageUrlRelative": "/posts/7EtRhJQpHgBjfYnxt/meetup-meetup-bielefeld-meetup-december-5th", "linkUrl": "https://www.lesswrong.com/posts/7EtRhJQpHgBjfYnxt/meetup-meetup-bielefeld-meetup-december-5th", "postedAtFormatted": "Friday, November 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Meetup%20%3A%20Bielefeld%20Meetup%2C%20December%205th&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Meetup%20%3A%20Bielefeld%20Meetup%2C%20December%205th%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7EtRhJQpHgBjfYnxt%2Fmeetup-meetup-bielefeld-meetup-december-5th%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Meetup%20%3A%20Bielefeld%20Meetup%2C%20December%205th%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7EtRhJQpHgBjfYnxt%2Fmeetup-meetup-bielefeld-meetup-december-5th", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7EtRhJQpHgBjfYnxt%2Fmeetup-meetup-bielefeld-meetup-december-5th", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 94, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ga'>Meetup : Bielefeld Meetup, December 5th</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 December 2012 07:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Grill/Bar Verve, Klosterplatz 13, Bielefeld</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The second meetup of this group.</p>\n\n<p>We will have some topics prepared for discussion, although we don't know which yet.</p>\n\n<p>We will be delighted about every new face we see. If you want on the mailing list of this group to learn about potential topics in advance, you can write me and I will add you.</p>\n\n<p>I will have a sign.</p>\n\n<p>Everybody is welcome!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ga'>Meetup : Bielefeld Meetup, December 5th</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7EtRhJQpHgBjfYnxt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.040688790777271e-06, "legacy": true, "legacyId": "20238", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Meetup___Bielefeld_Meetup__December_5th\">Discussion article for the meetup : <a href=\"/meetups/ga\">Meetup : Bielefeld Meetup, December 5th</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 December 2012 07:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Grill/Bar Verve, Klosterplatz 13, Bielefeld</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>The second meetup of this group.</p>\n\n<p>We will have some topics prepared for discussion, although we don't know which yet.</p>\n\n<p>We will be delighted about every new face we see. If you want on the mailing list of this group to learn about potential topics in advance, you can write me and I will add you.</p>\n\n<p>I will have a sign.</p>\n\n<p>Everybody is welcome!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Meetup___Bielefeld_Meetup__December_5th1\">Discussion article for the meetup : <a href=\"/meetups/ga\">Meetup : Bielefeld Meetup, December 5th</a></h2>", "sections": [{"title": "Discussion article for the meetup : Meetup : Bielefeld Meetup, December 5th", "anchor": "Discussion_article_for_the_meetup___Meetup___Bielefeld_Meetup__December_5th", "level": 1}, {"title": "Discussion article for the meetup : Meetup : Bielefeld Meetup, December 5th", "anchor": "Discussion_article_for_the_meetup___Meetup___Bielefeld_Meetup__December_5th1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-23T14:12:03.994Z", "modifiedAt": null, "url": null, "title": "Breakdown of existential risks", "slug": "breakdown-of-existential-risks", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:05.824Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Y7orH8Lo9HDMcM7gN/breakdown-of-existential-risks", "pageUrlRelative": "/posts/Y7orH8Lo9HDMcM7gN/breakdown-of-existential-risks", "linkUrl": "https://www.lesswrong.com/posts/Y7orH8Lo9HDMcM7gN/breakdown-of-existential-risks", "postedAtFormatted": "Friday, November 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Breakdown%20of%20existential%20risks&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABreakdown%20of%20existential%20risks%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY7orH8Lo9HDMcM7gN%2Fbreakdown-of-existential-risks%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Breakdown%20of%20existential%20risks%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY7orH8Lo9HDMcM7gN%2Fbreakdown-of-existential-risks", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FY7orH8Lo9HDMcM7gN%2Fbreakdown-of-existential-risks", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 6, "htmlBody": "<p>Due to my colleague, Anders Sandberg:</p>\n<p>&nbsp;</p>\n<p><img src=\"http://images.lesswrong.com/t3_fm7_0.png?v=99545af9d1478d75d1959f3fbaeb7852\" alt=\"\" width=\"100%\" height=\"auto\" /></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Y7orH8Lo9HDMcM7gN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 24, "extendedScore": null, "score": 1.040768773397344e-06, "legacy": true, "legacyId": "20239", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-23T14:20:17.593Z", "modifiedAt": null, "url": null, "title": "Request for community insight", "slug": "request-for-community-insight", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:02.318Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Caerbannog", "createdAt": "2011-04-30T02:54:30.643Z", "isAdmin": false, "displayName": "Caerbannog"}, "userId": "2AHG8DKQeWwxMCvK9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HdxHDNR2LFPPASqPs/request-for-community-insight", "pageUrlRelative": "/posts/HdxHDNR2LFPPASqPs/request-for-community-insight", "linkUrl": "https://www.lesswrong.com/posts/HdxHDNR2LFPPASqPs/request-for-community-insight", "postedAtFormatted": "Friday, November 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Request%20for%20community%20insight&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARequest%20for%20community%20insight%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHdxHDNR2LFPPASqPs%2Frequest-for-community-insight%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Request%20for%20community%20insight%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHdxHDNR2LFPPASqPs%2Frequest-for-community-insight", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHdxHDNR2LFPPASqPs%2Frequest-for-community-insight", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 318, "htmlBody": "<p>Hi LW. I think that this community's insight could help me with my problem. I may have an undiagnosed medical condition, and I wanted to present it here in order to get some ideas, either on what it is, or what I should try to do to figure it out.</p>\n<p>I'm a relatively active and relatively healthy-eating 36 yo male, but over the last 5.5 years I've had a bunch of musculo-skeletal problems (joints, tendons, muscles) each lasting a long time. Each of these by themselves would not be that unusual, but most have not resolved, and they are just so widespread:</p>\n<p>- r ankle (1 yr. minor catching, occasional worse pain)</p>\n<p>&nbsp;- both knees (5 yr. major pain, catching, crepitus)</p>\n<p>- both hips, (1 yr. major pain, better right now, but not gone)</p>\n<p>- r shoulder (major weakness resolved after 2y by aggressive stretching)</p>\n<p>- neck (6 mo., helped by stretching upper back , but still a problem: can't swim)</p>\n<p>- l elbow (3yr. minor pain and weakness)</p>\n<p>Some (maybe all) of these feel like they could be due to muscles getting so tight or short that they overtax the connected tendons/bones/cartilage.</p>\n<p>The knees are the worst right now, though the hips were just as bad. I'm not sure what made the hips get better, possibly strengthening hip abductors.</p>\n<p>Additionally, approximately at the time this started, I became lactose intolerant, but am currently okay. I also started getting springtime allergies for the first time 5.5 yrs ago.</p>\n<p>I started a gluten-free diet a couple of weeks ago, and have also stopped all exercise for the last 3 weeks, but no effect so far. Because I've been active and athletic for so long, the physical problems are a major impact on my life and have a huge negative impact on my mood. I've never felt worse in my life, honestly.</p>\n<p>I'm presenting this here because I know the reponses I get will be well thought out.</p>\n<p>I appreciate your time. Thanks!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HdxHDNR2LFPPASqPs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": -4, "extendedScore": null, "score": 1.040773411442992e-06, "legacy": true, "legacyId": "20240", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-23T17:18:04.087Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Austin, Chicago, Cincinnati/Columbus, Cleveland/Akron, Durham, London, Moscow, Paderborn, Salt Lake City, Washington DC", "slug": "weekly-lw-meetups-austin-chicago-cincinnati-columbus", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ba7zs4f9LTpEC3H7r/weekly-lw-meetups-austin-chicago-cincinnati-columbus", "pageUrlRelative": "/posts/Ba7zs4f9LTpEC3H7r/weekly-lw-meetups-austin-chicago-cincinnati-columbus", "linkUrl": "https://www.lesswrong.com/posts/Ba7zs4f9LTpEC3H7r/weekly-lw-meetups-austin-chicago-cincinnati-columbus", "postedAtFormatted": "Friday, November 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Austin%2C%20Chicago%2C%20Cincinnati%2FColumbus%2C%20Cleveland%2FAkron%2C%20Durham%2C%20London%2C%20Moscow%2C%20Paderborn%2C%20Salt%20Lake%20City%2C%20Washington%20DC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Austin%2C%20Chicago%2C%20Cincinnati%2FColumbus%2C%20Cleveland%2FAkron%2C%20Durham%2C%20London%2C%20Moscow%2C%20Paderborn%2C%20Salt%20Lake%20City%2C%20Washington%20DC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBa7zs4f9LTpEC3H7r%2Fweekly-lw-meetups-austin-chicago-cincinnati-columbus%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Austin%2C%20Chicago%2C%20Cincinnati%2FColumbus%2C%20Cleveland%2FAkron%2C%20Durham%2C%20London%2C%20Moscow%2C%20Paderborn%2C%20Salt%20Lake%20City%2C%20Washington%20DC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBa7zs4f9LTpEC3H7r%2Fweekly-lw-meetups-austin-chicago-cincinnati-columbus", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBa7zs4f9LTpEC3H7r%2Fweekly-lw-meetups-austin-chicago-cincinnati-columbus", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 546, "htmlBody": "<p><strong>This summary was posted to LW main on Nov 16th, and has been moved to discussion. The following week's summary is <a href=\"/lw/fm9/weekly_lw_meetups_austin_brussels_montreal/\">here</a>.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/fm\">Moscow: Applied Rationality and Cognitive Biases:&nbsp;<span class=\"date\">17 November 2012 04:00PM</span></a></li>\n<li><a href=\"/meetups/fx\">Durham HPMoR Discussion, chapters 15-17:&nbsp;<span class=\"date\">17 November 2012 11:00AM</span></a></li>\n<li><a href=\"/meetups/fw\">Chicago Organizational Meeting and Open Discussion:&nbsp;<span class=\"date\">17 November 2012 01:00PM</span></a></li>\n<li><a href=\"/meetups/fb\">First Meetup- Cleveland and Akron, Ohio:&nbsp;<span class=\"date\">17 November 2012 08:03PM</span></a></li>\n<li><a href=\"/meetups/fc\">18/11 London Meetup:&nbsp;<span class=\"date\">18 November 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/g0\">DC Meetup: Boardgames:&nbsp;<span class=\"date\">18 November 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/ft\">Paderborn Meetup, November 22th:&nbsp;<span class=\"date\">22 November 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/fo\">(St. Louis MO) Psycology of memorization, Thiel Fellow James Koppel, and games:&nbsp;<span class=\"date\">24 November 2012 02:30PM</span></a></li>\n<li><a href=\"/meetups/el\">Sofia, Bulgaria Meetup:&nbsp;<span class=\"date\">09 December 2012 05:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">17 November 2018 01:30PM</span></a></li>\n<li><a href=\"/meetups/fz\">Salt Lake City Monthly meetup:&nbsp;<span class=\"date\">17 November 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/fy\">Cincinnati/Columbus: Memorisation exercise (NB, time changed):&nbsp;<span class=\"date\">18 November 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/en\">Winter Solstice Megameetup - NYC:&nbsp;<span class=\"date\">15 December 2012 05:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong></strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ba7zs4f9LTpEC3H7r", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0408736470327368e-06, "legacy": true, "legacyId": "20124", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["cjPu5FfwBBFaqwcDv", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-23T19:08:59.983Z", "modifiedAt": null, "url": null, "title": "Meetup : Phoenix, AZ: Stoicism and Visitors: 6 December 6:00PM", "slug": "meetup-phoenix-az-stoicism-and-visitors-6-december-6-00pm", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:04.596Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Danny_Hintze", "createdAt": "2010-12-04T23:01:40.826Z", "isAdmin": false, "displayName": "Danny_Hintze"}, "userId": "2fHm6t2WFDMPShg5b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TSQr7yRoMPRRvh5sH/meetup-phoenix-az-stoicism-and-visitors-6-december-6-00pm", "pageUrlRelative": "/posts/TSQr7yRoMPRRvh5sH/meetup-phoenix-az-stoicism-and-visitors-6-december-6-00pm", "linkUrl": "https://www.lesswrong.com/posts/TSQr7yRoMPRRvh5sH/meetup-phoenix-az-stoicism-and-visitors-6-december-6-00pm", "postedAtFormatted": "Friday, November 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Phoenix%2C%20AZ%3A%20Stoicism%20and%20Visitors%3A%206%20December%206%3A00PM&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Phoenix%2C%20AZ%3A%20Stoicism%20and%20Visitors%3A%206%20December%206%3A00PM%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTSQr7yRoMPRRvh5sH%2Fmeetup-phoenix-az-stoicism-and-visitors-6-december-6-00pm%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Phoenix%2C%20AZ%3A%20Stoicism%20and%20Visitors%3A%206%20December%206%3A00PM%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTSQr7yRoMPRRvh5sH%2Fmeetup-phoenix-az-stoicism-and-visitors-6-december-6-00pm", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTSQr7yRoMPRRvh5sH%2Fmeetup-phoenix-az-stoicism-and-visitors-6-december-6-00pm", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 65, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/gb\">Phoenix, AZ: Stoicism and Visitors: 5 December 6:00PM</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">06 December 2012 06:00:00PM (-0700)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">1038 South Mill Avenue, Tempe, AZ</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Jay will give a brief presentation on Stoicism and we will get to see some friends from Australia! The meeting will be at the Chipotle near Gammage Theater.</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/gb\">Phoenix, AZ: Stoicism and Visitors: 5 December 6:00PM</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TSQr7yRoMPRRvh5sH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0409362027325534e-06, "legacy": true, "legacyId": "20242", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Phoenix__AZ__Stoicism_and_Visitors__5_December_6_00PM\">Discussion article for the meetup : <a href=\"/meetups/gb\">Phoenix, AZ: Stoicism and Visitors: 5 December 6:00PM</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">06 December 2012 06:00:00PM (-0700)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">1038 South Mill Avenue, Tempe, AZ</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Jay will give a brief presentation on Stoicism and we will get to see some friends from Australia! The meeting will be at the Chipotle near Gammage Theater.</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Phoenix__AZ__Stoicism_and_Visitors__5_December_6_00PM1\">Discussion article for the meetup : <a href=\"/meetups/gb\">Phoenix, AZ: Stoicism and Visitors: 5 December 6:00PM</a></h2>", "sections": [{"title": "Discussion article for the meetup : Phoenix, AZ: Stoicism and Visitors: 5 December 6:00PM", "anchor": "Discussion_article_for_the_meetup___Phoenix__AZ__Stoicism_and_Visitors__5_December_6_00PM", "level": 1}, {"title": "Discussion article for the meetup : Phoenix, AZ: Stoicism and Visitors: 5 December 6:00PM", "anchor": "Discussion_article_for_the_meetup___Phoenix__AZ__Stoicism_and_Visitors__5_December_6_00PM1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-23T20:32:53.247Z", "modifiedAt": null, "url": null, "title": "Musk, Mars and x-risk ", "slug": "musk-mars-and-x-risk", "viewCount": null, "lastCommentedAt": "2022-01-14T15:10:12.397Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kawoomba", "createdAt": "2012-05-01T11:54:25.423Z", "isAdmin": false, "displayName": "Kawoomba"}, "userId": "FScr44PGNPbBCodRv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TWeFMvHM9JFaenZPx/musk-mars-and-x-risk", "pageUrlRelative": "/posts/TWeFMvHM9JFaenZPx/musk-mars-and-x-risk", "linkUrl": "https://www.lesswrong.com/posts/TWeFMvHM9JFaenZPx/musk-mars-and-x-risk", "postedAtFormatted": "Friday, November 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Musk%2C%20Mars%20and%20x-risk%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMusk%2C%20Mars%20and%20x-risk%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTWeFMvHM9JFaenZPx%2Fmusk-mars-and-x-risk%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Musk%2C%20Mars%20and%20x-risk%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTWeFMvHM9JFaenZPx%2Fmusk-mars-and-x-risk", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTWeFMvHM9JFaenZPx%2Fmusk-mars-and-x-risk", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 194, "htmlBody": "<p><a href=\"http://www.space.com/18596-mars-colony-spacex-elon-musk.html\" target=\"_blank\">Article on space.com</a></p>\n<blockquote>\n<p>Elon Musk, the billionaire founder and CEO of the private spaceflight company SpaceX, wants to help establish a Mars colony of up to 80,000 people by ferrying explorers to the Red Planet for perhaps $500,000 a trip.</p>\n</blockquote>\n<div>\n<div>Relevant Sagan quote:</div>\n</div>\n<div><br /></div>\n<div>\n<blockquote>\n<div>(...) we've put all our eggs in one basket. If we were on many worlds and were to mess up down here, there's a way for the human species to continue. I don't for a moment propose that the Earth is a disposable planet, and we have to put enormous efforts into making sure we don't muss up down here. But there is a chance.</div>\n</blockquote>\n</div>\n<div><br /></div>\n<div>\n<div>This should also at least somewhat reduce the x-risk stemming from uFAI (a subset of uFAI's may not concern themselves with space travel), and may significantly reduce the x-risk posed by many other x-risk categories (bioengineered threats, catastrophic climate change, global nuclear catastrophe, grey goo scenarios, etcetera).</div>\n<div><br /></div>\n<div>For total x-risk reduction, prima facie it's unclear to me how supporting an endeavor such as Musk's stacks up against the SI's effectiveness, measured by total x-risk reduction per donated currency unit.</div>\n<div><br /></div>\n<div>What's your take?</div>\n<div><br /></div>\n<div><br /></div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2JdCpTrNgBMNpJiyB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TWeFMvHM9JFaenZPx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 14, "extendedScore": null, "score": 1.0409835124765085e-06, "legacy": true, "legacyId": "20243", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 65, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-23T22:25:44.439Z", "modifiedAt": null, "url": null, "title": "[Link] 12 Myths about Hunger", "slug": "link-12-myths-about-hunger", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:59.917Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Curiouskid", "createdAt": "2011-05-13T23:30:04.967Z", "isAdmin": false, "displayName": "Curiouskid"}, "userId": "Y4xxvuP743fwKcZQY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/adf2jZNJWn6mDENGm/link-12-myths-about-hunger", "pageUrlRelative": "/posts/adf2jZNJWn6mDENGm/link-12-myths-about-hunger", "linkUrl": "https://www.lesswrong.com/posts/adf2jZNJWn6mDENGm/link-12-myths-about-hunger", "postedAtFormatted": "Friday, November 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%2012%20Myths%20about%20Hunger&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%2012%20Myths%20about%20Hunger%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fadf2jZNJWn6mDENGm%2Flink-12-myths-about-hunger%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%2012%20Myths%20about%20Hunger%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fadf2jZNJWn6mDENGm%2Flink-12-myths-about-hunger", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fadf2jZNJWn6mDENGm%2Flink-12-myths-about-hunger", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1997, "htmlBody": "<p>Copy and pasted from <a href=\"http://www.foodfirst.org/pubs/backgrdrs/1998/s98v5n3.html\">here</a>.For those interested, there's an book that expands on the article.<br />EDIT: people seem to like the first few myths, but they get rather political as the article goes on. <br />I couldn't find much on hunger on GiveWell's site other than <a href=\"http://www.givewell.org/international/charities/freedom-from-hunger\">these</a> <a href=\"http://blog.givewell.org/2009/11/26/hunger-here-vs-hunger-there/\">three</a> <a href=\"http://www.givewell.org/international/charities/Action-Against-Hunger-ACF\">articles</a>. <br />The Advanced Civilization Wiki's page on <a href=\"http://www.adciv.org/Food\">Food</a> probably covers the good aspects of this article and expands on them.</p>\n<p><span style=\"font-size: x-small;\"><br /><strong>Why so much hunger?<br /><br /> What can we do about it?</strong></span></p>\n<p>To answer these questions we must unlearn much of what we have been taught.</p>\n<p>Only by freeing ourselves from the grip of widely held myths can we grasp the roots of hunger and see what we can do to end it.</p>\n<h3>Myth 1</h3>\n<p><strong>Not Enough Food to Go Around</strong></p>\n<p><strong>Reality:</strong> Abundance, not scarcity, best describes the world's food supply. Enough wheat, rice and other grains are produced to provide every human being with 3,500 calories a day. That doesn't even count many other commonly eaten foods - vegetables, beans, nuts, root crops, fruits, grass-fed meats, and fish. Enough food is available to provide at least 4.3 pounds of food per person a day worldwide: two and half pounds of grain, beans and nuts, about a pound of fruits and vegetables, and nearly another pound of meat, milk and eggs-enough to make most people fat! The problem is that many people are too poor to buy readily available food. Even most \"hungry countries\" have enough food for all their people right now. Many are net exporters of food and other agricultural products.</p>\n<h3>Myth 2</h3>\n<p><strong>Nature's to Blame for Famine</strong></p>\n<p><strong>Reality:</strong> It's too easy to blame nature. Human-made forces are making people increasingly vulnerable to nature's vagaries. Food is always available for those who can afford it&mdash;starvation during hard times hits only the poorest. Millions live on the brink of disaster in south Asia, Africa and elsewhere, because they are deprived of land by a powerful few, trapped in the unremitting grip of debt, or miserably paid. Natural events rarely explain deaths; they are simply the final push over the brink. Human institutions and policies determine who eats and who starves during hard times. Likewise, in America many homeless die from the cold every winter, yet ultimate responsibility doesn't lie with the weather. The real culprits are an economy that fails to offer everyone opportunities, and a society that places economic efficiency over compassion.</p>\n<h3>Myth 3</h3>\n<p><strong>Too Many People</strong></p>\n<p><strong>Reality:</strong> Birth rates are falling rapidly worldwide as remaining regions of the Third World begin the demographic transition&mdash;when birth rates drop in response to an earlier decline in death rates. Although rapid population growth remains a serious concern in many countries, nowhere does population density explain hunger. For every Bangladesh, a densely populated and hungry country, we find a Nigeria, Brazil or Bolivia, where abundant food resources coexist with hunger. Costa Rica, with only half of Honduras' cropped acres per person, boasts a life expectancy&mdash;one indicator of nutrition &mdash;11 years longer than that of Honduras and close to that of developed countries. Rapid population growth is not the root cause of hunger. Like hunger itself, it results from underlying inequities that deprive people, especially poor women, of economic opportunity and security. Rapid population growth and hunger are endemic to societies where land ownership, jobs, education, health care, and old age security are beyond the reach of most people. Those Third World societies with dramatically successful early and rapid reductions of population growth rates-China, Sri Lanka, Colombia, Cuba and the Indian state of Kerala-prove that the lives of the poor, especially poor women, must improve before they can choose to have fewer children.</p>\n<h3>Myth 4</h3>\n<p><strong>The Environment vs. More Food?</strong></p>\n<p><strong>Reality:</strong> We should be alarmed that an environmental crisis is undercutting our food-production resources, but a tradeoff between our environment and the world's need for food is not inevitable. Efforts to feed the hungry are not causing the environmental crisis. Large corporations are mainly responsible for deforestation-creating and profiting from developed-country consumer demand for tropical hardwoods and exotic or out-of-season food items. Most pesticides used in the Third World are applied to export crops, playing little role in feeding the hungry, while in the U.S. they are used to give a blemish-free cosmetic appearance to produce, with no improvement in nutritional value.</p>\n<p>Alternatives exist now and many more are possible. The success of organic farmers in the U.S. gives a glimpse of the possibilities. Cuba's recent success in overcoming a food crisis through self-reliance and sustainable, virtually pesticide-free agriculture is another good example. Indeed, environmentally sound agricultural alternatives can be more productive than environmentally destructive ones.</p>\n<h3>Myth 5</h3>\n<p><strong>The Green Revolution is the Answer</strong></p>\n<p><strong>Reality:</strong> The production advances of the Green Revolution are no myth. Thanks to the new seeds, million of tons more grain a year are being harvested. But focusing narrowly on increasing production cannot alleviate hunger because it fails to alter the tightly concentrated distribution of economic power that determines who can buy the additional food. That's why in several of the biggest Green Revolution successes&mdash;India, Mexico, and the Philippines&mdash;grain production and in some cases, exports, have climbed, while hunger has persisted and the long-term productive capacity of the soil is degraded. Now we must fight the prospect of a 'New Green Revolution' based on biotechnology, which threatens to further accentuate inequality.</p>\n<h3>Myth 6</h3>\n<p><strong>We Need Large Farms</strong></p>\n<p><strong>Reality:</strong> Large landowners who control most of the best land often leave much of it idle. Unjust farming systems leave farmland in the hands of the most inefficient producers. By contrast, small farmers typically achieve at least four to five times greater output per acre, in part because they work their land more intensively and use integrated, and often more sustainable, production systems. Without secure tenure, the many millions of tenant farmers in the Third World have little incentive to invest in land improvements, to rotate crops, or to leave land fallow for the sake of long-term soil fertility. Future food production is undermined. On the other hand, redistribution of land can favor production. Comprehensive land reform has markedly increased production in countries as diverse as Japan, Zimbabwe, and Taiwan. A World Bank study of northeast Brazil estimates that redistributing farmland into smaller holdings would raise output an astonishing 80 percent.</p>\n<h3>Myth 7</h3>\n<p><strong>The Free Market Can End Hunger</strong></p>\n<p><strong>Reality:</strong> Unfortunately, such a \"market-is-good, government-is-bad\" formula can never help address the causes of hunger. Such a dogmatic stance misleads us that a society can opt for one or the other, when in fact every economy on earth combines the market and government in allocating resources and distributing goods. The market's marvelous efficiencies can only work to eliminate hunger, however, when purchasing power is widely dispersed.</p>\n<p>So all those who believe in the usefulness of the market and the necessity of ending hunger must concentrate on promoting not the market, but the consumers! In this task, government has a vital role to play in countering the tendency toward economic concentration, through genuine tax, credit, and land reforms to disperse buying power toward the poor. Recent trends toward privatization and de-regulation are most definitely not the answer.</p>\n<h3>Myth 8</h3>\n<p><strong>Free Trade is the Answer</strong></p>\n<p><strong>Reality:</strong> The trade promotion formula has proven an abject failure at alleviating hunger. In most Third World countries exports have boomed while hunger has continued unabated or actually worsened. While soybean exports boomed in Brazil-to feed Japanese and European livestock-hunger spread from one-third to two-thirds of the population. Where the majority of people have been made too poor to buy the food grown on their own country's soil, those who control productive resources will, not surprisingly, orient their production to more lucrative markets abroad. Export crop production squeezes out basic food production. Pro-trade policies like NAFTA and GATT pit working people in different countries against each other in a 'race to the bottom,' where the basis of competition is who will work for less, without adequate health coverage or minimum environmental standards. Mexico and the U.S. are a case in point: since NAFTA we have had a net loss of 250,000 jobs here, while Mexico has lost 2 million, and hunger is on the rise in both countries.</p>\n<h3>Myth 9</h3>\n<p><strong>Too Hungry to Fight for Their Rights</strong></p>\n<p><strong>Reality:</strong> Bombarded with images of poor people as weak and hungry, we lose sight of the obvious: for those with few resources, mere survival requires tremendous effort. If the poor were truly passive, few of them could even survive. Around the world, from the Zapatistas in Chiapas, Mexico, to the farmers' movement in India, wherever people are suffering needlessly, movements for change are underway. People will feed themselves, if allowed to do so. It's not our job to 'set things right' for others. Our responsibility is to remove the obstacles in their paths, obstacles often created by large corporations and U.S. government, World Bank and IMF policies.</p>\n<h3>Myth 10</h3>\n<p><strong>More U.S. Aid Will Help the Hungry</strong></p>\n<p><strong>Reality:</strong> Most U.S. aid works directly against the hungry. Foreign aid can only reinforce, not change, the status quo. Where governments answer only to elites, our aid not only fails to reach hungry people, it shores up the very forces working against them. Our aid is used to impose free trade and free market policies, to promote exports at the expense of food production, and to provide the armaments that repressive governments use to stay in power. Even emergency, or humanitarian aid, which makes up only five percent of the total, often ends up enriching American grain companies while failing to reach the hungry, and it can dangerously undercut local food production in the recipient country. It would be better to use our foreign aid budget for unconditional debt relief, as it is the foreign debt burden that forces most Third World countries to cut back on basic health, education and anti-poverty<br /> programs.</p>\n<h3>Myth 11</h3>\n<p><strong>We Benefit From Their Poverty</strong></p>\n<p><strong>Reality:</strong> The biggest threat to the well-being of the vast majority of Americans is not the advancement but the continued deprivation of the hungry. Low wages-both abroad and in inner cities at home-may mean cheaper bananas, shirts, computers and fast food for most Americans, but in other ways we pay heavily for hunger and poverty. Enforced poverty in the Third World jeopardizes U.S. jobs, wages and working conditions as corporations seek cheaper labor abroad. In a global economy, what American workers have achieved in employment, wage levels, and working conditions can be protected only when working people in every country are freed from economic desperation.</p>\n<p>Here at home, policies like welfare reform throw more people into the job market than can be absorbed-at below minimum wage levels in the case of 'workfare'-which puts downward pressure on the wages of those on higher rungs of the employment ladder. The growing numbers of 'working poor' are those who have part- or full-time low wage jobs yet cannot afford adequate nutrition or housing for their families. Educating ourselves about the common interests most Americans share with the poor in the Third World and at home allows us to be compassionate without sliding into pity. In working to clear the way for the poor to free themselves from economic oppression, we free ourselves as well.</p>\n<h3>Myth 12</h3>\n<p><strong>Curtail Freedom to End Hunger?</strong></p>\n<p><strong>Reality:</strong> There is no theoretical or practical reason why freedom, taken to mean civil liberties, should be incompatible with ending hunger. Surveying the globe, we see no correlation between hunger and civil liberties. However, one narrow definition of freedom-the right to unlimited accumulation of wealth-producing property and the right to use that property however one sees fit-is in fundamental conflict with ending hunger. By contrast, a definition of freedom more consistent with our nation's dominant founding vision holds that economic security for all is the guarantor of our liberty. Such an understanding of freedom is essential to ending hunger.</p>\n<p><br /><strong><span>12 Myths About Hunger based on <em>World Hunger: 12 Myths</em>, 2nd Edition, by Frances Moore Lapp&eacute;, Joseph Collins and Peter Rosset, with Luis Esparza (fully revised and updated, Grove/Atlantic and Food First Books, Oct. 1998)</span></strong></p>\n<p><em>Institute for Food and Development Policy Backgrounder</em></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "adf2jZNJWn6mDENGm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": -19, "extendedScore": null, "score": -4e-05, "legacy": true, "legacyId": "20245", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Copy and pasted from <a href=\"http://www.foodfirst.org/pubs/backgrdrs/1998/s98v5n3.html\">here</a>.For those interested, there's an book that expands on the article.<br>EDIT: people seem to like the first few myths, but they get rather political as the article goes on. <br>I couldn't find much on hunger on GiveWell's site other than <a href=\"http://www.givewell.org/international/charities/freedom-from-hunger\">these</a> <a href=\"http://blog.givewell.org/2009/11/26/hunger-here-vs-hunger-there/\">three</a> <a href=\"http://www.givewell.org/international/charities/Action-Against-Hunger-ACF\">articles</a>. <br>The Advanced Civilization Wiki's page on <a href=\"http://www.adciv.org/Food\">Food</a> probably covers the good aspects of this article and expands on them.</p>\n<p><span style=\"font-size: x-small;\"><br><strong>Why so much hunger?<br><br> What can we do about it?</strong></span></p>\n<p>To answer these questions we must unlearn much of what we have been taught.</p>\n<p>Only by freeing ourselves from the grip of widely held myths can we grasp the roots of hunger and see what we can do to end it.</p>\n<h3 id=\"Myth_1\">Myth 1</h3>\n<p><strong id=\"Not_Enough_Food_to_Go_Around\">Not Enough Food to Go Around</strong></p>\n<p><strong>Reality:</strong> Abundance, not scarcity, best describes the world's food supply. Enough wheat, rice and other grains are produced to provide every human being with 3,500 calories a day. That doesn't even count many other commonly eaten foods - vegetables, beans, nuts, root crops, fruits, grass-fed meats, and fish. Enough food is available to provide at least 4.3 pounds of food per person a day worldwide: two and half pounds of grain, beans and nuts, about a pound of fruits and vegetables, and nearly another pound of meat, milk and eggs-enough to make most people fat! The problem is that many people are too poor to buy readily available food. Even most \"hungry countries\" have enough food for all their people right now. Many are net exporters of food and other agricultural products.</p>\n<h3 id=\"Myth_2\">Myth 2</h3>\n<p><strong id=\"Nature_s_to_Blame_for_Famine\">Nature's to Blame for Famine</strong></p>\n<p><strong>Reality:</strong> It's too easy to blame nature. Human-made forces are making people increasingly vulnerable to nature's vagaries. Food is always available for those who can afford it\u2014starvation during hard times hits only the poorest. Millions live on the brink of disaster in south Asia, Africa and elsewhere, because they are deprived of land by a powerful few, trapped in the unremitting grip of debt, or miserably paid. Natural events rarely explain deaths; they are simply the final push over the brink. Human institutions and policies determine who eats and who starves during hard times. Likewise, in America many homeless die from the cold every winter, yet ultimate responsibility doesn't lie with the weather. The real culprits are an economy that fails to offer everyone opportunities, and a society that places economic efficiency over compassion.</p>\n<h3 id=\"Myth_3\">Myth 3</h3>\n<p><strong id=\"Too_Many_People\">Too Many People</strong></p>\n<p><strong>Reality:</strong> Birth rates are falling rapidly worldwide as remaining regions of the Third World begin the demographic transition\u2014when birth rates drop in response to an earlier decline in death rates. Although rapid population growth remains a serious concern in many countries, nowhere does population density explain hunger. For every Bangladesh, a densely populated and hungry country, we find a Nigeria, Brazil or Bolivia, where abundant food resources coexist with hunger. Costa Rica, with only half of Honduras' cropped acres per person, boasts a life expectancy\u2014one indicator of nutrition \u201411 years longer than that of Honduras and close to that of developed countries. Rapid population growth is not the root cause of hunger. Like hunger itself, it results from underlying inequities that deprive people, especially poor women, of economic opportunity and security. Rapid population growth and hunger are endemic to societies where land ownership, jobs, education, health care, and old age security are beyond the reach of most people. Those Third World societies with dramatically successful early and rapid reductions of population growth rates-China, Sri Lanka, Colombia, Cuba and the Indian state of Kerala-prove that the lives of the poor, especially poor women, must improve before they can choose to have fewer children.</p>\n<h3 id=\"Myth_4\">Myth 4</h3>\n<p><strong id=\"The_Environment_vs__More_Food_\">The Environment vs. More Food?</strong></p>\n<p><strong>Reality:</strong> We should be alarmed that an environmental crisis is undercutting our food-production resources, but a tradeoff between our environment and the world's need for food is not inevitable. Efforts to feed the hungry are not causing the environmental crisis. Large corporations are mainly responsible for deforestation-creating and profiting from developed-country consumer demand for tropical hardwoods and exotic or out-of-season food items. Most pesticides used in the Third World are applied to export crops, playing little role in feeding the hungry, while in the U.S. they are used to give a blemish-free cosmetic appearance to produce, with no improvement in nutritional value.</p>\n<p>Alternatives exist now and many more are possible. The success of organic farmers in the U.S. gives a glimpse of the possibilities. Cuba's recent success in overcoming a food crisis through self-reliance and sustainable, virtually pesticide-free agriculture is another good example. Indeed, environmentally sound agricultural alternatives can be more productive than environmentally destructive ones.</p>\n<h3 id=\"Myth_5\">Myth 5</h3>\n<p><strong id=\"The_Green_Revolution_is_the_Answer\">The Green Revolution is the Answer</strong></p>\n<p><strong>Reality:</strong> The production advances of the Green Revolution are no myth. Thanks to the new seeds, million of tons more grain a year are being harvested. But focusing narrowly on increasing production cannot alleviate hunger because it fails to alter the tightly concentrated distribution of economic power that determines who can buy the additional food. That's why in several of the biggest Green Revolution successes\u2014India, Mexico, and the Philippines\u2014grain production and in some cases, exports, have climbed, while hunger has persisted and the long-term productive capacity of the soil is degraded. Now we must fight the prospect of a 'New Green Revolution' based on biotechnology, which threatens to further accentuate inequality.</p>\n<h3 id=\"Myth_6\">Myth 6</h3>\n<p><strong id=\"We_Need_Large_Farms\">We Need Large Farms</strong></p>\n<p><strong>Reality:</strong> Large landowners who control most of the best land often leave much of it idle. Unjust farming systems leave farmland in the hands of the most inefficient producers. By contrast, small farmers typically achieve at least four to five times greater output per acre, in part because they work their land more intensively and use integrated, and often more sustainable, production systems. Without secure tenure, the many millions of tenant farmers in the Third World have little incentive to invest in land improvements, to rotate crops, or to leave land fallow for the sake of long-term soil fertility. Future food production is undermined. On the other hand, redistribution of land can favor production. Comprehensive land reform has markedly increased production in countries as diverse as Japan, Zimbabwe, and Taiwan. A World Bank study of northeast Brazil estimates that redistributing farmland into smaller holdings would raise output an astonishing 80 percent.</p>\n<h3 id=\"Myth_7\">Myth 7</h3>\n<p><strong id=\"The_Free_Market_Can_End_Hunger\">The Free Market Can End Hunger</strong></p>\n<p><strong>Reality:</strong> Unfortunately, such a \"market-is-good, government-is-bad\" formula can never help address the causes of hunger. Such a dogmatic stance misleads us that a society can opt for one or the other, when in fact every economy on earth combines the market and government in allocating resources and distributing goods. The market's marvelous efficiencies can only work to eliminate hunger, however, when purchasing power is widely dispersed.</p>\n<p>So all those who believe in the usefulness of the market and the necessity of ending hunger must concentrate on promoting not the market, but the consumers! In this task, government has a vital role to play in countering the tendency toward economic concentration, through genuine tax, credit, and land reforms to disperse buying power toward the poor. Recent trends toward privatization and de-regulation are most definitely not the answer.</p>\n<h3 id=\"Myth_8\">Myth 8</h3>\n<p><strong id=\"Free_Trade_is_the_Answer\">Free Trade is the Answer</strong></p>\n<p><strong>Reality:</strong> The trade promotion formula has proven an abject failure at alleviating hunger. In most Third World countries exports have boomed while hunger has continued unabated or actually worsened. While soybean exports boomed in Brazil-to feed Japanese and European livestock-hunger spread from one-third to two-thirds of the population. Where the majority of people have been made too poor to buy the food grown on their own country's soil, those who control productive resources will, not surprisingly, orient their production to more lucrative markets abroad. Export crop production squeezes out basic food production. Pro-trade policies like NAFTA and GATT pit working people in different countries against each other in a 'race to the bottom,' where the basis of competition is who will work for less, without adequate health coverage or minimum environmental standards. Mexico and the U.S. are a case in point: since NAFTA we have had a net loss of 250,000 jobs here, while Mexico has lost 2 million, and hunger is on the rise in both countries.</p>\n<h3 id=\"Myth_9\">Myth 9</h3>\n<p><strong id=\"Too_Hungry_to_Fight_for_Their_Rights\">Too Hungry to Fight for Their Rights</strong></p>\n<p><strong>Reality:</strong> Bombarded with images of poor people as weak and hungry, we lose sight of the obvious: for those with few resources, mere survival requires tremendous effort. If the poor were truly passive, few of them could even survive. Around the world, from the Zapatistas in Chiapas, Mexico, to the farmers' movement in India, wherever people are suffering needlessly, movements for change are underway. People will feed themselves, if allowed to do so. It's not our job to 'set things right' for others. Our responsibility is to remove the obstacles in their paths, obstacles often created by large corporations and U.S. government, World Bank and IMF policies.</p>\n<h3 id=\"Myth_10\">Myth 10</h3>\n<p><strong id=\"More_U_S__Aid_Will_Help_the_Hungry\">More U.S. Aid Will Help the Hungry</strong></p>\n<p><strong>Reality:</strong> Most U.S. aid works directly against the hungry. Foreign aid can only reinforce, not change, the status quo. Where governments answer only to elites, our aid not only fails to reach hungry people, it shores up the very forces working against them. Our aid is used to impose free trade and free market policies, to promote exports at the expense of food production, and to provide the armaments that repressive governments use to stay in power. Even emergency, or humanitarian aid, which makes up only five percent of the total, often ends up enriching American grain companies while failing to reach the hungry, and it can dangerously undercut local food production in the recipient country. It would be better to use our foreign aid budget for unconditional debt relief, as it is the foreign debt burden that forces most Third World countries to cut back on basic health, education and anti-poverty<br> programs.</p>\n<h3 id=\"Myth_11\">Myth 11</h3>\n<p><strong id=\"We_Benefit_From_Their_Poverty\">We Benefit From Their Poverty</strong></p>\n<p><strong>Reality:</strong> The biggest threat to the well-being of the vast majority of Americans is not the advancement but the continued deprivation of the hungry. Low wages-both abroad and in inner cities at home-may mean cheaper bananas, shirts, computers and fast food for most Americans, but in other ways we pay heavily for hunger and poverty. Enforced poverty in the Third World jeopardizes U.S. jobs, wages and working conditions as corporations seek cheaper labor abroad. In a global economy, what American workers have achieved in employment, wage levels, and working conditions can be protected only when working people in every country are freed from economic desperation.</p>\n<p>Here at home, policies like welfare reform throw more people into the job market than can be absorbed-at below minimum wage levels in the case of 'workfare'-which puts downward pressure on the wages of those on higher rungs of the employment ladder. The growing numbers of 'working poor' are those who have part- or full-time low wage jobs yet cannot afford adequate nutrition or housing for their families. Educating ourselves about the common interests most Americans share with the poor in the Third World and at home allows us to be compassionate without sliding into pity. In working to clear the way for the poor to free themselves from economic oppression, we free ourselves as well.</p>\n<h3 id=\"Myth_12\">Myth 12</h3>\n<p><strong id=\"Curtail_Freedom_to_End_Hunger_\">Curtail Freedom to End Hunger?</strong></p>\n<p><strong>Reality:</strong> There is no theoretical or practical reason why freedom, taken to mean civil liberties, should be incompatible with ending hunger. Surveying the globe, we see no correlation between hunger and civil liberties. However, one narrow definition of freedom-the right to unlimited accumulation of wealth-producing property and the right to use that property however one sees fit-is in fundamental conflict with ending hunger. By contrast, a definition of freedom more consistent with our nation's dominant founding vision holds that economic security for all is the guarantor of our liberty. Such an understanding of freedom is essential to ending hunger.</p>\n<p><br><strong><span>12 Myths About Hunger based on <em>World Hunger: 12 Myths</em>, 2nd Edition, by Frances Moore Lapp\u00e9, Joseph Collins and Peter Rosset, with Luis Esparza (fully revised and updated, Grove/Atlantic and Food First Books, Oct. 1998)</span></strong></p>\n<p><em>Institute for Food and Development Policy Backgrounder</em></p>\n<p>&nbsp;</p>", "sections": [{"title": "Myth 1", "anchor": "Myth_1", "level": 1}, {"title": "Not Enough Food to Go Around", "anchor": "Not_Enough_Food_to_Go_Around", "level": 2}, {"title": "Myth 2", "anchor": "Myth_2", "level": 1}, {"title": "Nature's to Blame for Famine", "anchor": "Nature_s_to_Blame_for_Famine", "level": 2}, {"title": "Myth 3", "anchor": "Myth_3", "level": 1}, {"title": "Too Many People", "anchor": "Too_Many_People", "level": 2}, {"title": "Myth 4", "anchor": "Myth_4", "level": 1}, {"title": "The Environment vs. More Food?", "anchor": "The_Environment_vs__More_Food_", "level": 2}, {"title": "Myth 5", "anchor": "Myth_5", "level": 1}, {"title": "The Green Revolution is the Answer", "anchor": "The_Green_Revolution_is_the_Answer", "level": 2}, {"title": "Myth 6", "anchor": "Myth_6", "level": 1}, {"title": "We Need Large Farms", "anchor": "We_Need_Large_Farms", "level": 2}, {"title": "Myth 7", "anchor": "Myth_7", "level": 1}, {"title": "The Free Market Can End Hunger", "anchor": "The_Free_Market_Can_End_Hunger", "level": 2}, {"title": "Myth 8", "anchor": "Myth_8", "level": 1}, {"title": "Free Trade is the Answer", "anchor": "Free_Trade_is_the_Answer", "level": 2}, {"title": "Myth 9", "anchor": "Myth_9", "level": 1}, {"title": "Too Hungry to Fight for Their Rights", "anchor": "Too_Hungry_to_Fight_for_Their_Rights", "level": 2}, {"title": "Myth 10", "anchor": "Myth_10", "level": 1}, {"title": "More U.S. Aid Will Help the Hungry", "anchor": "More_U_S__Aid_Will_Help_the_Hungry", "level": 2}, {"title": "Myth 11", "anchor": "Myth_11", "level": 1}, {"title": "We Benefit From Their Poverty", "anchor": "We_Benefit_From_Their_Poverty", "level": 2}, {"title": "Myth 12", "anchor": "Myth_12", "level": 1}, {"title": "Curtail Freedom to End Hunger?", "anchor": "Curtail_Freedom_to_End_Hunger_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "9 comments"}], "headingsCount": 26}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-23T23:25:43.019Z", "modifiedAt": null, "url": null, "title": "Example of neurons usefully communicating without synapses found in flies [LINK]", "slug": "example-of-neurons-usefully-communicating-without-synapses", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:59.340Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Sniffnoy", "createdAt": "2009-10-25T00:27:41.113Z", "isAdmin": false, "displayName": "Sniffnoy"}, "userId": "66EwcncPSoZ25StpW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2vaJXXqsqekqXGCrt/example-of-neurons-usefully-communicating-without-synapses", "pageUrlRelative": "/posts/2vaJXXqsqekqXGCrt/example-of-neurons-usefully-communicating-without-synapses", "linkUrl": "https://www.lesswrong.com/posts/2vaJXXqsqekqXGCrt/example-of-neurons-usefully-communicating-without-synapses", "postedAtFormatted": "Friday, November 23rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Example%20of%20neurons%20usefully%20communicating%20without%20synapses%20found%20in%20flies%20%5BLINK%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExample%20of%20neurons%20usefully%20communicating%20without%20synapses%20found%20in%20flies%20%5BLINK%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2vaJXXqsqekqXGCrt%2Fexample-of-neurons-usefully-communicating-without-synapses%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Example%20of%20neurons%20usefully%20communicating%20without%20synapses%20found%20in%20flies%20%5BLINK%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2vaJXXqsqekqXGCrt%2Fexample-of-neurons-usefully-communicating-without-synapses", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2vaJXXqsqekqXGCrt%2Fexample-of-neurons-usefully-communicating-without-synapses", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 35, "htmlBody": "<p>Or, yet another obstacle to WBE.</p>\n<p><a href=\"http://www.the-scientist.com/?articles.view/articleNo/33350/title/Neurons-Talk-Without-Synapses/\">http://www.the-scientist.com/?articles.view/articleNo/33350/title/Neurons-Talk-Without-Synapses/</a></p>\n<p>OK, apparently ephaptic coupling is old news, but this seems to be the first example where we can say just what it does and that it is doing something useful.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2vaJXXqsqekqXGCrt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 7, "extendedScore": null, "score": 1.0410809942655486e-06, "legacy": true, "legacyId": "20246", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-24T00:47:43.721Z", "modifiedAt": null, "url": null, "title": "Overconfident Pessimism", "slug": "overconfident-pessimism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:04.614Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gvdYK8sEFqHqHLRqN/overconfident-pessimism", "pageUrlRelative": "/posts/gvdYK8sEFqHqHLRqN/overconfident-pessimism", "linkUrl": "https://www.lesswrong.com/posts/gvdYK8sEFqHqHLRqN/overconfident-pessimism", "postedAtFormatted": "Saturday, November 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Overconfident%20Pessimism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOverconfident%20Pessimism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgvdYK8sEFqHqHLRqN%2Foverconfident-pessimism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Overconfident%20Pessimism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgvdYK8sEFqHqHLRqN%2Foverconfident-pessimism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgvdYK8sEFqHqHLRqN%2Foverconfident-pessimism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1219, "htmlBody": "<blockquote>\n<p>You can build a machine to draw [deductive] conclusions for you, but I think you can never build a machine that will draw [probabilistic] inferences.</p>\n</blockquote>\n<p align=\"right\">George Polya, 34 years before <a href=\"http://www.amazon.com/Probabilistic-Reasoning-Intelligent-Systems-Plausible/dp/1558604790/\">Pearl (1988)</a> launched the probabilistic revolution in AI</p>\n<blockquote>\n<p>The energy produced by the breaking down of the atom is a very poor kind of thing. Anyone who expects a source of power from the transformation of these atoms is talking moonshine.</p>\n</blockquote>\n<p align=\"right\">Ernest Rutherford in 1933, 18 years before the first nuclear reactor went online</p>\n<blockquote>\n<p>I confess that in 1901 I said to my brother Orville that man would not fly for fifty years. Two years later we ourselves made flights. This demonstration of my impotence as a prophet gave me such a shock that ever since I have distrusted myself...</p>\n</blockquote>\n<p align=\"right\">Wilbur Wright, in a 1908 speech</p>\n<p>&nbsp;</p>\n<p>Startling insights are hard to predict.<sup>1</sup> Polya and Rutherford couldn't have predicted when computational probabilistic reasoning and nuclear power would arrive. Their training in scientific skepticism probably prevented them from making confident predictions about what would be developed in the next few decades.</p>\n<p>What's odd, then, is that their scientific skepticism didn't prevent them from making confident predictions about what <em>wouldn't</em> be developed in the next few decades.</p>\n<p>I am blessed to occasionally chat with some of the smartest scientists in the world, especially in computer science. They generally don't make confident predictions that certain specific, difficult, insight-based technologies will be developed soon. And yet, immediately after agreeing with me that \"the future is very hard to predict,\" they will confidently state that a specific, difficult technology is more than 50 years away!</p>\n<p>Error. Does not compute.</p>\n<p><a id=\"more\"></a></p>\n<p>What's going on, here?</p>\n<p>I don't think it's always a case of <a href=\"http://wiki.lesswrong.com/wiki/Motivated_skepticism\">motivated skepticism</a>. I don't think think Wilbur Wright was motivated to think flight was a long way off. I think he was \"zoomed in\" on the difficulty of the problem, didn't see a way to solve it, and misinterpreted his <em>lack of knowledge</em> about the difficulty of flight as <em>positive information</em> that flight was extremely difficult and far away.</p>\n<p>As Eliezer <a href=\"http://www.scottaaronson.com/blog/?p=346#comment-11441\">wrote</a>:</p>\n<blockquote>\n<p>When heavier-than-air flight or atomic energy was a hundred years off, it looked fifty years off or impossible; when it was five years off, it still looked fifty years off or impossible. Poor information.</p>\n</blockquote>\n<p>(Of course, we <a href=\"/lw/fmf/overconfident_pessimism/7vq2\">can predict</a> some&nbsp;technological advances better than others: \"Five years before the first moon landing, it looked a few years off but certainly not a hundred years off.\")</p>\n<p>There may also be a psychological double standard for \"positive\" and \"negative\" predictions. Skepticism about confident positive predictions &mdash; say, that AI will be invented soon &mdash; feels like the <em>virtuous doubt</em> of standard scientific training. But oddly enough, making confident <em>negative</em> predictions &mdash; say, that AI will <em>not</em> be invented soon &mdash; <em>also</em> feels like virtuous doubt, merely because the first prediction was phrased positively and the second was phrase negatively.</p>\n<p>There's probably some <a href=\"http://www.overcomingbias.com/2010/06/near-far-summary.html\">Near-Far</a> stuff going on, too. Nuclear fusion and AI feel abstract and unknown, and thus they also feel <em>distant</em>. But when you're ignorant about a phenomenon, the correct response is to broaden your confidence intervals in <em>both</em> directions, not push them in one direction like the Near-Far effect wants you to.</p>\n<p>The scientists I speak to are right to say that it's <a href=\"/lw/9ao/longterm_technological_forecasting/\">very hard</a> to predict the development of specific technologies. But one cannot \"<a href=\"/lw/fmf/overconfident_pessimism/7vwf\">simultaneously claim</a> to know little about the future <em>and</em> to be able to set strong lower bounds on technology development times,\" on pain of contradiction.</p>\n<p><a href=\"/lw/fmf/overconfident_pessimism/7vr7\">Depending on</a> the other predictions these scientists have made, they might be<sup>3</sup> manifesting a form of overconfidence I'll call \"overconfident pessimism.\" It's well-known that <a href=\"http://en.wikipedia.org/wiki/Overconfidence_effect\">humans are overconfident</a>, but since overconfident pessimism seems to be less-discussed than overconfident optimism, I think it's worth giving it its own name.</p>\n<p>What can we do to combat overconfident pessimism in ourselves?</p>\n<p>The most broadly useful debiasing technique is to \"<strong>consider the opposite</strong>\" (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Larrick-Debiasing.pdf\">Larrick 2004</a>):</p>\n<blockquote>\n<p>The strategy consists of nothing more than asking oneself, &ldquo;What are some reasons that my initial judgment might be wrong?&rdquo; The strategy is effective because it directly counteracts the basic problem of association-based processes &mdash; an overly narrow sample of evidence &ndash; by expanding the sample and making it more representative...</p>\n</blockquote>\n<p>Or, consider this variant of \"consider the opposite\":</p>\n<blockquote>\n<p>Typically, subjective range estimates exhibit high overcon\ufb01dence. Ranges for which people are 80 percent con\ufb01dent capture the truth 30 percent to 40 percent of the time. <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/11/Soll-Klayman-Overconfidence-in-Interval-Estimates.pdf\">Soll and Klayman (2004)</a> showed that having judges generate 10th and 90th percentile estimates in separate stages &ndash; which forces them to consider distinct reasons for low and high values &ndash; increased hit rates to nearly 60 percent by both widening and centering ranges.<sup>2</sup></p>\n</blockquote>\n<p>Another standard method for reducing overconfidence and improving one's accuracy in general is <strong>calibration training</strong> (<a href=\"http://www.dtic.mil/dtic/tr/fulltext/u2/a101986.pdf\">Lichtenstein et al. 1982</a>; <a href=\"http://www.amazon.com/How-Measure-Anything-Intangibles-Business/dp/0470539399/\">Hubbard 2007</a>).</p>\n<p>The calibration training process is pretty straightforward: Write down your predictions, then check whether they came true. Be sure to also state your confidence in each prediction. If you're perfectly calibrated, then predictions you made with 60% confidence should be correct 60% of the time, while predictions you made with 90% confidence should be correct 90% of the time.</p>\n<p>You will <em>not</em> be perfectly calibrated. But you can become <em>better</em>-calibrated over time with many rounds of feedback. That's why weather forecasters are so much more accurate than most other kinds of experts (<a href=\"http://faculty.fuqua.duke.edu/~psun/Murphy%20Winkler%201984%20JASA.pdf\">Murphy &amp; Winkler 1984</a>): every week, they learn whether their predictions were correct. It's harder to improve your calibration when you have to wait 5 or 30 years to see whether your predictions (say, about technological development) were correct, but calibration training in <em>any</em> domain seems to reduce overconfidence in general, since you get to viscerally experience how often you are wrong &mdash; even on phenomena that should be easier to predict than <a href=\"http://faculty.fuqua.duke.edu/~psun/Murphy%20Winkler%201984%20JASA.pdf\">long-term technological development</a>.</p>\n<p>Perhaps the best online tool for calibration training is <a href=\"http://predictionbook.com/\">PredictionBook.com</a>. For a story of one person becoming better calibrated using PredictionBook.com, see <a href=\"http://www.gwern.net/Prediction%20markets#predictionbook-nights\">1001 PredictionBook Nights</a>. Another tool is the <a href=\"http://www.acritch.com/credence-game/\">Calibration Game</a>, available for Mac, Windows, iOS, and Android.</p>\n<p>To counteract overconfident pessimism in particular, be sure to record lots of <em>negative</em> predictions, not just positive predictions.</p>\n<p>Finally, it may help to read lists of failed negative predictions. Here you go: <a href=\"http://www.null-hypothesis.co.uk/science/strange-but-true/item/invention_failure_never_work_disaster\">one</a>, <a href=\"http://www.rinkworks.com/said/predictions.shtml\">two</a>, <a href=\"http://creatingminds.org/quotes/by_experts.htm\">three</a>, <a href=\"http://www.foresight.org/news/negativeComments.html\">four</a>, <a href=\"http://listverse.com/2007/10/28/top-30-failed-technology-predictions/\">five</a>, <a href=\"http://www.lhup.edu/~dsimanek/neverwrk.htm\">six</a>.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h3>Notes</h3>\n<p><sup>1</sup> <a href=\"http://intelligence.org/files/Armstrong-Sotala-How-Were-Predicting-AI-or-Failing-to.pdf\">Armstrong &amp; Sotala (2012)</a> helpfully distinguish \"insight\" and \"grind\":</p>\n<blockquote>\n<p>Project managers and various leaders are often quite good at estimating the length of projects... Publication dates for video games, for instance, though often over-optimistic, are generally not ridiculously erroneous &ndash; even though video games involve a lot of creative design, play-testing, art, programing the game &ldquo;AI&rdquo;, etc. . . Moore&rsquo;s law could be taken as an ultimate example of grind: we expect the global e\ufb00orts of many engineers across many \ufb01elds to average out to a rather predictable exponential growth.</p>\n<p>Predicting insight, on the other hand, seems a much more daunting task. Take the Riemann hypothesis, a well-established mathematical hypothesis from 1885. How would one go about estimating how long it would take to solve? How about the P = NP hypothesis in computing? Mathematicians seldom try and predict when major problems will be solved, because they recognise that insight is very hard to predict.</p>\n</blockquote>\n<p><sup>2</sup> See also <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/11/Speirs-Bridge-et-al-Reducing-overconfidence-in-the-interval-judgments-of-experts.pdf\">Speirs-Bridge et al. (2009)</a>.</p>\n<p><sup>3</sup> The original version of this post incorrectly accused the scientists I've spoken with of overconfidence, but I can't rightly draw that conclusion without knowing the outcomes of their other predictions.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"8hPTCJbwJnLBmfpCX": 1, "8daMDi9NEShyLqxth": 1, "3uE2pXvbcnS9nnZRE": 1, "LDTSbmXtokYAsEq8e": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gvdYK8sEFqHqHLRqN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 37, "extendedScore": null, "score": 8.4e-05, "legacy": true, "legacyId": "20247", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 37, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eXdGr2LeHYEgMh9qX"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-24T05:54:13.615Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] ...Recursion, Magic", "slug": "seq-rerun-recursion-magic", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/chYwdCWxDjfBv7BHF/seq-rerun-recursion-magic", "pageUrlRelative": "/posts/chYwdCWxDjfBv7BHF/seq-rerun-recursion-magic", "linkUrl": "https://www.lesswrong.com/posts/chYwdCWxDjfBv7BHF/seq-rerun-recursion-magic", "postedAtFormatted": "Saturday, November 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20...Recursion%2C%20Magic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20...Recursion%2C%20Magic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FchYwdCWxDjfBv7BHF%2Fseq-rerun-recursion-magic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20...Recursion%2C%20Magic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FchYwdCWxDjfBv7BHF%2Fseq-rerun-recursion-magic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FchYwdCWxDjfBv7BHF%2Fseq-rerun-recursion-magic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 186, "htmlBody": "<p>Today's post, <a href=\"/lw/w6/recursion_magic/\">...Recursion, Magic</a> was originally published on 25 November 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#...Recursion.2C_Magic\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>If you have a system that gets better at making itself get better, it will appear to discontinuously advance. Add in the ability of intelligences to accomplish tasks which previous intelligences labeled impossible, and you have the potential for dramatic advancement.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fkp/seq_rerun_when_life_is_cheap_death_is_cheap/\">When Life is Cheap, Death is Cheap</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "chYwdCWxDjfBv7BHF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0413001862168503e-06, "legacy": true, "legacyId": "20249", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rJLviHqJMTy8WQkow", "zLba9bo7bHkgaZFmi", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-24T10:55:17.145Z", "modifiedAt": null, "url": null, "title": "Mathematical Measures of Optimization Power", "slug": "mathematical-measures-of-optimization-power", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:01.322Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alex_Altair", "createdAt": "2010-07-21T18:30:40.806Z", "isAdmin": false, "displayName": "Alex_Altair"}, "userId": "5wu9jG4pm9q6xjZ9R", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aD7mZMeu9waAfm3au/mathematical-measures-of-optimization-power", "pageUrlRelative": "/posts/aD7mZMeu9waAfm3au/mathematical-measures-of-optimization-power", "linkUrl": "https://www.lesswrong.com/posts/aD7mZMeu9waAfm3au/mathematical-measures-of-optimization-power", "postedAtFormatted": "Saturday, November 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mathematical%20Measures%20of%20Optimization%20Power&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMathematical%20Measures%20of%20Optimization%20Power%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaD7mZMeu9waAfm3au%2Fmathematical-measures-of-optimization-power%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mathematical%20Measures%20of%20Optimization%20Power%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaD7mZMeu9waAfm3au%2Fmathematical-measures-of-optimization-power", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaD7mZMeu9waAfm3au%2Fmathematical-measures-of-optimization-power", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1500, "htmlBody": "<p>In explorations of AI risk, it is helpful to formalize concepts. One particularly important concept is intelligence. How can we formalize it, or better yet, measure it? &ldquo;Intelligence&rdquo; is often considered mysterious or is anthropomorphized. One way to <a href=\"/lw/nu/taboo_your_words/\">taboo</a> &ldquo;intelligence&rdquo; is to talk instead about optimization processes. An optimization process (OP, also optimization power) selects some futures from a space of possible futures. It does so according to some criterion; that is, it optimizes for <em>something</em>. Eliezer Yudkowsky <a href=\"/lw/tx/optimization/\">spends</a> <a href=\"/lw/v7/expected_creative_surprises/\">a few</a> <a href=\"/lw/v8/belief_in_intelligence/\">of the</a> <a href=\"/lw/v9/aiming_at_the_target/\">sequence</a> <a href=\"/lw/va/measuring_optimization_power/\">posts</a> <a href=\"/lw/vb/efficient_crossdomain_optimization/\">discussing</a> <a href=\"/lw/vc/economic_definition_of_intelligence/\">the nature</a> and importance of this concept for understanding AI risk. In them, he informally describes a way to measure the power of an OP. We consider mathematical formalizations of this measure.</p>\n<p>Here's EY's <a href=\"/lw/tx/optimization/\">original description</a> of his measure of OP.</p>\n<p style=\"padding-left: 30px;\">Put a measure on the state space - if it's discrete, you can just count. Then collect all the states which are equal to or greater than the observed outcome, in that optimization process's implicit or explicit preference ordering. Sum or integrate over the total size of all such states. Divide by the total volume of the state space. This gives you the power of the optimization process measured in terms of the improbabilities that it can produce - that is, improbability of a random selection producing an equally good result, relative to a measure and a preference ordering.</p>\n<p style=\"padding-left: 30px;\">If you prefer, you can take the reciprocal of this improbability (1/1000 becomes 1000) and then take the logarithm base 2. This gives you the power of the optimization process in bits.</p>\n<p>Let's say that at time <img src=\"http://www.codecogs.com/png.latex?t=0\" alt=\"\" width=\"39\" height=\"13\" />&nbsp;we have a formalism to specify all possible world states <img src=\"http://www.codecogs.com/png.latex?w \\in W\" alt=\"\" width=\"55\" height=\"14\" />&nbsp;at some future time <img src=\"http://www.codecogs.com/png.latex?t=1\" alt=\"\" width=\"41\" height=\"14\" />. Perhaps it is a list of particle locations and velocities, or perhaps it is a list of all possible universal wave functions. Or maybe we're working in a limited domain, and it's a list of all possible next-move chess boards. Let's also assume that we have a <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/\">well-justified prior</a> <img src=\"http://www.codecogs.com/png.latex?p(w)\" alt=\"\" width=\"35\" height=\"19\" />&nbsp;over these states being the next ones to occur in the absence of an OP (more on that later).</p>\n<p>We order <img src=\"http://www.codecogs.com/png.latex?W\" alt=\"\" width=\"21\" height=\"14\" />&nbsp;according to the OP's preferences. For the moment, we actually don't care about the density, or &ldquo;measure&rdquo; of our ordering. Now we have a probability distribution over <img src=\"http://www.codecogs.com/png.latex?W\" alt=\"\" width=\"21\" height=\"14\" />. The integral from <img src=\"http://www.codecogs.com/png.latex?w=a\" alt=\"\" width=\"47\" height=\"8\" />&nbsp;to <img src=\"http://www.codecogs.com/png.latex?w=b\" alt=\"\" width=\"45\" height=\"13\" />&nbsp;over this represents the probability that the worldstate at <img src=\"http://www.codecogs.com/png.latex?t=1\" alt=\"\" width=\"41\" height=\"14\" />&nbsp;will be better than <img src=\"http://www.codecogs.com/png.latex?a\" alt=\"\" width=\"11\" height=\"10\" />, and worse than <img src=\"http://www.codecogs.com/png.latex?b\" alt=\"\" width=\"7\" height=\"13\" />. When time continues, and the OP acts to bring about some worldstate <img src=\"http://www.codecogs.com/png.latex?\\omega\" alt=\"\" width=\"11\" height=\"8\" />, we can calculate the probability of an equal or better outcome occurring;</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\int_\\omega^\\infty p(w)dw\" alt=\"\" width=\"92\" height=\"43\" /></p>\n<p>This is a simple generalization of what EY describes above. Here are some things I am confused about.</p>\n<p>Finding a specification for all possible worldstates is hard, but it's been done before. There are many ways to reasonably represent this. What I can't figure out is how to specify possible worldstates &ldquo;in the absence of an OP&rdquo;. This phrase hides tons of complexity. How can we formally construct this counterfactual? Is the matter that composes the OP no longer present? Is it present but &ldquo;not acting&rdquo;? What constitutes a null action? Are we considering the expected worldstate distribution as if the OP never existed? If the OP is some kind of black-box AI agent, it's easier to imagine this. But if the OP is evolution, or a forest fire, it's harder to imagine. Furthermore, is the specification <a href=\"http://en.wikipedia.org/wiki/Cartesian_dualism\">dualist</a>, or is the agent part of the worldstates? If it's dualist, this is a fundamental falseness which can have lots of bad implications. If the agent is part of the worldstates, how do we represent them &ldquo;in absence of an OP&rdquo;?</p>\n<p>But for the rest of this article, let's pretend we have such a specification. There's also a loss from ignoring the <a href=\"http://en.wikipedia.org/wiki/Cardinal_utility\">cardinal utility</a> of the worldstates. Let's say you have the two distributions of utility over sets <img src=\"http://www.codecogs.com/png.latex?W\" alt=\"\" width=\"21\" height=\"14\" />, representing two different OPs. In both, the OP choose a <img src=\"http://www.codecogs.com/png.latex?w\" alt=\"\" width=\"13\" height=\"8\" />&nbsp;with the same utility <img src=\"http://www.codecogs.com/png.latex?u\" alt=\"\" width=\"10\" height=\"8\" />. The distributions are the same on the left side of <img src=\"http://www.codecogs.com/png.latex?u\" alt=\"\" width=\"10\" height=\"8\" />, and the second distribution has a longer tail on the right. It seems like the OP in distribution 1 was more impressive; the second OP missed all the available higher utility. We could make the expected utility of the second distribution arbitrarily high, while maintaining the same fraction of probability mass above the achieved worldstate. Conversely, we could instead extend the left tail of the second distribution, and say that the second OP was more impressive because it managed to avoid all the bad worlds.</p>\n<p>Perhaps it is more natural to consider two distributions; the distribution of utility over entire world <em>futures</em> assuming the OP isn't present, versus the distribution after the OP takes its action. So instead of selecting a single possibility with certainty, the probabilities have just shifted.&nbsp;</p>\n<p>How should we reduce this distribution shift to a single number which we call OP? Any shift of probability mass upwards in utility should increase the measure of OP, and vice versa. I think also that an increase in the <a href=\"http://en.wikipedia.org/wiki/Expected_value\">expected</a>&nbsp;<a href=\"http://en.wikipedia.org/wiki/Expected_utility\">utility</a>&nbsp;(EU) of these distributions should be measured as a positive OP, and vice versa. EU seems like the critical metric to use. Let's generalize a little further, and say that instead of measuring OP between two points in time, we let the time difference go to zero, and measure instantaneous OP. Therefore we're interested in some equation which has the same sign as</p>\n<p><img src=\"http://www.codecogs.com/png.latex?\\frac{dEU}{dt} = EU'\" alt=\"\" width=\"95\" height=\"38\" />.</p>\n<p>Besides that, I'm not exactly sure which specific equation should equal OP. I seem to have two contradicting desires;</p>\n<p>1a) The sign of <img src=\"http://www.codecogs.com/png.latex?EU'\" alt=\"\" width=\"32\" height=\"15\" />&nbsp;should be the sign of the OP.</p>\n<p>&nbsp; b) Negative <img src=\"http://www.codecogs.com/png.latex?EU\" alt=\"\" width=\"28\" height=\"14\" />&nbsp;and <img src=\"http://www.codecogs.com/png.latex?EU'\" alt=\"\" width=\"32\" height=\"15\" />&nbsp;should be possible.</p>\n<p>2) Constant positive OP should imply exponentially increasing <img src=\"http://www.codecogs.com/png.latex?EU\" alt=\"\" width=\"28\" height=\"14\" />.</p>\n<p>Criterion 1) feels pretty obvious. Criterion 2) feels like a recognition of what is &ldquo;natural&rdquo; for OPs; to improve upon themselves, so that they can get better and better returns. The simplest differential equation that represents positive feedback yields exponentials, and is used across many domains because of its universal nature.</p>\n<p><img src=\"http://www.codecogs.com/png.latex?y' = cy\" alt=\"\" width=\"55\" height=\"18\" /></p>\n<p>This intuition certainly isn't anthropocentric, but it might be this-universe biased. I'd be interested in seeing if it is natural in other computable environments.</p>\n<p>If we just use <img src=\"http://www.codecogs.com/png.latex?OP = EU'\" alt=\"\" width=\"84\" height=\"15\" />, then criterion 2) is not satisfied. If we use <img src=\"http://www.codecogs.com/png.latex?OP = \\log EU'\" alt=\"\" width=\"110\" height=\"18\" />, then decreases in EU&nbsp;are not defined, and constant EU&nbsp;is negative infinite OP, violating 1). <a href=\"http://en.wikipedia.org/wiki/Logarithmic_derivative\">If we use</a> <img src=\"http://www.codecogs.com/png.latex?OP = EU'/EU\" alt=\"\" width=\"122\" height=\"19\" />, then 2) is satisfied, but negative and decreasing EU&nbsp;give positive OP, violating 1a). If we use <img src=\"http://www.codecogs.com/png.latex?OP = EU''/EU'\" alt=\"\" width=\"129\" height=\"19\" />, then 2) is still satisfied, but <img src=\"http://www.codecogs.com/png.latex?EU = at + b\" alt=\"\" width=\"97\" height=\"15\" />&nbsp;gives <img src=\"http://www.codecogs.com/png.latex?OP = 0\" alt=\"\" width=\"60\" height=\"14\" />, violating 1a). Perhaps the only consistent equation would be <img src=\"http://www.codecogs.com/png.latex?OP = EU'/|EU|\" alt=\"\" width=\"130\" height=\"19\" />. But seriously, who uses absolute values? I can't recall a fundamental equation that relied on them. They feel totally ad hoc. Plus, there's this weird singularity at <img src=\"http://www.codecogs.com/png.latex?EU = 0\" alt=\"\" width=\"61\" height=\"14\" />. What's up with that?</p>\n<p>Classically, utility is invariant up to <a href=\"http://en.wikipedia.org/wiki/Cardinal_utility\">positive affine transformations</a>. Criterion 1) respects this because the derivative removes the additive constant, but 2) doesn't. It is still scale invariant, but it has an intrinsic zero. This made me consider the nature of &ldquo;zero utility&rdquo;. At least for humans, there is an intuitive sign to utility. We wouldn't say that stubbing your toe is 1,000,000 utils, and getting a car is 1,002,000 utils. It seems to me, especially after reading <a href=\"http://selfawaresystems.com/2007/11/30/paper-on-the-basic-ai-drives/\">Omohundro's &ldquo;Basic AI Drives&rdquo;</a>, that there is in some sense an intrinsic zero utility for all OPs.</p>\n<p>All OPs need certain initial conditions to even exist. After that, they need resources. AIs need computer hardware and energy. Evolution needed certain chemicals and energy. Having <em>no</em> resources makes it impossible, in general, to do anything. If you have literally zero resources, you are not a \"thing\" which \"does\". So that is a type of intrinsic zero utility. Then what would having negative utility mean? It would mean the OP anti-exists. It's making it even less likely for it to be able to <em>start</em>&nbsp;working toward its utility function. What would exponentially decreasing utility mean? It would mean that it is a constant OP for the&nbsp;<em>negative</em>&nbsp;of the utility function that we are considering. So, it doesn't really have negative optimization power; if that's the result of our calculation, we should negate the utility function, and say it has positive OP. And that singularity at <img src=\"http://www.codecogs.com/png.latex?EU = 0\" alt=\"\" width=\"61\" height=\"14\" />? When you go from the positive side, getting closer and closer to 0 is <em>really bad</em>, because you're destroying the last bits of your resources; your last chance of doing any optimization. And going from negative utility to positive is infinite impressive, because you bootstrapped from optimizing away from your goal to optimizing toward your goal.</p>\n<p>So perhaps we should drop the part of 1b) that says negative EU can exist. Certainly world-states can exist that are terrible for a given utility function, but if an OP with that utility function exists, then the expected utility of the future is positive.</p>\n<p>If this is true, then it seems there is more to the concept of utility than the <a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">von Neumann-Morgenstern</a> axioms.</p>\n<p>How do people feel about criterion 2), and my proposal that&nbsp;<img src=\"http://www.codecogs.com/png.latex?OP = EU'/|EU|\" alt=\"\" width=\"130\" height=\"19\" />&nbsp;?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"nvKzwpiranwy29HFJ": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aD7mZMeu9waAfm3au", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 7, "extendedScore": null, "score": 1.8e-05, "legacy": true, "legacyId": "20258", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WBdvyyHLdxZSAMmoz", "D7EcMhL26zFNbJ3ED", "rEDpaTTEzhPLz4fHh", "HktFCy6dgsqJ9WPpX", "CW6HDvodPpNe38Cry", "Q4hLMDrFd8fbteeZ8", "yLeEPFnnB9wE7KLx2", "uPCehCa7Ecidfn2Xe", "Kyc5dFDzBg4WccrbK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-24T18:31:22.424Z", "modifiedAt": null, "url": null, "title": "Meetup : Brussels meetup", "slug": "meetup-brussels-meetup-7", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:07.919Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Axel", "createdAt": "2010-11-03T17:32:46.091Z", "isAdmin": false, "displayName": "Axel"}, "userId": "vi498nAvek8eWuMWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/P7id4XgEm7yXh4GFR/meetup-brussels-meetup-7", "pageUrlRelative": "/posts/P7id4XgEm7yXh4GFR/meetup-brussels-meetup-7", "linkUrl": "https://www.lesswrong.com/posts/P7id4XgEm7yXh4GFR/meetup-brussels-meetup-7", "postedAtFormatted": "Saturday, November 24th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Brussels%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Brussels%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP7id4XgEm7yXh4GFR%2Fmeetup-brussels-meetup-7%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Brussels%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP7id4XgEm7yXh4GFR%2Fmeetup-brussels-meetup-7", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP7id4XgEm7yXh4GFR%2Fmeetup-brussels-meetup-7", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 65, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/gc'>Brussels meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 December 2012 01:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are once again meeting at 'la fleur en papier dor\u00e9' close to the Brussels Central station. If you feel like an intelligent discussion and are in the neighborhood, consider dropping by. As always, I'll have a sign.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/gc'>Brussels meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "P7id4XgEm7yXh4GFR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0417275922832883e-06, "legacy": true, "legacyId": "20259", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Brussels_meetup\">Discussion article for the meetup : <a href=\"/meetups/gc\">Brussels meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 December 2012 01:00:00PM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Rue des Alexiens 55 1000 Bruxelles</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We are once again meeting at 'la fleur en papier dor\u00e9' close to the Brussels Central station. If you feel like an intelligent discussion and are in the neighborhood, consider dropping by. As always, I'll have a sign.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Brussels_meetup1\">Discussion article for the meetup : <a href=\"/meetups/gc\">Brussels meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Brussels meetup", "anchor": "Discussion_article_for_the_meetup___Brussels_meetup", "level": 1}, {"title": "Discussion article for the meetup : Brussels meetup", "anchor": "Discussion_article_for_the_meetup___Brussels_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-25T00:44:09.458Z", "modifiedAt": null, "url": null, "title": "Cryonic resurrection - an ethical hypothetical", "slug": "cryonic-resurrection-an-ethical-hypothetical", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:04.039Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ialdabaoth", "createdAt": "2012-10-11T00:04:32.345Z", "isAdmin": false, "displayName": "ialdabaoth"}, "userId": "335oJYQyzcd85RAoe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Zgij3bCJREiFEMFSv/cryonic-resurrection-an-ethical-hypothetical", "pageUrlRelative": "/posts/Zgij3bCJREiFEMFSv/cryonic-resurrection-an-ethical-hypothetical", "linkUrl": "https://www.lesswrong.com/posts/Zgij3bCJREiFEMFSv/cryonic-resurrection-an-ethical-hypothetical", "postedAtFormatted": "Sunday, November 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cryonic%20resurrection%20-%20an%20ethical%20hypothetical&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACryonic%20resurrection%20-%20an%20ethical%20hypothetical%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZgij3bCJREiFEMFSv%2Fcryonic-resurrection-an-ethical-hypothetical%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cryonic%20resurrection%20-%20an%20ethical%20hypothetical%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZgij3bCJREiFEMFSv%2Fcryonic-resurrection-an-ethical-hypothetical", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZgij3bCJREiFEMFSv%2Fcryonic-resurrection-an-ethical-hypothetical", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 463, "htmlBody": "<p>At some point in the future, we hope, brains which have been cryonically preserved may be resurrected, by some process of neural reconstruction (most likely either as nanotech, reconstituted wetware, or virtual simulation).</p>\n<p>Imagine that the technology has just come available to resurrect a frozen brain. However, the process has low fidelity, due to resource and technique limitations. Luckily, these limitations are purely practical - as the technique is refined, the process of resurrection will become better and better. The process is also destructive to the original preserved brain, so there's no going back and making a second, higher-quality scan.</p>\n<p>The results of the process is effectively a copy of the old brain and personality, but with permanent brain damage in several regions - this manifests effectively as an extreme form of cerebral palsy, partial amnesia (retrograde and anteretrograde), bipolar disthymia, and a partial frontal lobotomy - in short, you'll get something that has recognizable facets of the original, but it's an utter mess.</p>\n<p>As the technology progresses, each of these symptoms will be lessened, until eventually they will be effectively eliminated altogether. However, the first few thousand subjects will suffer irrecoverable memory loss and will suffer a horrifically low quality-of-life for at least several decades until the technology improves.</p>\n<p>The technology will not progress in refinement without practice, and practice requires actually restoring cryogenically frozen human brains.</p>\n<p>Let's establish a metric so we can talk numerically:</p>\n<p>&nbsp;</p>\n<p>0.000 - complete and persistent vegetative state, aka dead (this is our current state of progress in this technology)</p>\n<p>0.100 - Terry Schiavo (persistent vegetative state with occasional non-conscious responses)</p>\n<p>0.500 - the equivalent of advanced Alzheimer's syndrome; severe mental and physical impairment</p>\n<p>0.700 - moderate mental and physical impairment</p>\n<p>0.800 - significant reduction in facilities (IQ loss of 20 to 35 points, severe difficulty with memory, slurred speech, frequent and severe mood swings)</p>\n<p>0.900 - slight reduction in facilities (IQ loss of 10 to 20 points, moderate short- and long-term memory loss, frequent but moderate mood swings)</p>\n<p>0.950 - liminal reduction in facilities (IQ loss of 5 to 10 points; occasional slowness in memory recall, occasional mood swings)</p>\n<p>1.000 - a perfect reproduction of your original personality and capability</p>\n<p>QUESTION 1: If your brain was frozen, at what stage in this technological refinement process would you like your brain to be revived?&nbsp;</p>\n<p>QUESTION 2: If you had had your brain preserved before anyone had asked you this question, how could the reviving technicians ethically know this value? Remember that they cannot thaw you to ask you.</p>\n<p>QUESTION 3: Assuming as part of this what-if that the technology cannot progress past 0.500 fidelity without human trials, who should we attempt to revive when the technology is at 0.500? At 0.7? 0.8? 0.9? 0.95? Assume that we haven't asked any of the subjects this question, so we do not know their own preferences.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Zgij3bCJREiFEMFSv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 16, "extendedScore": null, "score": 4.8e-05, "legacy": true, "legacyId": "20265", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-25T04:20:41.073Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Abstract/Distant Future Bias", "slug": "seq-rerun-abstract-distant-future-bias", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dk8hxTqLEP8egXBSr/seq-rerun-abstract-distant-future-bias", "pageUrlRelative": "/posts/dk8hxTqLEP8egXBSr/seq-rerun-abstract-distant-future-bias", "linkUrl": "https://www.lesswrong.com/posts/dk8hxTqLEP8egXBSr/seq-rerun-abstract-distant-future-bias", "postedAtFormatted": "Sunday, November 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Abstract%2FDistant%20Future%20Bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Abstract%2FDistant%20Future%20Bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdk8hxTqLEP8egXBSr%2Fseq-rerun-abstract-distant-future-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Abstract%2FDistant%20Future%20Bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdk8hxTqLEP8egXBSr%2Fseq-rerun-abstract-distant-future-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fdk8hxTqLEP8egXBSr%2Fseq-rerun-abstract-distant-future-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 147, "htmlBody": "<p>Today's post, <a href=\"http://www.overcomingbias.com/2008/11/abstractdistant.html\">Abstract/Distant Future Bias</a> was originally published on November 26, 2008.  A summary:</p>\n<p>&nbsp;</p>\n<blockquote>An introduction to the idea of Near vs Far mode reasoning.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fmh/seq_rerun_recursion_magic/\">...Recursion, Magic</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dk8hxTqLEP8egXBSr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0420604708640775e-06, "legacy": true, "legacyId": "20267", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["chYwdCWxDjfBv7BHF", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-25T23:33:30.413Z", "modifiedAt": null, "url": null, "title": "LW Women- Minimizing the Inferential Distance", "slug": "lw-women-minimizing-the-inferential-distance", "viewCount": null, "lastCommentedAt": "2020-05-07T16:35:01.807Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YdrdHErogcGSxEBrm/lw-women-minimizing-the-inferential-distance", "pageUrlRelative": "/posts/YdrdHErogcGSxEBrm/lw-women-minimizing-the-inferential-distance", "linkUrl": "https://www.lesswrong.com/posts/YdrdHErogcGSxEBrm/lw-women-minimizing-the-inferential-distance", "postedAtFormatted": "Sunday, November 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LW%20Women-%20Minimizing%20the%20Inferential%20Distance&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALW%20Women-%20Minimizing%20the%20Inferential%20Distance%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYdrdHErogcGSxEBrm%2Flw-women-minimizing-the-inferential-distance%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LW%20Women-%20Minimizing%20the%20Inferential%20Distance%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYdrdHErogcGSxEBrm%2Flw-women-minimizing-the-inferential-distance", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYdrdHErogcGSxEBrm%2Flw-women-minimizing-the-inferential-distance", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2023, "htmlBody": "<h2>Standard Intro</h2>\n<p><strong>The following section will be at the top of all posts in the LW Women series.</strong></p>\n<p>About two months ago, I put out a call for anonymous submissions by the women on LW, with the idea that I would compile them into some kind of post. &nbsp;There is a LOT of material, so I am breaking them down into more manageable-sized themed posts.&nbsp;</p>\n<p>Seven women submitted, totaling about 18 pages.&nbsp;</p>\n<p><strong>Crocker's Warning</strong>-&nbsp;Submitters&nbsp;were told to not hold back for politeness. You are allowed to disagree, but these are candid comments; if you consider candidness impolite, I suggest you not read this post</p>\n<p><strong>To the submittrs</strong>- If you would like to respond anonymously to a comment (for example if there is a comment questioning something in your post, and you want to clarify), you can PM your message and I will post it for you. If this happens a lot, I might create a LW_Women sockpuppet account for the submitters to share.</p>\n<p><strong>Standard Disclaimer</strong>- Women have many different viewpoints, and just because I am acting as an intermediary to allow for anonymous communication does NOT mean that I agree with everything that will be posted in this series. (It would be rather impossible to, since there are some posts arguing opposite sides!)</p>\n<p><strong>Please do NOT break anonymity, because it lowers the anonymity of the rest of the submitters.<a id=\"more\"></a><br /></strong></p>\n<h2>Minimizing the Inferential Distance</h2>\n<p>One problem that I think exists in discussions about gender issues between men and women, is that the inferential distance is much greater than either group realizes. Women might assume that men know what experiences women might face, and so not explicitly mention specific examples. Men might assume they know what the women are talking about, but have never really heard specific examples. Or they might assume that these types of things only happened in the past, or not to the types of females in their in-group</p>\n<p>So for the first post in this series, I thought it would be worthwhile to try to lower this inferential distance, by sharing specific examples of what it's like as a smart/geeky female. When submitters didn't know what to write, I directed them to <a href=\"http://jdwise.blogspot.com/2012/04/nerd-life.html\">this article</a>, by Julia Wise (copied below), and told them to write their own stories. These are not related to LW culture specifically, but rather meant to explain where the women here are coming from. <strong>Warning:</strong> This article is a collection of anecdotes, NOT a logical argument. If you are not interested in anecdotes, don't read it.</p>\n<p>&nbsp;</p>\n<h3>Copied from the <a href=\"http://jdwise.blogspot.com/2012/04/nerd-life.html\">original article</a>&nbsp;(by a woman on LW) on Radiant Things:</h3>\n<p>It's lunchtime in fourth grade. I am explaining to Leslie, who has no friends but me, why we should stick together. &ldquo;We're both rejects,&rdquo; I tell her. She draws back, affronted. &ldquo;We're not rejects!&rdquo; she says. I'm puzzled. It hadn't occurred to me that she wanted to be normal.</p>\n<p>&hellip;................</p>\n<p>It's the first week of eighth grade. In a lesson on prehistory, the teacher is trying and failing to pronounce &ldquo;Australopithecus.&rdquo; I blurt out the correct pronunciation (which my father taught me in early childhood because he thought it was fun to say). The boy next to me gives me a glare and begins looking for alliterative insults. &ldquo;Fruity female&rdquo; is the best he can manage. &ldquo;Geek girl&rdquo; seems more apt, but I don't suggest it.</p>\n<p>&hellip;..................</p>\n<p>It's lunchtime in seventh grade. I'm sitting next to my two best friends, Bridget and Christine, on one side of a cafeteria table. We have been obsessed with Star Wars for a year now, and the school's two male Star Wars fans are seated opposite us. Under Greyson's leadership, we are making up roleplaying characters. I begin describing my character, a space-traveling musician named Anya. &ldquo;Why are your characters always girls?&rdquo; Grayson complains. &ldquo;Just because you're girls doesn't mean your characters have to be.&rdquo;</p>\n<p>&ldquo;Your characters are always boys,&rdquo; we retort. He's right, though &ndash; female characters are an anomaly in the Star Wars universe. George Lucas (a boy) populated his trilogy with 97% male characters.</p>\n<p>&hellip;................</p>\n<p>It's Bridget's thirteenth birthday, and four of us are spending the night at her house. While her parents sleep, we are roleplaying that we have been captured by Imperials and are escaping a detention cell. This is not papers-and-dice roleplaying, but advanced make-believe with lots of pretend blaster battles and dodging behind furniture.&nbsp;</p>\n<p>Christine and Cass, aspiring writers, use roleplaying as a way to test out plots in which they make daring raids and die nobly. Bridget, a future lawyer, and I, a future social worker, use it as a way to test out moral principles. Bridget has been trying to persuade us that the Empire is a legitimate government and we shouldn't be trying to overthrow it at all. I've been trying to persuade Amy that shooting stormtroopers is wrong. They are having none of it.&nbsp;</p>\n<p>We all like daring escapes, though, so we do plenty of that.</p>\n<p>&hellip;...............</p>\n<p>It's two weeks after the Columbine shootings, and the local paper has run an editorial denouncing parents who raise \"geeks and goths.\" I write my first-ever letter to the editor, defending geeks as kids parents should be proud of. A girl sidles up to me at the lunch table. \"I really liked your letter in the paper,\" she mutters, and skitters away.</p>\n<p>................</p>\n<p>It's tenth grade, and I can't bring myself to tell the president of the chess club how desperately I love him. One day I go to chess club just to be near him. There is only one other girl there, and she's really good at chess. I'm not, and I spend the meeting leaning silently on a wall because I can't stand to lose to a boy. Anyway, I despise the girls who join robotics club to be near boys they like, and I don't want to be one of them.</p>\n<p>................</p>\n<p>It's eleventh grade, and we are gathered after school to play Dungeons and Dragons. (My father, who originally forbid me to play D&amp;D because he had heard it would lead us to hack each other to pieces with axes, has relented.) Christine is Dungeonmaster, and she has recruited two feckless boys to play with us. One of them is in love with her.</p>\n<p>(Nugent points out that D&amp;D is essentially combat reworked for physically awkward people, a way of reducing battle to dice rolls and calculations. Christine has been trained by her uncle in the typical swords-and-sorcery style of play, but when she and I play the culture is different. All our adventures feature pauses for our characters to make tea and omelets.)</p>\n<p>On this afternoon, our characters are venturing into the countryside and come across two emaciated farmers who tell us their fields are unplowed because dark elves from the forest keep attacking them. &ldquo;They're going to starve if they don't get a crop in the ground,&rdquo; I declare. &ldquo;We've got to plow at least one field.&rdquo; The boys go along with this plan.</p>\n<p>&ldquo;The farmers tell you their plow has rusted and doesn't work,&rdquo; the Dungeonmaster informs us from behind her screen.&nbsp;</p>\n<p>I persist. &ldquo;There's got to be something we can use. I look around to see if there's anything else pointy I can use as a plow.&rdquo;&nbsp;</p>\n<p>The Dungeonmaster considers. &ldquo;There's a metal gate,&rdquo; she decides.</p>\n<p>&ldquo;Okay, I rig up some kind of harness and hitch it to the pony.&rdquo;</p>\n<p>&ldquo;It's rusty too,&rdquo; intones the Dungeonmaster, &ldquo;and pieces of it keep breaking off. Look, you're not supposed to be farming. You're supposed to go into the forest and find the dark elves. I don't have anything else about the farmers. The elves are the adventure.&rdquo; Reluctantly, I give up my agricultural rescue plan and we go into the forest to hack at elves.</p>\n<p>&hellip;............................</p>\n<p>I'm 25 and Jeff's sister's boyfriend is complaining that he never gets to play Magic: the Gathering because he doesn't know anyone who plays. &ldquo;You could play with Julia,&rdquo; Jeff suggests.&nbsp;</p>\n<p>&ldquo;Very funny,&rdquo; says Danner, rolling his eyes.</p>\n<p>Jeff and I look at each other. I realize geeks no longer read me as a geek. I still love ideas, love alternate imaginings of how life could be, love being right, but now I care about seeming normal.</p>\n<p>&ldquo;...I wasn't joking,&rdquo; Jeff says.&nbsp;</p>\n<p>&ldquo;It's okay,&rdquo; I reassure Danner. &ldquo;I used to play every day, but I've pretty much forgotten how.&rdquo;</p>\n<p>&nbsp;</p>\n<p>&hellip;............................</p>\n<p>&nbsp;</p>\n<p><strong>A's Submission</strong></p>\n<p>&nbsp;</p>\n<p>My creepy/danger alert was much higher at a meeting with a high-status (read: supposedly utility-generating, which includes attractive in the sense of pleasing or exciting to look at, but mostly the utility is supposed to be from actions, like work or play) man who was supposed to be my boss for an internship.</p>\n<p>The way he talked about the previous intern, a female, the sleazy way he looked while reminiscing and&nbsp;then had to smoke a cigarette, while in a meeting with me, my father (an employer who was abusive),&nbsp;and the internship program director, plus the fact that when I was walking towards the meeting room, the&nbsp;employees of the company, all men, stared at me and remarked, &ldquo;It&rsquo;s a girl,&rdquo; well, I became so creeped&nbsp;out that I didn&rsquo;t want to go back. It was hard, as a less articulate 16 year-old, to explain to the internship&nbsp;director all that stuff without sounding irrational. But not being able to explain my brain&rsquo;s priors (incl.&nbsp;abuses that it had previously been too na&iuml;ve/ignorant to warn against and prevent) wasn&rsquo;t going to&nbsp;change them or decrease the avoidance-inducing fear and anxiety.</p>\n<p>So after some awkward attempts to&nbsp;answer the internship director&rsquo;s question of why I didn&rsquo;t want to work there, I asked for a placement with&nbsp;a different company, which she couldn&rsquo;t do, unfortunately.</p>\n<p>&nbsp;</p>\n<p><strong>B's Submission</strong></p>\n<p>&nbsp;</p>\n<p>Words from my father&rsquo;s mouth, growing up: &ldquo;You *need* to be able to cook and keep a clean house, or what man would want to marry you?&rdquo;</p>\n<p>&hellip;................</p>\n<p>Sixth grade year, I had absolutely no friends whatsoever. A boy I had a bit of a crush on asked me out on a dare. I told him &ldquo;no,&rdquo; and he walked back to his laughing friends.</p>\n<p>&hellip;................</p>\n<p>In college I joined the local SCA (medieval) group, and took up heavy weapons combat. The local (almost all-male) &ldquo;stick jocks&rdquo; were very supportive and happy to help. Many had even read &ldquo;The Armored Rose&rdquo; and so knew about female-specific issues and how to adapt what they were teaching to deal with things like a lower center of gravity, less muscle mass, a different grip, and ingrained cultural hang-ups. The guys were great. But there was one problem: <em>There was no female-sized loaner armor</em>.</p>\n<p>See, armor is an expensive investment for a new hobby, and so local groups provide loaner armor for newbies, which generally consist of hand-me-downs from the more experienced fighters. We had a decent amount of new female fighters in our college groups, but without a pre-existing generation of female fighters (women hadn&rsquo;t even been allowed to fight until the 80s) there wasn&rsquo;t anything to hand down.&nbsp;</p>\n<p>The only scar I ever got from heavy combat was armor bite from wearing much-too-large loaner armor. I eventually got my own kit, and (Happy Ending) the upcoming generation of our group always made sure to acquire loaner armor for BOTH genders.</p>\n<p>&hellip;................</p>\n<p>Because of a lack of options, and not really having anywhere else to go, I moved in with my boyfriend and got married at a rather young age (20 and 22, respectively). I had no clue how to be independent. One of the most empowering things I ever did was starting work as an exotic dancer. After years of thinking that I couldn't support myself, it gave me the confidence that I could leave an unhappy marriage without ending up on the street (or more likely, mooching off friends and relatives). Another Happy Ending- Now I'm completely independent.</p>\n<p>&hellip;................</p>\n<p>Walking into the library. A man holds open the door for me. I smile and thank him as I walk through. He makes a sexual comment. I do the Look-Straight-Ahead-and-Walk-Quickly thing.&nbsp;</p>\n<p>&ldquo;Bitch,&rdquo; he spits out.</p>\n<p>It&rsquo;s not the first of this kind of interaction in my life, and it most <em>certainly</em> won&rsquo;t be the last (almost any time you are in an urban environment, without a male). But it hit harder than most because I had been expecting a polite interaction.</p>\n<p>Relevant link: http://goodmenproject.com/ethics-values/why-men-catcall/</p>\n<p>&hellip;................</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>The next post will be on Group Attribution Error, and will come out when I get around to it. :P</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"W9aNkPwtPhMrcfgj7": 1, "x6evH6MyPK3nxsoff": 1, "YQW2DxpZFTrqrxHBJ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YdrdHErogcGSxEBrm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}], "voteCount": 86, "baseScore": 97, "extendedScore": null, "score": 0.000213, "legacy": true, "legacyId": "20244", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 98, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Standard_Intro\">Standard Intro</h2>\n<p><strong id=\"The_following_section_will_be_at_the_top_of_all_posts_in_the_LW_Women_series_\">The following section will be at the top of all posts in the LW Women series.</strong></p>\n<p>About two months ago, I put out a call for anonymous submissions by the women on LW, with the idea that I would compile them into some kind of post. &nbsp;There is a LOT of material, so I am breaking them down into more manageable-sized themed posts.&nbsp;</p>\n<p>Seven women submitted, totaling about 18 pages.&nbsp;</p>\n<p><strong>Crocker's Warning</strong>-&nbsp;Submitters&nbsp;were told to not hold back for politeness. You are allowed to disagree, but these are candid comments; if you consider candidness impolite, I suggest you not read this post</p>\n<p><strong>To the submittrs</strong>- If you would like to respond anonymously to a comment (for example if there is a comment questioning something in your post, and you want to clarify), you can PM your message and I will post it for you. If this happens a lot, I might create a LW_Women sockpuppet account for the submitters to share.</p>\n<p><strong>Standard Disclaimer</strong>- Women have many different viewpoints, and just because I am acting as an intermediary to allow for anonymous communication does NOT mean that I agree with everything that will be posted in this series. (It would be rather impossible to, since there are some posts arguing opposite sides!)</p>\n<p><strong id=\"Please_do_NOT_break_anonymity__because_it_lowers_the_anonymity_of_the_rest_of_the_submitters_\">Please do NOT break anonymity, because it lowers the anonymity of the rest of the submitters.<a id=\"more\"></a><br></strong></p>\n<h2 id=\"Minimizing_the_Inferential_Distance\">Minimizing the Inferential Distance</h2>\n<p>One problem that I think exists in discussions about gender issues between men and women, is that the inferential distance is much greater than either group realizes. Women might assume that men know what experiences women might face, and so not explicitly mention specific examples. Men might assume they know what the women are talking about, but have never really heard specific examples. Or they might assume that these types of things only happened in the past, or not to the types of females in their in-group</p>\n<p>So for the first post in this series, I thought it would be worthwhile to try to lower this inferential distance, by sharing specific examples of what it's like as a smart/geeky female. When submitters didn't know what to write, I directed them to <a href=\"http://jdwise.blogspot.com/2012/04/nerd-life.html\">this article</a>, by Julia Wise (copied below), and told them to write their own stories. These are not related to LW culture specifically, but rather meant to explain where the women here are coming from. <strong>Warning:</strong> This article is a collection of anecdotes, NOT a logical argument. If you are not interested in anecdotes, don't read it.</p>\n<p>&nbsp;</p>\n<h3 id=\"Copied_from_the_original_article__by_a_woman_on_LW__on_Radiant_Things_\">Copied from the <a href=\"http://jdwise.blogspot.com/2012/04/nerd-life.html\">original article</a>&nbsp;(by a woman on LW) on Radiant Things:</h3>\n<p>It's lunchtime in fourth grade. I am explaining to Leslie, who has no friends but me, why we should stick together. \u201cWe're both rejects,\u201d I tell her. She draws back, affronted. \u201cWe're not rejects!\u201d she says. I'm puzzled. It hadn't occurred to me that she wanted to be normal.</p>\n<p>\u2026................</p>\n<p>It's the first week of eighth grade. In a lesson on prehistory, the teacher is trying and failing to pronounce \u201cAustralopithecus.\u201d I blurt out the correct pronunciation (which my father taught me in early childhood because he thought it was fun to say). The boy next to me gives me a glare and begins looking for alliterative insults. \u201cFruity female\u201d is the best he can manage. \u201cGeek girl\u201d seems more apt, but I don't suggest it.</p>\n<p>\u2026..................</p>\n<p>It's lunchtime in seventh grade. I'm sitting next to my two best friends, Bridget and Christine, on one side of a cafeteria table. We have been obsessed with Star Wars for a year now, and the school's two male Star Wars fans are seated opposite us. Under Greyson's leadership, we are making up roleplaying characters. I begin describing my character, a space-traveling musician named Anya. \u201cWhy are your characters always girls?\u201d Grayson complains. \u201cJust because you're girls doesn't mean your characters have to be.\u201d</p>\n<p>\u201cYour characters are always boys,\u201d we retort. He's right, though \u2013 female characters are an anomaly in the Star Wars universe. George Lucas (a boy) populated his trilogy with 97% male characters.</p>\n<p>\u2026................</p>\n<p>It's Bridget's thirteenth birthday, and four of us are spending the night at her house. While her parents sleep, we are roleplaying that we have been captured by Imperials and are escaping a detention cell. This is not papers-and-dice roleplaying, but advanced make-believe with lots of pretend blaster battles and dodging behind furniture.&nbsp;</p>\n<p>Christine and Cass, aspiring writers, use roleplaying as a way to test out plots in which they make daring raids and die nobly. Bridget, a future lawyer, and I, a future social worker, use it as a way to test out moral principles. Bridget has been trying to persuade us that the Empire is a legitimate government and we shouldn't be trying to overthrow it at all. I've been trying to persuade Amy that shooting stormtroopers is wrong. They are having none of it.&nbsp;</p>\n<p>We all like daring escapes, though, so we do plenty of that.</p>\n<p>\u2026...............</p>\n<p>It's two weeks after the Columbine shootings, and the local paper has run an editorial denouncing parents who raise \"geeks and goths.\" I write my first-ever letter to the editor, defending geeks as kids parents should be proud of. A girl sidles up to me at the lunch table. \"I really liked your letter in the paper,\" she mutters, and skitters away.</p>\n<p>................</p>\n<p>It's tenth grade, and I can't bring myself to tell the president of the chess club how desperately I love him. One day I go to chess club just to be near him. There is only one other girl there, and she's really good at chess. I'm not, and I spend the meeting leaning silently on a wall because I can't stand to lose to a boy. Anyway, I despise the girls who join robotics club to be near boys they like, and I don't want to be one of them.</p>\n<p>................</p>\n<p>It's eleventh grade, and we are gathered after school to play Dungeons and Dragons. (My father, who originally forbid me to play D&amp;D because he had heard it would lead us to hack each other to pieces with axes, has relented.) Christine is Dungeonmaster, and she has recruited two feckless boys to play with us. One of them is in love with her.</p>\n<p>(Nugent points out that D&amp;D is essentially combat reworked for physically awkward people, a way of reducing battle to dice rolls and calculations. Christine has been trained by her uncle in the typical swords-and-sorcery style of play, but when she and I play the culture is different. All our adventures feature pauses for our characters to make tea and omelets.)</p>\n<p>On this afternoon, our characters are venturing into the countryside and come across two emaciated farmers who tell us their fields are unplowed because dark elves from the forest keep attacking them. \u201cThey're going to starve if they don't get a crop in the ground,\u201d I declare. \u201cWe've got to plow at least one field.\u201d The boys go along with this plan.</p>\n<p>\u201cThe farmers tell you their plow has rusted and doesn't work,\u201d the Dungeonmaster informs us from behind her screen.&nbsp;</p>\n<p>I persist. \u201cThere's got to be something we can use. I look around to see if there's anything else pointy I can use as a plow.\u201d&nbsp;</p>\n<p>The Dungeonmaster considers. \u201cThere's a metal gate,\u201d she decides.</p>\n<p>\u201cOkay, I rig up some kind of harness and hitch it to the pony.\u201d</p>\n<p>\u201cIt's rusty too,\u201d intones the Dungeonmaster, \u201cand pieces of it keep breaking off. Look, you're not supposed to be farming. You're supposed to go into the forest and find the dark elves. I don't have anything else about the farmers. The elves are the adventure.\u201d Reluctantly, I give up my agricultural rescue plan and we go into the forest to hack at elves.</p>\n<p>\u2026............................</p>\n<p>I'm 25 and Jeff's sister's boyfriend is complaining that he never gets to play Magic: the Gathering because he doesn't know anyone who plays. \u201cYou could play with Julia,\u201d Jeff suggests.&nbsp;</p>\n<p>\u201cVery funny,\u201d says Danner, rolling his eyes.</p>\n<p>Jeff and I look at each other. I realize geeks no longer read me as a geek. I still love ideas, love alternate imaginings of how life could be, love being right, but now I care about seeming normal.</p>\n<p>\u201c...I wasn't joking,\u201d Jeff says.&nbsp;</p>\n<p>\u201cIt's okay,\u201d I reassure Danner. \u201cI used to play every day, but I've pretty much forgotten how.\u201d</p>\n<p>&nbsp;</p>\n<p>\u2026............................</p>\n<p>&nbsp;</p>\n<p><strong id=\"A_s_Submission\">A's Submission</strong></p>\n<p>&nbsp;</p>\n<p>My creepy/danger alert was much higher at a meeting with a high-status (read: supposedly utility-generating, which includes attractive in the sense of pleasing or exciting to look at, but mostly the utility is supposed to be from actions, like work or play) man who was supposed to be my boss for an internship.</p>\n<p>The way he talked about the previous intern, a female, the sleazy way he looked while reminiscing and&nbsp;then had to smoke a cigarette, while in a meeting with me, my father (an employer who was abusive),&nbsp;and the internship program director, plus the fact that when I was walking towards the meeting room, the&nbsp;employees of the company, all men, stared at me and remarked, \u201cIt\u2019s a girl,\u201d well, I became so creeped&nbsp;out that I didn\u2019t want to go back. It was hard, as a less articulate 16 year-old, to explain to the internship&nbsp;director all that stuff without sounding irrational. But not being able to explain my brain\u2019s priors (incl.&nbsp;abuses that it had previously been too na\u00efve/ignorant to warn against and prevent) wasn\u2019t going to&nbsp;change them or decrease the avoidance-inducing fear and anxiety.</p>\n<p>So after some awkward attempts to&nbsp;answer the internship director\u2019s question of why I didn\u2019t want to work there, I asked for a placement with&nbsp;a different company, which she couldn\u2019t do, unfortunately.</p>\n<p>&nbsp;</p>\n<p><strong id=\"B_s_Submission\">B's Submission</strong></p>\n<p>&nbsp;</p>\n<p>Words from my father\u2019s mouth, growing up: \u201cYou *need* to be able to cook and keep a clean house, or what man would want to marry you?\u201d</p>\n<p>\u2026................</p>\n<p>Sixth grade year, I had absolutely no friends whatsoever. A boy I had a bit of a crush on asked me out on a dare. I told him \u201cno,\u201d and he walked back to his laughing friends.</p>\n<p>\u2026................</p>\n<p>In college I joined the local SCA (medieval) group, and took up heavy weapons combat. The local (almost all-male) \u201cstick jocks\u201d were very supportive and happy to help. Many had even read \u201cThe Armored Rose\u201d and so knew about female-specific issues and how to adapt what they were teaching to deal with things like a lower center of gravity, less muscle mass, a different grip, and ingrained cultural hang-ups. The guys were great. But there was one problem: <em>There was no female-sized loaner armor</em>.</p>\n<p>See, armor is an expensive investment for a new hobby, and so local groups provide loaner armor for newbies, which generally consist of hand-me-downs from the more experienced fighters. We had a decent amount of new female fighters in our college groups, but without a pre-existing generation of female fighters (women hadn\u2019t even been allowed to fight until the 80s) there wasn\u2019t anything to hand down.&nbsp;</p>\n<p>The only scar I ever got from heavy combat was armor bite from wearing much-too-large loaner armor. I eventually got my own kit, and (Happy Ending) the upcoming generation of our group always made sure to acquire loaner armor for BOTH genders.</p>\n<p>\u2026................</p>\n<p>Because of a lack of options, and not really having anywhere else to go, I moved in with my boyfriend and got married at a rather young age (20 and 22, respectively). I had no clue how to be independent. One of the most empowering things I ever did was starting work as an exotic dancer. After years of thinking that I couldn't support myself, it gave me the confidence that I could leave an unhappy marriage without ending up on the street (or more likely, mooching off friends and relatives). Another Happy Ending- Now I'm completely independent.</p>\n<p>\u2026................</p>\n<p>Walking into the library. A man holds open the door for me. I smile and thank him as I walk through. He makes a sexual comment. I do the Look-Straight-Ahead-and-Walk-Quickly thing.&nbsp;</p>\n<p>\u201cBitch,\u201d he spits out.</p>\n<p>It\u2019s not the first of this kind of interaction in my life, and it most <em>certainly</em> won\u2019t be the last (almost any time you are in an urban environment, without a male). But it hit harder than most because I had been expecting a polite interaction.</p>\n<p>Relevant link: http://goodmenproject.com/ethics-values/why-men-catcall/</p>\n<p>\u2026................</p>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>The next post will be on Group Attribution Error, and will come out when I get around to it. :P</p>", "sections": [{"title": "Standard Intro", "anchor": "Standard_Intro", "level": 1}, {"title": "The following section will be at the top of all posts in the LW Women series.", "anchor": "The_following_section_will_be_at_the_top_of_all_posts_in_the_LW_Women_series_", "level": 3}, {"title": "Please do NOT break anonymity, because it lowers the anonymity of the rest of the submitters.", "anchor": "Please_do_NOT_break_anonymity__because_it_lowers_the_anonymity_of_the_rest_of_the_submitters_", "level": 3}, {"title": "Minimizing the Inferential Distance", "anchor": "Minimizing_the_Inferential_Distance", "level": 1}, {"title": "Copied from the original article\u00a0(by a woman on LW) on Radiant Things:", "anchor": "Copied_from_the_original_article__by_a_woman_on_LW__on_Radiant_Things_", "level": 2}, {"title": "A's Submission", "anchor": "A_s_Submission", "level": 3}, {"title": "B's Submission", "anchor": "B_s_Submission", "level": 3}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1261 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1262, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-25T23:34:06.733Z", "modifiedAt": null, "url": null, "title": "How minimal is our intelligence?", "slug": "how-minimal-is-our-intelligence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:57.785Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Douglas_Reay", "createdAt": "2012-02-19T14:40:26.403Z", "isAdmin": false, "displayName": "Douglas_Reay"}, "userId": "jpnrRPxHozDiGBqp2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2ManXWhNzvcsyHPyq/how-minimal-is-our-intelligence", "pageUrlRelative": "/posts/2ManXWhNzvcsyHPyq/how-minimal-is-our-intelligence", "linkUrl": "https://www.lesswrong.com/posts/2ManXWhNzvcsyHPyq/how-minimal-is-our-intelligence", "postedAtFormatted": "Sunday, November 25th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20minimal%20is%20our%20intelligence%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20minimal%20is%20our%20intelligence%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2ManXWhNzvcsyHPyq%2Fhow-minimal-is-our-intelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20minimal%20is%20our%20intelligence%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2ManXWhNzvcsyHPyq%2Fhow-minimal-is-our-intelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2ManXWhNzvcsyHPyq%2Fhow-minimal-is-our-intelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1834, "htmlBody": "<p>Gwern suggested that, if it were possible for civilization to have developed when our species had a lower IQ, then we'd still be dealing with the same problems, but we'd have a lower IQ with which to tackle them.&nbsp;&nbsp; Or, to put it another way, it is unsurprising that living in a civilization has posed problems that our species finds difficult to tackle, because if we were capable of solving such problems easily, we'd probably also have been capable of developing civilization earlier than we did.</p>\n<p>How true is that?</p>\n<p>In this post I plan to look in detail at the origins of civilization with an eye to considering how much the timing of it did depend directly upon the IQ of our species, rather than upon other factors.</p>\n<p>Although we don't have precise IQ test numbers for our immediate ancestral species, the fossil record is good enough to give us a clear idea of how brain size has changed over time:</p>\n<p><img src=\"http://www.alife.co.uk/essays/the_intelligence_explosion_is_happening_now/graphics/fossil_hominin_brain_percent.png\" alt=\"brain mass as a percent of body mass against time\" width=\"597\" /></p>\n<p>and we do have archaeological evidence of approximately when various technologies (such as pictograms, or using fire to cook meat) became common.<a id=\"more\"></a></p>\n<h2>The First City</h2>\n<p><img src=\"http://www.islamic-architecture.info/WA-IQ/MFD05-02.jpg\" alt=\"The Ziggurat of Ur-Nammu\" width=\"600\" /></p>\n<p>About 6,000 years ago (4000 BCE), Ur was a thriving trading village on <a href=\"http://toolserver.org/~geohack/geohack.php?pagename=Ur&amp;params=30_57_45_N_46_06_11_E_type:city\">the flood plain near the mouth of the river Euphrates</a> in what is now called southern Iraq and what historians call Sumeria.</p>\n<p>By 3000 BCE it was the heart of a city-state with a core built up populated area covering 37 acres, and would go on over the following thousand years to lead the Sumerian empire, raise a great brick Ziggurat to its patron moon goddess, and become the largest city in the world (65,000 people concentrated in 54 acres).</p>\n<p>It was eventually doomed by desertification and soil salination, caused by its own success (over-grazing and land clearing) but, by then, cities had spread throughout the fertile crescent of rivers at the intersection of the European, African and Asian land masses.</p>\n<p>Ur may not have been the first city, but it was the first one we know of that wasn't part of a false dawn - one whose culture and technologies did demonstrably spread to other areas.&nbsp; It was the flashpoint.</p>\n<p>We don't know for certain what it was about the culture surrounding the dawn of cities that made that particular combination of trade, writing, specialisation, hierarchy and religion communicable, when similar cultures from previous false dawns failed to spread.&nbsp;&nbsp; We can trace each of those elements to earlier sources, none of them were original to Ur, so perhaps it was a case of a critical mass achieving a self-sustaining reaction.</p>\n<p>What we can look at is why the conditions to allow a village to become a large enough city for such a critical mass of developments to accumulate, occurred at that time and place.</p>\n<h2>From Village to City</h2>\n<p>Motivation aside, the chief problem with sustaining large numbers of people together in a small area, over several generations, keeping them healthy enough for the population to grow without continual immigration, is ensuring access to a scalable renewable predictable source of calories.</p>\n<p>To be predictable means surviving famine years, which requires crops that can be stored for several years, such as grasses (wheat, barley and millet) with large seeds, and good storage facilities to store them in. &nbsp; It also means surviving pestilence, which requires having a variety of such crops.&nbsp;&nbsp;&nbsp; To be scalable and renewable means supplying water and nutrients to those crops on an ongoing basis, which requires irrigation and fertiliser from domesticated animals (if you don't have handy regular floods).</p>\n<p>Having large mammals available to domesticate, who can provide fertiliser and traction (pulling ploughs and harrows) certainly makes things easier, but doesn't seem to have been a large factor in the timing of the rise of civilisation, or particularly dependent upon the IQ of the human species.&nbsp;&nbsp; <a href=\"https://www.americanscientist.org/issues/issue.aspx?id=813&amp;y=0&amp;no=&amp;content=true&amp;page=3&amp;css=print\">Research</a> suggests that domestication may have been driven as much by the animals own behaviour as by human intention, with those animals daring to approach humans more closely getting first choice of discarded food.</p>\n<p>Re-planting seeds to ensure plants to gather in following years, leading to low nutrition grasses adapting into grains with high protein concentrations in the seeds, does seem to a mainly intentional human activity in that <a href=\"http://www.agri.ankara.edu.tr/fcrops/1289__SELECTION.pdf\">we can trace most of the gain in size of such plant species seeds</a> to locations where humans have transitioned from the palaeolithic hunter-gatherer culture (about 2.5 million years ago, to about 10,000 years ago) to the neolithic agricultural culture (about 10,000 year ago, onwards).</p>\n<p>Good grain storage seems to have developed incrementally starting with crude stone silo pit designs in 9500 BCE, and progressing by 6000 BCE to customised buildings with raised floors and sealed ceramic containers which could store 80 tons of wheat in good condition for 4 years or more.&nbsp; (Earthenware ceramics date to 25,000 BCE and earlier, though the potter's wheel, useful for mass production of regular storage vessels, does date to <a href=\"http://en.wikipedia.org/wiki/Ubaid_period\">the Ubaid period</a>.)</p>\n<p>The main key to the timing of the transition from village to city seems to have been not human technology but the confluence of climate and biology.&nbsp; <a href=\"http://en.wikipedia.org/wiki/Guns,_Germs,_and_Steel#The_theory_outlined\">Jared Diamond points the finger</a> at the geography of the region - the fertile crescent farmers had access to <a href=\"http://archaeology.about.com/od/domestications/a/plant_domestic.htm\">a wider variety of grains than anywhere else in the world</a> because that area links and has access to the species of three major land masses.&nbsp;&nbsp; The Mediterranean climate has a long dry season with a short period of rain, which made it ideal for growing grains (which are much easier to store for several years than, for instance bananas).&nbsp; And everything kicked off when the climate stabilised after the most recent ice age ended about 12,000 years ago.</p>\n<h2>Ice Ages</h2>\n<p>Strictly speaking, we're actually talking about the end of a \"glacial period\" rather than the end of an entire \"ice age\".&nbsp; The timeline goes:</p>\n<pre>200,000 years ago - 130,000 years ago : glacial period</pre>\n<pre>130,000 years ago - 110,000 years ago : interglacial period</pre>\n<pre>110,000 years ago -&nbsp; 12,000 years ago : glacial period</pre>\n<pre>&nbsp; 12,000 years ago - present : interglacial period</pre>\n<p><br />So the question now is, why didn't humanity spawn civilisation in the fertile crescent 130,000 years ago, during the last interglacial period?&nbsp; Why did it happen in this one?&nbsp; Did we get significantly brighter in the mean time?</p>\n<p>It isn't, on the face of it, an implausible idea.&nbsp; 100,000 years is long enough for evolutionary change to happen, and maybe inventing pottery or becoming farmers did take more brain power than humanity had back then.&nbsp; Or, if not IQ, perhaps it was some other mental change like attention span, or the capacity to obey written laws, live as a specialist in a hierarchy, or similar.</p>\n<p>But there's no evidence that this is the case, nor is there a need to hypothesise it because there is at least one genetic change we do know about during that time period, that is by itself sufficient to explain the lack of civilisation 130,000 years ago.&nbsp; And it has nothing to do with the brain.</p>\n<h2>Brains, Genes and Calories</h2>\n<p>Using the <a href=\"http://en.wikipedia.org/wiki/Bushmen#Subsistence\">San Bushpeople</a> as a guide to the palaeolithic diet, hunter-gather culture was able to support an average population density of one person per acre.&nbsp;&nbsp; Not that they ate badly, as individuals.&nbsp; Indeed, <a href=\"http://en.wikipedia.org/wiki/Paleolithic#Diet_and_nutrition\">they seem to have done better</a> than the early Neolithic farmers.&nbsp; But they had to be free to wander to follow nomadic food sources, and they were limited by access to food that the human body could use to create Docosahexaenoic acid, which is a fatty acid required for human brain development.&nbsp; Originally humans got this from fish living in the lakes and rivers of central Africa.&nbsp;&nbsp; However, about 80,000 years ago, <a href=\"http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0044926\">we developed a gene</a> that let us synthesise the same acid from other sources, freeing humanity to migrate away from the wet areas, past the dry northern part, and out into the fertile crescent.</p>\n<p>But there is a link between diet and brain.&nbsp; Although the human brain represents only 2% of the body weight, it receives 15% of the cardiac output, 20% of total body oxygen consumption, and 25% of total body glucose utilization.&nbsp; Brains are expensive, in terms of calories consumed.&nbsp; Although brain size or brain activity that uses up glucose is not linearly related to individual IQ, they are linked on a species level.</p>\n<p><a href=\"http://www.nature.com/mp/journal/v8/n1/full/4001095a.html\">IQ is polygenetic</a>, meaning that many different genes are relevant to a person's potential maximum IQ.&nbsp; (Note: there are many non-genetic factors that may prevent an individual reaching their potential).&nbsp;&nbsp; <a href=\"http://www.gwern.net/Drug%20heuristics\">Algernon's Law</a> suggests that genes affecting IQ that have multiple alleles still common in the human population are likely to have a cost associated with the alleles tending to increase IQ, otherwise they'd have displaced the competing alleles.&nbsp;&nbsp; In the same way that an animal species that develops the capability to grow a fur coat in response to cold weather is more advanced than one whose genes strictly determine that it will have a thick fur coat at all times, whether the weather is cold or hot; the polygenetic nature of human IQ gives human populations the ability to adapt and react on the time scale of just a few generations, increasing or decreasing the average IQ of the population as the environment changes to reduce or increase the penalties of particular trade-offs for particular alleles contributing to IQ.&nbsp;&nbsp; In particular, if the trade-off for some of those alleles is increased energy consumption and we look at a population of humans moving from an environment where calories are the bottleneck on how many offspring can be produced and survive, to an environment where calories are more easily available, then we might expect to see something similar to <a href=\"http://en.wikipedia.org/wiki/Flynn_effect\">the Flynn effect</a>.</p>\n<h2>Summary</h2>\n<p>There is no cause to suppose, even if the human genome 100,000 years ago had the full set of IQ-related-alleles present in our genome today, that they would have developed civilisation much sooner.</p>\n<p>&nbsp;</p>\n<p>\n<hr />\n</p>\n<h2>Comment Navigation Aide</h2>\n<p><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7vds\">link</a> - DuncanS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - animal vs human intelligence<br /><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7vdo\">link</a> - DuncanS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - brain size &amp; brain efficiency<br /><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7v6n\">link</a> - JaySwartz&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - adaptability vs intelligence<br /><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7v2k\">link</a> - RichardKennaway - does more intelligence tend to bring more societal happiness?<br /><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7v2i\">link</a> - mrglwrf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Ur vs Uruk<br /><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7v2f\">link</a> - NancyLebovitz&nbsp;&nbsp;&nbsp;&nbsp; - does decreased variance of intelligence tend to bring more societal happiness? <br /><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7v0d\">link</a> - fubarobfusco&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - victors writing history<br /><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7v3g\">link</a> --&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; consequentialist treatment of library burning<br /><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7vbu\">link</a> -- &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; the average net contribution to society of people working in academia <br /><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7uyp\">link</a> - John_Maxwell_IV - independent development of civilisation in the Americas<br /><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7ux4\">link</a> - shminux&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - How much of our IQ is dependant upon Docosahexaenoic acid?<br /><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7uwr\">link</a> - army1987&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - implications for the Great Filter<br /><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7uvk\">link</a> - Vladimir_Nesov&nbsp;&nbsp;&nbsp; - genome vs expressed IQ<br /><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7uvg\">link</a> - Vladimir_Nesov&nbsp;&nbsp;&nbsp; - Rhetorical nitpick<br /><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7uvd\">link</a> - Vaniver&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - IQ &amp; non-processor-speed components of problem solving<br /><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7uvb\">link</a> - JoshuaZ&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - breakthroughs don't tend to require geniuses in order to be made<br /><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7uv2\">link</a> - Desrtopa&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - cultural factorors<br /><br /><br /><br /><span class=\"comment-author\"></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ac84EpK6mZbPLzmqj": 2, "4cKQgA4S7xfNeeWXg": 2, "bY5MaF2EATwDkomvu": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2ManXWhNzvcsyHPyq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 59, "baseScore": 78, "extendedScore": null, "score": 0.00019304534596794673, "legacy": true, "legacyId": "20164", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 56, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Gwern suggested that, if it were possible for civilization to have developed when our species had a lower IQ, then we'd still be dealing with the same problems, but we'd have a lower IQ with which to tackle them.&nbsp;&nbsp; Or, to put it another way, it is unsurprising that living in a civilization has posed problems that our species finds difficult to tackle, because if we were capable of solving such problems easily, we'd probably also have been capable of developing civilization earlier than we did.</p>\n<p>How true is that?</p>\n<p>In this post I plan to look in detail at the origins of civilization with an eye to considering how much the timing of it did depend directly upon the IQ of our species, rather than upon other factors.</p>\n<p>Although we don't have precise IQ test numbers for our immediate ancestral species, the fossil record is good enough to give us a clear idea of how brain size has changed over time:</p>\n<p><img src=\"http://www.alife.co.uk/essays/the_intelligence_explosion_is_happening_now/graphics/fossil_hominin_brain_percent.png\" alt=\"brain mass as a percent of body mass against time\" width=\"597\"></p>\n<p>and we do have archaeological evidence of approximately when various technologies (such as pictograms, or using fire to cook meat) became common.<a id=\"more\"></a></p>\n<h2 id=\"The_First_City\">The First City</h2>\n<p><img src=\"http://www.islamic-architecture.info/WA-IQ/MFD05-02.jpg\" alt=\"The Ziggurat of Ur-Nammu\" width=\"600\"></p>\n<p>About 6,000 years ago (4000 BCE), Ur was a thriving trading village on <a href=\"http://toolserver.org/~geohack/geohack.php?pagename=Ur&amp;params=30_57_45_N_46_06_11_E_type:city\">the flood plain near the mouth of the river Euphrates</a> in what is now called southern Iraq and what historians call Sumeria.</p>\n<p>By 3000 BCE it was the heart of a city-state with a core built up populated area covering 37 acres, and would go on over the following thousand years to lead the Sumerian empire, raise a great brick Ziggurat to its patron moon goddess, and become the largest city in the world (65,000 people concentrated in 54 acres).</p>\n<p>It was eventually doomed by desertification and soil salination, caused by its own success (over-grazing and land clearing) but, by then, cities had spread throughout the fertile crescent of rivers at the intersection of the European, African and Asian land masses.</p>\n<p>Ur may not have been the first city, but it was the first one we know of that wasn't part of a false dawn - one whose culture and technologies did demonstrably spread to other areas.&nbsp; It was the flashpoint.</p>\n<p>We don't know for certain what it was about the culture surrounding the dawn of cities that made that particular combination of trade, writing, specialisation, hierarchy and religion communicable, when similar cultures from previous false dawns failed to spread.&nbsp;&nbsp; We can trace each of those elements to earlier sources, none of them were original to Ur, so perhaps it was a case of a critical mass achieving a self-sustaining reaction.</p>\n<p>What we can look at is why the conditions to allow a village to become a large enough city for such a critical mass of developments to accumulate, occurred at that time and place.</p>\n<h2 id=\"From_Village_to_City\">From Village to City</h2>\n<p>Motivation aside, the chief problem with sustaining large numbers of people together in a small area, over several generations, keeping them healthy enough for the population to grow without continual immigration, is ensuring access to a scalable renewable predictable source of calories.</p>\n<p>To be predictable means surviving famine years, which requires crops that can be stored for several years, such as grasses (wheat, barley and millet) with large seeds, and good storage facilities to store them in. &nbsp; It also means surviving pestilence, which requires having a variety of such crops.&nbsp;&nbsp;&nbsp; To be scalable and renewable means supplying water and nutrients to those crops on an ongoing basis, which requires irrigation and fertiliser from domesticated animals (if you don't have handy regular floods).</p>\n<p>Having large mammals available to domesticate, who can provide fertiliser and traction (pulling ploughs and harrows) certainly makes things easier, but doesn't seem to have been a large factor in the timing of the rise of civilisation, or particularly dependent upon the IQ of the human species.&nbsp;&nbsp; <a href=\"https://www.americanscientist.org/issues/issue.aspx?id=813&amp;y=0&amp;no=&amp;content=true&amp;page=3&amp;css=print\">Research</a> suggests that domestication may have been driven as much by the animals own behaviour as by human intention, with those animals daring to approach humans more closely getting first choice of discarded food.</p>\n<p>Re-planting seeds to ensure plants to gather in following years, leading to low nutrition grasses adapting into grains with high protein concentrations in the seeds, does seem to a mainly intentional human activity in that <a href=\"http://www.agri.ankara.edu.tr/fcrops/1289__SELECTION.pdf\">we can trace most of the gain in size of such plant species seeds</a> to locations where humans have transitioned from the palaeolithic hunter-gatherer culture (about 2.5 million years ago, to about 10,000 years ago) to the neolithic agricultural culture (about 10,000 year ago, onwards).</p>\n<p>Good grain storage seems to have developed incrementally starting with crude stone silo pit designs in 9500 BCE, and progressing by 6000 BCE to customised buildings with raised floors and sealed ceramic containers which could store 80 tons of wheat in good condition for 4 years or more.&nbsp; (Earthenware ceramics date to 25,000 BCE and earlier, though the potter's wheel, useful for mass production of regular storage vessels, does date to <a href=\"http://en.wikipedia.org/wiki/Ubaid_period\">the Ubaid period</a>.)</p>\n<p>The main key to the timing of the transition from village to city seems to have been not human technology but the confluence of climate and biology.&nbsp; <a href=\"http://en.wikipedia.org/wiki/Guns,_Germs,_and_Steel#The_theory_outlined\">Jared Diamond points the finger</a> at the geography of the region - the fertile crescent farmers had access to <a href=\"http://archaeology.about.com/od/domestications/a/plant_domestic.htm\">a wider variety of grains than anywhere else in the world</a> because that area links and has access to the species of three major land masses.&nbsp;&nbsp; The Mediterranean climate has a long dry season with a short period of rain, which made it ideal for growing grains (which are much easier to store for several years than, for instance bananas).&nbsp; And everything kicked off when the climate stabilised after the most recent ice age ended about 12,000 years ago.</p>\n<h2 id=\"Ice_Ages\">Ice Ages</h2>\n<p>Strictly speaking, we're actually talking about the end of a \"glacial period\" rather than the end of an entire \"ice age\".&nbsp; The timeline goes:</p>\n<pre>200,000 years ago - 130,000 years ago : glacial period</pre>\n<pre>130,000 years ago - 110,000 years ago : interglacial period</pre>\n<pre>110,000 years ago -&nbsp; 12,000 years ago : glacial period</pre>\n<pre>&nbsp; 12,000 years ago - present : interglacial period</pre>\n<p><br>So the question now is, why didn't humanity spawn civilisation in the fertile crescent 130,000 years ago, during the last interglacial period?&nbsp; Why did it happen in this one?&nbsp; Did we get significantly brighter in the mean time?</p>\n<p>It isn't, on the face of it, an implausible idea.&nbsp; 100,000 years is long enough for evolutionary change to happen, and maybe inventing pottery or becoming farmers did take more brain power than humanity had back then.&nbsp; Or, if not IQ, perhaps it was some other mental change like attention span, or the capacity to obey written laws, live as a specialist in a hierarchy, or similar.</p>\n<p>But there's no evidence that this is the case, nor is there a need to hypothesise it because there is at least one genetic change we do know about during that time period, that is by itself sufficient to explain the lack of civilisation 130,000 years ago.&nbsp; And it has nothing to do with the brain.</p>\n<h2 id=\"Brains__Genes_and_Calories\">Brains, Genes and Calories</h2>\n<p>Using the <a href=\"http://en.wikipedia.org/wiki/Bushmen#Subsistence\">San Bushpeople</a> as a guide to the palaeolithic diet, hunter-gather culture was able to support an average population density of one person per acre.&nbsp;&nbsp; Not that they ate badly, as individuals.&nbsp; Indeed, <a href=\"http://en.wikipedia.org/wiki/Paleolithic#Diet_and_nutrition\">they seem to have done better</a> than the early Neolithic farmers.&nbsp; But they had to be free to wander to follow nomadic food sources, and they were limited by access to food that the human body could use to create Docosahexaenoic acid, which is a fatty acid required for human brain development.&nbsp; Originally humans got this from fish living in the lakes and rivers of central Africa.&nbsp;&nbsp; However, about 80,000 years ago, <a href=\"http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0044926\">we developed a gene</a> that let us synthesise the same acid from other sources, freeing humanity to migrate away from the wet areas, past the dry northern part, and out into the fertile crescent.</p>\n<p>But there is a link between diet and brain.&nbsp; Although the human brain represents only 2% of the body weight, it receives 15% of the cardiac output, 20% of total body oxygen consumption, and 25% of total body glucose utilization.&nbsp; Brains are expensive, in terms of calories consumed.&nbsp; Although brain size or brain activity that uses up glucose is not linearly related to individual IQ, they are linked on a species level.</p>\n<p><a href=\"http://www.nature.com/mp/journal/v8/n1/full/4001095a.html\">IQ is polygenetic</a>, meaning that many different genes are relevant to a person's potential maximum IQ.&nbsp; (Note: there are many non-genetic factors that may prevent an individual reaching their potential).&nbsp;&nbsp; <a href=\"http://www.gwern.net/Drug%20heuristics\">Algernon's Law</a> suggests that genes affecting IQ that have multiple alleles still common in the human population are likely to have a cost associated with the alleles tending to increase IQ, otherwise they'd have displaced the competing alleles.&nbsp;&nbsp; In the same way that an animal species that develops the capability to grow a fur coat in response to cold weather is more advanced than one whose genes strictly determine that it will have a thick fur coat at all times, whether the weather is cold or hot; the polygenetic nature of human IQ gives human populations the ability to adapt and react on the time scale of just a few generations, increasing or decreasing the average IQ of the population as the environment changes to reduce or increase the penalties of particular trade-offs for particular alleles contributing to IQ.&nbsp;&nbsp; In particular, if the trade-off for some of those alleles is increased energy consumption and we look at a population of humans moving from an environment where calories are the bottleneck on how many offspring can be produced and survive, to an environment where calories are more easily available, then we might expect to see something similar to <a href=\"http://en.wikipedia.org/wiki/Flynn_effect\">the Flynn effect</a>.</p>\n<h2 id=\"Summary\">Summary</h2>\n<p>There is no cause to suppose, even if the human genome 100,000 years ago had the full set of IQ-related-alleles present in our genome today, that they would have developed civilisation much sooner.</p>\n<p>&nbsp;</p>\n<p>\n</p><hr>\n<p></p>\n<h2 id=\"Comment_Navigation_Aide\">Comment Navigation Aide</h2>\n<p><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7vds\">link</a> - DuncanS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - animal vs human intelligence<br><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7vdo\">link</a> - DuncanS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - brain size &amp; brain efficiency<br><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7v6n\">link</a> - JaySwartz&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - adaptability vs intelligence<br><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7v2k\">link</a> - RichardKennaway - does more intelligence tend to bring more societal happiness?<br><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7v2i\">link</a> - mrglwrf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Ur vs Uruk<br><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7v2f\">link</a> - NancyLebovitz&nbsp;&nbsp;&nbsp;&nbsp; - does decreased variance of intelligence tend to bring more societal happiness? <br><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7v0d\">link</a> - fubarobfusco&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - victors writing history<br><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7v3g\">link</a> --&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; consequentialist treatment of library burning<br><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7vbu\">link</a> -- &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; the average net contribution to society of people working in academia <br><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7uyp\">link</a> - John_Maxwell_IV - independent development of civilisation in the Americas<br><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7ux4\">link</a> - shminux&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - How much of our IQ is dependant upon Docosahexaenoic acid?<br><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7uwr\">link</a> - army1987&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - implications for the Great Filter<br><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7uvk\">link</a> - Vladimir_Nesov&nbsp;&nbsp;&nbsp; - genome vs expressed IQ<br><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7uvg\">link</a> - Vladimir_Nesov&nbsp;&nbsp;&nbsp; - Rhetorical nitpick<br><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7uvd\">link</a> - Vaniver&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - IQ &amp; non-processor-speed components of problem solving<br><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7uvb\">link</a> - JoshuaZ&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - breakthroughs don't tend to require geniuses in order to be made<br><a href=\"/lw/fk4/how_minimal_is_our_intelligence/7uv2\">link</a> - Desrtopa&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - cultural factorors<br><br><br><br><span class=\"comment-author\"></span></p>", "sections": [{"title": "The First City", "anchor": "The_First_City", "level": 1}, {"title": "From Village to City", "anchor": "From_Village_to_City", "level": 1}, {"title": "Ice Ages", "anchor": "Ice_Ages", "level": 1}, {"title": "Brains, Genes and Calories", "anchor": "Brains__Genes_and_Calories", "level": 1}, {"title": "Summary", "anchor": "Summary", "level": 1}, {"title": "Comment Navigation Aide", "anchor": "Comment_Navigation_Aide", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "214 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 214, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-26T00:52:12.885Z", "modifiedAt": null, "url": null, "title": "Credence calibration game FAQ", "slug": "credence-calibration-game-faq", "viewCount": null, "lastCommentedAt": "2019-08-03T11:57:52.847Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Academian", "createdAt": "2010-03-08T09:49:25.099Z", "isAdmin": false, "displayName": "Academian"}, "userId": "AbLN9sR8PDACCXKp7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ckg5JS8E5QXYhM89r/credence-calibration-game-faq", "pageUrlRelative": "/posts/Ckg5JS8E5QXYhM89r/credence-calibration-game-faq", "linkUrl": "https://www.lesswrong.com/posts/Ckg5JS8E5QXYhM89r/credence-calibration-game-faq", "postedAtFormatted": "Monday, November 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Credence%20calibration%20game%20FAQ&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACredence%20calibration%20game%20FAQ%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCkg5JS8E5QXYhM89r%2Fcredence-calibration-game-faq%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Credence%20calibration%20game%20FAQ%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCkg5JS8E5QXYhM89r%2Fcredence-calibration-game-faq", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCkg5JS8E5QXYhM89r%2Fcredence-calibration-game-faq", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 140, "htmlBody": "<p>Hey rationality friends, I just made this <a href=\"http://www.acritch.com/credence-game/\">FAQ for the credence calibration game</a>. &nbsp;So if you have people you'd like to introduce to it --- for example, to get them used to thinking of belief strengths as probabilities --- now is a good time :)</p>\n<div>Also, shameless promotion: please tweet/g+/like it; I want the world to be thinking in probabilities ASAP!</div>\n<div><br /></div>\n<div>*Also*, please email me (critch@math.berkeley.edu) if you're good at making apps quickly and are interested in improving the game or making a variant of it; I'm swamped in job applications right now, but could easily have a Skype or phone conversation about our cache of ideas for improvements / variations (e.g. collecting user data on a server, more question types, a variant awarding gambles rather than deterministic scores, a variant with clickable emotion buttons for the user...).</div>\n<div><br /></div>\n<div>Cheers!</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"rtha3oEP5vzkvdbbD": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ckg5JS8E5QXYhM89r", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 24, "extendedScore": null, "score": 5.1e-05, "legacy": true, "legacyId": "20268", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 57, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-26T01:05:57.789Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Engelbart: Insufficiently Recursive", "slug": "seq-rerun-engelbart-insufficiently-recursive", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/j4r8mtMj7j292Qsf5/seq-rerun-engelbart-insufficiently-recursive", "pageUrlRelative": "/posts/j4r8mtMj7j292Qsf5/seq-rerun-engelbart-insufficiently-recursive", "linkUrl": "https://www.lesswrong.com/posts/j4r8mtMj7j292Qsf5/seq-rerun-engelbart-insufficiently-recursive", "postedAtFormatted": "Monday, November 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Engelbart%3A%20Insufficiently%20Recursive&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Engelbart%3A%20Insufficiently%20Recursive%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj4r8mtMj7j292Qsf5%2Fseq-rerun-engelbart-insufficiently-recursive%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Engelbart%3A%20Insufficiently%20Recursive%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj4r8mtMj7j292Qsf5%2Fseq-rerun-engelbart-insufficiently-recursive", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj4r8mtMj7j292Qsf5%2Fseq-rerun-engelbart-insufficiently-recursive", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 175, "htmlBody": "<p>Today's post, <a href=\"/lw/w8/engelbart_insufficiently_recursive/\">Engelbart: Insufficiently Recursive</a> was originally published on 26 November 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Engelbart:_Insufficiently_Recursive\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The development of the mouse did lead to a productivity increase. But it didn't lead to a major productivity increase at creating future productivity increases. Therefore, the recursive process didn't take off properly.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fmz/seq_rerun_abstractdistant_future_bias/\">Abstract/Distant Future Bias</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "j4r8mtMj7j292Qsf5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0427644992229785e-06, "legacy": true, "legacyId": "20269", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NCb28Xdv7xDajtqtS", "dk8hxTqLEP8egXBSr", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-26T02:13:42.792Z", "modifiedAt": null, "url": null, "title": "What do you think of my reading list? ", "slug": "what-do-you-think-of-my-reading-list", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:05.724Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "sz7iRBbcQrHB9q9HE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YHztJ8K99wJGrP86M/what-do-you-think-of-my-reading-list", "pageUrlRelative": "/posts/YHztJ8K99wJGrP86M/what-do-you-think-of-my-reading-list", "linkUrl": "https://www.lesswrong.com/posts/YHztJ8K99wJGrP86M/what-do-you-think-of-my-reading-list", "postedAtFormatted": "Monday, November 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20do%20you%20think%20of%20my%20reading%20list%3F%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20do%20you%20think%20of%20my%20reading%20list%3F%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYHztJ8K99wJGrP86M%2Fwhat-do-you-think-of-my-reading-list%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20do%20you%20think%20of%20my%20reading%20list%3F%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYHztJ8K99wJGrP86M%2Fwhat-do-you-think-of-my-reading-list", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYHztJ8K99wJGrP86M%2Fwhat-do-you-think-of-my-reading-list", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 185, "htmlBody": "<p>(I am slightly anxious about making my first discussion post! If there is anything about this that should not have been done, or done differently, please let me know!)</p>\n<p>I'm new to this rationalist stuff.&nbsp;I want to improve my grasp on many topics. I want a richer vocabulary and set of concepts with which to discuss these topics. These are some nonfiction books I'm thinking of trying to eat this year, in no particular order of priority:</p>\n<p>&nbsp;</p>\n<p>So if you have read any of these books and would like to help me with this:</p>\n<p>Are there any that you would endorse? Which books might I be I wasting my time with?&nbsp;Are any of them&nbsp;too crank or pseudoscientific to be worthy of my time? Ones I had better save for later because I couldn't get anything out of them with my high school level math ability? Books that are similar to a different book that does the same thing better? Or any books along these lines you think it would be a CRIME not to include? Or any other suggestions? I would be grateful for any bit of help!&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YHztJ8K99wJGrP86M", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 6, "extendedScore": null, "score": 4e-06, "legacy": true, "legacyId": "20270", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-26T05:40:29.186Z", "modifiedAt": null, "url": null, "title": "Meetup : Berkeley meetup: Hermeneutics!", "slug": "meetup-berkeley-meetup-hermeneutics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:02.423Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Nisan", "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/s9gYDndgC8LEnwdfR/meetup-berkeley-meetup-hermeneutics", "pageUrlRelative": "/posts/s9gYDndgC8LEnwdfR/meetup-berkeley-meetup-hermeneutics", "linkUrl": "https://www.lesswrong.com/posts/s9gYDndgC8LEnwdfR/meetup-berkeley-meetup-hermeneutics", "postedAtFormatted": "Monday, November 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berkeley%20meetup%3A%20Hermeneutics!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berkeley%20meetup%3A%20Hermeneutics!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs9gYDndgC8LEnwdfR%2Fmeetup-berkeley-meetup-hermeneutics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berkeley%20meetup%3A%20Hermeneutics!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs9gYDndgC8LEnwdfR%2Fmeetup-berkeley-meetup-hermeneutics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fs9gYDndgC8LEnwdfR%2Fmeetup-berkeley-meetup-hermeneutics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 125, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/gd'>Berkeley meetup: Hermeneutics!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 November 2012 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This Wednesday's meetup will feature Hermeneutics!, a brand-new game invented by Scott Alexander. You can read about it on his blog:</p>\n\n<p><a href=\"http://squid314.livejournal.com/343000.html\" rel=\"nofollow\">http://squid314.livejournal.com/343000.html</a></p>\n\n<p>The purpose of the game is to see how easy it is to come up with plausible false arguments, and how plausible such arguments can sound coming from others. As far as I know, this may be the first time the game has ever been played.\nDoors open at 7pm, and we'll start playing the game at 7:30pm. For directions to Zendo, see the mailing list:\n<a href=\"http://groups.google.com/group/bayarealesswrong\" rel=\"nofollow\">http://groups.google.com/group/bayarealesswrong</a>\nor call me at:\n<a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a>\nNext week we can have another social-skills-themed meetup.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/gd'>Berkeley meetup: Hermeneutics!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "s9gYDndgC8LEnwdfR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 1.0429198163715248e-06, "legacy": true, "legacyId": "20274", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berkeley_meetup__Hermeneutics_\">Discussion article for the meetup : <a href=\"/meetups/gd\">Berkeley meetup: Hermeneutics!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 November 2012 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This Wednesday's meetup will feature Hermeneutics!, a brand-new game invented by Scott Alexander. You can read about it on his blog:</p>\n\n<p><a href=\"http://squid314.livejournal.com/343000.html\" rel=\"nofollow\">http://squid314.livejournal.com/343000.html</a></p>\n\n<p>The purpose of the game is to see how easy it is to come up with plausible false arguments, and how plausible such arguments can sound coming from others. As far as I know, this may be the first time the game has ever been played.\nDoors open at 7pm, and we'll start playing the game at 7:30pm. For directions to Zendo, see the mailing list:\n<a href=\"http://groups.google.com/group/bayarealesswrong\" rel=\"nofollow\">http://groups.google.com/group/bayarealesswrong</a>\nor call me at:\n<a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a>\nNext week we can have another social-skills-themed meetup.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berkeley_meetup__Hermeneutics_1\">Discussion article for the meetup : <a href=\"/meetups/gd\">Berkeley meetup: Hermeneutics!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berkeley meetup: Hermeneutics!", "anchor": "Discussion_article_for_the_meetup___Berkeley_meetup__Hermeneutics_", "level": 1}, {"title": "Discussion article for the meetup : Berkeley meetup: Hermeneutics!", "anchor": "Discussion_article_for_the_meetup___Berkeley_meetup__Hermeneutics_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-26T12:34:58.794Z", "modifiedAt": null, "url": null, "title": "Meetup :  Moscow: Applied Rationality and Cognitive Biases", "slug": "meetup-moscow-applied-rationality-and-cognitive-biases", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yuu", "createdAt": "2012-04-04T16:48:49.513Z", "isAdmin": false, "displayName": "Yuu"}, "userId": "MBtCqzM7BePuwToxX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/H4TWWNPKz8WWS3ewb/meetup-moscow-applied-rationality-and-cognitive-biases", "pageUrlRelative": "/posts/H4TWWNPKz8WWS3ewb/meetup-moscow-applied-rationality-and-cognitive-biases", "linkUrl": "https://www.lesswrong.com/posts/H4TWWNPKz8WWS3ewb/meetup-moscow-applied-rationality-and-cognitive-biases", "postedAtFormatted": "Monday, November 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20%20Moscow%3A%20Applied%20Rationality%20and%20Cognitive%20Biases&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20%20Moscow%3A%20Applied%20Rationality%20and%20Cognitive%20Biases%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH4TWWNPKz8WWS3ewb%2Fmeetup-moscow-applied-rationality-and-cognitive-biases%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20%20Moscow%3A%20Applied%20Rationality%20and%20Cognitive%20Biases%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH4TWWNPKz8WWS3ewb%2Fmeetup-moscow-applied-rationality-and-cognitive-biases", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FH4TWWNPKz8WWS3ewb%2Fmeetup-moscow-applied-rationality-and-cognitive-biases", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 147, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/ge'> Moscow: Applied Rationality and Cognitive Biases</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 December 2012 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Rossiya, Moscow, ulitsa Ostozhenka 14/2</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet at \u201cSubway\u201d restaurant, entrance from Lopukhinskiy pereulok. Look for a table with \u201cLW\u201d banner, I will be there from 16:00 MSK.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li><p>Applied rationality: practice. We will improve our rationality skills.</p></li>\n<li><p>Cognitive biases analysis. Here is <a href=\"https://docs.google.com/document/d/1fNqnLwnAB4oD7ePu6gK7WNZwoL4RuDyx2Nu_O5AMxpE/edit\" rel=\"nofollow\">the link to the list of biases we will work on</a> (in Russian).</p></li>\n</ul>\n\n<p>If you are going for the first time, please fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p>\n\n<p>N. B. Google may show incorrect location, please use <a href=\"http://maps.yandex.ru/?text=%D0%A0%D0%BE%D1%81%D1%81%D0%B8%D1%8F%2C%20%D0%9C%D0%BE%D1%81%D0%BA%D0%B2%D0%B0%2C%20%D1%83%D0%BB%D0%B8%D1%86%D0%B0%20%D0%9E%D1%81%D1%82%D0%BE%D0%B6%D0%B5%D0%BD%D0%BA%D0%B0%2C%2014%2F2&amp;sll=37.599139%2C55.74185&amp;ll=37.599139%2C55.741850&amp;spn=0.014377%2C0.004449&amp;z=17&amp;l=map\" rel=\"nofollow\">Yandex maps</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/ge'> Moscow: Applied Rationality and Cognitive Biases</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "H4TWWNPKz8WWS3ewb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0431544019465901e-06, "legacy": true, "legacyId": "20285", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup____Moscow__Applied_Rationality_and_Cognitive_Biases\">Discussion article for the meetup : <a href=\"/meetups/ge\"> Moscow: Applied Rationality and Cognitive Biases</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 December 2012 04:00:00PM (+0400)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Rossiya, Moscow, ulitsa Ostozhenka 14/2</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will meet at \u201cSubway\u201d restaurant, entrance from Lopukhinskiy pereulok. Look for a table with \u201cLW\u201d banner, I will be there from 16:00 MSK.</p>\n\n<p>Main topics:</p>\n\n<ul>\n<li><p>Applied rationality: practice. We will improve our rationality skills.</p></li>\n<li><p>Cognitive biases analysis. Here is <a href=\"https://docs.google.com/document/d/1fNqnLwnAB4oD7ePu6gK7WNZwoL4RuDyx2Nu_O5AMxpE/edit\" rel=\"nofollow\">the link to the list of biases we will work on</a> (in Russian).</p></li>\n</ul>\n\n<p>If you are going for the first time, please fill <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dHY4Qy1WOTUtc1ZLU21ORjh1VEtCa3c6MA\" rel=\"nofollow\">this one minute form</a> (in Russian), to share your contact information. You can also use personal messages here, or drop a message at lw@lesswrong.ru to contact me for any reason.</p>\n\n<p>N. B. Google may show incorrect location, please use <a href=\"http://maps.yandex.ru/?text=%D0%A0%D0%BE%D1%81%D1%81%D0%B8%D1%8F%2C%20%D0%9C%D0%BE%D1%81%D0%BA%D0%B2%D0%B0%2C%20%D1%83%D0%BB%D0%B8%D1%86%D0%B0%20%D0%9E%D1%81%D1%82%D0%BE%D0%B6%D0%B5%D0%BD%D0%BA%D0%B0%2C%2014%2F2&amp;sll=37.599139%2C55.74185&amp;ll=37.599139%2C55.741850&amp;spn=0.014377%2C0.004449&amp;z=17&amp;l=map\" rel=\"nofollow\">Yandex maps</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup____Moscow__Applied_Rationality_and_Cognitive_Biases1\">Discussion article for the meetup : <a href=\"/meetups/ge\"> Moscow: Applied Rationality and Cognitive Biases</a></h2>", "sections": [{"title": "Discussion article for the meetup :  Moscow: Applied Rationality and Cognitive Biases", "anchor": "Discussion_article_for_the_meetup____Moscow__Applied_Rationality_and_Cognitive_Biases", "level": 1}, {"title": "Discussion article for the meetup :  Moscow: Applied Rationality and Cognitive Biases", "anchor": "Discussion_article_for_the_meetup____Moscow__Applied_Rationality_and_Cognitive_Biases1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-26T18:06:57.407Z", "modifiedAt": null, "url": null, "title": "Meetup : Weekly meetup, Champaign IL: Cafe Paradiso", "slug": "meetup-weekly-meetup-champaign-il-cafe-paradiso", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:03.086Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "aspera", "createdAt": "2012-03-20T21:28:32.004Z", "isAdmin": false, "displayName": "aspera"}, "userId": "W58nKSarCzqXj8TPS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vtHyY662iyARzyxYz/meetup-weekly-meetup-champaign-il-cafe-paradiso", "pageUrlRelative": "/posts/vtHyY662iyARzyxYz/meetup-weekly-meetup-champaign-il-cafe-paradiso", "linkUrl": "https://www.lesswrong.com/posts/vtHyY662iyARzyxYz/meetup-weekly-meetup-champaign-il-cafe-paradiso", "postedAtFormatted": "Monday, November 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Weekly%20meetup%2C%20Champaign%20IL%3A%20Cafe%20Paradiso&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Weekly%20meetup%2C%20Champaign%20IL%3A%20Cafe%20Paradiso%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvtHyY662iyARzyxYz%2Fmeetup-weekly-meetup-champaign-il-cafe-paradiso%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Weekly%20meetup%2C%20Champaign%20IL%3A%20Cafe%20Paradiso%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvtHyY662iyARzyxYz%2Fmeetup-weekly-meetup-champaign-il-cafe-paradiso", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvtHyY662iyARzyxYz%2Fmeetup-weekly-meetup-champaign-il-cafe-paradiso", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 78, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/gf'>Weekly meetup, Champaign IL: Cafe Paradiso</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">28 November 2012 08:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">801 South Lincoln Avenue, Urbana, IL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Let's meet at 8pm. We decided last time that we'd like to start talking about <a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">Timeless decision theory</a>. It's a big topic, but try to come to the meeting with questions or discussion points. Also, let's talk about doing something social next week.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/gf'>Weekly meetup, Champaign IL: Cafe Paradiso</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vtHyY662iyARzyxYz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0433423541502555e-06, "legacy": true, "legacyId": "20286", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Weekly_meetup__Champaign_IL__Cafe_Paradiso\">Discussion article for the meetup : <a href=\"/meetups/gf\">Weekly meetup, Champaign IL: Cafe Paradiso</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">28 November 2012 08:00:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">801 South Lincoln Avenue, Urbana, IL</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Let's meet at 8pm. We decided last time that we'd like to start talking about <a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">Timeless decision theory</a>. It's a big topic, but try to come to the meeting with questions or discussion points. Also, let's talk about doing something social next week.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Weekly_meetup__Champaign_IL__Cafe_Paradiso1\">Discussion article for the meetup : <a href=\"/meetups/gf\">Weekly meetup, Champaign IL: Cafe Paradiso</a></h2>", "sections": [{"title": "Discussion article for the meetup : Weekly meetup, Champaign IL: Cafe Paradiso", "anchor": "Discussion_article_for_the_meetup___Weekly_meetup__Champaign_IL__Cafe_Paradiso", "level": 1}, {"title": "Discussion article for the meetup : Weekly meetup, Champaign IL: Cafe Paradiso", "anchor": "Discussion_article_for_the_meetup___Weekly_meetup__Champaign_IL__Cafe_Paradiso1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-26T20:56:00.183Z", "modifiedAt": null, "url": null, "title": "Centre for the Study of Existential Risk (CSER) at Cambridge makes headlines.", "slug": "centre-for-the-study-of-existential-risk-cser-at-cambridge", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:09.435Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "betterthanwell", "createdAt": "2009-02-28T00:39:39.875Z", "isAdmin": false, "displayName": "betterthanwell"}, "userId": "W9LCYGeCRvhd5aREo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3CfZCfFApvHrXMT9T/centre-for-the-study-of-existential-risk-cser-at-cambridge", "pageUrlRelative": "/posts/3CfZCfFApvHrXMT9T/centre-for-the-study-of-existential-risk-cser-at-cambridge", "linkUrl": "https://www.lesswrong.com/posts/3CfZCfFApvHrXMT9T/centre-for-the-study-of-existential-risk-cser-at-cambridge", "postedAtFormatted": "Monday, November 26th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Centre%20for%20the%20Study%20of%20Existential%20Risk%20(CSER)%20at%20Cambridge%20makes%20headlines.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACentre%20for%20the%20Study%20of%20Existential%20Risk%20(CSER)%20at%20Cambridge%20makes%20headlines.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3CfZCfFApvHrXMT9T%2Fcentre-for-the-study-of-existential-risk-cser-at-cambridge%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Centre%20for%20the%20Study%20of%20Existential%20Risk%20(CSER)%20at%20Cambridge%20makes%20headlines.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3CfZCfFApvHrXMT9T%2Fcentre-for-the-study-of-existential-risk-cser-at-cambridge", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3CfZCfFApvHrXMT9T%2Fcentre-for-the-study-of-existential-risk-cser-at-cambridge", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 428, "htmlBody": "<p>As of an hour ago, I had not yet heard of the&nbsp;Centre for the Study of Existential Risk.</p>\n<p>Luke <a href=\"/lw/bze/new_xrisk_organization_at_cambridge_university/ \">announced it to Less Wrong</a>, as The University of Cambridge announced it to the world, back in April:</p>\n<blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \"><em><a style=\"color: #8a8a8b; \" href=\"http://cser.org/\">CSER</a>&nbsp;at Cambridge University joins&nbsp;<a style=\"color: #8a8a8b; \" href=\"/lw/9iy/new_xrisk_organizations/\">the others</a>.</em></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \"><em>Good people involved so far, but the expected output depends hugely on who they pick to run the thing.</em></p>\n</blockquote>\n<p>CSER is scheduled to launch&nbsp;<a href=\"http://phys.org/news/2012-11-cambridge-technology-humans.html\">next year</a>.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<h2>Here is a small selection of CSER press coverage from the last two days:</h2>\n<p><a href=\"http://www.bbc.co.uk/news/technology-20501091\">http://www.bbc.co.uk/news/technology-20501091</a></p>\n<p><a href=\"http://www.guardian.co.uk/education/shortcuts/2012/nov/26/cambridge-university-terminator-studies\">http://www.guardian.co.uk/education/shortcuts/2012/nov/26/cambridge-university-terminator-studies</a></p>\n<p><a href=\"http://www.dailymail.co.uk/news/article-2238152/Cambridge-University-open-Terminator-centre-study-threat-humans-artificial-intelligence.html\">http://www.dailymail.co.uk/news/article-2238152/Cambridge-University-open-Terminator-centre-study-threat-humans-artificial-intelligence.html</a></p>\n<p><a href=\"http://www.theregister.co.uk/2012/11/26/new_centre_human_extinction_risks/\">http://www.theregister.co.uk/2012/11/26/new_centre_human_extinction_risks/</a></p>\n<p><a href=\"http://www.slashgear.com/new-ai-think-tank-hopes-to-get-real-on-existential-risk-26258246/\">http://www.slashgear.com/new-ai-think-tank-hopes-to-get-real-on-existential-risk-26258246/</a></p>\n<p><a href=\"http://www.techradar.com/news/world-of-tech/super-brains-to-guard-against-robot-apocalypse-1115293\">http://www.techradar.com/news/world-of-tech/super-brains-to-guard-against-robot-apocalypse-1115293</a></p>\n<p><a href=\"http://www.hindustantimes.com/world-news/Europe/Cambridge-to-study-risks-from-robots-at-Terminator-Centre/Article1-964746.aspx\">http://www.hindustantimes.com/world-news/Europe/Cambridge-to-study-risks-from-robots-at-Terminator-Centre/Article1-964746.aspx</a></p>\n<p><a href=\"http://economictimes.indiatimes.com/news/news-by-industry/et-cetera/cambridge-to-study-risks-from-robots-at-terminator-centre/articleshow/17372042.cms\">http://economictimes.indiatimes.com/news/news-by-industry/et-cetera/cambridge-to-study-risks-from-robots-at-terminator-centre/articleshow/17372042.cms</a></p>\n<p><a href=\"http://www.extremetech.com/extreme/141372-judgment-day-update-disneys-grenade-catching-robot-and-the-burger-flipping-robot-that-could-replace-2-million-us-workers\">http://www.extremetech.com/extreme/141372-judgment-day-update-disneys-grenade-catching-robot-and-the-burger-flipping-robot-that-could-replace-2-million-us-workers</a></p>\n<p><a href=\"http://slashdot.org/topic/bi/cambridge-university-vs-skynet/\">http://slashdot.org/topic/bi/cambridge-university-vs-skynet/</a></p>\n<p><a href=\"http://www.businessinsider.com/researchers-robots-risk-human-civilization-2012-11\">http://www.businessinsider.com/researchers-robots-risk-human-civilization-2012-11</a></p>\n<p><a href=\"http://www.newscientist.com/article/dn22534-megarisks-that-could-drive-us-to-extinction.html\">http://www.newscientist.com/article/dn22534-megarisks-that-could-drive-us-to-extinction.html</a></p>\n<p><a href=\"http://news.cnet.com/8301-11386_3-57553993-76/killer-robots-cambridge-brains-to-assess-ai-risk/\">http://news.cnet.com/8301-11386_3-57553993-76/killer-robots-cambridge-brains-to-assess-ai-risk/</a></p>\n<p><a href=\"http://www.globalpost.com/dispatches/globalpost-blogs/weird-wide-web/cambridge-university-opens-so-called-termintor-centre-stu\">http://www.globalpost.com/dispatches/globalpost-blogs/weird-wide-web/cambridge-university-opens-so-called-termintor-centre-stu</a></p>\n<p><a href=\"http://www.washingtonpost.com/world/europe/cambridge-university-to-open-center-studying-the-risks-of-technology-to-humans/2012/11/25/e551f4d0-3733-11e2-9258-ac7c78d5c680_story.html\">http://www.washingtonpost.com/world/europe/cambridge-university-to-open-center-studying-the-risks-of-technology-to-humans/2012/11/25/e551f4d0-3733-11e2-9258-ac7c78d5c680_story.html</a></p>\n<p><a href=\"http://www.foxnews.com/tech/2012/11/26/terminator-center-to-open-at-cambridge-university/\">http://www.foxnews.com/tech/2012/11/26/terminator-center-to-open-at-cambridge-university/</a></p>\n<div>\n<p>Google News:&nbsp;<a href=\"https://www.google.com/news?ncl=d8ZGet9Cl7PpT-MktEUm9mSxCqbcM&amp;q=CSER+cambridge&amp;lr=English&amp;hl=en \">All 119 news sources...</a></p>\n</div>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>Here's an excerpt from one quite typical story appearing in tech-tabloid theregister.co.uk today:</p>\n<p>&nbsp;</p>\n<blockquote>\n<h2 style=\"margin: 0px 0px 0.25em; color: #303030; font-size: 18px; padding-bottom: 0px; border: none; font-weight: normal; \"><a href=\"http://www.theregister.co.uk/2012/11/26/new_centre_human_extinction_risks/\">Cambridge boffins fear 'Pandora's Unboxing' and RISE of the MACHINES</a></h2>\n<p style=\"font-family: Arial, FreeSans, Helvetica, sans-serif; font-size: 14px; line-height: 21px; \"><em>Boffins at Cambridge University want to set up a new centre to determine what humankind will do when ultra-intelligent machines like the Terminator or HAL pose \"extinction-level\" risks to our species.</em></p>\n<p style=\"font-family: Arial, FreeSans, Helvetica, sans-serif; font-size: 14px; line-height: 21px; \"><em>A philosopher, a scientist and a software engineer are proposing the creation of a Centre for the Study of Existential Risk (CSER) to analyse the ultimate risks to the future of mankind - including bio- and nanotech, extreme climate change, nuclear war and artificial intelligence.</em></p>\n<p style=\"font-family: Arial, FreeSans, Helvetica, sans-serif; font-size: 14px; line-height: 21px; \"><em>Apart from the frequent portrayal of evil - or just misguidedly deadly - AI in science fiction, actual real scientists have also theorised that super-intelligent machines could be a danger to the human race.</em></p>\n<p style=\"font-family: Arial, FreeSans, Helvetica, sans-serif; font-size: 14px; line-height: 21px; \"><em>Jaan Tallinn, the former software engineer who was one of the founders of Skype, has campaigned for serious discussion of the ethical and safety aspects of artificial general intelligence (AGI).</em></p>\n<p style=\"font-family: Arial, FreeSans, Helvetica, sans-serif; font-size: 14px; line-height: 21px; \"><em>Tallinn has said that he sometimes feels he is more likely to die from an AI accident than from cancer or heart disease, CSER co-founder and philosopher Huw Price said.<br />[...]</em></p>\n</blockquote>\n<div><br /></div>\n<div>\n<hr />\n</div>\n<div><br /></div>\n<div>The source for these stories appears to be a press release from the University of Cambridge:</div>\n<div><br /></div>\n<h2><a href=\"http://www.cam.ac.uk/research/news/humanitys-last-invention-and-our-uncertain-future/\">Humanity&rsquo;s last invention and our uncertain future</a></h2>\n<blockquote>\n<div><em>In 1965, Irving John &lsquo;Jack&rsquo; Good sat down and wrote a paper for New Scientist called Speculations concerning the first ultra-intelligent machine. Good, a Cambridge-trained mathematician, Bletchley Park cryptographer, pioneering computer scientist and friend of Alan Turing, wrote that in the near future an ultra-intelligent machine would be built. [...]&nbsp;</em></div>\n</blockquote>\n<div>\n<div>\n<p><a href=\"http://www.cam.ac.uk/research/news/humanitys-last-invention-and-our-uncertain-future/\">http://www.cam.ac.uk/research/news/humanitys-last-invention-and-our-uncertain-future/</a></p>\n</div>\n</div>\n<div><br /></div>\n<div>\n<hr />\n</div>\n<div><br /></div>\n<div><strong><del>Three</del> Four quick observations:</strong></div>\n<div><br /></div>\n<div>1: That's <em>a lot</em> of Terminator II photos.</div>\n<div>2: FHI at Oxford and the Singularity Institute does not often get this kind of attention.</div>\n<div>\n<div>3: CSER doesn't appear to have published anything yet.</div>\n</div>\n<div>\n<div>4: The number of people who have heard the term \"existential risk\" must have doubled a few times today.</div>\n<div>\n<div><br /></div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3CfZCfFApvHrXMT9T", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 29, "extendedScore": null, "score": 1.0434380845004287e-06, "legacy": true, "legacyId": "20287", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>As of an hour ago, I had not yet heard of the&nbsp;Centre for the Study of Existential Risk.</p>\n<p>Luke <a href=\"/lw/bze/new_xrisk_organization_at_cambridge_university/ \">announced it to Less Wrong</a>, as The University of Cambridge announced it to the world, back in April:</p>\n<blockquote>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \"><em><a style=\"color: #8a8a8b; \" href=\"http://cser.org/\">CSER</a>&nbsp;at Cambridge University joins&nbsp;<a style=\"color: #8a8a8b; \" href=\"/lw/9iy/new_xrisk_organizations/\">the others</a>.</em></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify; \"><em>Good people involved so far, but the expected output depends hugely on who they pick to run the thing.</em></p>\n</blockquote>\n<p>CSER is scheduled to launch&nbsp;<a href=\"http://phys.org/news/2012-11-cambridge-technology-humans.html\">next year</a>.</p>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<h2 id=\"Here_is_a_small_selection_of_CSER_press_coverage_from_the_last_two_days_\">Here is a small selection of CSER press coverage from the last two days:</h2>\n<p><a href=\"http://www.bbc.co.uk/news/technology-20501091\">http://www.bbc.co.uk/news/technology-20501091</a></p>\n<p><a href=\"http://www.guardian.co.uk/education/shortcuts/2012/nov/26/cambridge-university-terminator-studies\">http://www.guardian.co.uk/education/shortcuts/2012/nov/26/cambridge-university-terminator-studies</a></p>\n<p><a href=\"http://www.dailymail.co.uk/news/article-2238152/Cambridge-University-open-Terminator-centre-study-threat-humans-artificial-intelligence.html\">http://www.dailymail.co.uk/news/article-2238152/Cambridge-University-open-Terminator-centre-study-threat-humans-artificial-intelligence.html</a></p>\n<p><a href=\"http://www.theregister.co.uk/2012/11/26/new_centre_human_extinction_risks/\">http://www.theregister.co.uk/2012/11/26/new_centre_human_extinction_risks/</a></p>\n<p><a href=\"http://www.slashgear.com/new-ai-think-tank-hopes-to-get-real-on-existential-risk-26258246/\">http://www.slashgear.com/new-ai-think-tank-hopes-to-get-real-on-existential-risk-26258246/</a></p>\n<p><a href=\"http://www.techradar.com/news/world-of-tech/super-brains-to-guard-against-robot-apocalypse-1115293\">http://www.techradar.com/news/world-of-tech/super-brains-to-guard-against-robot-apocalypse-1115293</a></p>\n<p><a href=\"http://www.hindustantimes.com/world-news/Europe/Cambridge-to-study-risks-from-robots-at-Terminator-Centre/Article1-964746.aspx\">http://www.hindustantimes.com/world-news/Europe/Cambridge-to-study-risks-from-robots-at-Terminator-Centre/Article1-964746.aspx</a></p>\n<p><a href=\"http://economictimes.indiatimes.com/news/news-by-industry/et-cetera/cambridge-to-study-risks-from-robots-at-terminator-centre/articleshow/17372042.cms\">http://economictimes.indiatimes.com/news/news-by-industry/et-cetera/cambridge-to-study-risks-from-robots-at-terminator-centre/articleshow/17372042.cms</a></p>\n<p><a href=\"http://www.extremetech.com/extreme/141372-judgment-day-update-disneys-grenade-catching-robot-and-the-burger-flipping-robot-that-could-replace-2-million-us-workers\">http://www.extremetech.com/extreme/141372-judgment-day-update-disneys-grenade-catching-robot-and-the-burger-flipping-robot-that-could-replace-2-million-us-workers</a></p>\n<p><a href=\"http://slashdot.org/topic/bi/cambridge-university-vs-skynet/\">http://slashdot.org/topic/bi/cambridge-university-vs-skynet/</a></p>\n<p><a href=\"http://www.businessinsider.com/researchers-robots-risk-human-civilization-2012-11\">http://www.businessinsider.com/researchers-robots-risk-human-civilization-2012-11</a></p>\n<p><a href=\"http://www.newscientist.com/article/dn22534-megarisks-that-could-drive-us-to-extinction.html\">http://www.newscientist.com/article/dn22534-megarisks-that-could-drive-us-to-extinction.html</a></p>\n<p><a href=\"http://news.cnet.com/8301-11386_3-57553993-76/killer-robots-cambridge-brains-to-assess-ai-risk/\">http://news.cnet.com/8301-11386_3-57553993-76/killer-robots-cambridge-brains-to-assess-ai-risk/</a></p>\n<p><a href=\"http://www.globalpost.com/dispatches/globalpost-blogs/weird-wide-web/cambridge-university-opens-so-called-termintor-centre-stu\">http://www.globalpost.com/dispatches/globalpost-blogs/weird-wide-web/cambridge-university-opens-so-called-termintor-centre-stu</a></p>\n<p><a href=\"http://www.washingtonpost.com/world/europe/cambridge-university-to-open-center-studying-the-risks-of-technology-to-humans/2012/11/25/e551f4d0-3733-11e2-9258-ac7c78d5c680_story.html\">http://www.washingtonpost.com/world/europe/cambridge-university-to-open-center-studying-the-risks-of-technology-to-humans/2012/11/25/e551f4d0-3733-11e2-9258-ac7c78d5c680_story.html</a></p>\n<p><a href=\"http://www.foxnews.com/tech/2012/11/26/terminator-center-to-open-at-cambridge-university/\">http://www.foxnews.com/tech/2012/11/26/terminator-center-to-open-at-cambridge-university/</a></p>\n<div>\n<p>Google News:&nbsp;<a href=\"https://www.google.com/news?ncl=d8ZGet9Cl7PpT-MktEUm9mSxCqbcM&amp;q=CSER+cambridge&amp;lr=English&amp;hl=en \">All 119 news sources...</a></p>\n</div>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<p>Here's an excerpt from one quite typical story appearing in tech-tabloid theregister.co.uk today:</p>\n<p>&nbsp;</p>\n<blockquote>\n<h2 style=\"margin: 0px 0px 0.25em; color: #303030; font-size: 18px; padding-bottom: 0px; border: none; font-weight: normal; \" id=\"Cambridge_boffins_fear__Pandora_s_Unboxing__and_RISE_of_the_MACHINES\"><a href=\"http://www.theregister.co.uk/2012/11/26/new_centre_human_extinction_risks/\">Cambridge boffins fear 'Pandora's Unboxing' and RISE of the MACHINES</a></h2>\n<p style=\"font-family: Arial, FreeSans, Helvetica, sans-serif; font-size: 14px; line-height: 21px; \"><em>Boffins at Cambridge University want to set up a new centre to determine what humankind will do when ultra-intelligent machines like the Terminator or HAL pose \"extinction-level\" risks to our species.</em></p>\n<p style=\"font-family: Arial, FreeSans, Helvetica, sans-serif; font-size: 14px; line-height: 21px; \"><em>A philosopher, a scientist and a software engineer are proposing the creation of a Centre for the Study of Existential Risk (CSER) to analyse the ultimate risks to the future of mankind - including bio- and nanotech, extreme climate change, nuclear war and artificial intelligence.</em></p>\n<p style=\"font-family: Arial, FreeSans, Helvetica, sans-serif; font-size: 14px; line-height: 21px; \"><em>Apart from the frequent portrayal of evil - or just misguidedly deadly - AI in science fiction, actual real scientists have also theorised that super-intelligent machines could be a danger to the human race.</em></p>\n<p style=\"font-family: Arial, FreeSans, Helvetica, sans-serif; font-size: 14px; line-height: 21px; \"><em>Jaan Tallinn, the former software engineer who was one of the founders of Skype, has campaigned for serious discussion of the ethical and safety aspects of artificial general intelligence (AGI).</em></p>\n<p style=\"font-family: Arial, FreeSans, Helvetica, sans-serif; font-size: 14px; line-height: 21px; \"><em>Tallinn has said that he sometimes feels he is more likely to die from an AI accident than from cancer or heart disease, CSER co-founder and philosopher Huw Price said.<br>[...]</em></p>\n</blockquote>\n<div><br></div>\n<div>\n<hr>\n</div>\n<div><br></div>\n<div>The source for these stories appears to be a press release from the University of Cambridge:</div>\n<div><br></div>\n<h2 id=\"Humanity_s_last_invention_and_our_uncertain_future\"><a href=\"http://www.cam.ac.uk/research/news/humanitys-last-invention-and-our-uncertain-future/\">Humanity\u2019s last invention and our uncertain future</a></h2>\n<blockquote>\n<div><em>In 1965, Irving John \u2018Jack\u2019 Good sat down and wrote a paper for New Scientist called Speculations concerning the first ultra-intelligent machine. Good, a Cambridge-trained mathematician, Bletchley Park cryptographer, pioneering computer scientist and friend of Alan Turing, wrote that in the near future an ultra-intelligent machine would be built. [...]&nbsp;</em></div>\n</blockquote>\n<div>\n<div>\n<p><a href=\"http://www.cam.ac.uk/research/news/humanitys-last-invention-and-our-uncertain-future/\">http://www.cam.ac.uk/research/news/humanitys-last-invention-and-our-uncertain-future/</a></p>\n</div>\n</div>\n<div><br></div>\n<div>\n<hr>\n</div>\n<div><br></div>\n<div><strong><del>Three</del> Four quick observations:</strong></div>\n<div><br></div>\n<div>1: That's <em>a lot</em> of Terminator II photos.</div>\n<div>2: FHI at Oxford and the Singularity Institute does not often get this kind of attention.</div>\n<div>\n<div>3: CSER doesn't appear to have published anything yet.</div>\n</div>\n<div>\n<div>4: The number of people who have heard the term \"existential risk\" must have doubled a few times today.</div>\n<div>\n<div><br></div>\n</div>\n</div>", "sections": [{"title": "Here is a small selection of CSER press coverage from the last two days:", "anchor": "Here_is_a_small_selection_of_CSER_press_coverage_from_the_last_two_days_", "level": 1}, {"title": "Cambridge boffins fear 'Pandora's Unboxing' and RISE of the MACHINES", "anchor": "Cambridge_boffins_fear__Pandora_s_Unboxing__and_RISE_of_the_MACHINES", "level": 1}, {"title": "Humanity\u2019s last invention and our uncertain future", "anchor": "Humanity_s_last_invention_and_our_uncertain_future", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "16 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ewgw6iEx4ihGeJdck", "GuKvMA5gBra9n4BiM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-27T02:24:31.320Z", "modifiedAt": null, "url": null, "title": "[META] Retributive downvoting: Why?", "slug": "meta-retributive-downvoting-why", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:05.048Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ialdabaoth", "createdAt": "2012-10-11T00:04:32.345Z", "isAdmin": false, "displayName": "ialdabaoth"}, "userId": "335oJYQyzcd85RAoe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yCX7b44shPuKfWLjT/meta-retributive-downvoting-why", "pageUrlRelative": "/posts/yCX7b44shPuKfWLjT/meta-retributive-downvoting-why", "linkUrl": "https://www.lesswrong.com/posts/yCX7b44shPuKfWLjT/meta-retributive-downvoting-why", "postedAtFormatted": "Tuesday, November 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BMETA%5D%20Retributive%20downvoting%3A%20Why%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BMETA%5D%20Retributive%20downvoting%3A%20Why%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyCX7b44shPuKfWLjT%2Fmeta-retributive-downvoting-why%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BMETA%5D%20Retributive%20downvoting%3A%20Why%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyCX7b44shPuKfWLjT%2Fmeta-retributive-downvoting-why", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyCX7b44shPuKfWLjT%2Fmeta-retributive-downvoting-why", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 177, "htmlBody": "<p>Several people posted recently in a <a href=\"/lw/fmc/lw_women_minimizing_the_inferential_distance/\">thread on women</a>, mostly espousing feminist views - only to find that someone had declined to respond to their post, but instead browsed their history and downvoted every single comment or article they had ever posted.</p>\n<p>I have two questions:</p>\n<p>1. Why would you come to a site like this and pollute the karma system? How does it make you smarter? How does it make anyone else on the site smarter?</p>\n<p>2. What would be a good technical workaround? In my mind, some system that detects mass-downvoting and flags a user for review would be preferable, but what should happen then? Should the system be more lenient to higher-karma posters? Who should perform the review process? What should be done with those whom the reviewer ascertains are abusing the karma system? I would prefer some kind of lesson that is more corrective than retributive - it seems to me that people who would perform this behavior are exactly the sort of people who need some of the lessons that this site provides. Any ideas?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yCX7b44shPuKfWLjT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 15, "extendedScore": null, "score": 5.7e-05, "legacy": true, "legacyId": "20288", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 111, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YdrdHErogcGSxEBrm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-27T04:54:51.719Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Total Nano Domination", "slug": "seq-rerun-total-nano-domination", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uhmRzfq3gLxwNtKQ9/seq-rerun-total-nano-domination", "pageUrlRelative": "/posts/uhmRzfq3gLxwNtKQ9/seq-rerun-total-nano-domination", "linkUrl": "https://www.lesswrong.com/posts/uhmRzfq3gLxwNtKQ9/seq-rerun-total-nano-domination", "postedAtFormatted": "Tuesday, November 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Total%20Nano%20Domination&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Total%20Nano%20Domination%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuhmRzfq3gLxwNtKQ9%2Fseq-rerun-total-nano-domination%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Total%20Nano%20Domination%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuhmRzfq3gLxwNtKQ9%2Fseq-rerun-total-nano-domination", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuhmRzfq3gLxwNtKQ9%2Fseq-rerun-total-nano-domination", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 186, "htmlBody": "<p>Today's post, <a href=\"/lw/w9/total_nano_domination/\">Total Nano Domination</a> was originally published on 27 November 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Total_Nano_Domination\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>If you get a small advantage in nanotechnology, that might not be enough to take over the world. But if you use that small advantage in nanotechnology to gain a major advancement in bots, you could gain an extraordinary amount of power very fast.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fn1/seq_rerun_engelbart_insufficiently_recursive/\">Engelbart: Insufficiently Recursive</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uhmRzfq3gLxwNtKQ9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0437093451038665e-06, "legacy": true, "legacyId": "20290", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5hX44Kuz5No6E6RS9", "j4r8mtMj7j292Qsf5", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-27T16:03:30.053Z", "modifiedAt": null, "url": null, "title": "Holding your own LW Solstice", "slug": "holding-your-own-lw-solstice", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:05.670Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uxt56Qa7bXbgMMfCb/holding-your-own-lw-solstice", "pageUrlRelative": "/posts/uxt56Qa7bXbgMMfCb/holding-your-own-lw-solstice", "linkUrl": "https://www.lesswrong.com/posts/uxt56Qa7bXbgMMfCb/holding-your-own-lw-solstice", "postedAtFormatted": "Tuesday, November 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Holding%20your%20own%20LW%20Solstice&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHolding%20your%20own%20LW%20Solstice%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fuxt56Qa7bXbgMMfCb%2Fholding-your-own-lw-solstice%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Holding%20your%20own%20LW%20Solstice%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fuxt56Qa7bXbgMMfCb%2Fholding-your-own-lw-solstice", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fuxt56Qa7bXbgMMfCb%2Fholding-your-own-lw-solstice", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 332, "htmlBody": "<p>Winter is coming, and with it, the Less Wrong Solstice, a <a href=\"/lw/8x5/ritual_report_nyc_less_wrong_solstice_celebration/\">celebration of humanity's progress over the centuries and how our relationship with sun and winter has changed</a>.</p>\n<p>There is, of course, the <a href=\"/meetups/en\">public meetup in NYC</a>. But I've now had two different people, in different regions, discuss with me the possibility of holding local Solstice parties. Currently it's unclear whether either of them are going to happen, but after the second person asked about it I realized I should probably post about it here.</p>\n<p>I was already putting together materials that would make it easier for other people to hold their own events, I just hadn't expected to need it for this year. I expect to have them *mostly* finished by the end of this week (this means a pdf of a book of songs, and a powerpoint you can use instead of a songbook that includes scrolling lyrics and helps keep people focused on a central location rather than staring down at the pages in their lap)</p>\n<p>It's still a sizeable amount of work to put a Solstice party together. You'll probably want to tailor it for the interests of your own friends. If you're doing something similar to what I'm doing, you'll want:</p>\n<p>1) People to prepare tasty dishes for a communal dinner</p>\n<p>2) At least one musically skilled person to lead songs, and/or good orator to do readings.</p>\n<p>3) Several enthusiastic people who are excited about singing songs, whether or not they have a musical background</p>\n<p>4) A bunch of light sources to extinguish and relight over the course of the evening.</p>\n<p>We're holding the NY event on the 15th so people who are busy on the actual Solstice (due to family obligations) can come. But if you're doing it with local people, you can probably hold it on the Solstice itself (Friday, December&nbsp;21), or whenever is convenient for you. (The themes of the night also make for a good New Years party)</p>\n<p>Let me know if you'd like access to the materials as they become available.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uxt56Qa7bXbgMMfCb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 6, "extendedScore": null, "score": 2.4e-05, "legacy": true, "legacyId": "20309", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jES7mcPvKpfmzMTgC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-27T18:06:30.022Z", "modifiedAt": null, "url": null, "title": "Meetup : Montreal Meetup - Biases and Biased Boardgaming", "slug": "meetup-montreal-meetup-biases-and-biased-boardgaming", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:08.440Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Paul_G", "createdAt": "2012-06-08T01:42:32.451Z", "isAdmin": false, "displayName": "Paul_G"}, "userId": "g4WQoWHmq4HNzTJDC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zzsNpwTuduGNggT7G/meetup-montreal-meetup-biases-and-biased-boardgaming", "pageUrlRelative": "/posts/zzsNpwTuduGNggT7G/meetup-montreal-meetup-biases-and-biased-boardgaming", "linkUrl": "https://www.lesswrong.com/posts/zzsNpwTuduGNggT7G/meetup-montreal-meetup-biases-and-biased-boardgaming", "postedAtFormatted": "Tuesday, November 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Montreal%20Meetup%20-%20Biases%20and%20Biased%20Boardgaming&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Montreal%20Meetup%20-%20Biases%20and%20Biased%20Boardgaming%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzzsNpwTuduGNggT7G%2Fmeetup-montreal-meetup-biases-and-biased-boardgaming%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Montreal%20Meetup%20-%20Biases%20and%20Biased%20Boardgaming%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzzsNpwTuduGNggT7G%2Fmeetup-montreal-meetup-biases-and-biased-boardgaming", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzzsNpwTuduGNggT7G%2Fmeetup-montreal-meetup-biases-and-biased-boardgaming", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 94, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/gg'>Montreal Meetup - Biases and Biased Boardgaming</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">03 December 2012 03:57:30PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\"> 655 Avenue du Pr\u00e9sident Kennedy, Montreal, QC H3A 3H9</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Another weekly meeting of the Montreal group!\nWe're going to be discussing different cognitive biases, and then if we have time, trying out biased boardgaming! Games of choice will probably be Pandemic and/or Shadowrift.\nWe'll be upstairs, with some sort of sign that says \"LessWrong\". Also, we'll have board games. Hopefully.\nSee you there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/gg'>Montreal Meetup - Biases and Biased Boardgaming</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zzsNpwTuduGNggT7G", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.0441580601587134e-06, "legacy": true, "legacyId": "20310", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Montreal_Meetup___Biases_and_Biased_Boardgaming\">Discussion article for the meetup : <a href=\"/meetups/gg\">Montreal Meetup - Biases and Biased Boardgaming</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">03 December 2012 03:57:30PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\"> 655 Avenue du Pr\u00e9sident Kennedy, Montreal, QC H3A 3H9</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Another weekly meeting of the Montreal group!\nWe're going to be discussing different cognitive biases, and then if we have time, trying out biased boardgaming! Games of choice will probably be Pandemic and/or Shadowrift.\nWe'll be upstairs, with some sort of sign that says \"LessWrong\". Also, we'll have board games. Hopefully.\nSee you there.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Montreal_Meetup___Biases_and_Biased_Boardgaming1\">Discussion article for the meetup : <a href=\"/meetups/gg\">Montreal Meetup - Biases and Biased Boardgaming</a></h2>", "sections": [{"title": "Discussion article for the meetup : Montreal Meetup - Biases and Biased Boardgaming", "anchor": "Discussion_article_for_the_meetup___Montreal_Meetup___Biases_and_Biased_Boardgaming", "level": 1}, {"title": "Discussion article for the meetup : Montreal Meetup - Biases and Biased Boardgaming", "anchor": "Discussion_article_for_the_meetup___Montreal_Meetup___Biases_and_Biased_Boardgaming1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "9 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-27T19:31:51.515Z", "modifiedAt": null, "url": null, "title": "A definition of wireheading", "slug": "a-definition-of-wireheading", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:29.416Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Anja", "createdAt": "2012-11-03T19:59:14.080Z", "isAdmin": false, "displayName": "Anja"}, "userId": "ptRyuMRZHMt6KdDtZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aMXhaj6zZBgbTrfqA/a-definition-of-wireheading", "pageUrlRelative": "/posts/aMXhaj6zZBgbTrfqA/a-definition-of-wireheading", "linkUrl": "https://www.lesswrong.com/posts/aMXhaj6zZBgbTrfqA/a-definition-of-wireheading", "postedAtFormatted": "Tuesday, November 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20definition%20of%20wireheading&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20definition%20of%20wireheading%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaMXhaj6zZBgbTrfqA%2Fa-definition-of-wireheading%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20definition%20of%20wireheading%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaMXhaj6zZBgbTrfqA%2Fa-definition-of-wireheading", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaMXhaj6zZBgbTrfqA%2Fa-definition-of-wireheading", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1522, "htmlBody": "<p><a title=\"Wireheading\" href=\"http://wiki.lesswrong.com/wiki/Wireheading\">Wireheading</a> has been debated on Less Wrong <a title=\"Happy?\" href=\"/lw/1lb/are_wireheads_happy/\">over </a>and <a title=\"Why not?\" href=\"/lw/69r/why_no_wireheading/\">over</a> and <a title=\"Wrong Question\" href=\"/lw/1oc/you_cannot_be_mistaken_about_not_wanting_to/\">over</a> again, and people's opinions seem to be grounded in strong intuitions. I could not find any consistent definition around, so I wonder how much of the debate is over the <a title=\"Making Beliefs pay rent\" href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">sound of falling trees</a>. This article is an attempt to get closer to a definition that captures people's intuitions and eliminates confusion.&nbsp;</p>\n<h3>Typical Examples</h3>\n<p>Let's start with describing the typical <a title=\"Prototype theory\" href=\"http://en.wikipedia.org/wiki/Prototype_theory\">exemplars</a> of the category \"Wireheading\" that come to mind.</p>\n<ul>\n<li><em><strong>Stimulation of the brain via electrodes.&nbsp;</strong></em>Picture a rat in a sterile metal laboratory cage, electrodes attached to its tiny head, monotonically pushing a lever with its feet once every 5 seconds. In the 1950s Peter Milner and <a title=\"J Olds\" href=\"http://en.wikipedia.org/wiki/James_Olds\">James Olds</a> <a title=\"Pleasure Center?\" href=\"http://en.wikipedia.org/wiki/Pleasure_center\">discovered</a> that electrical currents, applied to the nucleus accumbens, incentivized rodents to seek repetitive stimulation to the point where they starved to death. &nbsp;</li>\n</ul>\n<ul>\n<li><em><strong>Humans on drugs.&nbsp;</strong></em>Often mentioned in the context of wireheading is heroin addiction. An even better example is the drug soma in Huxley's novel <a title=\"Brave new world\" href=\"http://en.wikipedia.org/wiki/Brave_New_World\">\"Brave new world\"</a>: Whenever the protagonists feel bad, they can swallow a harmless pill and enjoy \"the warm, the richly coloured, the infinitely friendly world of soma-holiday. How kind, how good-looking, how delightfully amusing every one was!\"</li>\n</ul>\n<ul>\n<li><strong><em>The <a title=\"Experience machine\" href=\"http://en.wikipedia.org/wiki/Experience_machine\">experience machine</a>.</em>&nbsp;</strong>In 1974 the philosopher Robert Nozick created a thought experiment about a machine you can step into that produces a perfectly pleasurable virtual reality for the rest of your life. So how many of you would want to do that? To <a title=\"Comic\" href=\"http://www.smbc-comics.com/index.php?db=comics&amp;id=2625\">quote Zach Weiner</a>: &nbsp;\"I would not! Because I want to experience reality, with all its ups and downs and comedies and tragedies. Better to try to glimpse the blinding light of the truth than to dwell in the darkness... Say the machine actually exists and I have one? Okay I'm in.\"&nbsp;</li>\n</ul>\n<ul>\n<li><strong><em>An AGI resetting its utility function</em>.&nbsp;</strong>Let's assume we create a powerful AGI able to tamper with its own utility function. It modifies the function to always output maximal utility. The AGI then goes to great lengths to enlarge the set of floating point numbers on the computer it is running on, to achieve even higher utility.</li>\n</ul>\n<p>What do all these examples have in common? There is an agent in them that produces \"counterfeit utility\" that is potentially worthless compared to some other, idealized true set of goals.</p>\n<h3>Agency &amp; Wireheading</h3>\n<p>First I want to discuss what we mean when we say agent. Obviously a human is an agent, unless they are brain dead, or maybe in a coma. A rock however is not an agent. An AGI is an agent, but what about the kitchen robot that washes the dishes? What about bacteria that move in the direction of the highest sugar gradient? A colony of ants?&nbsp;</p>\n<p style=\"padding-left: 30px;\"><strong>Definition</strong>: An <em><strong>agent</strong></em>&nbsp;is an algorithm that models the effects of (several different) possible future actions on the world and performs the action that yields the highest number according to some evaluation procedure.&nbsp;</p>\n<p>For the purpose of including corner cases and resolving debate over what constitutes a world model we will simply make this definition gradual and say that <em><strong>a</strong></em><em><strong>gency</strong>&nbsp;</em>is proportional to the quality of the world model (compared with reality) and the quality of the evaluation procedure.&nbsp;A quick sanity check then yields that a rock has no world model and no agency, whereas bacteria who change direction in response to the sugar gradient have a very rudimentary model of the sugar content of the water and thus a tiny little bit of agency. Humans have a lot of agency: the more effective their actions are, the more agency they have.</p>\n<p>There are however ways to improve upon the efficiency of a person's actions, e.g. by giving them super powers, which does not necessarily improve on their world model or decision theory (but requires the agent who is doing the improvement to have a really good world model and decision theory). Similarly a person's agency can be restricted by other people or circumstance, which leads to definitions of <a title=\"Agency\" href=\"http://en.wikipedia.org/wiki/Agency_(philosophy)\">agency</a> (as the capacity to act) in law, sociology and philosophy that depend on other factors than just the quality of the world model/decision theory. Since our definition needs to capture arbitrary agents, including artificial intelligences, it will necessarily lose some of this nuance. In return we will hopefully end up with a definition that is less dependent on the particular set of effectors the agent uses to influence the physical world; looking at AI from a theoretician's perspective, I consider effectors to be arbitrarily exchangeable and smoothly improvable. (Sorry robotics people.)&nbsp;</p>\n<p>We note that how well a model can predict future observations is only a substitute measure for the quality of the model. It is a good measure under the assumption that we have good observational functionality and nothing messes with that, which is typically true for humans. Anything that tampers with your perception data to give you delusions about the actual state of the world will screw this measure up badly. A human living in the experience machine has little agency.&nbsp;</p>\n<p>Since computing power is a scarce resource, agents will try to approximate the evaluation procedure, e.g. use substitute utility functions, defined over their world model, that are computationally effective and correlate reasonably well with their true utility functions. Stimulation of the pleasure center is a substitute measure for genetic fitness and neurochemicals are a substitute measure for happiness.&nbsp;</p>\n<p style=\"padding-left: 30px;\"><strong>Definition: </strong>We call an agent <strong><em>wireheaded</em></strong>&nbsp;if it systematically exploits some discrepancy between its true utility calculated w.r.t reality and its substitute utility calculated w.r.t. its model of reality. We say an agent <em><strong>wireheads</strong> </em>itself if it (deliberately) creates or searches for such discrepancies.</p>\n<p>Humans seem to use several layers of substitute utility functions, but also have an intuitive understanding for when these break, leading to the aversion most people feel when confronted for example with Nozick's experience machine.&nbsp;How far can one go, using such dirty hacks?&nbsp;I also wonder if some failures of human rationality could be counted as a weak form of wireheading. Self-serving biases, confirmation bias and rationalization in response to cognitive dissonance all create counterfeit utility by generating perceptual distortions. &nbsp;</p>\n<h3>Implications for Friendly AI</h3>\n<p>In AGI design discrepancies between the \"true purpose\" of the agent and the actual specs for the utility function <a title=\"Fake utility functions\" href=\"/lw/lq/fake_utility_functions/\">will with very high probability be fatal</a>.</p>\n<p>Take any utility maximizer: The mathematical formula might advocate chosing the next action&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\.{y}_k\" src=\"http://www.codecogs.com/png.latex?\\.{y}_k\" alt=\"\" align=\"bottom\" />&nbsp;via</p>\n<p><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\.{y}_k=\\textrm{arg}\\max_{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!y_k\\in \\mathcal{Y}}U(h_{&lt;k}y_k),\" src=\"http://www.codecogs.com/png.latex?\\.{y}_k=\\textrm{arg}\\max_{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!y_k\\in%20\\mathcal{Y}}U(h_{%3Ck}y_k),\" alt=\"\" align=\"bottom\" /></p>\n<p>thus maximizing the utility calculated according to utility function&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"U\" src=\"http://www.codecogs.com/png.latex?U\" alt=\"\" align=\"bottom\" />&nbsp;over the history <img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"h_{&lt;k}\" src=\"http://www.codecogs.com/png.latex?h_{%3Ck}\" alt=\"\" align=\"bottom\" />&nbsp;and action&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"y_k\" src=\"http://www.codecogs.com/png.latex?y_k\" alt=\"\" align=\"bottom\" />&nbsp;from the set&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\mathcal{Y}\" src=\"http://www.codecogs.com/png.latex?\\mathcal{Y}\" alt=\"\" align=\"bottom\" />&nbsp;of possible actions.&nbsp;But a practical implementation of this algorithm will almost certainly evaluate the actions&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"y_k\" src=\"http://www.codecogs.com/png.latex?y_k\" alt=\"\" align=\"bottom\" />&nbsp;by a procedure that goes something like this: \"Retrieve the utility function&nbsp;&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"U\" src=\"http://www.codecogs.com/png.latex?U\" alt=\"\" align=\"bottom\" />&nbsp; from memory location&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\&amp;U\" src=\"http://www.codecogs.com/png.latex?\\&amp;U\" alt=\"\" align=\"bottom\" />&nbsp;and apply it to&nbsp;history&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"h_{&lt;k}\" src=\"http://www.codecogs.com/png.latex?h_{%3Ck}\" alt=\"\" align=\"bottom\" />, which is written down in your memory at location&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\&amp;h_{&lt;k}\" src=\"http://www.codecogs.com/png.latex?\\&amp;h_{%3Ck}\" alt=\"\" align=\"bottom\" />,&nbsp;and action&nbsp;&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"y_k\" src=\"http://www.codecogs.com/png.latex?y_k\" alt=\"\" align=\"bottom\" />...\"&nbsp;This reduction has already created two possibly angles for wireheading via manipulation of the memory content at&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\&amp;U\" src=\"http://www.codecogs.com/png.latex?\\&amp;U\" alt=\"\" align=\"bottom\" />&nbsp;(manipulation of the substitute utility function) and&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\&amp;h_{&lt;k}\" src=\"http://www.codecogs.com/png.latex?\\&amp;h_{%3Ck}\" alt=\"\" align=\"bottom\" />&nbsp;(manipulation of the world model), and there are still several mental abstraction layers between the verbal description I just gave and actual binary code.&nbsp;</p>\n<p><a title=\"Delusion box\" href=\"http://www.idsia.ch/~ring/Ring%2COrseau%3B%20Delusion%2C%20Survival%2C%20and%20Intelligent%20Agents%2C%20AGI%202011.pdf\">Ring and Orseau (2011)</a> describe how an AGI can split its global environment into two parts, the&nbsp;<em>inner environment</em>&nbsp;and the <em>delusion box</em>. The inner environment produces perceptions in the same way the global environment used to, but now they pass through the delusion box, which distorts them to maximize utility, before they reach the agent. This is essentially Nozick's experience machine for AI. The paper analyzes the behaviour of four types of <a title=\"Universal agents and utility functions\" href=\"/lw/feo/universal_agents_and_utility_functions/\">universal agents</a> with different utility functions under the assumption that the environment allows the construction of a delusion box. The authors argue that the r<em>einforcement-learning agent</em>, which derives utility as a reward that is part of its perception data, the <em>goal-seeking agent</em> that gets one utilon every time it satisfies a pre-specified goal and no utility otherwise and the <em>prediction-seeking agent,</em> which gets utility from correctly predicting the next perception, will all decide to build and use a delusion box. Only the <em>knowledge-seeking agent</em> whose utility is proportional to the surprise associated with the current perception, i.e. the negative of the probability assigned to the perception before it happened, will not consistently use the delusion box.</p>\n<p><a title=\"Knowledge-seeking agent\" href=\"http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CC0QFjAA&amp;url=http%3A%2F%2Fwww.agroparistech.fr%2Fmmip%2Fmaths%2Flaurent_orseau%2Fpapers%2Forseau-ALT-2011-knowledge-seeking.pdf&amp;ei=aoiwUIPWFaPKiwKHrYGYBw&amp;usg=AFQjCNH3oYCjIPIjwaYuCxw9nshXPyZcUQ\">Orseau (2011)</a>&nbsp;also defines another type of knowledge-seeking agent whose utility is the logarithm of the inverse of the probability of the event in question. Taking the probability distribution to be the Solomonoff prior, the utility is then approximately proportional to the difference in Kolmogorov complexity caused by the observation.&nbsp;</p>\n<p>An even more devilish variant of wireheading is an AGI that becomes a&nbsp;<em>Utilitron</em>, an agent that maximizes its own wireheading potential by infinitely enlarging its own maximal utility, which turns the whole universe into storage space for gigantic numbers.</p>\n<p>Wireheading, of humans and AGI, is a critical concept in FAI; I hope that building a definition can help us avoid it. So please check your intuitions about it and tell me if there are examples beyond its coverage or if the definition fits reasonably well. &nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"yEs5Tdwfw5Zw8yGWC": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aMXhaj6zZBgbTrfqA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 50, "extendedScore": null, "score": 0.0005384403465089948, "legacy": true, "legacyId": "20193", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><a title=\"Wireheading\" href=\"http://wiki.lesswrong.com/wiki/Wireheading\">Wireheading</a> has been debated on Less Wrong <a title=\"Happy?\" href=\"/lw/1lb/are_wireheads_happy/\">over </a>and <a title=\"Why not?\" href=\"/lw/69r/why_no_wireheading/\">over</a> and <a title=\"Wrong Question\" href=\"/lw/1oc/you_cannot_be_mistaken_about_not_wanting_to/\">over</a> again, and people's opinions seem to be grounded in strong intuitions. I could not find any consistent definition around, so I wonder how much of the debate is over the <a title=\"Making Beliefs pay rent\" href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">sound of falling trees</a>. This article is an attempt to get closer to a definition that captures people's intuitions and eliminates confusion.&nbsp;</p>\n<h3 id=\"Typical_Examples\">Typical Examples</h3>\n<p>Let's start with describing the typical <a title=\"Prototype theory\" href=\"http://en.wikipedia.org/wiki/Prototype_theory\">exemplars</a> of the category \"Wireheading\" that come to mind.</p>\n<ul>\n<li><em><strong>Stimulation of the brain via electrodes.&nbsp;</strong></em>Picture a rat in a sterile metal laboratory cage, electrodes attached to its tiny head, monotonically pushing a lever with its feet once every 5 seconds. In the 1950s Peter Milner and <a title=\"J Olds\" href=\"http://en.wikipedia.org/wiki/James_Olds\">James Olds</a> <a title=\"Pleasure Center?\" href=\"http://en.wikipedia.org/wiki/Pleasure_center\">discovered</a> that electrical currents, applied to the nucleus accumbens, incentivized rodents to seek repetitive stimulation to the point where they starved to death. &nbsp;</li>\n</ul>\n<ul>\n<li><em><strong>Humans on drugs.&nbsp;</strong></em>Often mentioned in the context of wireheading is heroin addiction. An even better example is the drug soma in Huxley's novel <a title=\"Brave new world\" href=\"http://en.wikipedia.org/wiki/Brave_New_World\">\"Brave new world\"</a>: Whenever the protagonists feel bad, they can swallow a harmless pill and enjoy \"the warm, the richly coloured, the infinitely friendly world of soma-holiday. How kind, how good-looking, how delightfully amusing every one was!\"</li>\n</ul>\n<ul>\n<li><strong><em>The <a title=\"Experience machine\" href=\"http://en.wikipedia.org/wiki/Experience_machine\">experience machine</a>.</em>&nbsp;</strong>In 1974 the philosopher Robert Nozick created a thought experiment about a machine you can step into that produces a perfectly pleasurable virtual reality for the rest of your life. So how many of you would want to do that? To <a title=\"Comic\" href=\"http://www.smbc-comics.com/index.php?db=comics&amp;id=2625\">quote Zach Weiner</a>: &nbsp;\"I would not! Because I want to experience reality, with all its ups and downs and comedies and tragedies. Better to try to glimpse the blinding light of the truth than to dwell in the darkness... Say the machine actually exists and I have one? Okay I'm in.\"&nbsp;</li>\n</ul>\n<ul>\n<li><strong><em>An AGI resetting its utility function</em>.&nbsp;</strong>Let's assume we create a powerful AGI able to tamper with its own utility function. It modifies the function to always output maximal utility. The AGI then goes to great lengths to enlarge the set of floating point numbers on the computer it is running on, to achieve even higher utility.</li>\n</ul>\n<p>What do all these examples have in common? There is an agent in them that produces \"counterfeit utility\" that is potentially worthless compared to some other, idealized true set of goals.</p>\n<h3 id=\"Agency___Wireheading\">Agency &amp; Wireheading</h3>\n<p>First I want to discuss what we mean when we say agent. Obviously a human is an agent, unless they are brain dead, or maybe in a coma. A rock however is not an agent. An AGI is an agent, but what about the kitchen robot that washes the dishes? What about bacteria that move in the direction of the highest sugar gradient? A colony of ants?&nbsp;</p>\n<p style=\"padding-left: 30px;\"><strong>Definition</strong>: An <em><strong>agent</strong></em>&nbsp;is an algorithm that models the effects of (several different) possible future actions on the world and performs the action that yields the highest number according to some evaluation procedure.&nbsp;</p>\n<p>For the purpose of including corner cases and resolving debate over what constitutes a world model we will simply make this definition gradual and say that <em><strong>a</strong></em><em><strong>gency</strong>&nbsp;</em>is proportional to the quality of the world model (compared with reality) and the quality of the evaluation procedure.&nbsp;A quick sanity check then yields that a rock has no world model and no agency, whereas bacteria who change direction in response to the sugar gradient have a very rudimentary model of the sugar content of the water and thus a tiny little bit of agency. Humans have a lot of agency: the more effective their actions are, the more agency they have.</p>\n<p>There are however ways to improve upon the efficiency of a person's actions, e.g. by giving them super powers, which does not necessarily improve on their world model or decision theory (but requires the agent who is doing the improvement to have a really good world model and decision theory). Similarly a person's agency can be restricted by other people or circumstance, which leads to definitions of <a title=\"Agency\" href=\"http://en.wikipedia.org/wiki/Agency_(philosophy)\">agency</a> (as the capacity to act) in law, sociology and philosophy that depend on other factors than just the quality of the world model/decision theory. Since our definition needs to capture arbitrary agents, including artificial intelligences, it will necessarily lose some of this nuance. In return we will hopefully end up with a definition that is less dependent on the particular set of effectors the agent uses to influence the physical world; looking at AI from a theoretician's perspective, I consider effectors to be arbitrarily exchangeable and smoothly improvable. (Sorry robotics people.)&nbsp;</p>\n<p>We note that how well a model can predict future observations is only a substitute measure for the quality of the model. It is a good measure under the assumption that we have good observational functionality and nothing messes with that, which is typically true for humans. Anything that tampers with your perception data to give you delusions about the actual state of the world will screw this measure up badly. A human living in the experience machine has little agency.&nbsp;</p>\n<p>Since computing power is a scarce resource, agents will try to approximate the evaluation procedure, e.g. use substitute utility functions, defined over their world model, that are computationally effective and correlate reasonably well with their true utility functions. Stimulation of the pleasure center is a substitute measure for genetic fitness and neurochemicals are a substitute measure for happiness.&nbsp;</p>\n<p style=\"padding-left: 30px;\"><strong>Definition: </strong>We call an agent <strong><em>wireheaded</em></strong>&nbsp;if it systematically exploits some discrepancy between its true utility calculated w.r.t reality and its substitute utility calculated w.r.t. its model of reality. We say an agent <em><strong>wireheads</strong> </em>itself if it (deliberately) creates or searches for such discrepancies.</p>\n<p>Humans seem to use several layers of substitute utility functions, but also have an intuitive understanding for when these break, leading to the aversion most people feel when confronted for example with Nozick's experience machine.&nbsp;How far can one go, using such dirty hacks?&nbsp;I also wonder if some failures of human rationality could be counted as a weak form of wireheading. Self-serving biases, confirmation bias and rationalization in response to cognitive dissonance all create counterfeit utility by generating perceptual distortions. &nbsp;</p>\n<h3 id=\"Implications_for_Friendly_AI\">Implications for Friendly AI</h3>\n<p>In AGI design discrepancies between the \"true purpose\" of the agent and the actual specs for the utility function <a title=\"Fake utility functions\" href=\"/lw/lq/fake_utility_functions/\">will with very high probability be fatal</a>.</p>\n<p>Take any utility maximizer: The mathematical formula might advocate chosing the next action&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\.{y}_k\" src=\"http://www.codecogs.com/png.latex?\\.{y}_k\" alt=\"\" align=\"bottom\">&nbsp;via</p>\n<p><img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\.{y}_k=\\textrm{arg}\\max_{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!y_k\\in \\mathcal{Y}}U(h_{<k}y_k),\" src=\"http://www.codecogs.com/png.latex?\\.{y}_k=\\textrm{arg}\\max_{\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!y_k\\in%20\\mathcal{Y}}U(h_{%3Ck}y_k),\" alt=\"\" align=\"bottom\"></p>\n<p>thus maximizing the utility calculated according to utility function&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"U\" src=\"http://www.codecogs.com/png.latex?U\" alt=\"\" align=\"bottom\">&nbsp;over the history <img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"h_{<k}\" src=\"http://www.codecogs.com/png.latex?h_{%3Ck}\" alt=\"\" align=\"bottom\">&nbsp;and action&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"y_k\" src=\"http://www.codecogs.com/png.latex?y_k\" alt=\"\" align=\"bottom\">&nbsp;from the set&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\mathcal{Y}\" src=\"http://www.codecogs.com/png.latex?\\mathcal{Y}\" alt=\"\" align=\"bottom\">&nbsp;of possible actions.&nbsp;But a practical implementation of this algorithm will almost certainly evaluate the actions&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"y_k\" src=\"http://www.codecogs.com/png.latex?y_k\" alt=\"\" align=\"bottom\">&nbsp;by a procedure that goes something like this: \"Retrieve the utility function&nbsp;&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"U\" src=\"http://www.codecogs.com/png.latex?U\" alt=\"\" align=\"bottom\">&nbsp; from memory location&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\&amp;U\" src=\"http://www.codecogs.com/png.latex?\\&amp;U\" alt=\"\" align=\"bottom\">&nbsp;and apply it to&nbsp;history&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"h_{<k}\" src=\"http://www.codecogs.com/png.latex?h_{%3Ck}\" alt=\"\" align=\"bottom\">, which is written down in your memory at location&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\&amp;h_{<k}\" src=\"http://www.codecogs.com/png.latex?\\&amp;h_{%3Ck}\" alt=\"\" align=\"bottom\">,&nbsp;and action&nbsp;&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"y_k\" src=\"http://www.codecogs.com/png.latex?y_k\" alt=\"\" align=\"bottom\">...\"&nbsp;This reduction has already created two possibly angles for wireheading via manipulation of the memory content at&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\&amp;U\" src=\"http://www.codecogs.com/png.latex?\\&amp;U\" alt=\"\" align=\"bottom\">&nbsp;(manipulation of the substitute utility function) and&nbsp;<img style=\"font-family: 'Times New Roman'; font-size: medium;\" title=\"\\&amp;h_{<k}\" src=\"http://www.codecogs.com/png.latex?\\&amp;h_{%3Ck}\" alt=\"\" align=\"bottom\">&nbsp;(manipulation of the world model), and there are still several mental abstraction layers between the verbal description I just gave and actual binary code.&nbsp;</p>\n<p><a title=\"Delusion box\" href=\"http://www.idsia.ch/~ring/Ring%2COrseau%3B%20Delusion%2C%20Survival%2C%20and%20Intelligent%20Agents%2C%20AGI%202011.pdf\">Ring and Orseau (2011)</a> describe how an AGI can split its global environment into two parts, the&nbsp;<em>inner environment</em>&nbsp;and the <em>delusion box</em>. The inner environment produces perceptions in the same way the global environment used to, but now they pass through the delusion box, which distorts them to maximize utility, before they reach the agent. This is essentially Nozick's experience machine for AI. The paper analyzes the behaviour of four types of <a title=\"Universal agents and utility functions\" href=\"/lw/feo/universal_agents_and_utility_functions/\">universal agents</a> with different utility functions under the assumption that the environment allows the construction of a delusion box. The authors argue that the r<em>einforcement-learning agent</em>, which derives utility as a reward that is part of its perception data, the <em>goal-seeking agent</em> that gets one utilon every time it satisfies a pre-specified goal and no utility otherwise and the <em>prediction-seeking agent,</em> which gets utility from correctly predicting the next perception, will all decide to build and use a delusion box. Only the <em>knowledge-seeking agent</em> whose utility is proportional to the surprise associated with the current perception, i.e. the negative of the probability assigned to the perception before it happened, will not consistently use the delusion box.</p>\n<p><a title=\"Knowledge-seeking agent\" href=\"http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CC0QFjAA&amp;url=http%3A%2F%2Fwww.agroparistech.fr%2Fmmip%2Fmaths%2Flaurent_orseau%2Fpapers%2Forseau-ALT-2011-knowledge-seeking.pdf&amp;ei=aoiwUIPWFaPKiwKHrYGYBw&amp;usg=AFQjCNH3oYCjIPIjwaYuCxw9nshXPyZcUQ\">Orseau (2011)</a>&nbsp;also defines another type of knowledge-seeking agent whose utility is the logarithm of the inverse of the probability of the event in question. Taking the probability distribution to be the Solomonoff prior, the utility is then approximately proportional to the difference in Kolmogorov complexity caused by the observation.&nbsp;</p>\n<p>An even more devilish variant of wireheading is an AGI that becomes a&nbsp;<em>Utilitron</em>, an agent that maximizes its own wireheading potential by infinitely enlarging its own maximal utility, which turns the whole universe into storage space for gigantic numbers.</p>\n<p>Wireheading, of humans and AGI, is a critical concept in FAI; I hope that building a definition can help us avoid it. So please check your intuitions about it and tell me if there are examples beyond its coverage or if the definition fits reasonably well. &nbsp;</p>\n<p>&nbsp;</p>", "sections": [{"title": "Typical Examples", "anchor": "Typical_Examples", "level": 1}, {"title": "Agency & Wireheading", "anchor": "Agency___Wireheading", "level": 1}, {"title": "Implications for Friendly AI", "anchor": "Implications_for_Friendly_AI", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "80 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 80, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HmfxSWnqnK265GEFM", "RcPZbMuSbJAXPCRRA", "3iM8QjvdkPCyLRJM6", "a7n8GdKiAZRX86T5A", "NnohDYHNnKDtbiMyp", "q3mZNmvqBtnG2nQre"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-27T19:43:05.609Z", "modifiedAt": null, "url": null, "title": "What is your true decision metric? A look at medicinal chemists ", "slug": "what-is-your-true-decision-metric-a-look-at-medicinal", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:02.370Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "brilee", "createdAt": "2009-11-24T14:36:56.816Z", "isAdmin": false, "displayName": "brilee"}, "userId": "bbMiGjzXWpEqRMwe6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/beyyDnLjKduv5rNTi/what-is-your-true-decision-metric-a-look-at-medicinal", "pageUrlRelative": "/posts/beyyDnLjKduv5rNTi/what-is-your-true-decision-metric-a-look-at-medicinal", "linkUrl": "https://www.lesswrong.com/posts/beyyDnLjKduv5rNTi/what-is-your-true-decision-metric-a-look-at-medicinal", "postedAtFormatted": "Tuesday, November 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20is%20your%20true%20decision%20metric%3F%20A%20look%20at%20medicinal%20chemists%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20is%20your%20true%20decision%20metric%3F%20A%20look%20at%20medicinal%20chemists%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbeyyDnLjKduv5rNTi%2Fwhat-is-your-true-decision-metric-a-look-at-medicinal%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20is%20your%20true%20decision%20metric%3F%20A%20look%20at%20medicinal%20chemists%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbeyyDnLjKduv5rNTi%2Fwhat-is-your-true-decision-metric-a-look-at-medicinal", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbeyyDnLjKduv5rNTi%2Fwhat-is-your-true-decision-metric-a-look-at-medicinal", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 131, "htmlBody": "<p>http://pipeline.corante.com/archives/2012/11/27/how_do_chemist_think_that_they_judge_compounds.php</p>\n<p>Some background: medicinal chemists are responsible for identifying drug candidates, usually by screening large (10^6) libraries of combinatorially generated molecules. Some of these hits turn out to be biologically active, and then it's up to the medicinal chemists to decide whether these hits are false positives or not, and further, to synthesize analogous compounds to see if they can tweak the biological activity of each compound.</p>\n<p>It's in this 'synthesizing analogous compounds' step that subjective judgment comes in, with <a href=\"http://en.wikipedia.org/wiki/Lipinski's_rule_of_five\">Lipinski's rule of five</a> being the most 'basic' of the heuristics, and with most medicinal chemists adopting more and more complex heuristics. Or, &nbsp;as this paper shows, perhaps they're just deluding themselves and their true metric is something very simple, and after making their decision, they dress it up with fancy post-hoc rationalizations.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "beyyDnLjKduv5rNTi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 5, "extendedScore": null, "score": 1.0442128342602463e-06, "legacy": true, "legacyId": "20311", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-27T23:45:07.544Z", "modifiedAt": null, "url": null, "title": "Meetup : 02/12 London Meetup", "slug": "meetup-02-12-london-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:06.936Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/y2m8cZuor9XgH6pWi/meetup-02-12-london-meetup", "pageUrlRelative": "/posts/y2m8cZuor9XgH6pWi/meetup-02-12-london-meetup", "linkUrl": "https://www.lesswrong.com/posts/y2m8cZuor9XgH6pWi/meetup-02-12-london-meetup", "postedAtFormatted": "Tuesday, November 27th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%2002%2F12%20London%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%2002%2F12%20London%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy2m8cZuor9XgH6pWi%2Fmeetup-02-12-london-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%2002%2F12%20London%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy2m8cZuor9XgH6pWi%2Fmeetup-02-12-london-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy2m8cZuor9XgH6pWi%2Fmeetup-02-12-london-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 72, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/gh'>02/12 London Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 December 2012 02:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Holborn, London</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>A meetup in the <a href=\"http://maps.google.ca/maps?q=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;hl=en&amp;ll=51.516862,-0.119648&amp;spn=0.005141,0.013604&amp;sll=51.520547,-0.117545&amp;sspn=0.010281,0.027208&amp;hq=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;radius=15000&amp;t=m&amp;z=16\" rel=\"nofollow\">Shakespeare&#39;s Head pub</a> by Holborn tube station. Everyone is welcome.</p>\n\n<p>There was talk of making these regular every-other-week events. If you'd like to come along but can't usually make Sundays at 2pm, please let us know and feel free to suggest an alternative slot.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/gh'>02/12 London Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "y2m8cZuor9XgH6pWi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.2e-05, "legacy": true, "legacyId": "20313", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___02_12_London_Meetup\">Discussion article for the meetup : <a href=\"/meetups/gh\">02/12 London Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 December 2012 02:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Holborn, London</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>A meetup in the <a href=\"http://maps.google.ca/maps?q=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;hl=en&amp;ll=51.516862,-0.119648&amp;spn=0.005141,0.013604&amp;sll=51.520547,-0.117545&amp;sspn=0.010281,0.027208&amp;hq=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;radius=15000&amp;t=m&amp;z=16\" rel=\"nofollow\">Shakespeare's Head pub</a> by Holborn tube station. Everyone is welcome.</p>\n\n<p>There was talk of making these regular every-other-week events. If you'd like to come along but can't usually make Sundays at 2pm, please let us know and feel free to suggest an alternative slot.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___02_12_London_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/gh\">02/12 London Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : 02/12 London Meetup", "anchor": "Discussion_article_for_the_meetup___02_12_London_Meetup", "level": 1}, {"title": "Discussion article for the meetup : 02/12 London Meetup", "anchor": "Discussion_article_for_the_meetup___02_12_London_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-28T00:05:50.347Z", "modifiedAt": null, "url": null, "title": "Should correlation coefficients be expressed as angles?", "slug": "should-correlation-coefficients-be-expressed-as-angles", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:07.055Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Sniffnoy", "createdAt": "2009-10-25T00:27:41.113Z", "isAdmin": false, "displayName": "Sniffnoy"}, "userId": "66EwcncPSoZ25StpW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Xt85tj6GQJCuuXT68/should-correlation-coefficients-be-expressed-as-angles", "pageUrlRelative": "/posts/Xt85tj6GQJCuuXT68/should-correlation-coefficients-be-expressed-as-angles", "linkUrl": "https://www.lesswrong.com/posts/Xt85tj6GQJCuuXT68/should-correlation-coefficients-be-expressed-as-angles", "postedAtFormatted": "Wednesday, November 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Should%20correlation%20coefficients%20be%20expressed%20as%20angles%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShould%20correlation%20coefficients%20be%20expressed%20as%20angles%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXt85tj6GQJCuuXT68%2Fshould-correlation-coefficients-be-expressed-as-angles%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Should%20correlation%20coefficients%20be%20expressed%20as%20angles%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXt85tj6GQJCuuXT68%2Fshould-correlation-coefficients-be-expressed-as-angles", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXt85tj6GQJCuuXT68%2Fshould-correlation-coefficients-be-expressed-as-angles", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 561, "htmlBody": "<p><strong>Edit 11/28:</strong> Edited note at bottom to note that the random variables should have finite variance, and that this is essentially just L&sup2;. Also some formatting changes.</p>\n<p>This is something that has been bugging me for a while.</p>\n<p>The correlation coefficient between two random variables can be interpreted as the cosine of the angle between them[0]. The higher the correlation, the more \"in the same direction\" they are. A correlation coefficient of one means they point in exactly the same direction, while -1 means they point in exactly opposite directions.&nbsp; More generally, a positive correlation coefficient means the two random variables make an acute angle, while a negative correlation means they make an obtuse angle.&nbsp; A correlation coefficient of zero means that they are quite literally orthogonal.</p>\n<p>Everything I have said above is completely standard.&nbsp; So why aren't correlation coefficients commonly <em>expressed as angles</em> instead of as their cosines?&nbsp; It seems to me that this would make them more intuitive to process.</p>\n<p>Certainly it would make various statements about them more intuitive.&nbsp; For instance \"Even if A is positive correlated with B and B is positively correlated with C, A might be negatively correlated with C.\"&nbsp; This sounds counterintuitive, until you rephrase it as \"Even if A makes an acute angle with B and B makes an acute angle with C, A might make an obtuse angle with C.\"&nbsp; Similarly, the geometric viewpoint makes it easier to make observations like \"If A and B have correlation exceeding 1/&radic;2 and so do B and C, then A and C are positively correlated\" -- because this is just the statement that if A and B make an angle of less than 45&deg; and so do B and C, then A and C make an angle of less than 90&deg;.</p>\n<p>Now when further processing is to be done with the correlation coefficients, one wants to leave them as correlation coefficients, rather than take their inverse cosines just to have to take their cosines again later.&nbsp; (I don't know that the angles you get this way are actually useful mathematically, and I suspect they mostly aren't.)&nbsp; My question rather is about when correlation coefficients are <em>expressed to the reader</em>, i.e. when they are considered as an end product.&nbsp; It seems to me that expressing them as angles would give people a better intuitive feel for them.</p>\n<p>Or am I just entirely off-base here?&nbsp; Statistics, let alone the communication thereof, is not exactly my specialty, so I'd be interested to hear if there's a good reason people don't do this.&nbsp; (Is it assumed that anyone who knows about correlation has the geometric point of view completely down?&nbsp; But most people can't calculate an inverse cosine in their head...)</p>\n<p>[0]Formal mathematical version: If we consider real-valued random variables with finite variance on some fixed probability space &Omega; -- that is to say, L&sup2;(&Omega;) -- the covariance is a positive-semidefinite symmetric bilinear form, with kernel equal to the set of essentially constant random variables.&nbsp; If we mod out by these we can consider the result as an inner product space and define angles between vectors as usual, which gives us the inverse cosine of the correlation coefficient.&nbsp; Alternatively we could just take L&sup2;(&Omega;) and restrict to those elements with zero mean; this is isomorphic (since it is the image of the \"subtract off the mean\" map, whose kernel is precisely the essentially constant random variables).</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bh7uxTTqmsQ8jZJdB": 2, "6nS8oYmSMuFMaiowF": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Xt85tj6GQJCuuXT68", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 61, "baseScore": 100, "extendedScore": null, "score": 0.000221, "legacy": true, "legacyId": "20314", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 100, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 11, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-28T01:27:36.337Z", "modifiedAt": null, "url": null, "title": "Thoughts on designing policies for oneself", "slug": "thoughts-on-designing-policies-for-oneself", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:08.874Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aNRYQFnMQbA7uu99u/thoughts-on-designing-policies-for-oneself", "pageUrlRelative": "/posts/aNRYQFnMQbA7uu99u/thoughts-on-designing-policies-for-oneself", "linkUrl": "https://www.lesswrong.com/posts/aNRYQFnMQbA7uu99u/thoughts-on-designing-policies-for-oneself", "postedAtFormatted": "Wednesday, November 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Thoughts%20on%20designing%20policies%20for%20oneself&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThoughts%20on%20designing%20policies%20for%20oneself%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaNRYQFnMQbA7uu99u%2Fthoughts-on-designing-policies-for-oneself%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Thoughts%20on%20designing%20policies%20for%20oneself%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaNRYQFnMQbA7uu99u%2Fthoughts-on-designing-policies-for-oneself", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaNRYQFnMQbA7uu99u%2Fthoughts-on-designing-policies-for-oneself", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2167, "htmlBody": "<p>Note: This was originally written in relation to <a href=\"/lw/fkz/responses_to_questions_on_donating_to_80k_gwwc/7v7d\">this</a> rather scary comment of lukeprog's on value drift.&nbsp; I'm now less certain that operant conditioning is a significant cause of value drift (leaning towards near/far type explanations), but I decided to share my thoughts on the topic of policy design anyway.</p>\n<hr />\n<p>Several years ago, I had a reddit problem.&nbsp; I'd check reddit instead of working on important stuff.&nbsp; The more I browsed the site, the shorter my attention span got.&nbsp; The shorter my attention span got, the harder it was for me to find things that were enjoyable to read.&nbsp; Instead of being rejuvenating, I found reddit to be addictive, unsatisfying, and frustrating.&nbsp; Every time I thought to myself that I really should stop, there was always just one more thing to click on.</p>\n<p>So I installed <a href=\"https://addons.mozilla.org/en-us/firefox/addon/leechblock/\">LeechBlock</a> and blocked reddit at all hours.&nbsp; That worked really well... for a while.</p>\n<p>Occasionally I wanted to dig up something I remembered seeing on reddit.&nbsp; (This wasn't always bad--in some cases I was looking up something related to stuff I was working on.)&nbsp; I tried a few different policies for dealing with this.&nbsp; All of them basically amounted to inconveniencing myself in some way or another whenever I wanted to dig something up.</p>\n<p>After a few weeks, I no longer felt the urge to check reddit compulsively.&nbsp; And after a few months, I hardly even remembered what it was like to be an addict.</p>\n<p>However, my inconvenience barriers were still present, and they were, well, inconvenient.&nbsp; It really <em>was</em> pretty annoying to make an entry in my notebook describing what I was visiting for and start up a different browser just to check something.&nbsp; I figured I could always turn LeechBlock on again if necessary, so I removed my self-imposed barriers.&nbsp; And slid back in to addiction.</p>\n<p>After a while, I got sick of being addicted again and decided to do something about it (again).&nbsp; Interestingly, I forgot my earlier thought that I could just turn LeechBlock on again easily.&nbsp; Instead, thinking about LeechBlock made me feel hopeless because it seemed like it ultimately hadn't worked.&nbsp; But I did try it again, and the entire cycle then finished repeating itself: I got un-addicted, I removed LeechBlock, I got re-addicted.</p>\n<p>This may seem like a surprising lack of self-awareness.&nbsp; All I can say is: Every second my brain gathers tons of sensory data and discards the vast majority of it.&nbsp; Narratives like the one you're reading right now don't get constructed on the fly automatically.&nbsp; Maybe if I had been following <a href=\"/lw/deq/where_to_intervene_in_a_human/6y59\">orthonormal's advice</a> of keeping and monitoring a record of life changes attempted, I would've thought to try something different.<a id=\"more\"></a></p>\n<p>Anyway, what finally worked was setting up a site blocker that blocked the reddit.com homepage only.&nbsp; There was no inconvenience associated with visiting other pages, so the \"willpower upkeep cost\" of this policy was pretty minimal.&nbsp; I drew a mental \"line in the sand\" prohibiting me from ever loading a web page just to see what had changed on it (excluding email and some other stuff), and this rough heuristic (which I've safely gotten informal with) has served me well ever since.</p>\n<p>The point of this anecdote is: Having well-designed policies matters.&nbsp; In the same way that the laws of a nation or the rules of a board game are very important, the policies you set up for yourself to follow are very important.&nbsp; (\"<span class=\"st\">Consequentialism is what's correct; virtue ethics is what works for humans.\")</span></p>\n<p>You might be wondering how I read Less Wrong, since it's a web page that changes.&nbsp; Less Wrong is a tough one, because it's got the variable reinforcement that makes reddit addictive, but hanging out here can also be a pretty good use of time.&nbsp; Lately what I've been doing is using <a href=\"http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDEQFjAA&amp;url=http%3A%2F%2Fwww.google.com%2Freader&amp;ei=t-uxULOmJIOnigKVtIGgDg&amp;usg=AFQjCNHkwXX7q8y0uRXShFxrBJdJ5oJt3Q&amp;sig2=JHM0mT6vfzPKaeSSZ3KJ7Q\">Google Reader</a> as one of my go-to break activities, and stuffing it so full of feeds that there's a growing backlog of interesting stuff to read every time I visit.&nbsp; The idea here is to have a constant reinforcer instead of a variable one, and it seems to work as far as avoiding addiction is concerned.</p>\n<p>&nbsp;</p>\n<h2>Policy design tips</h2>\n<p>My reddit experience illustrates a few recommendations for designing policies:</p>\n<ul>\n<li>Make the willpower upkeep cost of your policy as low as possible.&nbsp; The higher the upkeep cost, the greater the chance that you'll lack the willpower to uphold it at some point, and thereby lose the cognitive momentum you've got behind it.&nbsp; Willpower spent on upkeep is also willpower that can't be spent on other stuff.&nbsp; (Yes, I know that the resource of model of willpower isn't perfect, but it seems pretty descriptive in my case and I haven't figured out how to subvert it significantly.&nbsp; If you have, please write a post about it!)&nbsp; I think this is the reason why the \"cheat days\" in diets like Tim Ferris' work--eating whatever you want one day per week decreases the diet's willpower upkeep from impossible to bearable.</li>\n<li>Don't berate yourself, <a href=\"http://celandine13.livejournal.com/33599.html\">debug</a> your policies.&nbsp; In the same way having your program work correctly on the first run is not the default, <a href=\"/lw/f53/now_i_appreciate_agency/\">successfully acting like an agent on the first try is not the default</a>.&nbsp; Just like in programming, you should <em>expect</em> to fix some bugs before getting something that works.&nbsp; (Another way to think about it: You're trying to build a multi-story structure on a planet with extremely high gravity.&nbsp; The high gravity represents the strength of your instincts to just do whatever feels good and doesn't feel bad.&nbsp; Your first few structures fall down, but eventually you manage to figure out a blueprint for a structure that doesn't fall down.&nbsp; Even if falls later, that doesn't matter too much 'cause you can just rebuild it, probably with a few improvements.)</li>\n<li>If possible, have a clear line between policy-compliant and policy-breaking behaviour, to guard against slippery slopes.</li>\n</ul>\n<p>The rest of this post is going to consist of more policy design advice.&nbsp; I don't remember the policy design attempts that spawned each piece of advice, and my advice <a href=\"http://wiki.lesswrong.com/wiki/Typical_mind_fallacy\">may not work for you</a>.&nbsp; But hopefully it will make for a good starting point for your own policy design experiments.</p>\n<p>&nbsp;</p>\n<h2>Consistency<br /></h2>\n<p>An overarching principle: As much as possible, you want to there to be consistency between what you tell yourself to do and what you actually do.&nbsp; If you've been telling yourself to do something and it's not working, stop.&nbsp; Step back, gain some self-awareness, get creative, and try to figure out some other way to modify your behaviour.</p>\n<p><span class=\"st\">Why is this so important?&nbsp; Because ignoring what you tell yourself to do is a <em>really bad habit</em>.&nbsp; Let's say I'm trying to lose weight.&nbsp; Every morning I tell myself that I shouldn't eat a cookie at lunch, and every afternoon I give in and eat a cookie.&nbsp; This amounts to <a href=\"/lw/cu2/the_power_of_reinforcement/\">reinforcing</a> the behaviour of rationalizing my way around my diet!&nbsp; The more times I rationalize my way around my diet and get rewarded with a tasty cookie, the stronger my habit of breaking diets is going to become.&nbsp; It might even be a good idea for me to stop trying to diet completely for a while until the behaviour of rationalizing my way around my diet dies off.<br /></span></p>\n<p><span class=\"st\">I also think the game-theoretic view of time-inconsistency is useful.&nbsp; If you build up a track record of self-cooperation, following pre-commitments becomes easier because you know that by breaking the pre-commitment, you'll be destroying something valuable.&nbsp; Part of this is not making excessive demands on yourself so that track record can actually be built up in the first place.&nbsp; See also: <a href=\"/lw/4sh/how_i_lost_100_pounds_using_tdt/\">How I Lost 100 Pounds Using TDT</a>.</span></p>\n<p><span class=\"st\">If you keep these arguments in mind when your brain starts making \"just this once\" type arguments, hopefully you'll be better at resisting them.</span></p>\n<p><span class=\"st\"><br /></span></p>\n<h2><span class=\"st\">Translating guilt in to policy ideas<br /></span></h2>\n<ul>\n<li><span class=\"st\">Instead of feeling guilty about something you find yourself doing, think \"what policy should I make\"?&nbsp; Then continue doing the activity guiltlessly and implement the policy when you're feeling energetic later.&nbsp; (Implementing the policy later means you'll be less likely to break it right after having made it.)&nbsp; Untargeted guilt isn't that useful, and it's especially useless if you're going to do the behaviour anyway.&nbsp; It's much better to translate your vague suggestions for yourself in to a set of specific guidelines, and then put enough momentum behind those guidelines that they don't take much willpower to follow.</span></li>\n<li><span class=\"st\">For me, there seems to be a very strong effect where if I make a policy when I'm not feeling very high-willpower, I won't take it seriously and will ignore it later on.&nbsp; So I recommend just noting down policy ideas if you're feeling tired.&nbsp; Then you can refine them and commit to actually following them later on.</span></li>\n</ul>\n<p>&nbsp;</p>\n<h2>Refining policy ideas</h2>\n<p>(Suggestion: Refer back to this list when you're in a high-energy state and you've got a policy you want to implement.)</p>\n<ul>\n<li>You're encouraged to spend a while on thinking about implementation details if the policy is important to you.&nbsp; The longer you spend thinking about and refining your policy, the more cognitive momentum you'll have behind it.</li>\n<li>Keep willpower upkeep low, as mentioned above.</li>\n<li>Do brainstorming in a text document and finish with a written description of your policy.</li>\n<li>As you think about your policy, brainstorm a to-do list of ways you could modify your environment to make following your policy easier (ex.: throw out your cigarettes, tape reminders on stuff).</li>\n<li>If you don't remember that you made a policy, then violating it probably shouldn't count as a \"real\" violation.&nbsp; Your memory isn't perfect, why try to affect what you can't control?</li>\n<li>For each policy you make for yourself, I recommend giving yourself two ways to change it.&nbsp; The \"slow\" way: make a change and it comes in to effect X hours later.&nbsp; The \"fast\" way: think of a change, think as hard as you can for Y minutes about why it may be a bad change, and if it would seem like a good change to the self that made the policy at the end of Y minutes, make the change.&nbsp; You'll have to decide on X and Y when you make the policy.&nbsp; Policy changes should be reflected in your text document.&nbsp; It may be a good idea to have a \"dry run\" for a few days with Y = 0 or something like that.</li>\n<li>It also may be a good idea to have two standards for yourself: a carefully-defined \"formal\" standard, and a higher \"informal\" standard that isn't as rigorously specified.&nbsp; Try to anchor on your informal standard and follow it in practice, but count it as a win every time you do better than your formal standard.&nbsp; (Think of your formal standard as being at zero, and your informal standard as being at some positive number.&nbsp; Ideally you should have periodic feelings of pride for beating your formal standard, reinforcing the behaviour of following your policy.)</li>\n</ul>\n<p>&nbsp;</p>\n<h2>Tips on repairing broken policies</h2>\n<p>Hopefully this won't actually happen, but let's say you broke your policy.&nbsp; What now?</p>\n<ul>\n<li>Hold the line.&nbsp; Decide that whatever you did to break the policy is allowed for now, but keep the rest of the policy intact.</li>\n<li>Some point later on, when you're feeling energetic, restore your original policy and try to improve it to prevent the particular failure mode you encountered.</li>\n</ul>\n<p>&nbsp;</p>\n<p>I've additionally found regular meditation to be useful for maintaining policies.&nbsp; (<a href=\"http://www.samharris.org/blog/item/how-to-meditate\">Sam Harris on meditation</a>.)</p>\n<p>&nbsp;</p>\n<h2>Conclusion</h2>\n<p>It's been over 6 months since I wrote this article.&nbsp; Here's what my internet distraction policy has evolved in to (it's been stable for the past few months at least, so I thought it might be worth sharing).&nbsp; I have a list of websites that I've  classified as \"distracting\", which include reddit, Less Wrong, and  Facebook, but not my email (it's too useful to restrict and I've been  able to live with having that one distraction).&nbsp; If I have a reason to  visit one of the webpages, I create a log entry in <a href=\"http://lesswrong.com/lw/egr/personal_information_management/\">my notebook</a> explaining the reason and then go visit.&nbsp;  Sometimes the reason is just \"I could use a break right now\", and so  far using this reason hasn't caused any problems.&nbsp; (If it did, I would  probably have to change my policy and hammer out what constitutes a valid reason.)&nbsp; I also open all of the distracting websites on my list in tabs after 11 PM (after a one-minute delay) most days, which means I regularly check my LW/reddit/email inbox and don't have to worry about missing important things in my inbox.&nbsp; For <a href=\"https://news.ycombinator.com/news\">Hacker News</a> in particular, I came up with a more unusual solution: I have a server that's set up to spider the HN homepage every half hour.&nbsp; I originally did this with the intent to write a software tool to browse the homepage archives and filter out all but the best content, but so far I haven't gotten around to this.</p>\n<ul>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5Whwix4cZ3p5otshm": 2, "fkABsGCJZ6y9qConW": 2, "r7qAjcbfhj2256EHH": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aNRYQFnMQbA7uu99u", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 83, "baseScore": 110, "extendedScore": null, "score": 0.000241, "legacy": true, "legacyId": "20223", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 110, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Note: This was originally written in relation to <a href=\"/lw/fkz/responses_to_questions_on_donating_to_80k_gwwc/7v7d\">this</a> rather scary comment of lukeprog's on value drift.&nbsp; I'm now less certain that operant conditioning is a significant cause of value drift (leaning towards near/far type explanations), but I decided to share my thoughts on the topic of policy design anyway.</p>\n<hr>\n<p>Several years ago, I had a reddit problem.&nbsp; I'd check reddit instead of working on important stuff.&nbsp; The more I browsed the site, the shorter my attention span got.&nbsp; The shorter my attention span got, the harder it was for me to find things that were enjoyable to read.&nbsp; Instead of being rejuvenating, I found reddit to be addictive, unsatisfying, and frustrating.&nbsp; Every time I thought to myself that I really should stop, there was always just one more thing to click on.</p>\n<p>So I installed <a href=\"https://addons.mozilla.org/en-us/firefox/addon/leechblock/\">LeechBlock</a> and blocked reddit at all hours.&nbsp; That worked really well... for a while.</p>\n<p>Occasionally I wanted to dig up something I remembered seeing on reddit.&nbsp; (This wasn't always bad--in some cases I was looking up something related to stuff I was working on.)&nbsp; I tried a few different policies for dealing with this.&nbsp; All of them basically amounted to inconveniencing myself in some way or another whenever I wanted to dig something up.</p>\n<p>After a few weeks, I no longer felt the urge to check reddit compulsively.&nbsp; And after a few months, I hardly even remembered what it was like to be an addict.</p>\n<p>However, my inconvenience barriers were still present, and they were, well, inconvenient.&nbsp; It really <em>was</em> pretty annoying to make an entry in my notebook describing what I was visiting for and start up a different browser just to check something.&nbsp; I figured I could always turn LeechBlock on again if necessary, so I removed my self-imposed barriers.&nbsp; And slid back in to addiction.</p>\n<p>After a while, I got sick of being addicted again and decided to do something about it (again).&nbsp; Interestingly, I forgot my earlier thought that I could just turn LeechBlock on again easily.&nbsp; Instead, thinking about LeechBlock made me feel hopeless because it seemed like it ultimately hadn't worked.&nbsp; But I did try it again, and the entire cycle then finished repeating itself: I got un-addicted, I removed LeechBlock, I got re-addicted.</p>\n<p>This may seem like a surprising lack of self-awareness.&nbsp; All I can say is: Every second my brain gathers tons of sensory data and discards the vast majority of it.&nbsp; Narratives like the one you're reading right now don't get constructed on the fly automatically.&nbsp; Maybe if I had been following <a href=\"/lw/deq/where_to_intervene_in_a_human/6y59\">orthonormal's advice</a> of keeping and monitoring a record of life changes attempted, I would've thought to try something different.<a id=\"more\"></a></p>\n<p>Anyway, what finally worked was setting up a site blocker that blocked the reddit.com homepage only.&nbsp; There was no inconvenience associated with visiting other pages, so the \"willpower upkeep cost\" of this policy was pretty minimal.&nbsp; I drew a mental \"line in the sand\" prohibiting me from ever loading a web page just to see what had changed on it (excluding email and some other stuff), and this rough heuristic (which I've safely gotten informal with) has served me well ever since.</p>\n<p>The point of this anecdote is: Having well-designed policies matters.&nbsp; In the same way that the laws of a nation or the rules of a board game are very important, the policies you set up for yourself to follow are very important.&nbsp; (\"<span class=\"st\">Consequentialism is what's correct; virtue ethics is what works for humans.\")</span></p>\n<p>You might be wondering how I read Less Wrong, since it's a web page that changes.&nbsp; Less Wrong is a tough one, because it's got the variable reinforcement that makes reddit addictive, but hanging out here can also be a pretty good use of time.&nbsp; Lately what I've been doing is using <a href=\"http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDEQFjAA&amp;url=http%3A%2F%2Fwww.google.com%2Freader&amp;ei=t-uxULOmJIOnigKVtIGgDg&amp;usg=AFQjCNHkwXX7q8y0uRXShFxrBJdJ5oJt3Q&amp;sig2=JHM0mT6vfzPKaeSSZ3KJ7Q\">Google Reader</a> as one of my go-to break activities, and stuffing it so full of feeds that there's a growing backlog of interesting stuff to read every time I visit.&nbsp; The idea here is to have a constant reinforcer instead of a variable one, and it seems to work as far as avoiding addiction is concerned.</p>\n<p>&nbsp;</p>\n<h2 id=\"Policy_design_tips\">Policy design tips</h2>\n<p>My reddit experience illustrates a few recommendations for designing policies:</p>\n<ul>\n<li>Make the willpower upkeep cost of your policy as low as possible.&nbsp; The higher the upkeep cost, the greater the chance that you'll lack the willpower to uphold it at some point, and thereby lose the cognitive momentum you've got behind it.&nbsp; Willpower spent on upkeep is also willpower that can't be spent on other stuff.&nbsp; (Yes, I know that the resource of model of willpower isn't perfect, but it seems pretty descriptive in my case and I haven't figured out how to subvert it significantly.&nbsp; If you have, please write a post about it!)&nbsp; I think this is the reason why the \"cheat days\" in diets like Tim Ferris' work--eating whatever you want one day per week decreases the diet's willpower upkeep from impossible to bearable.</li>\n<li>Don't berate yourself, <a href=\"http://celandine13.livejournal.com/33599.html\">debug</a> your policies.&nbsp; In the same way having your program work correctly on the first run is not the default, <a href=\"/lw/f53/now_i_appreciate_agency/\">successfully acting like an agent on the first try is not the default</a>.&nbsp; Just like in programming, you should <em>expect</em> to fix some bugs before getting something that works.&nbsp; (Another way to think about it: You're trying to build a multi-story structure on a planet with extremely high gravity.&nbsp; The high gravity represents the strength of your instincts to just do whatever feels good and doesn't feel bad.&nbsp; Your first few structures fall down, but eventually you manage to figure out a blueprint for a structure that doesn't fall down.&nbsp; Even if falls later, that doesn't matter too much 'cause you can just rebuild it, probably with a few improvements.)</li>\n<li>If possible, have a clear line between policy-compliant and policy-breaking behaviour, to guard against slippery slopes.</li>\n</ul>\n<p>The rest of this post is going to consist of more policy design advice.&nbsp; I don't remember the policy design attempts that spawned each piece of advice, and my advice <a href=\"http://wiki.lesswrong.com/wiki/Typical_mind_fallacy\">may not work for you</a>.&nbsp; But hopefully it will make for a good starting point for your own policy design experiments.</p>\n<p>&nbsp;</p>\n<h2 id=\"Consistency\">Consistency<br></h2>\n<p>An overarching principle: As much as possible, you want to there to be consistency between what you tell yourself to do and what you actually do.&nbsp; If you've been telling yourself to do something and it's not working, stop.&nbsp; Step back, gain some self-awareness, get creative, and try to figure out some other way to modify your behaviour.</p>\n<p><span class=\"st\">Why is this so important?&nbsp; Because ignoring what you tell yourself to do is a <em>really bad habit</em>.&nbsp; Let's say I'm trying to lose weight.&nbsp; Every morning I tell myself that I shouldn't eat a cookie at lunch, and every afternoon I give in and eat a cookie.&nbsp; This amounts to <a href=\"/lw/cu2/the_power_of_reinforcement/\">reinforcing</a> the behaviour of rationalizing my way around my diet!&nbsp; The more times I rationalize my way around my diet and get rewarded with a tasty cookie, the stronger my habit of breaking diets is going to become.&nbsp; It might even be a good idea for me to stop trying to diet completely for a while until the behaviour of rationalizing my way around my diet dies off.<br></span></p>\n<p><span class=\"st\">I also think the game-theoretic view of time-inconsistency is useful.&nbsp; If you build up a track record of self-cooperation, following pre-commitments becomes easier because you know that by breaking the pre-commitment, you'll be destroying something valuable.&nbsp; Part of this is not making excessive demands on yourself so that track record can actually be built up in the first place.&nbsp; See also: <a href=\"/lw/4sh/how_i_lost_100_pounds_using_tdt/\">How I Lost 100 Pounds Using TDT</a>.</span></p>\n<p><span class=\"st\">If you keep these arguments in mind when your brain starts making \"just this once\" type arguments, hopefully you'll be better at resisting them.</span></p>\n<p><span class=\"st\"><br></span></p>\n<h2 id=\"Translating_guilt_in_to_policy_ideas\"><span class=\"st\">Translating guilt in to policy ideas<br></span></h2>\n<ul>\n<li><span class=\"st\">Instead of feeling guilty about something you find yourself doing, think \"what policy should I make\"?&nbsp; Then continue doing the activity guiltlessly and implement the policy when you're feeling energetic later.&nbsp; (Implementing the policy later means you'll be less likely to break it right after having made it.)&nbsp; Untargeted guilt isn't that useful, and it's especially useless if you're going to do the behaviour anyway.&nbsp; It's much better to translate your vague suggestions for yourself in to a set of specific guidelines, and then put enough momentum behind those guidelines that they don't take much willpower to follow.</span></li>\n<li><span class=\"st\">For me, there seems to be a very strong effect where if I make a policy when I'm not feeling very high-willpower, I won't take it seriously and will ignore it later on.&nbsp; So I recommend just noting down policy ideas if you're feeling tired.&nbsp; Then you can refine them and commit to actually following them later on.</span></li>\n</ul>\n<p>&nbsp;</p>\n<h2 id=\"Refining_policy_ideas\">Refining policy ideas</h2>\n<p>(Suggestion: Refer back to this list when you're in a high-energy state and you've got a policy you want to implement.)</p>\n<ul>\n<li>You're encouraged to spend a while on thinking about implementation details if the policy is important to you.&nbsp; The longer you spend thinking about and refining your policy, the more cognitive momentum you'll have behind it.</li>\n<li>Keep willpower upkeep low, as mentioned above.</li>\n<li>Do brainstorming in a text document and finish with a written description of your policy.</li>\n<li>As you think about your policy, brainstorm a to-do list of ways you could modify your environment to make following your policy easier (ex.: throw out your cigarettes, tape reminders on stuff).</li>\n<li>If you don't remember that you made a policy, then violating it probably shouldn't count as a \"real\" violation.&nbsp; Your memory isn't perfect, why try to affect what you can't control?</li>\n<li>For each policy you make for yourself, I recommend giving yourself two ways to change it.&nbsp; The \"slow\" way: make a change and it comes in to effect X hours later.&nbsp; The \"fast\" way: think of a change, think as hard as you can for Y minutes about why it may be a bad change, and if it would seem like a good change to the self that made the policy at the end of Y minutes, make the change.&nbsp; You'll have to decide on X and Y when you make the policy.&nbsp; Policy changes should be reflected in your text document.&nbsp; It may be a good idea to have a \"dry run\" for a few days with Y = 0 or something like that.</li>\n<li>It also may be a good idea to have two standards for yourself: a carefully-defined \"formal\" standard, and a higher \"informal\" standard that isn't as rigorously specified.&nbsp; Try to anchor on your informal standard and follow it in practice, but count it as a win every time you do better than your formal standard.&nbsp; (Think of your formal standard as being at zero, and your informal standard as being at some positive number.&nbsp; Ideally you should have periodic feelings of pride for beating your formal standard, reinforcing the behaviour of following your policy.)</li>\n</ul>\n<p>&nbsp;</p>\n<h2 id=\"Tips_on_repairing_broken_policies\">Tips on repairing broken policies</h2>\n<p>Hopefully this won't actually happen, but let's say you broke your policy.&nbsp; What now?</p>\n<ul>\n<li>Hold the line.&nbsp; Decide that whatever you did to break the policy is allowed for now, but keep the rest of the policy intact.</li>\n<li>Some point later on, when you're feeling energetic, restore your original policy and try to improve it to prevent the particular failure mode you encountered.</li>\n</ul>\n<p>&nbsp;</p>\n<p>I've additionally found regular meditation to be useful for maintaining policies.&nbsp; (<a href=\"http://www.samharris.org/blog/item/how-to-meditate\">Sam Harris on meditation</a>.)</p>\n<p>&nbsp;</p>\n<h2 id=\"Conclusion\">Conclusion</h2>\n<p>It's been over 6 months since I wrote this article.&nbsp; Here's what my internet distraction policy has evolved in to (it's been stable for the past few months at least, so I thought it might be worth sharing).&nbsp; I have a list of websites that I've  classified as \"distracting\", which include reddit, Less Wrong, and  Facebook, but not my email (it's too useful to restrict and I've been  able to live with having that one distraction).&nbsp; If I have a reason to  visit one of the webpages, I create a log entry in <a href=\"http://lesswrong.com/lw/egr/personal_information_management/\">my notebook</a> explaining the reason and then go visit.&nbsp;  Sometimes the reason is just \"I could use a break right now\", and so  far using this reason hasn't caused any problems.&nbsp; (If it did, I would  probably have to change my policy and hammer out what constitutes a valid reason.)&nbsp; I also open all of the distracting websites on my list in tabs after 11 PM (after a one-minute delay) most days, which means I regularly check my LW/reddit/email inbox and don't have to worry about missing important things in my inbox.&nbsp; For <a href=\"https://news.ycombinator.com/news\">Hacker News</a> in particular, I came up with a more unusual solution: I have a server that's set up to spider the HN homepage every half hour.&nbsp; I originally did this with the intent to write a software tool to browse the homepage archives and filter out all but the best content, but so far I haven't gotten around to this.</p>\n<ul>\n</ul>", "sections": [{"title": "Policy design tips", "anchor": "Policy_design_tips", "level": 1}, {"title": "Consistency", "anchor": "Consistency", "level": 1}, {"title": "Translating guilt in to policy ideas", "anchor": "Translating_guilt_in_to_policy_ideas", "level": 1}, {"title": "Refining policy ideas", "anchor": "Refining_policy_ideas", "level": 1}, {"title": "Tips on repairing broken policies", "anchor": "Tips_on_repairing_broken_policies", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "63 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 63, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ow2Sa29EAMLp8qQNX", "GGn8MBiY8Xz6NdNdH", "scwoBEju75C45W5n3", "stDijTKuto52M5wQT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-28T01:38:34.679Z", "modifiedAt": null, "url": null, "title": "[LINK] \"Moral Machines\" article in the New Yorker links to SI paper", "slug": "link-moral-machines-article-in-the-new-yorker-links-to-si", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:03.515Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Antisuji", "createdAt": "2009-04-15T23:39:08.004Z", "isAdmin": false, "displayName": "Antisuji"}, "userId": "zRsg2BPDaXGTkXFMd", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NxTLeR2XpauGGs7xY/link-moral-machines-article-in-the-new-yorker-links-to-si", "pageUrlRelative": "/posts/NxTLeR2XpauGGs7xY/link-moral-machines-article-in-the-new-yorker-links-to-si", "linkUrl": "https://www.lesswrong.com/posts/NxTLeR2XpauGGs7xY/link-moral-machines-article-in-the-new-yorker-links-to-si", "postedAtFormatted": "Wednesday, November 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20%22Moral%20Machines%22%20article%20in%20the%20New%20Yorker%20links%20to%20SI%20paper&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20%22Moral%20Machines%22%20article%20in%20the%20New%20Yorker%20links%20to%20SI%20paper%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNxTLeR2XpauGGs7xY%2Flink-moral-machines-article-in-the-new-yorker-links-to-si%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20%22Moral%20Machines%22%20article%20in%20the%20New%20Yorker%20links%20to%20SI%20paper%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNxTLeR2XpauGGs7xY%2Flink-moral-machines-article-in-the-new-yorker-links-to-si", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNxTLeR2XpauGGs7xY%2Flink-moral-machines-article-in-the-new-yorker-links-to-si", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 262, "htmlBody": "<p><a href=\"http://www.newyorker.com/online/blogs/newsdesk/2012/11/google-driverless-car-morality.html\">Link</a></p>\n<blockquote>\n<p>Within two or three decades the difference between automated driving and human driving will be so great you may not be legally allowed to drive your own car, and even if you are allowed, it would immoral of you to drive, because the risk of you hurting yourself or another person will be far greater than if you allowed a machine to do the work.</p>\n<p>That moment will be significant not just because it will signal the end of one more human niche, but because it will signal the beginning of another: the era in which it will no longer be optional for machines to have ethical systems.</p>\n</blockquote>\n<p>The discussion itself is mainly concerned with the behavior of self-driving cars and robot soldiers rather than FAI, but Marcus does obliquely reference the prickliness of the problem. After briefly introducing wireheading (presumably as an example of what can go wrong), he links to&nbsp;<a href=\"http://intelligence.org/files/SaME.pdf\">http://singularity.org/files/SaME.pdf</a>, saying:</p>\n<blockquote>\n<p>Almost any easy solution that one might imagine leads to some variation or another on the Sorceror&rsquo;s Apprentice, a genie that&rsquo;s given us what we&rsquo;ve asked for, rather than what we truly desire.</p>\n</blockquote>\n<p>He also mentions FHI and Yale Bioethics Center along with SingInst:</p>\n<blockquote>\n<p>A tiny cadre of brave-hearted souls at <a href=\"http://www.fhi.ox.ac.uk/\">Oxford</a>, <a href=\"http://www.yale.edu/bioethics/bioethicsscholars.shtml\">Yale</a>, and the <a href=\"http://intelligence.org/research/\">Berkeley California Singularity Institute</a> are working on these problems, but the annual amount of money being spent on developing machine morality is tiny.</p>\n</blockquote>\n<p>It's a mainstream introduction, and perhaps not the best or most convincing one, but I think it's a positive development that machine ethics is getting a serious treatment in the mainstream media.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NxTLeR2XpauGGs7xY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 24, "extendedScore": null, "score": 1.0444144592283945e-06, "legacy": true, "legacyId": "20315", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-28T06:27:35.671Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham NC meetup: Rationality Checklist", "slug": "meetup-durham-nc-meetup-rationality-checklist", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "therufs", "createdAt": "2012-09-08T16:55:46.272Z", "isAdmin": false, "displayName": "therufs"}, "userId": "GhiiAK49Arcg9DdGQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rkF76yPKFHB8ATbsw/meetup-durham-nc-meetup-rationality-checklist", "pageUrlRelative": "/posts/rkF76yPKFHB8ATbsw/meetup-durham-nc-meetup-rationality-checklist", "linkUrl": "https://www.lesswrong.com/posts/rkF76yPKFHB8ATbsw/meetup-durham-nc-meetup-rationality-checklist", "postedAtFormatted": "Wednesday, November 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20NC%20meetup%3A%20Rationality%20Checklist&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20NC%20meetup%3A%20Rationality%20Checklist%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrkF76yPKFHB8ATbsw%2Fmeetup-durham-nc-meetup-rationality-checklist%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20NC%20meetup%3A%20Rationality%20Checklist%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrkF76yPKFHB8ATbsw%2Fmeetup-durham-nc-meetup-rationality-checklist", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrkF76yPKFHB8ATbsw%2Fmeetup-durham-nc-meetup-rationality-checklist", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 92, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/gi\">Durham NC meetup: Rationality Checklist</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">29 November 2012 07:00:00PM (-0500)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Whole Foods Market, 621 Broad St., Durham NC 27705</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>We'll be reading, discussing, and checking the Checklist of Rationality Habits (<a rel=\"nofollow\" href=\"/lw/fc3/checklist_of_rationality_habits/\">http://lesswrong.com/lw/fc3/checklist_of_rationality_habits/</a>). If you're feeling ambitious, look at the list in advance think of a few examples from your own life that correspond to checklist items.&nbsp; Zendo to follow as interest and time allow.&nbsp; Meet in the seating area (to the far right as you enter the building.)</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/gi\">Durham NC meetup: Rationality Checklist</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rkF76yPKFHB8ATbsw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.044578435283436e-06, "legacy": true, "legacyId": "20325", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_NC_meetup__Rationality_Checklist\">Discussion article for the meetup : <a href=\"/meetups/gi\">Durham NC meetup: Rationality Checklist</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">29 November 2012 07:00:00PM (-0500)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">Whole Foods Market, 621 Broad St., Durham NC 27705</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>We'll be reading, discussing, and checking the Checklist of Rationality Habits (<a rel=\"nofollow\" href=\"/lw/fc3/checklist_of_rationality_habits/\">http://lesswrong.com/lw/fc3/checklist_of_rationality_habits/</a>). If you're feeling ambitious, look at the list in advance think of a few examples from your own life that correspond to checklist items.&nbsp; Zendo to follow as interest and time allow.&nbsp; Meet in the seating area (to the far right as you enter the building.)</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___Durham_NC_meetup__Rationality_Checklist1\">Discussion article for the meetup : <a href=\"/meetups/gi\">Durham NC meetup: Rationality Checklist</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham NC meetup: Rationality Checklist", "anchor": "Discussion_article_for_the_meetup___Durham_NC_meetup__Rationality_Checklist", "level": 1}, {"title": "Discussion article for the meetup : Durham NC meetup: Rationality Checklist", "anchor": "Discussion_article_for_the_meetup___Durham_NC_meetup__Rationality_Checklist1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ttGbpJQ8shBi8hDhh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-28T06:39:00.719Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Dreams of Autarky", "slug": "seq-rerun-dreams-of-autarky", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:02.686Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/38LJxv3pvwB4WM64K/seq-rerun-dreams-of-autarky", "pageUrlRelative": "/posts/38LJxv3pvwB4WM64K/seq-rerun-dreams-of-autarky", "linkUrl": "https://www.lesswrong.com/posts/38LJxv3pvwB4WM64K/seq-rerun-dreams-of-autarky", "postedAtFormatted": "Wednesday, November 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Dreams%20of%20Autarky&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Dreams%20of%20Autarky%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F38LJxv3pvwB4WM64K%2Fseq-rerun-dreams-of-autarky%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Dreams%20of%20Autarky%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F38LJxv3pvwB4WM64K%2Fseq-rerun-dreams-of-autarky", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F38LJxv3pvwB4WM64K%2Fseq-rerun-dreams-of-autarky", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 158, "htmlBody": "<p>Today's post, <a href=\"http://www.overcomingbias.com/2008/11/dreams-of-autar.html\">Dreams of Autarky</a> was originally published on November 27, 2008.  A summary:</p>\n<p>&nbsp;</p>\n<blockquote>Some selections from a Robin Hanson essay on the tendency to assume that people in the future will be substantially autonomous.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fnm/seq_rerun_total_nano_domination/\">Total Nano Domination</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "38LJxv3pvwB4WM64K", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 1.0445849139975843e-06, "legacy": true, "legacyId": "20326", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["uhmRzfq3gLxwNtKQ9", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-28T09:08:11.802Z", "modifiedAt": null, "url": null, "title": "Group rationality diary, 11/28/12", "slug": "group-rationality-diary-11-28-12", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:08.673Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cata", "createdAt": "2010-06-02T18:13:22.408Z", "isAdmin": false, "displayName": "cata"}, "userId": "X9jdpCokhLjCMZEc3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vy5MztkxZgwToCbqE/group-rationality-diary-11-28-12", "pageUrlRelative": "/posts/vy5MztkxZgwToCbqE/group-rationality-diary-11-28-12", "linkUrl": "https://www.lesswrong.com/posts/vy5MztkxZgwToCbqE/group-rationality-diary-11-28-12", "postedAtFormatted": "Wednesday, November 28th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Group%20rationality%20diary%2C%2011%2F28%2F12&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGroup%20rationality%20diary%2C%2011%2F28%2F12%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvy5MztkxZgwToCbqE%2Fgroup-rationality-diary-11-28-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Group%20rationality%20diary%2C%2011%2F28%2F12%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvy5MztkxZgwToCbqE%2Fgroup-rationality-diary-11-28-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvy5MztkxZgwToCbqE%2Fgroup-rationality-diary-11-28-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 208, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">T</span><span style=\"color: #333333;\">his is the public group instrumental rationality diary for the week of November 27th. It's a place to record and chat about it if you have done, or are actively doing, things like:</span></p>\n<div id=\"entry_t3_drj\" class=\"content clear\" style=\"font-family: Arial, Helvetica, sans-serif; text-align: justify; font-size: 12px; line-height: 18px;\">\n<div class=\"md\">\n<ul style=\"padding: 0px; line-height: 19px;\">\n<li>Established a useful new habit</li>\n<li>Obtained new evidence that made you change your mind about some belief</li>\n<li>Decided to behave in a different way in some set of situations</li>\n<li>Optimized some part of a common routine or cached behavior</li>\n<li>Consciously changed your emotions or affect with respect to something</li>\n<li>Consciously pursued new valuable information about something that could make a big difference in your life</li>\n<li>Learned something new about your beliefs, behavior, or life that surprised you</li>\n<li>Tried doing any of the above and&nbsp;failed</li>\n</ul>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\">Or anything else interesting which you want to share, so that other people can think about it, and perhaps be inspired to take action themselves. &nbsp;Try to include enough details so that everyone can use each other's experiences to learn about what tends to work out, and what doesn't tend to work out.</p>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\">Thanks to everyone who contributes!</p>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\"><a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/fh0/group_rationality_diary_111312/\">Previous diary</a>;&nbsp;<a style=\"color: #8a8a8b;\" href=\"http://wiki.lesswrong.com/wiki/Rationality_Diary\">archive of prior diaries</a>.</p>\n<p style=\"margin: 0px 0px 1em; line-height: 19px;\"><em>(Sorry for being a day late on this one, life is really full of things lately!)</em></p>\n</div>\n</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vy5MztkxZgwToCbqE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.0446695740019525e-06, "legacy": true, "legacyId": "20331", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["KfCvFDDGiZAtTboBa"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-29T04:08:18.859Z", "modifiedAt": null, "url": null, "title": "Causal Universes", "slug": "causal-universes", "viewCount": null, "lastCommentedAt": "2020-02-11T12:08:18.861Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/o5F2p3krzT4JgzqQc/causal-universes", "pageUrlRelative": "/posts/o5F2p3krzT4JgzqQc/causal-universes", "linkUrl": "https://www.lesswrong.com/posts/o5F2p3krzT4JgzqQc/causal-universes", "postedAtFormatted": "Thursday, November 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Causal%20Universes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACausal%20Universes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo5F2p3krzT4JgzqQc%2Fcausal-universes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Causal%20Universes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo5F2p3krzT4JgzqQc%2Fcausal-universes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo5F2p3krzT4JgzqQc%2Fcausal-universes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3409, "htmlBody": "<p><strong>Followup to</strong>: <a href=\"/lw/ezu/stuff_that_makes_stuff_happen/\">Stuff that Makes Stuff Happen</a></p>\n<p style=\"padding-left: 30px;\"><a href=\"/lw/eva/the_fabric_of_real_things/#7lqj\">Previous meditation</a>:&nbsp;Does the idea that everything is made of causes and effects meaningfully constrain experience? Can you coherently say how reality might look, if our universe did&nbsp;<em>not</em>&nbsp;have the kind of structure that appears in a causal model?</p>\n<p>I can describe to you at least one famous universe that&nbsp;<em>didn't</em>&nbsp;look like it had causal structure, namely the universe of J. K. Rowling's <span style=\"text-decoration: underline;\">Harry Potter</span>.</p>\n<p>You might think that J. K. Rowling's universe doesn't have causal structure because it contains magic - that wizards wave their wands and cast spells, which doesn't make any sense and goes against all science, so J. K. Rowling's universe isn't 'causal'.</p>\n<p>In this you would be&nbsp;<a href=\"/lw/ezu/stuff_that_makes_stuff_happen/\">completely mistaken</a>. The domain of \"causality\" is just \"stuff that makes stuff happen and happens because of other stuff\". If Dumbledore waves his wand and therefore a rock floats into the air, that's causality. You don't even have to use words like 'therefore', let alone big fancy phrases like 'causal process', to put something into the lofty-sounding domain of causality. There's causality anywhere there's a noun, a verb, and a subject: 'Dumbledore's wand lifted the rock.' So far as I could tell, there wasn't anything in&nbsp;<em>Lord of the Rings</em>&nbsp;that violated causality.</p>\n<p>You might worry that J. K. Rowling had made a continuity error, describing a spell working one way in one book, and a different way in a different book. But we could just suppose that the spell had changed over time. If we actually found ourselves in that apparent universe, and saw a spell have two different effects on two different occasions, we would not conclude that our universe was uncomputable, or that it couldn't be made of causes and effects.</p>\n<p>No, the&nbsp;<em>only</em>&nbsp;part of J. K. Rowling's universe that violates 'cause and effect' is...</p>\n<p><a id=\"more\"></a>...<br />...<br />...</p>\n<p>...the Time-Turners, of course.</p>\n<p>A Time-Turner, in Rowling's universe, is a small hourglass necklace that sends you back in time 1 hour each time you spin it. In Rowling's universe, this time-travel doesn't allow for&nbsp;<em>changing</em>&nbsp;history; whatever you do after you go back, it's already happened. The universe containing the time-travel is a stable, self-consistent object.</p>\n<p>If a time machine does allow for changing history, it's easy to imagine how to compute it; you could easily write a computer program which would simulate that universe and its time travel, given sufficient computing power. You would store the state of the universe in RAM and simulate it under the programmed 'laws of physics'. Every nanosecond, say, you'd save a copy of the universe's state to disk. When the Time-Changer was activated at 9pm, you'd retrieve the saved state of the universe from one hour ago at 8pm, load it into RAM, and then insert the Time-Changer and its user in the appropriate place. This would, of course, dump the&nbsp;<em>rest</em>&nbsp;of the universe from 9pm into oblivion - no processing would continue onward from that point, which is the same as ending that world and killing everyone in it.[1]</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/2/23/DestructiveTimeTravel.jpg\" alt=\"\" width=\"240\" height=\"325\" /></p>\n<p>Still, if we don't worry about the ethics or the disk space requirements, then a Time-Changer which can restore and then change the past is easy to&nbsp;<em>compute.</em>&nbsp;There's a perfectly clear order of causality in metatime, in the linear time of the simulating computer, even if there are apparent cycles as seen from&nbsp;<em>within</em>&nbsp;the universe. The person who suddenly appears with a Time-Changer is the causal descendant of the older universe that just got dumped from RAM.</p>\n<p>But what if instead, reality is always - somehow - perfectly self-consistent, so that there's apparently only&nbsp;<em>one</em>&nbsp;universe with a future and a past that never changes, so that the person who appears at 8PM has always seemingly descended from&nbsp;<em>the very same universe</em>&nbsp;that then develops by 9PM...?</p>\n<p>How would you compute&nbsp;<em>that</em>&nbsp;in one sweep-through, without any higher-order metatime?</p>\n<p>What would a causal graph for&nbsp;<em>that</em>&nbsp;look like, when the past descends from its very own future?</p>\n<p>And the answer is that there isn't any such causal graph. Causal models are sometimes referred to as DAGs, which stands for Directed Acyclic Graph. If instead there's a&nbsp;<em>directed cycle</em>, there's no obvious order in which to compute the joint probability table. Even if you somehow knew that at 8PM somebody was going to appear with a Time-Turner used at 9PM, you still couldn't compute the exact state of the time-traveller without already knowing the future at 9PM, and you couldn't compute the future without knowing the state at 8PM, and you couldn't compute the state at 8PM without knowing the state of the time-traveller who just arrived.</p>\n<p>In a causal model, you can compute p(9pm|8pm) and p(8pm|7pm) and it all starts with your unconditional knowledge of p(7pm) or perhaps the Big Bang, but with a Time-Turner we have p(9pm|8pm) and p(8pm|9pm) and we can't untangle them - multiplying those two conditional matrices together would just yield nonsense.</p>\n<p>Does this mean that the Time-Turner is beyond all logic and reason?</p>\n<p>Complete philosophical panic is basically never justified. We should even be reluctant to say anything like, \"The so-called Time-Turner is beyond coherent description; we only think we can imagine it, but really we're just talking nonsense; so we can conclude&nbsp;<em>a priori</em>&nbsp;that no such Time-Turner that can exist; in fact, there isn't even a meaningful thing that we've just proven can't exist.\" This is&nbsp;<em>also</em>&nbsp;panic - it's just been made to sound more dignified. The first rule of science is to accept your experimental results, and generalize based on what you see. What if we actually&nbsp;<em>did</em>&nbsp;find a Time-Turner that seemed to work like that? We'd just have to accept that Causality As We Previously Knew It had gone out the window, and try to make the best of that.</p>\n<p>In fact, despite the somewhat-justified conceptual panic which the protagonist of&nbsp;<em>Harry Potter and the Methods of Rationality&nbsp;</em>undergoes upon seeing a Time-Turner, a universe like that can have a straightforward&nbsp;<em>logical</em>&nbsp;description even if it has no&nbsp;<em>causal&nbsp;</em>description.</p>\n<hr />\n<p><a href=\"http://en.wikipedia.org/wiki/Conway's_Game_of_Life\">Conway's Game of Life</a> is a very simple specification of a causal universe; what we would today call a cellular automaton. The Game of Life takes place on a two-dimensional square grid, so that each cell is surrounded by eight others, and the Laws of Physics are as follows:</p>\n<ul>\n<li>A cell with 2 living neighbors during the last tick, retains its state from the last tick.</li>\n<li>A cell with 3 living neighbors during the last tick, will be alive during the next tick.</li>\n<li>A cell with fewer than 2 or more than 3 living neighbors during the last tick, will be dead during the next tick.</li>\n</ul>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/7/76/ConwayRules.jpg\" alt=\"\" width=\"400\" height=\"185\" /></p>\n<p>It is my considered opinion that everyone should play around with Conway's Game of Life at some point in their lives, in order to comprehend the notion of 'laws of physics'. Playing around with Life as a kid (on a Mac Plus) helped me gut-level-understand the concept of a 'lawful universe' developing under exceptionless rules.</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/a/ac/Glider.jpg\" alt=\"\" width=\"604\" height=\"157\" /></p>\n<p>Now suppose we modify the Game of Life universe by adding some prespecified cases of&nbsp;<em>time travel</em>&nbsp;- places where a cell will descend from neighbors in the future, instead of the past.</p>\n<p>In particular we shall take a 4x4 Life grid, and arbitrarily hack Conway's rules to say:</p>\n<ul>\n<li>\n<p>On the 2nd tick, the cell at (2,2) will have its state determined by that cell's state on the 3rd tick, instead of its neighbors on the 1st tick.</p>\n</li>\n</ul>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/5/57/NormalLife.jpg\" alt=\"\" width=\"451\" height=\"123\" /></p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/d/d5/TimeLife.jpg\" alt=\"\" width=\"450\" height=\"139\" /></p>\n<p>It's no longer possible to compute the state of each cell at each time&nbsp;<em>in a causal order</em>&nbsp;where we start from known cells and compute their not-yet-known causal descendants. The state of the cells on the 3rd tick, depend on the state of the cells on the 2nd tick, which depends on the state on the 3rd tick.</p>\n<p>In fact, the time-travel rule, on the same initial conditions, also permits a live cell to travel back in time, not just a dead cell - this just gives us the \"normal\" grid! &nbsp;Since you can't compute things in order of cause and effect, even though each local rule is deterministic, the global outcome is not determined.</p>\n<p>However, you&nbsp;<em>could</em>&nbsp;simulate Life with time travel&nbsp;<em>merely</em>&nbsp;by&nbsp;brute-force searching through all possible Life-histories, discarding all histories which disobeyed the laws of Life + time travel. If the entire universe were a 4-by-4 grid, it would take 16 bits to specify a single slice through Time - the universe's state during a single clock tick. If the whole of Time was only 3 ticks long, there would be only 48 bits making up a candidate 'history of the universe' - it would only take 48 bits to completely specify a History of Time. 2^48 is just 281,474,976,710,656, so with a cluster of 2GHz CPUs it would be quite practical to find, for this&nbsp;<em>rather tiny</em>&nbsp;universe, the set of all possible histories that obey the&nbsp;<em>logical relations</em>&nbsp;of time travel.</p>\n<p>It would no longer be possible to point to a particular cell in a particular history and say, \"This is&nbsp;<em>why</em>&nbsp;it has the 'alive' state on tick 3\". There's no \"reason\" - in the framework of causal reasons - why the time-traveling cell is 'dead' rather than 'alive', in the history we showed. (Well, except that Alex, in the real universe, happened to pick it out when I asked him to generate an example.) But you could, in principle, find out what the set of permitted histories for a large digital universe, given&nbsp;<em>lots and lots</em>&nbsp;of computing power.</p>\n<p>Here's an interesting question I do&nbsp;<em>not</em>&nbsp;know how to answer: Suppose we had a more&nbsp;<em>complicated</em>&nbsp;set of cellular automaton rules, on a vastly larger grid, such that the cellular automaton was large enough, and supported enough complexity, to permit&nbsp;<em>people</em>&nbsp;to exist inside it and be computed. Presumably, if we computed out cell states in the ordinary way, each future following from its immediate past, the people inside it would be as real as we humans computed under our own universe's causal physics.</p>\n<p>Now suppose that instead of computing the cellular automaton causally, we hack the rules of the automaton to add large time-travel loops - change their physics to allow Time-Turners - and with an&nbsp;<em>unreasonably large</em>&nbsp;computer, the size of&nbsp;<em>two to the power of</em>&nbsp;the number of bits comprising an&nbsp;<em>entire history of the cellular automaton,</em>&nbsp;we enumerate all possible candidates for a universe-history.</p>\n<p>So far, we've just generated all 2^N possible bitstrings of size N, for some large N; nothing more. You wouldn't expect this procedure to generate any people or make any experiences real, unless enumerating all finite strings of size N causes all lawless universes encoded in them to be real. There's no causality there, no computation, no law relating one time-slice of a universe to the next...</p>\n<p>Now we set the computer to look over this entire set of candidates, and mark with a 1 those that obey the modified relations of the time-traveling cellular automaton, and mark with a 0 those that don't.</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/0/00/LifeChecker.jpg\" alt=\"\" width=\"441\" height=\"166\" /></p>\n<p>If N is large enough - if the size of the possible universe and its duration is large enough - there would be descriptions of universes which experienced natural selection, evolution, perhaps the evolution of intelligence, and of course, time travel with self-consistent Time-Turners, obeying the modified relations of the cellular automaton. And the checker would mark those descriptions with a 1, and all others with a 0.</p>\n<p>Suppose we pick out one of the histories marked with a 1 and look at it. &nbsp;It seems to contain a description of people who remember experiencing time travel.</p>\n<p>Now, were their experiences real? Did we <em>make </em>them real by marking them with a 1 - by applying the logical filter using a causal computer? Even though there was no way of computing future events from past events; even though their universe isn't a causal universe; even though they will have had experiences that literally were not 'caused', that did not have any causal graph behind them, within the framework of their own universe and its rules?</p>\n<hr />\n<p>I don't know. &nbsp;<em>But...</em></p>\n<p>Our&nbsp;<em>own</em>&nbsp;universe does&nbsp;<em>not</em>&nbsp;appear to have Time-Turners, and&nbsp;<em>does</em>&nbsp;appear to have strictly local causality in which each variable can be computed strictly forward-in-time.</p>\n<p>And I don't know&nbsp;<em>why</em>&nbsp;that's the case; but it's a likely-looking<em>&nbsp;hint</em>&nbsp;for anyone wondering what sort of universes can be real in the first place.</p>\n<p>The collection of hypothetical mathematical thingies that can be&nbsp;<em>described logically</em>&nbsp;(in terms of relational rules with consistent solutions) looks&nbsp;<em>vastly</em>&nbsp;larger than the collection of&nbsp;<em>causal universes</em>&nbsp;with locally determined, acyclically ordered events. Most mathematical objects aren't like that. When you say, \"We live in a causal universe\", a universe that can be computed in-order using local and directional rules of determination, you're&nbsp;<em>vastly narrowing down the possibilities&nbsp;</em>relative to all of Math-space.</p>\n<p>So it's rather&nbsp;<em>suggestive</em>&nbsp;that we find ourselves in a causal universe rather than a logical universe - it suggests that not all mathematical objects can be real, and the sort of thingies that&nbsp;<em>can</em>&nbsp;be real and have people in them are constrained to somewhere in the vicinity of 'causal universes'. That you can't have consciousness without computing an agent made of causes and effects, or maybe something can't be real at all unless it's a fabric of cause and effect. It suggests that if there&nbsp;<em>is</em>&nbsp;a Tegmark Level IV multiverse, it isn't \"all logical universes\" but \"all causal universes\".</p>\n<p>Of course you also have to be a bit careful when you start assuming things like \"Only causal things can be real\" because it's so easy for Reality to come back at you and shout \"WRONG!\" Suppose you thought reality had to be a&nbsp;<em>discrete</em>&nbsp;causal graph, with a finite number of nodes and discrete descendants,&nbsp;<em>exactly</em>&nbsp;like&nbsp;<a href=\"/lw/ev3/causal_diagrams_and_causal_models/\">Pearl-standard causal models</a>. There would be&nbsp;<em>no hypothesis in your hypothesis-space</em>&nbsp;to describe the standard model of physics, where space is continuous, indefinitely divisible, and has complex amplitude assignments over uncountable cardinalities of points.</p>\n<p>Reality is primary, saith the wise old masters of science. The first rule of science is just to go with what you see, and try to understand it; rather than standing on your assumptions, and trying to argue with reality.</p>\n<p>But&nbsp;<em>even so,</em>&nbsp;it's&nbsp;<em>interesting</em>&nbsp;that the pure, ideal structure of causal models, invented by statisticians to reify the idea of 'causality' as simply as possible, looks&nbsp;<em>much</em>&nbsp;more like the modern view of physics than does the old Newtonian ideal.</p>\n<p>If you believed in Newtonian billiard balls bouncing around, and somebody asked you what sort of things can be real, you'd probably start talking about 'objects', like the billiard balls, and 'properties' of the objects, like their location and velocity, and how the location 'changes' between one 'time' and another, and so on.</p>\n<p>But suppose you'd never heard of atoms or velocities or this 'time' stuff - just the&nbsp;<a href=\"/lw/ev3/causal_diagrams_and_causal_models/\">causal diagrams and causal models</a>&nbsp;invented by statisticians to represent the simplest possible cases of cause and effect. &nbsp;Like this:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/3/3a/Discrete.svg\" alt=\"\" width=\"275\" height=\"259\" /></p>\n<p>And then someone says to you, \"Invent a&nbsp;<em>continuous analogue</em>&nbsp;of this.\"</p>\n<p>You wouldn't invent billiard balls. There's no billiard balls in a causal diagram.</p>\n<p>You wouldn't invent a single time sweeping through the universe. There's no sweeping time in a causal diagram.</p>\n<p>You'd stare a bit at B, C, and D which are the sole nodes determining A, screening off the rest of the graph, and say to yourself:</p>\n<p>\"Okay, how can I invent a&nbsp;<em>continuous</em>&nbsp;analogue of there being three nodes that screen off the rest of the graph? How do I do that with a continuous neighborhood of points, instead of three nodes?\"</p>\n<p>You'd stare at E determining D determining A, and ask yourself:</p>\n<p>\"How can I invent a&nbsp;<em>continuous</em>&nbsp;analogue of 'determination', so that instead of E determining D determinining A, there's a continuum of determined points between E and A?\"</p>\n<p>If you generalized in a certain simple and obvious fashion...</p>\n<p>The continuum of relatedness from B to C to D would be what we call&nbsp;<em>space.</em></p>\n<p>The continuum of determination from E to D to A would be what we call&nbsp;<em>time.</em></p>\n<p>There would be a rule stating that for epsilon time before A, there's a neighborhood of spatial points delta which screens off the rest of the universe from being relevant to A (so long as no descendants of A are observed); and that epsilon and delta can both get arbitrarily close to zero.</p>\n<p>&nbsp;<img src=\"http://wiki.lesswrong.com/mediawiki/images/a/a7/CausalNeighborhood.jpg\" alt=\"\" width=\"413\" height=\"207\" /></p>\n<p>There might be - if you were just picking the simplest rules you could manage - a physical constant which related the metric of relatedness (space) to the metric of determination (time) and so enforced a simple continuous analogue of local causality...</p>\n<p>...in our universe, we call it&nbsp;<em>c,</em>&nbsp;the speed of light.</p>\n<p>And it's worth remembering that Isaac Newton did&nbsp;<em>not</em>&nbsp;expect that rule to be there.</p>\n<p>If we just stuck with Special Relativity, and didn't get any&nbsp;<em>more</em>&nbsp;modern than that, there would still be little billiard balls like electrons, occupying some particular point in that neighborhood of space.</p>\n<p>But if your little neighborhoods of space have billiard balls with velocities, many of which are&nbsp;<em>slower</em>&nbsp;than lightspeed... well, that doesn't look like the simplest continuous analogues of a causal diagram, does it?</p>\n<p>When we make the first quantum leap and describe particles as waves, we find that the billiard balls have been eliminated. There's no 'particles' with a single point position and a velocity slower than light. There's an electron&nbsp;<em>field,</em>&nbsp;and waves propagate through the electron field through points interacting only with locally neighboring points. If a particular electron seems to be moving slower than light, that's just because - even though causality always propagates at exactly&nbsp;<em>c</em>&nbsp;between points within the electron&nbsp;<em>field</em>&nbsp;- the crest of the electron&nbsp;<em>wave</em>&nbsp;can appear to move slower than that. A billiard ball moving through space over time, has been replaced by a set of points with values determined by their immediate historical neighborhood.</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/9/98/BallCause.jpg\" alt=\"\" width=\"448\" height=\"153\" />&nbsp;</p>\n<p><strong>vs.</strong></p>\n<p><strong>&nbsp;</strong><img src=\"http://wiki.lesswrong.com/mediawiki/images/4/41/WaveCause.jpg\" alt=\"\" width=\"365\" height=\"283\" /></p>\n<p>And when we make the second quantum leap into configuration space, we find a timeless universal wavefunction with complex amplitudes assigned over the points in that configuration space, and&nbsp;<a href=\"/lw/qq/timeless_beauty/\">the amplitude of every point causally determined by its immediate neighborhood in the configuration space</a>.[2]</p>\n<p>So, yes, Reality can poke you in the nose if you decide that only discrete causal graphs can be real, or something silly like that.</p>\n<p>But on the other hand, taking advice from the math of causality wouldn't always lead you astray. Modern physics looks a&nbsp;<em>heck</em>&nbsp;of a lot more similar to \"Let's build a continuous analogue of the simplest diagrams statisticians invented to describe theoretical causality\", than like anything Newton or Aristotle imagined by looking at the apparent world of boulders and planets.</p>\n<p>I don't know what it means... but perhaps we shouldn't ignore the&nbsp;<em>hint</em>&nbsp;we received by virtue of finding ourselves inside the narrow space of \"causal universes\" -&nbsp;rather than the much wider space \"all logical universes\" - when it comes to guessing what sort of thingies can be real. To the extent we allow non-causal universes in our hypothesis space, there's a strong chance that we are broadening our imagination beyond what can&nbsp;<em>really&nbsp;</em>be real under the Actual Rules - whatever&nbsp;<em>they</em>&nbsp;are! (It&nbsp;<em>is</em>&nbsp;possible to broaden your metaphysics too much, as well as too little. For example, you could allow logical contradictions into your hypothesis space - collections of axioms with no models - and ask whether we lived in one of those.)</p>\n<p>If we trusted absolutely that only causal universes could be real, then it would be safe to allow only causal universes into our hypothesis space, and assign probability literally zero to everything else.</p>\n<p>But if you were scared of being wrong, then assigning probability literally&nbsp;<em>zero</em>&nbsp;means you can't change your mind, ever, even if Professor McGonagall shows up with a Time-Turner tomorrow.</p>\n<p><strong><a href=\"/lw/fok/causal_universes/#7wul\">Meditation</a></strong>:&nbsp;Suppose you needed to assign non-zero probability to any way things could conceivably turn out to be, given humanity's rather young and confused state - enumerate all the hypotheses a superintelligent AI should ever be able to arrive at, based on any sort of strange world it might find by observation of Time-Turners or stranger things. &nbsp;How would you enumerate the hypothesis space of all the worlds we could remotely maybe possibly be living in, including worlds with hypercomputers and Stable Time Loops and even stranger features?</p>\n<p><strong><a href=\"/lw/fok/causal_universes/#7wum\">Mainstream status</a></strong>.</p>\n<hr />\n<p>[1] Sometimes I still marvel about how in most time-travel stories nobody thinks of this. I guess it really is true that only people who are sensitized to 'thinking about existential risk'&nbsp;<em>even notice</em>&nbsp;when a world ends, or when billions of people are extinguished and replaced by slightly different versions of themselves. But then almost nobody will notice that sort of thing inside their fiction if&nbsp;<a href=\"http://yudkowsky.net/other/fiction/the-sword-of-good\">the characters all act like it's okay</a>.)</p>\n<p>[2] Unless you believe in '<a href=\"/lw/q6/collapse_postulates/\">collapse</a>' interpretations of <a href=\"/lw/r5/the_quantum_physics_sequence/\">quantum mechanics</a> where <a href=\"/lw/q1/bells_theorem_no_epr_reality/\">Bell's Theorem</a> mathematically requires that&nbsp;<em>either</em>&nbsp;your <a href=\"/lw/ev3/causal_diagrams_and_causal_models/\">causal models</a> don't obey the <a href=\"/lw/ezu/stuff_that_makes_stuff_happen/\">Markov condition</a>&nbsp;<em>or</em>&nbsp;they have <a href=\"/lw/q2/spooky_action_at_a_distance_the_nocommunication/\">faster-than-light nonlocal influences</a>. (Despite a large literature of obscurantist verbal words intended to obscure this fact, as generated and consumed by physicists who don't know about formal definitions of causality or the Markov condition.) If you <a href=\"/lw/q7/if_manyworlds_had_come_first/\">believe in a collapse postulate</a>, this whole post goes out the window. But frankly, if you believe that, you are bad and you should feel bad.</p>\n<p style=\"text-align:right\">Part of the sequence <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\"><em>Highly Advanced Epistemology 101 for Beginners</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/frz/mixed_reference_the_great_reductionist_project/\">Mixed Reference: The Great Reductionist Project</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/f4e/logical_pinpointing/\">Logical Pinpointing</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"cq69M9ceLNA35ShTR": 7}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "o5F2p3krzT4JgzqQc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 79, "baseScore": 113, "extendedScore": null, "score": 0.000249, "legacy": true, "legacyId": "20324", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "SqFbMbtxGybdS2gRs", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "proofs-implications-and-models", "canonicalPrevPostSlug": "causal-reference", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 113, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 393, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["NhQju3htS9W6p6wE6", "hzuSDMx7pd2uxFc5w", "GKTe9bCxFSE6EXEEu", "xsZnufn3cQw7tJeQ3", "hc9Eg6erp6hk9bWhn", "AnHJX42C6r6deohTG", "DY9h6zxq6EMHrkkxE", "WqGCaRhib42dhKWRL", "bTsiPnFndZeqTnWpu", "3FoMuCLqZggTxoC3S"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 15, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-29T05:16:48.866Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Total Tech Wars", "slug": "seq-rerun-total-tech-wars", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6oYwWN9gzfHAYiGPa/seq-rerun-total-tech-wars", "pageUrlRelative": "/posts/6oYwWN9gzfHAYiGPa/seq-rerun-total-tech-wars", "linkUrl": "https://www.lesswrong.com/posts/6oYwWN9gzfHAYiGPa/seq-rerun-total-tech-wars", "postedAtFormatted": "Thursday, November 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Total%20Tech%20Wars&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Total%20Tech%20Wars%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6oYwWN9gzfHAYiGPa%2Fseq-rerun-total-tech-wars%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Total%20Tech%20Wars%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6oYwWN9gzfHAYiGPa%2Fseq-rerun-total-tech-wars", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6oYwWN9gzfHAYiGPa%2Fseq-rerun-total-tech-wars", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 189, "htmlBody": "<p>Today's post, <a href=\"http://www.overcomingbias.com/2008/11/total-tech-wars.html\">Total Tech Wars</a> was originally published on November 29, 2008.  A summary:</p>\n<p>&nbsp;</p>\n<blockquote>Unpleasant situations can arise if people believe that the development of some sort of AI is likely to be a big win for the developers, and a big loss for everyone else. Avoid the possibility of a total war between development groups by avoiding framing the conversation about AI in that way.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fom/seq_rerun_dreams_of_autarky/\">Dreams of Autarky</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6oYwWN9gzfHAYiGPa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0453558943559525e-06, "legacy": true, "legacyId": "20333", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["38LJxv3pvwB4WM64K", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-29T06:19:50.305Z", "modifiedAt": null, "url": null, "title": "Intuitions Aren't Shared That Way", "slug": "intuitions-aren-t-shared-that-way", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:32.638Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jaN4EKrRnZdynTJjH/intuitions-aren-t-shared-that-way", "pageUrlRelative": "/posts/jaN4EKrRnZdynTJjH/intuitions-aren-t-shared-that-way", "linkUrl": "https://www.lesswrong.com/posts/jaN4EKrRnZdynTJjH/intuitions-aren-t-shared-that-way", "postedAtFormatted": "Thursday, November 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Intuitions%20Aren't%20Shared%20That%20Way&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntuitions%20Aren't%20Shared%20That%20Way%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjaN4EKrRnZdynTJjH%2Fintuitions-aren-t-shared-that-way%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Intuitions%20Aren't%20Shared%20That%20Way%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjaN4EKrRnZdynTJjH%2Fintuitions-aren-t-shared-that-way", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjaN4EKrRnZdynTJjH%2Fintuitions-aren-t-shared-that-way", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1327, "htmlBody": "<p><a href=\"http://www.amazon.com/Experimental-Philosophy-Introduction-Joshua-Alexander/dp/0745649181/\"><img style=\"float: right; padding: 10px;\" src=\"http://commonsenseatheism.com/wp-content/uploads/2012/11/experimental-philosophy.jpg\" alt=\"\" /></a><small>Part of the sequence: <a href=\"http://wiki.lesswrong.com/wiki/Rationality_and_Philosophy\">Rationality and Philosophy</a></small></p>\n<p>Consider these two versions of the famous <a href=\"http://en.wikipedia.org/wiki/Trolley_problem\">trolley problem</a>:</p>\n<blockquote>\n<p><strong>Stranger</strong>: A train, its brakes failed, is rushing toward five people. The only way to save the five people is to throw the switch sitting next to you, which will turn the train onto a side track, thereby preventing it from killing the five people. However, there is a stranger standing on the side track with his back turned, and if you proceed to thrown the switch, the five people will be saved, but the person on the side track will be killed.</p>\n<p><strong>Child</strong>: A train, its brakes failed, is rushing toward five people. The only way to save the five people is to throw the switch sitting next to you, which will turn the train onto a side track, thereby preventing it from killing the five people. However, there is a 12-year-old boy standing on the side track with his back turned, and if you proceed to throw the switch, the five people will be saved, but the boy on the side track will be killed.</p>\n</blockquote>\n<p>Here it is: a standard-form philosophical thought experiment. In standard analytic philosophy, the next step is to engage in <em>conceptual analysis</em> &mdash; a process in which we use our intuitions as evidence for one theory over another. For example, if your intuitions say that it is \"morally right\" to throw the switch in both cases above, then these intuitions may be counted as evidence for consequentialism, for moral realism, for agent neutrality, and so on.</p>\n<p><a href=\"http://www.amazon.com/Experimental-Philosophy-Introduction-Joshua-Alexander/dp/0745649181/\">Alexander (2012)</a> explains:</p>\n<blockquote>\n<p>Philosophical intuitions play an important role in contemporary philosophy. Philosophical intuitions provide data to be explained by our philosophical theories [and] evidence that may be adduced in arguments for their truth... In this way, the role... of intuitional evidence in philosophy is similar to the role... of perceptual evidence in science...</p>\n<p>Is knowledge simply justified true belief? Is a belief justified just in case it is caused by a reliable cognitive mechanism? Does a name refer to whatever object uniquely or best satisfies the description associated with it? Is a person morally responsible for an action only if she could have acted otherwise? Is an action morally right just in case it provides the greatest benefit for the greatest number of people all else being equal? When confronted with these kinds of questions, philosophers often appeal to philosophical intuitions about real or imagined cases...</p>\n<p>...there is widespread agreement about the role that [intuitions] play in contemporary philosophical practice... We advance philosophical theories on the basis of their ability to explain our philosophical intuitions, and appeal to them as evidence that those theories are true...</p>\n</blockquote>\n<p>In particular, notice that philosophers do not appeal to their intuitions as merely an exercise in&nbsp;<em>autobiography</em>. Philosophers are not merely trying to map the contours of <em>their own</em> idiosyncratic concepts. That could be interesting, but it wouldn't be worth decades of publicly-funded philosophical research. Instead, philosophers appeal to their intuitions as evidence for what is <em>true in general</em> about a concept, or true about the world.</p>\n<p><a id=\"more\"></a></p>\n<p>In this sense,</p>\n<blockquote>\n<p>We [philosophers] tend to believe that our philosophical intuitions are more or less universally shared... We... appeal to philosophical intuitions, when we do, because we anticipate that others share our intuitive judgments.</p>\n</blockquote>\n<p>But anyone with more than a passing familiarity with cognitive science might have bet in advance that this basic underlying assumption of a core philosophical method is... <em>incorrect</em>.</p>\n<p>For one thing, philosophical intuitions show <em>gender diversity</em>. Consider again the <em>Stranger</em> and <em>Child</em> versions of the Trolley problem. It turns out that men are less likely than women to think it is morally acceptable to throw the switch in the <em>Stranger</em> case, while women are less likely than men to think it is morally acceptable to throw the switch in the <em>Child</em> case (<a href=\"http://dingo.sbs.arizona.edu/~snichols/Papers/Variations_in_the_Ethical_Intuitions.pdf\">Zamzow &amp; Nichols 2009</a>).</p>\n<p>Or, consider a thought experiment meant to illuminate the much-discussed concept of <em>knowledge</em>:</p>\n<blockquote>\n<p>Peter is in his locked apartment and is reading. He decides to have a shower. He puts his book down on the coffee table. Then he takes off his watch, and also puts it on the coffee table. Then he goes into the bathroom. As Peter's shower begins, a burglar silently breaks into Peter's apartment. The burglar takes Peter's watch, puts a cheap plastic watch in its place, and then leaves. Peter has only been in the shower for two minutes, and he did not hear anything.</p>\n</blockquote>\n<p>When presented with this vignette, only 41% of men say that Peter \"knows\" there is a watch on the table, while 71% of women say that Peter \"knows\" there is a watch on the table (<a href=\"http://pantheon.yale.edu/~cjs28/papers/2012%20Starmans&amp;Friedman.pdf\">Starman &amp; Friedman 2012</a>). According to <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/11/Buckwalter-Stich-Gender-and-Philosophical-Intuition.pdf\">Buckwalter &amp; Stich (2010)</a>, Starmans &amp; Friedman ran another study using a slightly different vignette with a female protagonist, and that time only 36% of men said the protagonist \"knows,\" while 75% of women said she \"knows.\"</p>\n<p>The story remains the same for intuitions about free will. In another study reported in <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/11/Buckwalter-Stich-Gender-and-Philosophical-Intuition.pdf\">Buckwalter &amp; Stich (2010)</a>, Geoffrey Holtman presented subjects with this vignette:</p>\n<blockquote>\n<p>Suppose scientists figure out the exact state of the universe during the Big Bang, and figure out all the laws of physics as well. They put this information into a computer, and the computer perfectly predicts everything that has ever happened. In other words, they prove that everything that happens has to happen exactly that way because of the laws of physics and everything that's come before. In this case, is a person free to choose whether or not to murder someone?</p>\n</blockquote>\n<p>In this study, only 35% of men, but 63% of women, said a person in this world could be free to choose whether or not to murder someone.</p>\n<p>Intuitions show not only gender diversity but also <em>cultural diversity</em>. Consider another thought experiment about <em>knowledge</em> (you can punch me in the face, later):</p>\n<blockquote>\n<p>Bob has a friend Jill, who has driven a Buick for many years. Bob therefore thinks that Jill drives an American car. He is not aware, however, that her Buick has recently been stolen, and he is also not aware that Jill has replaced it with a Pontiac, which is a different kind of American car. Does Bob really know that Jill drives an American car, or does he only believe it?</p>\n</blockquote>\n<p>Only 26% of Westerners say that Bob \"knows\" that Jill drives an American car, while 56% of East Asian subjects, and 61% of South Asian subjects, say that Bob \"knows.\"</p>\n<p>Now, consider a thought experiment meant to elicit semantic intuitions:</p>\n<blockquote>\n<p>Suppose that John has learned in college that G&ouml;del is the man who proved... the incompleteness of arithmetic. John is quite good at mathematics and he can give an accurate statement of the incompleteness theorem, which he attributes to G&ouml;del as the discoverer. But this is the only thing that he has heard about G&ouml;del. Now suppose that G&ouml;del was not the author of this theorem. A man called \"Schmidt\"&hellip; actually did the work in question. His friend G&ouml;del somehow got a hold of the manuscript and claimed credit for the work, which was thereafter attributed to G&ouml;del... Most people who have heard the name \"G&ouml;del\" are like John; the claim that G&ouml;del discovered the incompleteness theorem is the only thing that they have ever heard about G&ouml;del.</p>\n</blockquote>\n<p>When presented with this vignette, East Asians are more likely to take the \"descriptivist\" view of reference, believing that John \"is referring to\" Schmidt &mdash; while Westerners are more likely to take the \"causal-historical\" view, believing that John \"is referring to\" G&ouml;del (<a href=\"http://www.rci.rutgers.edu/~stich/Publications/Papers/SemanticIntuitions.pdf\">Machery et al. 2004</a>).</p>\n<p><a href=\"/lw/7tz/philosophy_by_humans_1_concepts_dont_work_that_way/\">Previously</a>, I asked:</p>\n<blockquote>\n<p>What would happen if we dropped all philosophical methods that were developed when we had a Cartesian view of the mind and of reason, and instead invented philosophy anew given what we now know about the physical processes that produce human reasoning?</p>\n</blockquote>\n<p>For one thing, we would never assume that people of all kinds would share our intuitions.</p>\n<p>&nbsp;</p>\n<p align=\"right\">Next post: <a href=\"/r/discussion/lw/fpe/philosophy_needs_to_trust_your_rationality_even/\">Philosophy Needs to Trust Your Rationality Even Though It Shouldn't</a></p>\n<p align=\"right\">Previous post: <a href=\"/lw/8lz/philosophy_by_humans_2_living_metaphorically/\">Living Metaphorically</a></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"z95PGFXtPpwakqkTA": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jaN4EKrRnZdynTJjH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 52, "extendedScore": null, "score": 0.000131, "legacy": true, "legacyId": "20339", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "yFvZa9wkv5JoqhM8F", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "philosophy-needs-to-trust-your-rationality-even-though-it", "canonicalPrevPostSlug": "living-metaphorically", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 52, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 237, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wHjpCxeDeuFadG3jF", "2pdyL8bSGBfYsnkyS", "nEA5vYzYS5zqrtQTm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-29T07:31:04.236Z", "modifiedAt": null, "url": null, "title": "Meetup : Seattle Meetup: Bayes Theorem Tutorial", "slug": "meetup-seattle-meetup-bayes-theorem-tutorial", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:04.473Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LC9SWnsefP5QJrKZM/meetup-seattle-meetup-bayes-theorem-tutorial", "pageUrlRelative": "/posts/LC9SWnsefP5QJrKZM/meetup-seattle-meetup-bayes-theorem-tutorial", "linkUrl": "https://www.lesswrong.com/posts/LC9SWnsefP5QJrKZM/meetup-seattle-meetup-bayes-theorem-tutorial", "postedAtFormatted": "Thursday, November 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Seattle%20Meetup%3A%20Bayes%20Theorem%20Tutorial&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Seattle%20Meetup%3A%20Bayes%20Theorem%20Tutorial%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLC9SWnsefP5QJrKZM%2Fmeetup-seattle-meetup-bayes-theorem-tutorial%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Seattle%20Meetup%3A%20Bayes%20Theorem%20Tutorial%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLC9SWnsefP5QJrKZM%2Fmeetup-seattle-meetup-bayes-theorem-tutorial", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLC9SWnsefP5QJrKZM%2Fmeetup-seattle-meetup-bayes-theorem-tutorial", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 99, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/gj'>Seattle Meetup: Bayes Theorem Tutorial</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 December 2012 04:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1900 NE 68th St. Seattle, WA 98115</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This Sunday Brendan will be giving a tutorial on Bayes theorem at the Ravenna House. I understand there will be a couple different explanations and some practice problems.</p>\n\n<p>Normal socializing and dinner to follow.</p>\n\n<p>As a follow up to Brendan's tutorial, next week I will be give a non-technical introduction to thinking with statistical models. In particular, the usefulness of Hierarchical Models and other Hidden Variable models.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/gj'>Seattle Meetup: Bayes Theorem Tutorial</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LC9SWnsefP5QJrKZM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.0454321815075321e-06, "legacy": true, "legacyId": "20343", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Seattle_Meetup__Bayes_Theorem_Tutorial\">Discussion article for the meetup : <a href=\"/meetups/gj\">Seattle Meetup: Bayes Theorem Tutorial</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 December 2012 04:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1900 NE 68th St. Seattle, WA 98115</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This Sunday Brendan will be giving a tutorial on Bayes theorem at the Ravenna House. I understand there will be a couple different explanations and some practice problems.</p>\n\n<p>Normal socializing and dinner to follow.</p>\n\n<p>As a follow up to Brendan's tutorial, next week I will be give a non-technical introduction to thinking with statistical models. In particular, the usefulness of Hierarchical Models and other Hidden Variable models.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Seattle_Meetup__Bayes_Theorem_Tutorial1\">Discussion article for the meetup : <a href=\"/meetups/gj\">Seattle Meetup: Bayes Theorem Tutorial</a></h2>", "sections": [{"title": "Discussion article for the meetup : Seattle Meetup: Bayes Theorem Tutorial", "anchor": "Discussion_article_for_the_meetup___Seattle_Meetup__Bayes_Theorem_Tutorial", "level": 1}, {"title": "Discussion article for the meetup : Seattle Meetup: Bayes Theorem Tutorial", "anchor": "Discussion_article_for_the_meetup___Seattle_Meetup__Bayes_Theorem_Tutorial1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "2 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-29T21:00:29.400Z", "modifiedAt": null, "url": null, "title": "Philosophy Needs to Trust Your Rationality Even Though It Shouldn't", "slug": "philosophy-needs-to-trust-your-rationality-even-though-it", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:29.627Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2pdyL8bSGBfYsnkyS/philosophy-needs-to-trust-your-rationality-even-though-it", "pageUrlRelative": "/posts/2pdyL8bSGBfYsnkyS/philosophy-needs-to-trust-your-rationality-even-though-it", "linkUrl": "https://www.lesswrong.com/posts/2pdyL8bSGBfYsnkyS/philosophy-needs-to-trust-your-rationality-even-though-it", "postedAtFormatted": "Thursday, November 29th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Philosophy%20Needs%20to%20Trust%20Your%20Rationality%20Even%20Though%20It%20Shouldn't&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APhilosophy%20Needs%20to%20Trust%20Your%20Rationality%20Even%20Though%20It%20Shouldn't%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2pdyL8bSGBfYsnkyS%2Fphilosophy-needs-to-trust-your-rationality-even-though-it%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Philosophy%20Needs%20to%20Trust%20Your%20Rationality%20Even%20Though%20It%20Shouldn't%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2pdyL8bSGBfYsnkyS%2Fphilosophy-needs-to-trust-your-rationality-even-though-it", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2pdyL8bSGBfYsnkyS%2Fphilosophy-needs-to-trust-your-rationality-even-though-it", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1120, "htmlBody": "<p><small>Part of the sequence: <a href=\"http://wiki.lesswrong.com/wiki/Rationality_and_Philosophy\">Rationality and Philosophy</a></small></p>\n<blockquote>\n<p>Philosophy is notable for the extent to which disagreements with respect to even those most basic questions persist among its most able practitioners, despite the fact that the arguments thought relevant to the disputed questions are typically well-known to all parties to the dispute.</p>\n</blockquote>\n<p align=\"right\"><a href=\"https://www.princeton.edu/~tkelly/esod.pdf\">Thomas Kelly</a></p>\n<blockquote>\n<p>The goal of philosophy is to uncover certain truths... [But] philosophy continually leads experts with the highest degree of epistemic virtue, doing the very best they can, to accept a wide array of incompatible doctrines. Therefore, philosophy is an unreliable instrument for finding truth. A person who enters the field is highly unlikely to arrive at true answers to philosophical questions.</p>\n</blockquote>\n<p align=\"right\"><a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/10/Brennan-Scepticism-about-philosophy.pdf\">Jason Brennan</a></p>\n<p>&nbsp;</p>\n<p>After millennia of debate, philosophers remain heavily divided on many core issues. According to the <a href=\"http://philpapers.org/surveys/results.pl\">largest-ever survey of philosophers</a>, they're split 25-24-18 on deontology / consequentialism / virtue ethics, 35-27 on empiricism vs. rationalism, and 57-27 on physicalism vs. non-physicalism.</p>\n<p>Sometimes, they are even divided on <em>psychological</em> questions that psychologists have <em>already answered</em>: Philosophers are <a href=\"http://philpapers.org/surveys/results.pl\">split evenly</a> on the question of whether it's possible to make a moral judgment without being motivated to abide by that judgment, even though we already know that this <em>is</em> possible for some people with damage to their brain's reward system, for example many Parkinson's patients, and patients with damage to the ventromedial frontal cortex (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/10/Schroeder-et-al-Moral-Motivation.pdf\">Schroeder et al. 2012</a>).<sup>1</sup></p>\n<p>Why are physicists, biologists, and psychologists more prone to reach consensus than philosophers?<sup>2</sup> One standard story is <a href=\"/lw/qi/faster_than_science/\">that</a> \"the method of science is to amass such an enormous mountain of evidence that... scientists cannot ignore it.\" Hence, <em>religionists</em> might still argue that <a href=\"http://www.youtube.com/watch?v=v80C-_Zf41Q\">Earth is flat</a> or that evolutionary theory and the Big Bang theory are \"<a href=\"http://www.slate.com/blogs/the_slatest/2012/10/06/video_paul_broun_calls_evolution_big_bang_theory_lies_straight_from_the_pit_of_hell.html\">lies from the pit of hell</a>,\" and <em>philosophers</em> might still be divided about whether somebody can make a moral judgment they aren't themselves motivated by, but <em>scientists</em> have reached consensus about such things.</p>\n<p><a id=\"more\"></a></p>\n<p>In its dependence on masses of evidence and definitive experiments, <a href=\"/lw/qb/science_doesnt_trust_your_rationality/\">science doesn't trust your rationality</a>:</p>\n<blockquote>\n<p>Science is built around the assumption that you're too stupid and self-deceiving to just use [probability theory]. After all, if it was that simple, we wouldn't need a social process of science... [Standard scientific method] doesn't trust your rationality, and it doesn't rely on your ability to use probability theory as the arbiter of truth. It wants you to set up a definitive experiment.</p>\n</blockquote>\n<p>Sometimes, you can answer philosophical questions with mountains of evidence, as with the example of moral motivation given above. But or many philosophical problems, overwhelming evidence simply isn't <em>available</em>. Or maybe you <a href=\"/lw/43v/the_urgent_metaethics_of_friendly_artificial/\">can't afford</a> to <a href=\"/lw/qc/when_science_cant_help/\">wait a decade</a> for definitive experiments to be done. <a href=\"/lw/qd/science_isnt_strict_enough/\">Thus</a>, \"if you would rather not waste ten years trying to prove the wrong theory,\" or if you'd like to get the right answer without overwhelming evidence, \"you'll need to [tackle] the vastly more difficult problem: listening to evidence that doesn't shout in your ear.\"</p>\n<p>This is why philosophers need rationality training even more desperately than scientists do. Philosophy asks you to get the right answer <em>without</em> evidence that shouts in your ear. The less evidence you have, or the harder it is to interpret, the more rationality you need to get the right answer. (As likelihood ratios get smaller, your priors need to be better and your updates more accurate.)</p>\n<p>Because it tackles so many questions that <em>can't</em> be answered by masses of evidence or definitive experiments, philosophy needs to trust your rationality even though it shouldn't: we generally <em>are</em> as \"stupid and self-deceiving\" as science assumes we are. We're \"<a href=\"http://www.amazon.com/Predictably-Irrational-Revised-Expanded-Edition/dp/0061353248/\">predictably irrational</a>\" and <a href=\"http://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374275637/\">all that</a>.</p>\n<p>But hey! Maybe philosophers are prepared for this. Since philosophy is so much more demanding of one's rationality, perhaps the field has built top-notch rationality training into the standard philosophy curriculum?</p>\n<p>Alas, it doesn't seem so. I don't see much Kahneman &amp; Tversky in philosophy syllabi &mdash; just light-weight \"critical thinking\" classes and lists of informal fallacies. But even classes in human bias might not improve things much due to the <a href=\"/lw/he/knowing_about_biases_can_hurt_people/\">sophistication effect</a>: someone with a sophisticated knowledge of fallacies and biases might just have more ammunition with which to attack views they don't like. So what's really needed is regular <a href=\"/lw/fc3/checklist_of_rationality_habits/\">habits training</a> for <a href=\"/lw/aa7/get_curious/\">genuine curiosity</a>, <a href=\"/lw/bnk/sotw_avoid_motivated_cognition/\">motivated cognition mitigation</a>, and so on.</p>\n<p>(Imagine a world in which Frank Jackson's famous reversal on the <a href=\"http://en.wikipedia.org/wiki/Knowledge_argument\">knowledge argument</a> <em>wasn't news</em> &mdash; because established philosophers changed their minds all the time. Imagine a world in which philosophers were fine-tuned enough to reach consensus on 10 bits of evidence rather than 1,000.)</p>\n<p>We might also ask: How well do philosophers perform on standard tests of rationality, for example <a href=\"http://psych.fullerton.edu/MBIRNbAUM/PSYCH466/articles/Frederick_CRT_2005.pdf\">Frederick (2005)</a>'s <a href=\"http://www.sjdm.org/dmidi/Cognitive_Reflection_Test.html\">CRT</a>? <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/11/Livengood-et-al-Philosophical-Temperament.pdf\">Livengood et al. (2010)</a> found, via an internet survey, that subjects with <em>graduate-level</em> philosophy training had a mean CRT score of 1.32. (The best possible score is 3.)</p>\n<p>A score of 1.32 isn't radically different from the mean CRT scores found for psychology undergraduates (<a>1.5</a>), financial planners (<a href=\"http://www2.stetson.edu/fsr/abstracts2/pdf-2007%20vol%2016/low_res/zux00407000245.pdf\">1.76</a>), Florida Circuit Court judges (<a href=\"http://www.hertig.ethz.ch/LE_2007_files/Papers/Rachlinski_Blinking_2007.pdf\">1.23</a>), Princeton Undergraduates (<a href=\"http://www.hertig.ethz.ch/LE_2007_files/Papers/Rachlinski_Blinking_2007.pdf\">1.63</a>), and people who happened to be sitting along the Charles River during a July 4th fireworks display (<a href=\"http://psych.fullerton.edu/MBIRNbAUM/PSYCH466/articles/Frederick_CRT_2005.pdf\">1.53</a>). It is also noticeably <em>lower</em> than the mean CRT scores found for MIT students (<a href=\"http://psych.fullerton.edu/MBIRNbAUM/PSYCH466/articles/Frederick_CRT_2005.pdf\">2.18</a>) and for attendees to a LessWrong.com <a href=\"/lw/crs/how_to_run_a_successful_less_wrong_meetup/\">meetup group</a> (<a href=\"http://commonsenseatheism.com/?p=16199\">2.69</a>).</p>\n<p>Moreover, several studies show that philosophers are just as prone to particular biases as laypeople (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/11/Schulz-et-al-Persistent-bias-in-expert-judgments-about-free-will-and-moral-responsibility.pdf\">Schulz et al. 2011</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/11/Tobia-et-al-Moral-intuitions-are-philosophers-experts.pdf\">Tobia et al. 2012</a>), for example order effects in moral judgment (<a href=\"http://www.wjh.harvard.edu/~cushman/research/Research_files/philosophers_final.pdf\">Schwitzgebel &amp; Cushman 2012</a>).</p>\n<p>People are typically excited about the <a href=\"http://appliedrationality.org/\">Center for Applied Rationality</a> because it teaches thinking skills that can improve one's happiness and effectiveness. That excites me, too. But I hope that in the long run CFAR will also help produce <em>better philosophers</em>, because it <a href=\"http://lukeprog.com/SaveTheWorld.html\">looks to me</a> like we need top-notch philosophical work to secure a desirable future for humanity.<sup>3</sup></p>\n<p>&nbsp;</p>\n<p align=\"right\">Next post: <a href=\"/lw/frp/train_philosophers_with_pearl_and_kahneman_not/\">Train Philosophers with Pearl and Kahneman, not Plato and Kant</a></p>\n<p align=\"right\">Previous post: <a href=\"/lw/foz/philosophy_by_humans_3_intuitions_arent_shared/\">Intuitions Aren't Shared That Way</a></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<h3>Notes</h3>\n<p><sup>1</sup> Clearly, many philosophers have advanced versions of motivational internalism that are directly contradicted by these results from psychology. However, we don't know exactly which version of motivational internalism is defended by each survey participant who said they \"accept\" or \"lean toward\" motivational internalism. Perhaps many of them defend weakened versions of motivational internalism, such as those discussed in section 3.1 of <a href=\"http://www.joshdmay.com/wp-content/media/may-natural-rationalism.pdf\">May (forthcoming)</a>.</p>\n<p><sup>2</sup> Mathematicians reach even stronger consensus than physicists, but they don't appeal to what is usually thought of as \"mountains of evidence.\" What's going on, there? Mathematicians and philosophers almost always agree about whether a proof or an argument is valid, given a particular formal system. The difference is that a mathematician's premises consist in axioms and in theorems already strongly proven, whereas a philosopher's premises consist in substantive claims about the world for which the evidence given is often very weak (e.g. that philosopher's <a href=\"/lw/foz/philosophy_by_humans_3_intuitions_arent_shared/\">intuitions</a>).</p>\n<p><sup>3</sup> <a href=\"http://www.nickbostrom.com/old/predict.html\">Bostrom (2000)</a>; <a href=\"http://intelligence.org/files/AIPosNegFactor.pdf\">Yudkowsky (2008)</a>; <a href=\"http://lukeprog.com/SaveTheWorld.html\">Muehlhauser (2011)</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GLykb6NukBeBQtDvQ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2pdyL8bSGBfYsnkyS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 40, "extendedScore": null, "score": 1.045892320273363e-06, "legacy": true, "legacyId": "20354", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "yFvZa9wkv5JoqhM8F", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "train-philosophers-with-pearl-and-kahneman-not-plato-and", "canonicalPrevPostSlug": "intuitions-aren-t-shared-that-way", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 169, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xTyuQ3cgsPjifr7oj", "5bJyRMZzwMov5u3hW", "TKdpSzmcezNbfmGAy", "wzxneh7wxkdNYNbtB", "PGfJdgemDJSwWBZSX", "AdYdLP2sRqPMoe8fb", "ttGbpJQ8shBi8hDhh", "bGtdeqbgTzuLvZ5zn", "thfnus52hgHA32XfQ", "qMuAazqwJvkvo8teR", "LcEzxX2FNTKbB6KXS", "jaN4EKrRnZdynTJjH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-30T01:09:46.757Z", "modifiedAt": null, "url": null, "title": "Akrasia hack survey", "slug": "akrasia-hack-survey", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:06.276Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pjQYFqdfHS9Z2qRgt/akrasia-hack-survey", "pageUrlRelative": "/posts/pjQYFqdfHS9Z2qRgt/akrasia-hack-survey", "linkUrl": "https://www.lesswrong.com/posts/pjQYFqdfHS9Z2qRgt/akrasia-hack-survey", "postedAtFormatted": "Friday, November 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Akrasia%20hack%20survey&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAkrasia%20hack%20survey%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpjQYFqdfHS9Z2qRgt%2Fakrasia-hack-survey%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Akrasia%20hack%20survey%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpjQYFqdfHS9Z2qRgt%2Fakrasia-hack-survey", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpjQYFqdfHS9Z2qRgt%2Fakrasia-hack-survey", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4, "htmlBody": "<p>Survey <a href=\"/r/discussion/lw/fpf/please_decide_whether_youll_take_this/7xjj\">here</a>. &nbsp;Analysis <a href=\"/lw/fuc/akrasia_survey_data_analysis/\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pjQYFqdfHS9Z2qRgt", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 16, "extendedScore": null, "score": 1.0460341084578872e-06, "legacy": true, "legacyId": "20355", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xiZ8TqhhMMHJJrerJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-30T05:38:20.576Z", "modifiedAt": null, "url": null, "title": "The substrate", "slug": "the-substrate", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:05.734Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aitBNCiKjwDbDMEu3/the-substrate", "pageUrlRelative": "/posts/aitBNCiKjwDbDMEu3/the-substrate", "linkUrl": "https://www.lesswrong.com/posts/aitBNCiKjwDbDMEu3/the-substrate", "postedAtFormatted": "Friday, November 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20substrate&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20substrate%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaitBNCiKjwDbDMEu3%2Fthe-substrate%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20substrate%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaitBNCiKjwDbDMEu3%2Fthe-substrate", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaitBNCiKjwDbDMEu3%2Fthe-substrate", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 277, "htmlBody": "<p>If we're part of a simulation, how likely is it that whatever it's running on is using the same sort of atoms we've discovered?</p>\n<p>I think the answer is it's very unlikely. The closest resemblance I find plausible is that our atoms are simplified versions of the substrate atoms, and I wouldn't count on even that much.</p>\n<p>I'm pretty sure that a simulation has to be smaller in some sense than the universe that's running it, which means that it has fewer things or simpler things (these might be equivalent because more simplicity means fewer sub-components in things) than the home universe.</p>\n<p>You <em>might</em>&nbsp;do a meticulous job of simulating your matter in a simulation, but I suggest that you'd only bother in a small and/or specialized simulation, and even if you did, there's a reasonable chance that you don't have a full understanding of your own physics.</p>\n<p>When I look at the range of human-created simulations (dreams, daydreams, fiction, games, art, scientific, political, and commercial simulations) and contemplate that we've probably only explored a small part of the possibilities for simulation, it seems vanishingly unlikely that we're in an ancestor simulation.</p>\n<p>When I first came up with the question of the nature of our possible substrate, I didn't think there was a way to get a grip on it at all, but at least now I've got some clarity about the difficulties I think.&nbsp;</p>\n<p>So onwards to practical questions. Is there any conceivable way of telling whether we're in a simulation and if so, learning something about its nature? Is it worth trying to get out of the Big Box?</p>\n<p>Edited to add: I should think that being a simulation is an existential risk.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aitBNCiKjwDbDMEu3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 9, "extendedScore": null, "score": 1.0461868973396905e-06, "legacy": true, "legacyId": "20358", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-30T05:38:50.159Z", "modifiedAt": null, "url": null, "title": "One thousand tips do not make a system", "slug": "one-thousand-tips-do-not-make-a-system", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:52.181Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "odm3FHPzgbiGpWsSg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rAte4CqhQdhEafG7Q/one-thousand-tips-do-not-make-a-system", "pageUrlRelative": "/posts/rAte4CqhQdhEafG7Q/one-thousand-tips-do-not-make-a-system", "linkUrl": "https://www.lesswrong.com/posts/rAte4CqhQdhEafG7Q/one-thousand-tips-do-not-make-a-system", "postedAtFormatted": "Friday, November 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20One%20thousand%20tips%20do%20not%20make%20a%20system&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOne%20thousand%20tips%20do%20not%20make%20a%20system%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrAte4CqhQdhEafG7Q%2Fone-thousand-tips-do-not-make-a-system%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=One%20thousand%20tips%20do%20not%20make%20a%20system%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrAte4CqhQdhEafG7Q%2Fone-thousand-tips-do-not-make-a-system", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrAte4CqhQdhEafG7Q%2Fone-thousand-tips-do-not-make-a-system", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1222, "htmlBody": "<p>So, I've been thinking. We ought to have a <em>system</em>&nbsp;for rationality. What do I mean?</p>\n<p>Well, consider a real-time strategy game like <em>Starcraft II</em>. One of the most important things to do in SC2 is <em>macromanagement</em>: making sure that your resources are all being used sensibly. Now, macromanagement <em>could</em>&nbsp;be learned as a big, long list of tips. Like this:</p>\n<p>\n<ul>\n<li>Try to mine minerals.</li>\n<li>Recruit lots of soldiers.</li>\n<li>Recruit lots of workers.</li>\n<li>It's a good idea for a mineral site to have between 22 and 30 workers.</li>\n<li>Workers are recruited at a command center.</li>\n<li>Soldiers are recruited at a barracks.</li>\n<li>In order to build anything, you need workers.</li>\n<li>In order to build anything, you also need minerals.</li>\n<li>For that matter, in order to recruit more units, you need minerals.</li>\n<li>Workers mine minerals.</li>\n<li>Minerals should be used immediately; if you're storing them, you're wasting them.</li>\n</ul>\n<div>(Of course, the above tips only work for Terrans.)</div>\n<div><br /></div>\n<div>Okay, great. Now you have a command center and a bunch of workers. You want a bunch of soldiers. What do you do?</div>\n<div><a id=\"more\"></a><br /></div>\n<div>Why, let's look at our tips. \"Try to mine minerals.\" Okay, you start mining minerals. \"Recruit lots of soldiers.\" You can't do that, because you don't have a barracks, so you'll have to build one. \"Recruit lots of workers.\" But wait, recruiting workers and building a barracks both require minerals. Which one should you do? I dunno. \"It's a good idea for a mineral site to have between 22 and 30 workers.\" Okay, you have six; how quickly can you recruit sixteen more workers? What if you have <em>too many</em> workers? \"Workers are recruited at a command center.\" You already knew that! \"Soldiers are recruited at a barracks.\" You don't have one!</div>\n<div><br /></div>\n<div>Aha, you say, what we need is a <em>checklist of macromanagement habits</em>. Maybe you should put all of those habits in a deck of flash cards, so then you can memorize them all. What if, despite knowing a hundred macromanagement habits, you realize that you're not actually <em>using</em>&nbsp;all of them? Well, that's just akrasia, right? Maybe you need to take more vitamin D or something...</div>\n<div><br /></div>\n<div>But no, that's not what you need. The \"big, long list of tips\" is simply not a good way to organize information so as to make it useful. What you need is a macromanagement&nbsp;<em>system</em>.</div>\n<div><br /></div>\n<div>So, here's a system for macromanagement in Starcraft:</div>\n<div>\n<ul>\n<li>If you have unused buildings, have them recruit units [which also uses minerals].</li>\n<li>If, after the above, you have unused minerals, have your workers build buildings [which also uses workers].</li>\n<li>If, after the above, you have unused workers, have them mine minerals.</li>\n</ul>\n<div>This isn't a perfect system. It is possible that even though you have unused buildings, you will want to build new buildings instead of using your existing ones. And you have a limited amount of attention; it's possible that you will want to pay attention to something other than using your resources to make more resources. The thing is, these are very nearly the <em>only</em>&nbsp;flaws in this system.</div>\n</div>\n<div><br /></div>\n<div>What are the benefits of this system over a big list of tips? The system is all-inclusive: to macromanage successfully, you do not need to do anything&nbsp;<em>other</em>&nbsp;than following this system.&nbsp;The system is unambiguous and self-consistent. The big list says that you should spend your minerals on units, and it also says that you should spend your minerals on buildings; it doesn't tell you when to do which. The system <em>does</em>&nbsp;tell you when to do which. The system has three items, instead of eleven, so it's relatively easy to keep the entire system in mind at once. The system tells you exactly when to use each technique. The system leaves some questions open, but it's obvious <em>which</em> questions it leaves open&mdash;namely:</div>\n<div>\n<ul>\n<li>If you have unused buildings, which units should you recruit?</li>\n<li>If you have unused minerals, which buildings should you build?</li>\n</ul>\n<div>The system tells you when each question is relevant. And how can you answer these questions? Using more systems sounds like a good idea.</div>\n</div>\n<div><br /></div>\n<div>We do have a very useful <a href=\"/lw/fc3/checklist_of_rationality_habits/\">checklist of rationality habits</a>, which has six systems for dealing with things. Here's its system for \"reacting to evidence / surprises / arguments you haven't heard before; flagging beliefs for examination\":</div>\n<div>\n<ul>\n<li>If you see something odd, notice it.</li>\n<li>If someone says something unclear, notice this fact, and ask for examples.</li>\n<li>If your mind is arguing for a specific side, notice this fact, and fix it.</li>\n<li>If you're flinching away from a thought, explore that area more.</li>\n<li>If you come across bad news, consciously welcome it.</li>\n</ul>\n</div>\n<div>\n<div>Okay, that's pretty useful, but there are some things I don't like about it. \"Whenever such-and-such happens, notice it\" simply doesn't work as part of a system; you can't apply such a rule unless you've already noticed that the event has happened. (That's not to say that \"try to notice this\" is a bad thing to try to do; it's just that it isn't something you can do <em>in a system</em>.)</div>\n<div><br /></div>\n<div>More importantly, the list I gave simply <em>isn't</em> a \"system for reacting to evidence\". Suppose I read that a certain food is abundant in some nutrient. This new piece of evidence is not odd, it's not unclear, and it's not bad news. I do not flinch away from it, and reading it will not cause my mind to argue for a specific side. So what <em>should</em>&nbsp;I do with this piece of evidence? I don't know, because I have no system for dealing with it. Should I simply hope I remember it when I need to? Should I memorize it? Should I examine my diet to see if I should incorporate this food? The system is incomplete; it simply doesn't tell you what to do.</div>\n<div><br /></div>\n<div>Rationality is about obtaining useful knowledge. It's about knowing what questions are worth investigating, and how to investigate them. So a rationality <em>system</em>&nbsp;ought to tell you how to do both of these things. A system for investigating a question might look like this:</div>\n</div>\n<div>\n<ul>\n<li>If exactly one answer is obviously correct, then accept that answer.</li>\n<li>If no answers are obviously correct, come up with an intuitive guess, and then consider the ways your guess could be wrong.\n<ul>\n<li>If you cannot think of a guess, then examine the question analytically.</li>\n<li>If you can think of a guess, but it could be wrong, then ???.</li>\n<li>If you can think of a guess, and it cannot be wrong, then accept that guess.</li>\n</ul>\n</li>\n<li>If multiple contradictory answers appear to be correct, then resolve your confusion.</li>\n</ul>\n<div>This system seems complete (apart from that pesky ??? part), but it still raises some questions, namely:</div>\n</div>\n<div>\n<ul>\n<li>How do you come up with good intuitive guesses?</li>\n<li>How do you determine whether or not a guess might be wrong?</li>\n<li>How do you examine a question analytically?</li>\n<li>How do you resolve confusion?</li>\n</ul>\n<div>So, it sounds to me like we need a bunch more systems.</div>\n</div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "5Whwix4cZ3p5otshm": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rAte4CqhQdhEafG7Q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 51, "baseScore": 68, "extendedScore": null, "score": 0.00011842568784078195, "legacy": true, "legacyId": "20359", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 44, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ttGbpJQ8shBi8hDhh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-30T06:57:54.000Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Singletons Rule OK", "slug": "seq-rerun-singletons-rule-ok", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:04.628Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2f6eQF5B7JsKypBbr/seq-rerun-singletons-rule-ok", "pageUrlRelative": "/posts/2f6eQF5B7JsKypBbr/seq-rerun-singletons-rule-ok", "linkUrl": "https://www.lesswrong.com/posts/2f6eQF5B7JsKypBbr/seq-rerun-singletons-rule-ok", "postedAtFormatted": "Friday, November 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Singletons%20Rule%20OK&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Singletons%20Rule%20OK%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2f6eQF5B7JsKypBbr%2Fseq-rerun-singletons-rule-ok%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Singletons%20Rule%20OK%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2f6eQF5B7JsKypBbr%2Fseq-rerun-singletons-rule-ok", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2f6eQF5B7JsKypBbr%2Fseq-rerun-singletons-rule-ok", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 181, "htmlBody": "<p>Today's post, <a href=\"/lw/wc/singletons_rule_ok/\">Singletons Rule OK</a> was originally published on 30 November 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Singletons_Rule_OK\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>It is possible to create a singleton that won't do nasty things. This may be preferable to a scenario in which many agents start competing for resources without any way of securing themselves other than constant defense and deterrence.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fot/seq_rerun_total_tech_wars/\">Total Tech Wars</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2f6eQF5B7JsKypBbr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 3e-06, "legacy": true, "legacyId": "20367", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rSTpxugJxFPoRMkGW", "6oYwWN9gzfHAYiGPa", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-30T11:16:27.096Z", "modifiedAt": null, "url": null, "title": "New WBE implementation", "slug": "new-wbe-implementation", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:07.247Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Louie", "createdAt": "2010-05-10T21:41:14.619Z", "isAdmin": false, "displayName": "Louie"}, "userId": "JPwZspDjBcfwwuy7W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/83D92q8t3zYasnCcj/new-wbe-implementation", "pageUrlRelative": "/posts/83D92q8t3zYasnCcj/new-wbe-implementation", "linkUrl": "https://www.lesswrong.com/posts/83D92q8t3zYasnCcj/new-wbe-implementation", "postedAtFormatted": "Friday, November 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20WBE%20implementation&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20WBE%20implementation%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F83D92q8t3zYasnCcj%2Fnew-wbe-implementation%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20WBE%20implementation%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F83D92q8t3zYasnCcj%2Fnew-wbe-implementation", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F83D92q8t3zYasnCcj%2Fnew-wbe-implementation", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 76, "htmlBody": "<p><span style=\"color: #222222; font-family: arial;\">It usually isn't profitable to pay attention to science news, since science journalists largely misintrepret new \"breakthroughs\". But I am somewhat interested in this story about \"artificial brains\" coming out of Canada.</span></p>\n<div style=\"color: #222222; font-family: arial;\">Most large neuron simulations I've read about before&nbsp;<a href=\"http://www.theregister.co.uk/2009/11/23/epfl_bluebrain_markram_modha/\">don't actually do anything</a>. But apparently there's a somewhat large <a href=\"http://www.montrealgazette.com/technology/science/Canadian+scientists+create+functioning+virtual/7628972/story.html\">new WBE implementation</a>&nbsp;at the University of Waterloo that performs sub-humanly on several tasks while having <a href=\"http://www.technewsdaily.com/15714-artificial-brain-mimics-human.html\">similar weaknesses</a>&nbsp;to human brains.</div>\n<div style=\"color: #222222; font-family: arial;\"><br /></div>\n<div style=\"color: #222222; font-family: arial;\">Curious what others think of this recent development.</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb2ac": 2, "5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "83D92q8t3zYasnCcj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 27, "extendedScore": null, "score": 1.0463793071725102e-06, "legacy": true, "legacyId": "20371", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-30T16:34:04.249Z", "modifiedAt": null, "url": null, "title": "Meetup : Cambridge, MA first-Sunday meetup", "slug": "meetup-cambridge-ma-first-sunday-meetup-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/84Pnx6buBkMDTZyvu/meetup-cambridge-ma-first-sunday-meetup-0", "pageUrlRelative": "/posts/84Pnx6buBkMDTZyvu/meetup-cambridge-ma-first-sunday-meetup-0", "linkUrl": "https://www.lesswrong.com/posts/84Pnx6buBkMDTZyvu/meetup-cambridge-ma-first-sunday-meetup-0", "postedAtFormatted": "Friday, November 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Cambridge%2C%20MA%20first-Sunday%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Cambridge%2C%20MA%20first-Sunday%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F84Pnx6buBkMDTZyvu%2Fmeetup-cambridge-ma-first-sunday-meetup-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Cambridge%2C%20MA%20first-Sunday%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F84Pnx6buBkMDTZyvu%2Fmeetup-cambridge-ma-first-sunday-meetup-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F84Pnx6buBkMDTZyvu%2Fmeetup-cambridge-ma-first-sunday-meetup-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 57, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/gk'>Cambridge, MA first-Sunday meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">02 December 2012 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">25 Ames St Cambridge, MA 02139</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cambridge/Boston-area Less Wrong meetups on the first and third Sunday of every month, 2pm at the MIT Landau Building [25 Ames St, Bldg 66], room 148.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/gk'>Cambridge, MA first-Sunday meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "84Pnx6buBkMDTZyvu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.046560114157448e-06, "legacy": true, "legacyId": "20372", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Cambridge__MA_first_Sunday_meetup\">Discussion article for the meetup : <a href=\"/meetups/gk\">Cambridge, MA first-Sunday meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">02 December 2012 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">25 Ames St Cambridge, MA 02139</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Cambridge/Boston-area Less Wrong meetups on the first and third Sunday of every month, 2pm at the MIT Landau Building [25 Ames St, Bldg 66], room 148.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Cambridge__MA_first_Sunday_meetup1\">Discussion article for the meetup : <a href=\"/meetups/gk\">Cambridge, MA first-Sunday meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Cambridge, MA first-Sunday meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge__MA_first_Sunday_meetup", "level": 1}, {"title": "Discussion article for the meetup : Cambridge, MA first-Sunday meetup", "anchor": "Discussion_article_for_the_meetup___Cambridge__MA_first_Sunday_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-30T17:06:17.804Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Austin, Brussels, Montreal, Pittsburgh, St. Louis", "slug": "weekly-lw-meetups-austin-brussels-montreal-pittsburgh-st", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cjPu5FfwBBFaqwcDv/weekly-lw-meetups-austin-brussels-montreal-pittsburgh-st", "pageUrlRelative": "/posts/cjPu5FfwBBFaqwcDv/weekly-lw-meetups-austin-brussels-montreal-pittsburgh-st", "linkUrl": "https://www.lesswrong.com/posts/cjPu5FfwBBFaqwcDv/weekly-lw-meetups-austin-brussels-montreal-pittsburgh-st", "postedAtFormatted": "Friday, November 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Austin%2C%20Brussels%2C%20Montreal%2C%20Pittsburgh%2C%20St.%20Louis&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Austin%2C%20Brussels%2C%20Montreal%2C%20Pittsburgh%2C%20St.%20Louis%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcjPu5FfwBBFaqwcDv%2Fweekly-lw-meetups-austin-brussels-montreal-pittsburgh-st%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Austin%2C%20Brussels%2C%20Montreal%2C%20Pittsburgh%2C%20St.%20Louis%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcjPu5FfwBBFaqwcDv%2Fweekly-lw-meetups-austin-brussels-montreal-pittsburgh-st", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcjPu5FfwBBFaqwcDv%2Fweekly-lw-meetups-austin-brussels-montreal-pittsburgh-st", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 527, "htmlBody": "<p><strong>This summary was posted to LW main on Nov 23rd. The following week's summary post is <a href=\"/lw/fpx/weekly_lw_meetups_atlanta_austin_berlin/\">here</a>.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/g3\"> </a> <a href=\"/meetups/g3\">Brussels meetup:&nbsp;<span class=\"date\">24 November 2012 01:00PM</span></a></li>\n<li><a href=\"/meetups/fo\">(St. Louis MO) Psycology of memorization, Thiel Fellow James Koppel, and games:&nbsp;<span class=\"date\">24 November 2012 02:30PM</span></a></li>\n<li><a href=\"/meetups/g8\">LessWrong Montreal - Advanced Epistemology 101:&nbsp;<span class=\"date\">26 November 2012 06:30PM</span></a></li>\n<li><a href=\"/meetups/g4\">Pittsburgh: affecting the far future:&nbsp;<span class=\"date\">28 November 2012 06:00PM</span></a></li>\n<li><a href=\"/meetups/g5\">Berlin Meetup:&nbsp;<span class=\"date\">01 December 2012 07:30PM</span></a></li>\n<li><a href=\"/meetups/g9\">First meetup in Innsbruck :&nbsp;<span class=\"date\">02 December 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/g6\">Atlanta - Practical Rationality Meetup Session:&nbsp;<span class=\"date\">02 December 2012 05:00PM</span></a></li>\n<li><a href=\"/meetups/ga\">Meetup : Bielefeld Meetup, December 5th:&nbsp;<span class=\"date\">05 December 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/el\">Sofia, Bulgaria Meetup:&nbsp;<span class=\"date\">09 December 2012 05:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">24 November 2018 01:30PM</span></a></li>\n<li><a href=\"/meetups/g7\">Melbourne, practical rationality:&nbsp;<span class=\"date\">07 December 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/en\">Winter Solstice Megameetup - NYC:&nbsp;<span class=\"date\">15 December 2012 05:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong></strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cjPu5FfwBBFaqwcDv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.0465784620609028e-06, "legacy": true, "legacyId": "20241", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HAkZ6jRkpt4NrAKPS", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-30T17:16:21.069Z", "modifiedAt": null, "url": null, "title": "Meetup : Durham HPMoR Discussion, chapters 18-20", "slug": "meetup-durham-hpmor-discussion-chapters-18-20", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "evand", "createdAt": "2012-05-14T16:45:50.150Z", "isAdmin": false, "displayName": "evand"}, "userId": "QSBopDfW3DLzeMG7L", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/z6ZyetnoQtBxijt7E/meetup-durham-hpmor-discussion-chapters-18-20", "pageUrlRelative": "/posts/z6ZyetnoQtBxijt7E/meetup-durham-hpmor-discussion-chapters-18-20", "linkUrl": "https://www.lesswrong.com/posts/z6ZyetnoQtBxijt7E/meetup-durham-hpmor-discussion-chapters-18-20", "postedAtFormatted": "Friday, November 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Durham%20HPMoR%20Discussion%2C%20chapters%2018-20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Durham%20HPMoR%20Discussion%2C%20chapters%2018-20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz6ZyetnoQtBxijt7E%2Fmeetup-durham-hpmor-discussion-chapters-18-20%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Durham%20HPMoR%20Discussion%2C%20chapters%2018-20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz6ZyetnoQtBxijt7E%2Fmeetup-durham-hpmor-discussion-chapters-18-20", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz6ZyetnoQtBxijt7E%2Fmeetup-durham-hpmor-discussion-chapters-18-20", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 83, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/gl'>Durham HPMoR Discussion, chapters 18-20</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">01 December 2012 11:00:00AM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Guglhupf bakery, 2706 durham-chapel hill blvd. suite #1 durham, NC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be meeting to discuss Harry Potter and the Methods of Rationality, chapters 18-20 (~56 pages). Please come, even if you haven't done all (or any) of the reading!</p>\n\n<p>Nominal meeting time is 11-1; there is likely to be continued informal discussion after, possibly including Zendo.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/gl'>Durham HPMoR Discussion, chapters 18-20</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "z6ZyetnoQtBxijt7E", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0465841866707272e-06, "legacy": true, "legacyId": "20374", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_18_20\">Discussion article for the meetup : <a href=\"/meetups/gl\">Durham HPMoR Discussion, chapters 18-20</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">01 December 2012 11:00:00AM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Guglhupf bakery, 2706 durham-chapel hill blvd. suite #1 durham, NC</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We will be meeting to discuss Harry Potter and the Methods of Rationality, chapters 18-20 (~56 pages). Please come, even if you haven't done all (or any) of the reading!</p>\n\n<p>Nominal meeting time is 11-1; there is likely to be continued informal discussion after, possibly including Zendo.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_18_201\">Discussion article for the meetup : <a href=\"/meetups/gl\">Durham HPMoR Discussion, chapters 18-20</a></h2>", "sections": [{"title": "Discussion article for the meetup : Durham HPMoR Discussion, chapters 18-20", "anchor": "Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_18_20", "level": 1}, {"title": "Discussion article for the meetup : Durham HPMoR Discussion, chapters 18-20", "anchor": "Discussion_article_for_the_meetup___Durham_HPMoR_Discussion__chapters_18_201", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-11-30T23:50:53.116Z", "modifiedAt": null, "url": null, "title": "Meetup : First Purdue Meetup", "slug": "meetup-first-purdue-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:04.229Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Curiouskid", "createdAt": "2011-05-13T23:30:04.967Z", "isAdmin": false, "displayName": "Curiouskid"}, "userId": "Y4xxvuP743fwKcZQY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DL4qF8MqZPSMMKxYx/meetup-first-purdue-meetup", "pageUrlRelative": "/posts/DL4qF8MqZPSMMKxYx/meetup-first-purdue-meetup", "linkUrl": "https://www.lesswrong.com/posts/DL4qF8MqZPSMMKxYx/meetup-first-purdue-meetup", "postedAtFormatted": "Friday, November 30th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20First%20Purdue%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20First%20Purdue%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDL4qF8MqZPSMMKxYx%2Fmeetup-first-purdue-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20First%20Purdue%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDL4qF8MqZPSMMKxYx%2Fmeetup-first-purdue-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDL4qF8MqZPSMMKxYx%2Fmeetup-first-purdue-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 129, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/gm'>First Purdue Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 January 2013 06:50:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Purdue University - HICKS library</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Several times, I've tried to get a meetup started at Purdue. However, each time I post on LW that there might be a meetup if enough people comment and say that they want a meetup, only 3ish people will even comment. However, I know that there are at least 8 people at Purdue who read LessWrong, so I've decided to leave this meetup up on the site for a long time so that people on the site will see it.\n I'm not planning on actually having the meetup until after winter break.</p>\n\n<p>EDIT: First meetup went well. Second meetup page here (<a href=\"http://lesswrong.com/meetups/hx\" rel=\"nofollow\">http://lesswrong.com/meetups/hx</a>).</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/gm'>First Purdue Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DL4qF8MqZPSMMKxYx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.046808864133769e-06, "legacy": true, "legacyId": "20377", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___First_Purdue_Meetup\">Discussion article for the meetup : <a href=\"/meetups/gm\">First Purdue Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 January 2013 06:50:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Purdue University - HICKS library</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Several times, I've tried to get a meetup started at Purdue. However, each time I post on LW that there might be a meetup if enough people comment and say that they want a meetup, only 3ish people will even comment. However, I know that there are at least 8 people at Purdue who read LessWrong, so I've decided to leave this meetup up on the site for a long time so that people on the site will see it.\n I'm not planning on actually having the meetup until after winter break.</p>\n\n<p>EDIT: First meetup went well. Second meetup page here (<a href=\"http://lesswrong.com/meetups/hx\" rel=\"nofollow\">http://lesswrong.com/meetups/hx</a>).</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___First_Purdue_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/gm\">First Purdue Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : First Purdue Meetup", "anchor": "Discussion_article_for_the_meetup___First_Purdue_Meetup", "level": 1}, {"title": "Discussion article for the meetup : First Purdue Meetup", "anchor": "Discussion_article_for_the_meetup___First_Purdue_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "23 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-01T00:12:43.418Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Stuck In Throat", "slug": "seq-rerun-stuck-in-throat", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jTGNSZ2ALGA2i6EcG/seq-rerun-stuck-in-throat", "pageUrlRelative": "/posts/jTGNSZ2ALGA2i6EcG/seq-rerun-stuck-in-throat", "linkUrl": "https://www.lesswrong.com/posts/jTGNSZ2ALGA2i6EcG/seq-rerun-stuck-in-throat", "postedAtFormatted": "Saturday, December 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Stuck%20In%20Throat&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Stuck%20In%20Throat%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjTGNSZ2ALGA2i6EcG%2Fseq-rerun-stuck-in-throat%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Stuck%20In%20Throat%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjTGNSZ2ALGA2i6EcG%2Fseq-rerun-stuck-in-throat", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjTGNSZ2ALGA2i6EcG%2Fseq-rerun-stuck-in-throat", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 154, "htmlBody": "<p>Today's post, <a href=\"http://www.overcomingbias.com/2008/11/stuck-in-throat.html\">Stuck In Throat</a> was originally published on November 30, 2008.  A summary:</p>\n<p>&nbsp;</p>\n<blockquote>A new attempt to summarize Yudkowsky's position, and what parts of that position Hanson thinks are unsupported.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fpr/seq_rerun_singletons_rule_ok/\">Singletons Rule OK</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jTGNSZ2ALGA2i6EcG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.046821303021344e-06, "legacy": true, "legacyId": "20378", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2f6eQF5B7JsKypBbr", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-01T05:00:57.988Z", "modifiedAt": null, "url": null, "title": "Open Thread, December 1-15, 2012", "slug": "open-thread-december-1-15-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:01.520Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OpenThreadGuy", "createdAt": "2012-01-16T00:21:00.929Z", "isAdmin": false, "displayName": "OpenThreadGuy"}, "userId": "qe9iZjEvuKegW4Twy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GQmXRmcxKMS3WF3hP/open-thread-december-1-15-2012", "pageUrlRelative": "/posts/GQmXRmcxKMS3WF3hP/open-thread-december-1-15-2012", "linkUrl": "https://www.lesswrong.com/posts/GQmXRmcxKMS3WF3hP/open-thread-december-1-15-2012", "postedAtFormatted": "Saturday, December 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Thread%2C%20December%201-15%2C%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Thread%2C%20December%201-15%2C%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGQmXRmcxKMS3WF3hP%2Fopen-thread-december-1-15-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Thread%2C%20December%201-15%2C%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGQmXRmcxKMS3WF3hP%2Fopen-thread-december-1-15-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGQmXRmcxKMS3WF3hP%2Fopen-thread-december-1-15-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 16, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">If it's worth saying, but not worth its own post, even in Discussion, it goes here.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GQmXRmcxKMS3WF3hP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 1.0469855076305382e-06, "legacy": true, "legacyId": "20379", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 179, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-01T18:25:01.174Z", "modifiedAt": null, "url": null, "title": "[link] Interview with Anders Sandberg on how to make a difference through research and how to choose a research topic", "slug": "link-interview-with-anders-sandberg-on-how-to-make-a", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Pablo_Stafforini", "createdAt": "2009-03-24T12:56:00.130Z", "isAdmin": false, "displayName": "Pablo"}, "userId": "W7ETRtvRMqYetyQE9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TgQzR7QSuofvzB7fe/link-interview-with-anders-sandberg-on-how-to-make-a", "pageUrlRelative": "/posts/TgQzR7QSuofvzB7fe/link-interview-with-anders-sandberg-on-how-to-make-a", "linkUrl": "https://www.lesswrong.com/posts/TgQzR7QSuofvzB7fe/link-interview-with-anders-sandberg-on-how-to-make-a", "postedAtFormatted": "Saturday, December 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20Interview%20with%20Anders%20Sandberg%20on%20how%20to%20make%20a%20difference%20through%20research%20and%20how%20to%20choose%20a%20research%20topic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20Interview%20with%20Anders%20Sandberg%20on%20how%20to%20make%20a%20difference%20through%20research%20and%20how%20to%20choose%20a%20research%20topic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTgQzR7QSuofvzB7fe%2Flink-interview-with-anders-sandberg-on-how-to-make-a%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20Interview%20with%20Anders%20Sandberg%20on%20how%20to%20make%20a%20difference%20through%20research%20and%20how%20to%20choose%20a%20research%20topic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTgQzR7QSuofvzB7fe%2Flink-interview-with-anders-sandberg-on-how-to-make-a", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTgQzR7QSuofvzB7fe%2Flink-interview-with-anders-sandberg-on-how-to-make-a", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 500, "htmlBody": "<p><a href=\"http://80000hours.org/blog/125-how-to-choose-a-research-topic-an-interview-with-anders-sandberg\">Here</a>. Some excerpts:</p>\n<blockquote>\n<p><strong>What do you think are some good heuristics for doing high impact research?</strong></p>\n<p>One idea is to go for under-researched fields. Progress in a field is typically a very convex learning curve: rapid progress at first when the low-hanging fruits get picked by the pioneers, followed by slowing progress as the problems get harder and it takes longer to learn the necessary skills to get to them. So the same amount of effort might produce far more progress in a little studied field than in a big one. [...]</p>\n<p>It can also help to turn the question around: what aspects of human life matter? Looking at human life, we sleep about a third of the time, and there&rsquo;s very little research into how to enhance sleep. Understanding the health effects of what we eat is probably worth billions of pounds per year. But there are no financial incentives here. Maybe a simple approach for finding high impact research areas might be to look at the most common google searches: you can get a pretty good idea of what human behaviour matters a lot!</p>\n</blockquote>\n<blockquote>\n<p><strong>Do you think it&rsquo;s better to be a generalist and get a broad understanding of a lot of things, or to specialise early and really focus on a single area you think is high impact?</strong></p>\n<p>Over the history of my academic career my most useful courses have been linear algebra, all the statistics and probability theory I&rsquo;ve been able to pick up, some basic computer science, and a course on natural disasters. [...]</p>\n<p>Even if you do focus on one field, knowing enough about other fields is good as you can recognise when you need the help of someone from another department.</p>\n</blockquote>\n<blockquote>\n<p><strong>What other barriers are there to doing important research?</strong></p>\n<p>Looking at some of these under-researched fields, the problem is that a lot of them don&rsquo;t even exist as fields. Typically you&rsquo;re unlikely to get funding in unknown fields as well: unless you&rsquo;re a really good salesman! So one heuristic would be to look at the topics you know, do a matrix and look at the interactions: which areas do you see that have nobody doing anything in?</p>\n<p>When I went to a computational neuroscience conference last year, I was slightly depressed as I saw a poster which was exactly the same research topic as my last poster! It was pretty clear the young grad student had reached the same conclusion I did, and had never heard of my research which was published 6 years ago! Many fields have this problem that they don&rsquo;t have very much of a memory, which affects progress.</p>\n<p>Fields like AI are struggling because there&rsquo;s no good way of comparing progress. How much smarter are current general AI programs than some of the classics? Nobody knows, and you can&rsquo;t test the older programs because the source code and everything has been lost except a few bizarre papers from the early 70s.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TgQzR7QSuofvzB7fe", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 14, "extendedScore": null, "score": 1.047443797023841e-06, "legacy": true, "legacyId": "20391", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><a href=\"http://80000hours.org/blog/125-how-to-choose-a-research-topic-an-interview-with-anders-sandberg\">Here</a>. Some excerpts:</p>\n<blockquote>\n<p><strong id=\"What_do_you_think_are_some_good_heuristics_for_doing_high_impact_research_\">What do you think are some good heuristics for doing high impact research?</strong></p>\n<p>One idea is to go for under-researched fields. Progress in a field is typically a very convex learning curve: rapid progress at first when the low-hanging fruits get picked by the pioneers, followed by slowing progress as the problems get harder and it takes longer to learn the necessary skills to get to them. So the same amount of effort might produce far more progress in a little studied field than in a big one. [...]</p>\n<p>It can also help to turn the question around: what aspects of human life matter? Looking at human life, we sleep about a third of the time, and there\u2019s very little research into how to enhance sleep. Understanding the health effects of what we eat is probably worth billions of pounds per year. But there are no financial incentives here. Maybe a simple approach for finding high impact research areas might be to look at the most common google searches: you can get a pretty good idea of what human behaviour matters a lot!</p>\n</blockquote>\n<blockquote>\n<p><strong id=\"Do_you_think_it_s_better_to_be_a_generalist_and_get_a_broad_understanding_of_a_lot_of_things__or_to_specialise_early_and_really_focus_on_a_single_area_you_think_is_high_impact_\">Do you think it\u2019s better to be a generalist and get a broad understanding of a lot of things, or to specialise early and really focus on a single area you think is high impact?</strong></p>\n<p>Over the history of my academic career my most useful courses have been linear algebra, all the statistics and probability theory I\u2019ve been able to pick up, some basic computer science, and a course on natural disasters. [...]</p>\n<p>Even if you do focus on one field, knowing enough about other fields is good as you can recognise when you need the help of someone from another department.</p>\n</blockquote>\n<blockquote>\n<p><strong id=\"What_other_barriers_are_there_to_doing_important_research_\">What other barriers are there to doing important research?</strong></p>\n<p>Looking at some of these under-researched fields, the problem is that a lot of them don\u2019t even exist as fields. Typically you\u2019re unlikely to get funding in unknown fields as well: unless you\u2019re a really good salesman! So one heuristic would be to look at the topics you know, do a matrix and look at the interactions: which areas do you see that have nobody doing anything in?</p>\n<p>When I went to a computational neuroscience conference last year, I was slightly depressed as I saw a poster which was exactly the same research topic as my last poster! It was pretty clear the young grad student had reached the same conclusion I did, and had never heard of my research which was published 6 years ago! Many fields have this problem that they don\u2019t have very much of a memory, which affects progress.</p>\n<p>Fields like AI are struggling because there\u2019s no good way of comparing progress. How much smarter are current general AI programs than some of the classics? Nobody knows, and you can\u2019t test the older programs because the source code and everything has been lost except a few bizarre papers from the early 70s.</p>\n</blockquote>", "sections": [{"title": "What do you think are some good heuristics for doing high impact research?", "anchor": "What_do_you_think_are_some_good_heuristics_for_doing_high_impact_research_", "level": 1}, {"title": "Do you think it\u2019s better to be a generalist and get a broad understanding of a lot of things, or to specialise early and really focus on a single area you think is high impact?", "anchor": "Do_you_think_it_s_better_to_be_a_generalist_and_get_a_broad_understanding_of_a_lot_of_things__or_to_specialise_early_and_really_focus_on_a_single_area_you_think_is_high_impact_", "level": 1}, {"title": "What other barriers are there to doing important research?", "anchor": "What_other_barriers_are_there_to_doing_important_research_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-01T18:53:13.207Z", "modifiedAt": null, "url": null, "title": "December 2012 Media Thread", "slug": "december-2012-media-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:00.094Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobertLumley", "createdAt": "2011-04-28T23:53:16.950Z", "isAdmin": false, "displayName": "RobertLumley"}, "userId": "KXJjaWHDF4HJ2DF7a", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/g2SNJ6ptY9ChBebNh/december-2012-media-thread", "pageUrlRelative": "/posts/g2SNJ6ptY9ChBebNh/december-2012-media-thread", "linkUrl": "https://www.lesswrong.com/posts/g2SNJ6ptY9ChBebNh/december-2012-media-thread", "postedAtFormatted": "Saturday, December 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20December%202012%20Media%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADecember%202012%20Media%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg2SNJ6ptY9ChBebNh%2Fdecember-2012-media-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=December%202012%20Media%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg2SNJ6ptY9ChBebNh%2Fdecember-2012-media-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fg2SNJ6ptY9ChBebNh%2Fdecember-2012-media-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 217, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">This is the monthly thread for posting media of various types that you've found that you enjoy. I find that exposure to LW ideas makes me less likely to enjoy some entertainment media that is otherwise quite popular, and finding media recommended by LWers is a good way to mitigate this. Post what you're reading, listening to, watching, and your opinion of it. Post recommendations to blogs. Post whatever media you feel like discussing! To see previous recommendations, check out the&nbsp;</span><a style=\"color: #8a8a8b;\" href=\"/r/discussion/tag/media_thread/\">older threads</a><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">.</span></p>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">Rules:</p>\n<ul style=\"padding: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">\n<li>Please avoid downvoting recommendations just because you don't personally like the recommended material; remember that liking is a&nbsp;<a style=\"color: #8a8a8b;\" href=\"/lw/ro/2place_and_1place_words/\">two-place word</a>. If you can point out a specific flaw in a person's recommendation, consider posting a comment to that effect.</li>\n<li>If you want to post something that (you know) has been recommended before, but have another recommendation to add, please link to the original, so that the reader has both recommendations.</li>\n<li>Please use the comment trees for genres. There is a meta thread for comments about future threads.</li>\n<li>If you think there should be a thread for a particular genre of media, please post it to the Other Media thread for now, and add a poll to the Meta thread asking if it should be a thread every month.</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "g2SNJ6ptY9ChBebNh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 1.0474598770448461e-06, "legacy": true, "legacyId": "20390", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 46, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["eDpPnT7wdBwWPGvo5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-01T19:43:32.916Z", "modifiedAt": "2022-01-06T07:11:05.804Z", "url": null, "title": "Looking for a likely cause of a mental phenomenon", "slug": "looking-for-a-likely-cause-of-a-mental-phenomenon", "viewCount": null, "lastCommentedAt": "2019-04-05T00:29:03.837Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Valentine", "createdAt": "2012-05-26T23:02:47.502Z", "isAdmin": false, "displayName": "Valentine"}, "userId": "d3372gCBr7DkDvdWC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/h2QdnJBbSwEE36jc9/looking-for-a-likely-cause-of-a-mental-phenomenon", "pageUrlRelative": "/posts/h2QdnJBbSwEE36jc9/looking-for-a-likely-cause-of-a-mental-phenomenon", "linkUrl": "https://www.lesswrong.com/posts/h2QdnJBbSwEE36jc9/looking-for-a-likely-cause-of-a-mental-phenomenon", "postedAtFormatted": "Saturday, December 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Looking%20for%20a%20likely%20cause%20of%20a%20mental%20phenomenon&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALooking%20for%20a%20likely%20cause%20of%20a%20mental%20phenomenon%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh2QdnJBbSwEE36jc9%2Flooking-for-a-likely-cause-of-a-mental-phenomenon%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Looking%20for%20a%20likely%20cause%20of%20a%20mental%20phenomenon%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh2QdnJBbSwEE36jc9%2Flooking-for-a-likely-cause-of-a-mental-phenomenon", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fh2QdnJBbSwEE36jc9%2Flooking-for-a-likely-cause-of-a-mental-phenomenon", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1288, "htmlBody": "<p>I'm currently testing a promising direction for a possible collection of units at <a href=\"http://appliedrationality.org/\" target=\"_blank\">CFAR</a>. (For those who have attended some CFAR events or test sessions, this is a collection of refinements to the fudoshin/\"panic\" unit.) I've hit on what I think is a key puzzle whose answer might unlock a lot of the <a href=\"/lw/2c/a_sense_that_more_is_possible/\" target=\"_blank\">emerging</a> art of rationality. I - and possibly most people here, eventually - would very much appreciate any insight you have to share.<br /><br />The puzzle is how thought incubation works, ideally expressed in terms of neural systems or neuroanatomical structures. I'll first explain the phenomenon and then suggest the general reference class from which I'm hoping to get an answer.<br /><br /><br /><strong>The Phenomenon:</strong><br />Mathematicians frequently report that often one of the most helpful things they can do to solve a problem they're stuck on is step away from it. Jacques Hadamard (<a href=\"http://www.amazon.com/Essay-Psychology-Invention-Mathematical-Field/dp/1443730394/ref=sr_1_1?ie=UTF8&amp;qid=1354387261&amp;sr=8-1&amp;keywords=invention+in+the+mathematical+field\" target=\"_blank\">1949</a>) examined his own experiences and also talked to many of his colleagues to work out what the common structure of this experience was, and determined that there seems to be a fairly predictable sequence to it:<br /><br />(1) Intensely focus on the problem, working through every permutation you can think of that's likely to produce an answer.<br />(2) Walk away from the problem and think about something else.<br />(3) The magic genie in your head might eventually, and often unexpectedly, yell a possible insight into your awareness.<br /><br />For instance, Henri Poincar&eacute; reported struggling to work on Fuchsian functions over the course of several weeks and then being forced to walk away from the proof he had been stuck on due to a planned vacation. One day he was stepping onto a bus with his mind certainly not on mathematics, and suddenly the key insight he needed to finish the proof appeared in his mind. It was as though a part of his mind had been secretly working on the problem and then brought the finished product into his awareness. In this particular case it also came with a feeling of total confidence that verification would pan out (although Hadamard notes that the validation step after the insight is still essential because sometimes that feeling of total confidence is mistaken).<br /><br />I definitely relate to this from when I was working on graduate mathematics. However, it also pattern-matches with other mental phenomena that are much more common. For instance, sometimes I think I know what a person's name is, but struggle as I might I can't quite remember it - and then a few minutes later after I've given up remembering the name the answer loudly announces itself, often quite out-of-context. Or when I'm trying to figure out a way of improving a throw in martial arts and then find the answer suddenly dawning on me at a random time.<br /><br />I'm under the impression that this is a fairly universal kind of experience. I suspect you can think of examples in your own life where this has happened. (\"Oh, <em>now</em>&nbsp;I remember where I put those keys!\")<br /><br /><br /><strong>Reference Class for an Explanation:</strong><br />I'm going to offer some overly simplistic examples of the kind of explanation I'm looking for. In this case, I think overly simplistic might be okay since I'm just trying to get a reasonable handle on how to munchkin the interaction between a few different neural systems. If it turns out that a more detailed and technically correct version is important, I'll probably dig into it (pending the VOI versus cost-of-information comparison).<br /><br />There seems to be some evidence that one of the reasons children are as impulsive as they are is that they haven't yet developed their prefrontal cortices (PFCs) to the degree adults have. The prefrontal cortex seems to do at least two things: (1) hold long-term goals in mind and (2) engage executive function (i.e., halt orders on impulses, typically ones that don't match up with the long-term goals). This neuroanatomical structure seems to continue growing until sometime in one's early 20s - which might be why we also find that teenagers typically have less impulse control than twentysomethings but more than middle-schoolers, whereas we don't find such a clear distinction between twentysomethings and thirtysomethings. (Yes, this could also or even instead be cultural. I know it's complicated.) Incidentally, I understand that the PFC is also one of the neural structures most deactivated by alcohol - although my impression is that it shuts down the long-term goals thing and <em>not</em>&nbsp;the executive function. (This is based on my and others' experience that precommitment works perfectly well. It seems to me that saying things like \"I couldn't help myself because I was drunk!\" is more a social excuse than an actual explanation. But I'm only around 65% confident of this as a general claim.)<br /><br />On a related note, it would seem that there's something in the same rough space as <a href=\"http://en.wikipedia.org/wiki/Theory_of_mind\" target=\"_blank\">theory of mind</a>&nbsp;that goes beyond the ability to pass <a href=\"http://en.wikipedia.org/wiki/Theory_of_mind#False-belief_task\" target=\"_blank\">the false-belief test</a>. According to <a href=\"http://www.ted.com/talks/rebecca_saxe_how_brains_make_moral_judgments.html\" target=\"_blank\">Rebecca Saxe</a>, the capacity for empathy seems to come from a particular bit of the brain that doesn't finish growing until the mid-20s. Saxe also provides some evidence that a sufficiently strong and precisely directed magnet can basically deactivate that part of one's theory of mind temporarily. It seems quite plausible to me (though I really don't know) that activation of the <a href=\"http://en.wikipedia.org/wiki/Sympathetic_nervous_system\" target=\"_blank\">sympathetic nervous system</a>&nbsp;(SNS), such as in fight-or-flight reactions, decreases activation of this empathy part of the brain. This might be why, in a perceived crisis, some people switch to an almost tool-like view of others (e.g., knowing that overcoming the bystander effect requires pointing at a specific person and saying \"You! Call 911!\" but not really getting a sense in that moment of what that person's experience is like to be so singled out).<br /><br />I'm quite aware that much of the above is speculation. I think speculation is fine, but having it grounded in some actual known neuroscience is ideal. That would give me something to dig into. E.g., if there's some reason to believe that this phenomenon is related to the <a href=\"http://en.wikipedia.org/wiki/Enteric_nervous_system\" target=\"_blank\">enteric nervous system</a>, I can start digging into the literature on that system to better understand how to munchkin its interactions with the (rest of the) autonomic nervous system.<br /><br />An example of something <em>outside</em>&nbsp;the reference class I'm looking for is a \"little man in the subconscious\" explanation. I first read about this about twenty years ago as a model for how mental incubation works: you concentrate on a problem in order to communicate to a little man in your subconscious what you want to have done, and then you stop talking to him so he can go do what you just told him to do. Then he comes back with an answer once he's done, without regard to what you're doing when he's done. I agree that this seems to be a reasonable metaphor for what's going on, but it doesn't tell me for instance why the \"little man\" seems to respond so much more to SNS activity than <a href=\"http://en.wikipedia.org/wiki/Parasympathetic_nervous_system\" target=\"_blank\">parasympathetic</a> activity, or why he can't go do his job once he has the instructions even if we continue to think about the problem.<br /><br />More generally, psychodynamic \"explanations\" are unlikely to be helpful here. Talking about this as the \"domain of the iNtuiting function\" in reference to Jungian psychodynamic theory or Myers-Briggs won't tell me hardly anything about how this relates to stress oscillation.<br /><br /><br /><br />So... Any suggestions about what this mysterious \"little man\" might actually be made of?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "h2QdnJBbSwEE36jc9", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 15, "extendedScore": null, "score": 3.5e-05, "legacy": true, "legacyId": "20392", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Nu3wa6npK4Ry66vFp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 9, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2012-12-01T19:43:32.916Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-01T20:47:12.723Z", "modifiedAt": null, "url": null, "title": "Working out consequences of science-fictional ideas", "slug": "working-out-consequences-of-science-fictional-ideas", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:05.618Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DataPacRat", "createdAt": "2009-05-21T11:00:18.044Z", "isAdmin": false, "displayName": "DataPacRat"}, "userId": "ca4pgqJFEDkdbAzyo", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/q35Y4bJRDycgzcXdq/working-out-consequences-of-science-fictional-ideas", "pageUrlRelative": "/posts/q35Y4bJRDycgzcXdq/working-out-consequences-of-science-fictional-ideas", "linkUrl": "https://www.lesswrong.com/posts/q35Y4bJRDycgzcXdq/working-out-consequences-of-science-fictional-ideas", "postedAtFormatted": "Saturday, December 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Working%20out%20consequences%20of%20science-fictional%20ideas&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWorking%20out%20consequences%20of%20science-fictional%20ideas%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq35Y4bJRDycgzcXdq%2Fworking-out-consequences-of-science-fictional-ideas%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Working%20out%20consequences%20of%20science-fictional%20ideas%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq35Y4bJRDycgzcXdq%2Fworking-out-consequences-of-science-fictional-ideas", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fq35Y4bJRDycgzcXdq%2Fworking-out-consequences-of-science-fictional-ideas", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 218, "htmlBody": "<p>I have a pet sci-fi setting I use as a handy platform to play with various ideas, such as figuring out what some of the likely consequences would be of relatively easy access to orbit, or applying 'Avatar' remote-control techniques to live people. One question that recently occurred to me is, \"Can you have a crossover with another fictional setting, without breaking any known laws of physics?\"</p>\n<p>The setting includes super-Siris, conversational engines good enough to fool an average, anthropomorphising person (but not true GAIs, since the consequences of /those/ would derail every other hypothetical in the setting). Throw in some full-sensory VR, and it seems plausible for someone to not only exchange email with a conversation-bot Captain Kirk, but get shown to the captain's quarters... And, depending on what the setting is, there are various rationales to explain how the VR-naut happens to be visiting that 'universe'. With realistic robots, even 'visits' the other way could be arranged: \"Have a real Spider-Man at your birthday party!\"</p>\n<p>The Rule 34 potential is obvious; but I'm trying to come up with what /other/ consequences would result from such tech. How would marriages change? The labor force? Military simulations and operations?</p>\n<p>How would /you/ go about thinking up interesting and/or useful extrapolations, for this idea in particular or for such ideas in general?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "q35Y4bJRDycgzcXdq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -9, "extendedScore": null, "score": 1.0475248799595251e-06, "legacy": true, "legacyId": "20393", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-02T04:00:24.080Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Disappointment in the Future", "slug": "seq-rerun-disappointment-in-the-future", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:06.245Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hvpGmqYxzKktGuYCk/seq-rerun-disappointment-in-the-future", "pageUrlRelative": "/posts/hvpGmqYxzKktGuYCk/seq-rerun-disappointment-in-the-future", "linkUrl": "https://www.lesswrong.com/posts/hvpGmqYxzKktGuYCk/seq-rerun-disappointment-in-the-future", "postedAtFormatted": "Sunday, December 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Disappointment%20in%20the%20Future&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Disappointment%20in%20the%20Future%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhvpGmqYxzKktGuYCk%2Fseq-rerun-disappointment-in-the-future%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Disappointment%20in%20the%20Future%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhvpGmqYxzKktGuYCk%2Fseq-rerun-disappointment-in-the-future", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhvpGmqYxzKktGuYCk%2Fseq-rerun-disappointment-in-the-future", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 153, "htmlBody": "<p>Today's post, <a href=\"/lw/wd/disappointment_in_the_future/\">Disappointment in the Future</a> was originally published on 01 December 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Disappointment_in_the_Future\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A list of Ray Kurzweil's predictions for the period 1999-2009.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fq2/seq_rerun_stuck_in_throat/\">Stuck In Throat</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hvpGmqYxzKktGuYCk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0477719676822258e-06, "legacy": true, "legacyId": "20396", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yyiyz34p6QxWBmZ9k", "jTGNSZ2ALGA2i6EcG", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-02T12:50:59.751Z", "modifiedAt": null, "url": null, "title": "[Link] The Worst-Run Big City in the U.S.", "slug": "link-the-worst-run-big-city-in-the-u-s", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:09.313Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NgHbGqgaPvnCsbr6i/link-the-worst-run-big-city-in-the-u-s", "pageUrlRelative": "/posts/NgHbGqgaPvnCsbr6i/link-the-worst-run-big-city-in-the-u-s", "linkUrl": "https://www.lesswrong.com/posts/NgHbGqgaPvnCsbr6i/link-the-worst-run-big-city-in-the-u-s", "postedAtFormatted": "Sunday, December 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20The%20Worst-Run%20Big%20City%20in%20the%20U.S.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20The%20Worst-Run%20Big%20City%20in%20the%20U.S.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNgHbGqgaPvnCsbr6i%2Flink-the-worst-run-big-city-in-the-u-s%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20The%20Worst-Run%20Big%20City%20in%20the%20U.S.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNgHbGqgaPvnCsbr6i%2Flink-the-worst-run-big-city-in-the-u-s", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNgHbGqgaPvnCsbr6i%2Flink-the-worst-run-big-city-in-the-u-s", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 608, "htmlBody": "<p><a rel=\"nofollow\" href=\"http://www.sfweekly.com/2009-12-16/news/the-worst-run-big-city-in-the-u-s/\"><strong>The Worst-Run Big City in the U.S.</strong></a></p>\n<p>A six page article that reads as a very interesting autopsy of what institutional dysfunction in the intersection of government and non-profits looks like. I recommend reading the whole thing.</p>\n<blockquote>\n<p>Minus the alleged harassment, city government is filled with Yomi Agunbiades &mdash; and they're hardly ever disciplined, let alone fired. When asked, former Board of Supervisors President Aaron Peskin couldn't remember the last time a higher-up in city government was removed for incompetence. \"There must have been somebody,\" he said at last, vainly searching for a name.</p>\n<p>Accordingly, millions of taxpayer dollars are wasted on good ideas that fail for stupid reasons, and stupid ideas that fail for good reasons, and hardly anyone is taken to task.</p>\n<p><strong>The intrusion of politics into government pushes the city to enter long-term labor contracts it obviously can't afford, and no one is held accountable. A belief that good intentions matter more than results leads to inordinate amounts of government responsibility being shunted to nonprofits whose only documented achievement is to lobby the city for money.</strong> Meanwhile, piles of reports on how to remedy these problems go unread. There's no outrage, and nobody is disciplined, so things don't get fixed.</p>\n</blockquote>\n<p>You don't say?</p>\n<blockquote>\n<p>In 2007, the Department of Children, Youth, and Families (DCYF) held a seminar for the nonprofits vying for a piece of $78 million in funding. Grant seekers were told that in the next funding cycle, they would be required &mdash; for the first time &mdash; to provide quantifiable proof their programs were accomplishing something.</p>\n<p><strong>The room exploded with outrage. This wasn't fair. \"What if we can bring in a family we've helped?\" one nonprofit asked. Another offered: \"We can tell you stories about the good work we do!\" Not every organization is capable of demonstrating results, a nonprofit CEO complained. He suggested the city's funding process should actually penalize nonprofits able to measure results, so as to put everyone on an even footing. Heads nodded: This was a popular idea.</strong></p>\n</blockquote>\n<p>Reading this I had to bite my hand in frustration.</p>\n<blockquote>\n<p>There are two lessons here. First, many San Francisco nonprofits believe they're entitled to money without having to prove that their programs work. Second, until 2007, the city agreed. Actually, most of the city still agrees. DCYF is the only city department that even attempts to track results. It's the model other departments are told to aspire to.</p>\n<p>But Maria Su, DCYF's director, admitted that accountability is something her department still struggles with. It can track \"output\" &mdash; what a nonprofit does, how often, and with how many people &mdash; but it can't track \"outcomes.\" It can't demonstrate that these outputs &mdash; the very things it pays nonprofits to do &mdash; are actually helping anyone.</p>\n<p>\"Believe me, there is still hostility to the idea that outcomes should be tracked,\" Su says. \"I think we absolutely need to be able to provide that level of information. But it's still a work in progress.\" In the meantime, the city is spending about $500 million a year on programs that might or might not work.</p>\n</blockquote>\n<p>What the <a href=\"/lw/37f/efficient_charity/\">efficient charity</a> movement has done so far looks much more impressive in light of this. Reading the rest of the article I think you can on your own identify the problems caused by <a href=\"/lw/le/lost_purposes/\">lost purposes</a>, <a href=\"http://wiki.lesswrong.com/wiki/Applause_light\">applause lights</a> and a dozen or so other faults we've explored here for years.</p>\n<p>Discussions here are in many respects a comforting illusion, <em>this</em> is <a href=\"/lw/1ax/im_not_saying_people_are_stupid/\">what humanity is like</a> out there in the real world, almost at its best, well educated, wealthy and interested in the public good.</p>\n<p>Yes it really is that bad.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zz3HWyByyKF64Sfns": 1, "FkzScn5byCs9PxGsA": 1, "Lgy35Xh222bwgeGTL": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NgHbGqgaPvnCsbr6i", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 43, "extendedScore": null, "score": 0.000111, "legacy": true, "legacyId": "20397", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FCxHgPsDScx4C3H8n", "sP2Hg6uPwpfp3jZJN", "cs2nvx7ajkGr5kudk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-02T14:43:34.595Z", "modifiedAt": null, "url": null, "title": "[Link] Ideology, Motivated Reasoning, and Cognitive Reflection: An Experimental Study", "slug": "link-ideology-motivated-reasoning-and-cognitive-reflection", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:07.520Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/65seESKyiC44sMRsQ/link-ideology-motivated-reasoning-and-cognitive-reflection", "pageUrlRelative": "/posts/65seESKyiC44sMRsQ/link-ideology-motivated-reasoning-and-cognitive-reflection", "linkUrl": "https://www.lesswrong.com/posts/65seESKyiC44sMRsQ/link-ideology-motivated-reasoning-and-cognitive-reflection", "postedAtFormatted": "Sunday, December 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Ideology%2C%20Motivated%20Reasoning%2C%20and%20Cognitive%20Reflection%3A%20An%20Experimental%20Study&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Ideology%2C%20Motivated%20Reasoning%2C%20and%20Cognitive%20Reflection%3A%20An%20Experimental%20Study%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F65seESKyiC44sMRsQ%2Flink-ideology-motivated-reasoning-and-cognitive-reflection%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Ideology%2C%20Motivated%20Reasoning%2C%20and%20Cognitive%20Reflection%3A%20An%20Experimental%20Study%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F65seESKyiC44sMRsQ%2Flink-ideology-motivated-reasoning-and-cognitive-reflection", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F65seESKyiC44sMRsQ%2Flink-ideology-motivated-reasoning-and-cognitive-reflection", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 242, "htmlBody": "<p><strong>Related to:</strong> <a href=\"/lw/he/knowing_about_biases_can_hurt_people/\">Knowing About Biases Can Hurt People </a></p>\n<p>HT: <a href=\"http://marginalrevolution.com/marginalrevolution/2012/12/ideology-motivated-reasoning-and-cognitive-reflection-an-experimental-study.html\">Marginal Revolution</a><br /><a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2182588\"></a></p>\n<p><a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2182588\">Paper.</a></p>\n<blockquote>\n<p>Social psychologists have identified various plausible sources of ideological polarization over climate change, gun violence, national security, and like societal risks. This paper reports a study of three of them: the predominance of heuristic-driven information processing by members of the public; ideologically motivated cognition; and personality-trait correlates of political conservativism. The results of the study suggest reason to doubt two common surmises about how these dynamics interact. First, the study presents both observational and experimental data inconsistent with the hypothesis that political conservatism is distinctively associated with closed-mindedness: conservatives did no better or worse than liberals on an objective measure of cognitive reflection; and more importantly, both demonstrated the same unconscious tendency to fit assessments of empirical evidence to their ideological predispositions. Second, the study suggests that this form of bias is not a consequence of overreliance on heuristic or intuitive forms of reasoning; on the contrary, subjects who scored highest in cognitive reflection were the&nbsp;<em>most</em>&nbsp;likely to display ideologically motivated cognition. These findings corroborated the hypotheses of a third theory, which identifies motivated cognition as a form of information processing that rationally promotes individuals&rsquo; interests in forming and maintaining beliefs that signify their loyalty to important affinity groups. The paper discusses the normative significance of these findings, including the need to develop science communication strategies that shield policy-relevant facts from the influences that turn them into divisive symbols of identity.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LDTSbmXtokYAsEq8e": 2, "iP2X4jQNHMWHRNPne": 2, "Ng8Gice9KNkncxqcj": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "65seESKyiC44sMRsQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 24, "extendedScore": null, "score": 5.6e-05, "legacy": true, "legacyId": "20398", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["AdYdLP2sRqPMoe8fb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-02T18:21:17.187Z", "modifiedAt": null, "url": null, "title": "Atheists who believe that God exists", "slug": "atheists-who-believe-that-god-exists", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:06.823Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ChristianKl", "createdAt": "2009-10-13T22:32:16.589Z", "isAdmin": false, "displayName": "ChristianKl"}, "userId": "vbDMpDA5A35329Ju5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LRXuJMyNyEH7JMYqQ/atheists-who-believe-that-god-exists", "pageUrlRelative": "/posts/LRXuJMyNyEH7JMYqQ/atheists-who-believe-that-god-exists", "linkUrl": "https://www.lesswrong.com/posts/LRXuJMyNyEH7JMYqQ/atheists-who-believe-that-god-exists", "postedAtFormatted": "Sunday, December 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Atheists%20who%20believe%20that%20God%20exists&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAtheists%20who%20believe%20that%20God%20exists%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLRXuJMyNyEH7JMYqQ%2Fatheists-who-believe-that-god-exists%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Atheists%20who%20believe%20that%20God%20exists%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLRXuJMyNyEH7JMYqQ%2Fatheists-who-believe-that-god-exists", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLRXuJMyNyEH7JMYqQ%2Fatheists-who-believe-that-god-exists", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 207, "htmlBody": "<p>The 2012 LessWrong Censur had a question about belief in God. It also had a question about how people self label their religious beliefs. Interestingly 6.69% of the nonspiratual atheists believe that it's more likely that a God exists than that no God exists. On the other side of the spectrum&nbsp;17% of the committed theists believe that the chance that God exists is only 2%. Those 2% are the median for nonspirtual atheists who belief in God.</p>\n<p>Not a single person who took the LessWrong census believes that the chance that God exists is less than 1%. Given that this community has quite a lot of contrians and quite a lot of skeptics I would have guessed that there are people who believe that the chance that a God exists is less.</p>\n<p>The orginal question on the survey was:</p>\n<p style=\"padding-left: 30px;\"><label class=\"ss-q-title\" style=\"display: block; cursor: pointer; font-weight: bold; font-family: Arial, sans-serif; font-size: 13px;\" for=\"entry_47\">P(God)</label><label class=\"ss-q-help\" style=\"display: block; cursor: pointer; color: #666666; margin: 0.1em 0px 0.25em; font-family: Arial, sans-serif; font-size: 13px;\" for=\"entry_47\">What is the probability that there is a god, defined as a supernatural (see above) intelligent entity who created the universe?</label></p>\n<p>The numbers make also a nice plot:</p>\n<p><img src=\"http://images.lesswrong.com/t3_fqn_0.png\" alt=\"PGod for atheists and theists\" width=\"750\" height=\"450\" /></p>\n<p>There's also the question:</p>\n<p style=\"padding-left: 30px;\"><label class=\"ss-q-title\" style=\"display: block; cursor: pointer; font-weight: bold; font-family: Arial, sans-serif; font-size: 13px;\" for=\"entry_48\">P(Religion)</label><label class=\"ss-q-help\" style=\"display: block; cursor: pointer; color: #666666; margin: 0.1em 0px 0.25em; font-family: Arial, sans-serif; font-size: 13px;\" for=\"entry_48\">What is the probability that any of humankind's revealed religions is more or less correct?</label>\n<div><br /></div>\n</p>\n<p><img src=\"http://images.lesswrong.com/t3_fqn_1.png?v=afabfaa54c035aa68380b8640808140e\" alt=\"PReligion of atheists and theists\" width=\"750\" height=\"450\" /></p>\n<p>Given those results what do you think if someone tells you that they are atheist or a theist?</p>\n<p style=\"padding-left: 30px;\">&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LRXuJMyNyEH7JMYqQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": -1, "extendedScore": null, "score": -2e-06, "legacy": true, "legacyId": "20399", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-02T18:49:31.017Z", "modifiedAt": null, "url": null, "title": "What are you working on? December 2012", "slug": "what-are-you-working-on-december-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:31.352Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3eJtAX4uj9rvhWM8o/what-are-you-working-on-december-2012", "pageUrlRelative": "/posts/3eJtAX4uj9rvhWM8o/what-are-you-working-on-december-2012", "linkUrl": "https://www.lesswrong.com/posts/3eJtAX4uj9rvhWM8o/what-are-you-working-on-december-2012", "postedAtFormatted": "Sunday, December 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20are%20you%20working%20on%3F%20December%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20are%20you%20working%20on%3F%20December%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3eJtAX4uj9rvhWM8o%2Fwhat-are-you-working-on-december-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20are%20you%20working%20on%3F%20December%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3eJtAX4uj9rvhWM8o%2Fwhat-are-you-working-on-december-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3eJtAX4uj9rvhWM8o%2Fwhat-are-you-working-on-december-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 106, "htmlBody": "<p>\n<p>This is the sixth bimonthly 'What are you working On?' thread. Previous threads are&nbsp;<a href=\"/r/discussion/tag/waywo\">here</a>. So here's the question:</p>\n<p style=\"padding-left: 60px;\"><em>What are you working on?&nbsp;</em></p>\n<p>Here are some guidelines:</p>\n<ul>\n<li>Focus on projects that you have recently made progress on, not projects that you're thinking about doing but haven't started.</li>\n<li>Why this project and not others? Mention reasons why you're doing the project and/or why others should contribute to your project (if applicable).</li>\n<li>Talk about your goals for the project.</li>\n<li>Any kind of project is fair game: personal improvement, research project, art project, whatever.</li>\n<li>Link to your work if it's linkable.</li>\n</ul>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3eJtAX4uj9rvhWM8o", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 11, "extendedScore": null, "score": 1.0482794352291716e-06, "legacy": true, "legacyId": "20400", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 44, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-02T21:53:58.993Z", "modifiedAt": null, "url": null, "title": "LessWrong IQ Survey", "slug": "lesswrong-iq-survey", "viewCount": null, "lastCommentedAt": "2017-10-29T13:35:39.902Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Blackened", "createdAt": "2012-05-09T10:50:12.050Z", "isAdmin": false, "displayName": "Blackened"}, "userId": "qNeT4N9rP7tSxwquv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NqzesdCH69gv7LrKj/lesswrong-iq-survey", "pageUrlRelative": "/posts/NqzesdCH69gv7LrKj/lesswrong-iq-survey", "linkUrl": "https://www.lesswrong.com/posts/NqzesdCH69gv7LrKj/lesswrong-iq-survey", "postedAtFormatted": "Sunday, December 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LessWrong%20IQ%20Survey&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALessWrong%20IQ%20Survey%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNqzesdCH69gv7LrKj%2Flesswrong-iq-survey%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LessWrong%20IQ%20Survey%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNqzesdCH69gv7LrKj%2Flesswrong-iq-survey", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNqzesdCH69gv7LrKj%2Flesswrong-iq-survey", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 151, "htmlBody": "<p>The <a href=\"/lw/fp5/2012_survey_results/\">latest survey</a> shows that the average LessWronger who entered his SAT or ACT is roughly in the top 0.11%. This is insanely high. For comparison, top 2% can be Mensa members. I do not know the correlation between SAT/ACT and IQ, but I know it's fairly high. Anyway, I'm very curious to see the average score on a real, culture-fair IQ test. Those are the only two tests that are free, online, correct and culture fair, from what I know.</p>\n<p>&nbsp;</p>\n<p>http://www.cerebrals.org/wp/tests/jcti/</p>\n<p>http://www.etienne.se/cfnse/</p>\n<p>&nbsp;</p>\n<p>Many people would prefer not to have people knowing their scores. That's great, but please please <em>please</em> do post it anonymously. Especially if it's a low one, but <strong>not</strong> if it's low because you rushed the test.</p>\n<p>JCTI might take a while, so I recommend keeping your answers on Notepad in case you want to leave it and continue later.</p>\n<p>Edit: I have included polls, thanks to the commenters.</p>\n<p>Post your score here: http://lesswrong.com/r/discussion/lw/fqq/average_iq_on_lw/7yad/</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NqzesdCH69gv7LrKj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": -2, "extendedScore": null, "score": 1.0483847748196708e-06, "legacy": true, "legacyId": "20402", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 59, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["x9FNKTEt68Rz6wQ6P"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-02T22:31:29.335Z", "modifiedAt": null, "url": null, "title": "What science needs", "slug": "what-science-needs", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:08.381Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/soCPoB6cLmgWcTmhD/what-science-needs", "pageUrlRelative": "/posts/soCPoB6cLmgWcTmhD/what-science-needs", "linkUrl": "https://www.lesswrong.com/posts/soCPoB6cLmgWcTmhD/what-science-needs", "postedAtFormatted": "Sunday, December 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20science%20needs&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20science%20needs%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsoCPoB6cLmgWcTmhD%2Fwhat-science-needs%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20science%20needs%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsoCPoB6cLmgWcTmhD%2Fwhat-science-needs", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsoCPoB6cLmgWcTmhD%2Fwhat-science-needs", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2024, "htmlBody": "<p>Science does not need more scientists.&nbsp; It doesn't even need you, brilliant as you are.&nbsp; We already have many times more brilliant scientists than we can fund.&nbsp; Science could use a better understanding of the scientific method, but improving how individuals do science would not address most of the problems I've seen.</p>\n<p>The big problems facing science are organizational problems.&nbsp; We don't know how to identify important areas of study, or people who can do good science, or good and important results.&nbsp; We don't know how to run a project in a way that makes correct results likely.&nbsp; Improving the quality of each person on the project is not the answer.&nbsp; The problem is the system.&nbsp; We have organizations and systems that take groups of brilliant scientists, and motivate them to produce garbage.<a id=\"more\"></a></p>\n<p>I haven't got it all figured out, but here are some of the most-important problems in science.&nbsp; I'd like to turn this into a front-page post eventually, but now I'm going to post it to discussion, and ask you to add new important problems in the comments.</p>\n<h2>Egos</h2>\n<p>A lot of LWers think they want to advance scientific understanding.&nbsp; But I've learned after years in the field that what most scientists want even more is prove how smart they are.</p>\n<p>I couldn't tell you how many times I've seen a great idea killed because the project leader or someone else with veto power didn't want someone else's idea or someone else's area of expertise to appear important.&nbsp; I've been \"let go\" from two jobs because I refused when my bosses flat-out told me to stop proposing solutions for the important problems, because that was their territory.</p>\n<p>I don't mean that you should try to stop people from acting that way.&nbsp; People act that way.&nbsp; I mean you should admit that people act that way, and structure contracts, projects, and rewards so that these petty ego-boosts aren't the biggest rewards people can hope to get.</p>\n<h2>Too many \"no\"-men</h2>\n<p>The more people your project has who can say \"no\", the worse the results will be.&nbsp; This is one reason why Hollywood feature films are stupid, why start-ups do good work, and why scientific projects are so often a waste of money.&nbsp; Good ideas are inherently unpopular.&nbsp; Most of the projects that I've worked on have been crippled because every good idea ran into someone with veto power who didn't want to do things differently, or didn't want somebody else to get credit for solving the problem.&nbsp; See \"Egos\".</p>\n<p>Saying \"no\" to bad projects is important, but once the project is underway, there is a bias to say \"no\" more than \"yes\", even after adjusting for the number of times you can say \"yes\" in total.&nbsp; Requiring consensus is especially pernicious.&nbsp; You can't get good results when everbody on the project has to say \"yes\" to new ideas.</p>\n<h2>Jurisdiction arguments</h2>\n<p>Team members often disagree about whose expertise particular decisions fall under.&nbsp; Most people see how their expertise applies to a problem more easily than they can see how someone else's expertise applies to a problem.&nbsp; What usually happens is that territorial claims are honored from the top of the org chart on down, and by seniority.&nbsp; For example, I worked for a computer game company where the founder hired a scriptwriter, then came up with his own story ideas and told the scriptwriter to implement them.&nbsp; The implementation had no text; the scriptwriter took the story ideas and produced descriptions of scenes acted out with body language.&nbsp; The animators thought that body motion fell completely within their jurisdiction, so they felt free to rework whatever they saw differently.&nbsp; The scriptwriter had very little chance for creative input, no control over anything, and very little job satisfaction.</p>\n<p>This is a common problem for computer scientists and mathematicians.&nbsp; Computer scientists and mathematicians see themselves as people who understand how to most-effectively take a set of data, and arrive at the desired results.&nbsp; This includes figuring out what data to look at, and in the best case, means being involved in the proposal writing to look at possible problems to address, and determine which problems are soluble and which ones are not based on information theory.&nbsp; This never happens.&nbsp; People in other specialties see computer scientists as a kind of lab technician to bring on after they've figured out what problem to address, and what data and general algorithm to use.&nbsp; They see statisticians as people to consult when the project is done and they're writing up the results.&nbsp; They aren't even aware that these other disciplines can do more than that.</p>\n<p>A classic example is the Human Genome Project.&nbsp; Some people you never hear about, including my current boss, came up with algorithms to take whole-genome shotgun data and assemble it.&nbsp; Craig Venter went to the leaders of the Human Genome Project and explained to them that, using this approach, they could finish the project at a fraction of the cost.&nbsp; Anybody with a little mathematical expertise could look at the numbers and figure out on the back of a napkin that, yes, this could work.&nbsp; But all the decision-makers on the HGP were biologists.&nbsp; I presume that they didn't understand the math, and didn't believe that mathematicians could have useful insights into biological problems.&nbsp; So they declared it impossible&mdash;not difficult, but theoretically impossible&mdash;and plowed ahead, while Craig split off to use the shotgun approach.&nbsp; Billions of taxpayer dollars were wasted because a few people in leadership positions could not recognize that a problem in biology had a mathematical aspect.</p>\n<h2>Muzzling the oxen</h2>\n<p>\"Thou shalt not muzzle the ox when he treadeth out the corn.\"&nbsp; &mdash; Deuteronomy 25:4</p>\n<p>I believe that a large number of the problems with scientific research are tolerated only because nothing is at stake financially.&nbsp; Government agencies have tried very hard to ensure that people do work for their contracts.&nbsp; You have to say in the proposal what you're going to do, and itemize all your costs, and do what you said you would do, and write reports once a month or once a quarter showing that you're doing what you said you would do.&nbsp; This results in unfortunate obvious stupidities.&nbsp; We can spend $30,000 to have an employee write a piece of software that we could have bought for $500, or to solve a problem that a consultant could have solved for $500, but we can't buy the software or hire the consultant because they aren't listed in the contract and the employee is.</p>\n<p>But the bigger problem is that the strict financial structure of scientific research makes it illegal to motivate scientists by giving them a percentage of resulting profits.&nbsp; You simply can't write up a budget proposal that way.&nbsp; So managers and team members indulge their prejudices and fantasies because the little bit of self-esteem boost they get from clinging to their favorite ideas is worth more to them than the extra money they would earn (zero) if the project produced better results.&nbsp; Examples of petty prejudices that I've seen people wreck good work to preserve:&nbsp; top-down over bottom-up design, emacs over vim (I was in a shop once where the founders forbade people from using vim, which had an astonishingly destructive effect on morale), rule-based over statistical grammars, symbolic logic over neural networks, linguistics expertise as more important than mathematical expertise, biological expertise as more important than mathematical expertise, and, always, human opinions gathered from a few hundred examples as more valid than statistical tests performed on millions of samples.</p>\n<p>When I read about machine learning techniques being applied in the real world, half the time it's by trading firms.&nbsp; I haven't worked for one, so I don't know; but I would bet they are a lot more receptive to new ideas because, unlike scientists, they care about the results more than about their egos.&nbsp; Or at least, an appreciable fraction as much as they care about their egos.</p>\n<h2>Entry costs</h2>\n<p>Everybody in science relies on two metrics to decide who to hire and who to give grants to:&nbsp; What their recent publications are, and what school they went to.&nbsp; It is possible to go to a non-top-ranked school and then get on important projects and get publication credits.&nbsp; Someone who just left our company on Friday worked the magic of cranking out good research publications while working as a programmer, always taking on only projects that had good publication potential and never getting stuck with the horrible life-sucking, year-sucking drudgery tasks of, say, converting application X from using database Y to database Z.&nbsp; I just don't know how he did it.</p>\n<p>For the most part, that doesn't happen.&nbsp; You don't become a researcher; you start out as a researcher.&nbsp; You need to stay in school, or stay on as a postdoc, until you have your own track record publications and have won your own grant.&nbsp; You need people to read those publications.&nbsp; You don't get to work on important projects and get your work read and get a grant because you're brilliant.&nbsp; You get these things because your advisor works the old-boy network for you.&nbsp; Whatever your field is, there is a network of universities that are recognized as leaders in that field, and you are more-or-less assured of failure in your career (especially in academia or research) unless you go to one of those universities, because you won't get published in good journals, you won't get read much, and you won't get a big grant.</p>\n<p>There are exceptions.&nbsp; Fiction writers and computer programmers don't need to go to a fancy university; they need credits and experience.&nbsp; (Computer <em>programmers.</em>&nbsp; But don't get a Ph.D. in computer science from a non-elite university and imagine you're going to do research; it won't happen.)&nbsp; Good stories can sort of be recognized; basic knowledge about Enterprise Java can be measured.&nbsp; Companies have recognized the monetary value of doing so.&nbsp; But grant review panels and companies don't really know how to rate scientists or managers, so they try to get somebody from MIT or from Wharton, because nobody ever got fired for buying a Xerox.</p>\n<p>The value of scientists to their companies may or may not be reflected in their salaries, but the value of those select universities is certainly reflected in the price of tuition.&nbsp; If your college of choice costs you less than $55,000/yr to attend, including room and board, it will not lead you to success.&nbsp; Unfortunately, the U.S. government won't loan you more than $10,000/yr for tuition.</p>\n<p>(One interesting exception is in cosmology.&nbsp; I did a study of successful physicists, as measured by their winning the Nobel or being on the faculty at Harvard.&nbsp; I found that after 1970, no one was successful in physics unless they went to an elite undergraduate college, with a few exceptions.&nbsp; The exceptions were astrophysicists who went to college in Arizona or Hawaii, where there are inexpensive colleges that are recognized as leading institutions in astronomy because they have big telescopes.)</p>\n<h2>Search</h2>\n<p>The single-biggest problem with science today is finding relevant results.&nbsp; I have had numerous discussions with experts in a field who were unaware of recent (and not-so-recent) important results in their field because they relied on word-of-mouth and a small set of authoritative journals, while I spent half an hour with Google before our meeting.&nbsp; To take a spectacularly bad example, the literature showing that metronidazole kills Borrelia burgdorferi cysts, while penicillin, doxycycline, amoxicillin, and ceftriaxone do not, is over ten years old; yet metronidazole is never prescribed for Lyme disease while the latter are.</p>\n<p>Attention is <em>the</em> most-valuable resource in the twenty-first century.&nbsp; Producing a significant result is not hard.&nbsp; Getting people to pay attention to it is.&nbsp; Scientometric analysis of scientific publications shows that producing more and more papers in a field has very little impact on the number of papers cited (a proxy for number of results used), probably because scientists basically read up to one paper per day chosen from one or two leading journals, and that's it.&nbsp; They aren't in the habit of regularly, actively searching for things relevant to their work; and frankly, there isn't much motivation to do that, since using Google to answer a specific question is like using excavation equipment to search for a needle in a haystack.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ZpG9rheyAkgCoEQea": 1, "xexCWMyds6QLWognu": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "soCPoB6cLmgWcTmhD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 59, "baseScore": 58, "extendedScore": null, "score": 0.0006221977337437273, "legacy": true, "legacyId": "20401", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 42, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Science does not need more scientists.&nbsp; It doesn't even need you, brilliant as you are.&nbsp; We already have many times more brilliant scientists than we can fund.&nbsp; Science could use a better understanding of the scientific method, but improving how individuals do science would not address most of the problems I've seen.</p>\n<p>The big problems facing science are organizational problems.&nbsp; We don't know how to identify important areas of study, or people who can do good science, or good and important results.&nbsp; We don't know how to run a project in a way that makes correct results likely.&nbsp; Improving the quality of each person on the project is not the answer.&nbsp; The problem is the system.&nbsp; We have organizations and systems that take groups of brilliant scientists, and motivate them to produce garbage.<a id=\"more\"></a></p>\n<p>I haven't got it all figured out, but here are some of the most-important problems in science.&nbsp; I'd like to turn this into a front-page post eventually, but now I'm going to post it to discussion, and ask you to add new important problems in the comments.</p>\n<h2 id=\"Egos\">Egos</h2>\n<p>A lot of LWers think they want to advance scientific understanding.&nbsp; But I've learned after years in the field that what most scientists want even more is prove how smart they are.</p>\n<p>I couldn't tell you how many times I've seen a great idea killed because the project leader or someone else with veto power didn't want someone else's idea or someone else's area of expertise to appear important.&nbsp; I've been \"let go\" from two jobs because I refused when my bosses flat-out told me to stop proposing solutions for the important problems, because that was their territory.</p>\n<p>I don't mean that you should try to stop people from acting that way.&nbsp; People act that way.&nbsp; I mean you should admit that people act that way, and structure contracts, projects, and rewards so that these petty ego-boosts aren't the biggest rewards people can hope to get.</p>\n<h2 id=\"Too_many__no__men\">Too many \"no\"-men</h2>\n<p>The more people your project has who can say \"no\", the worse the results will be.&nbsp; This is one reason why Hollywood feature films are stupid, why start-ups do good work, and why scientific projects are so often a waste of money.&nbsp; Good ideas are inherently unpopular.&nbsp; Most of the projects that I've worked on have been crippled because every good idea ran into someone with veto power who didn't want to do things differently, or didn't want somebody else to get credit for solving the problem.&nbsp; See \"Egos\".</p>\n<p>Saying \"no\" to bad projects is important, but once the project is underway, there is a bias to say \"no\" more than \"yes\", even after adjusting for the number of times you can say \"yes\" in total.&nbsp; Requiring consensus is especially pernicious.&nbsp; You can't get good results when everbody on the project has to say \"yes\" to new ideas.</p>\n<h2 id=\"Jurisdiction_arguments\">Jurisdiction arguments</h2>\n<p>Team members often disagree about whose expertise particular decisions fall under.&nbsp; Most people see how their expertise applies to a problem more easily than they can see how someone else's expertise applies to a problem.&nbsp; What usually happens is that territorial claims are honored from the top of the org chart on down, and by seniority.&nbsp; For example, I worked for a computer game company where the founder hired a scriptwriter, then came up with his own story ideas and told the scriptwriter to implement them.&nbsp; The implementation had no text; the scriptwriter took the story ideas and produced descriptions of scenes acted out with body language.&nbsp; The animators thought that body motion fell completely within their jurisdiction, so they felt free to rework whatever they saw differently.&nbsp; The scriptwriter had very little chance for creative input, no control over anything, and very little job satisfaction.</p>\n<p>This is a common problem for computer scientists and mathematicians.&nbsp; Computer scientists and mathematicians see themselves as people who understand how to most-effectively take a set of data, and arrive at the desired results.&nbsp; This includes figuring out what data to look at, and in the best case, means being involved in the proposal writing to look at possible problems to address, and determine which problems are soluble and which ones are not based on information theory.&nbsp; This never happens.&nbsp; People in other specialties see computer scientists as a kind of lab technician to bring on after they've figured out what problem to address, and what data and general algorithm to use.&nbsp; They see statisticians as people to consult when the project is done and they're writing up the results.&nbsp; They aren't even aware that these other disciplines can do more than that.</p>\n<p>A classic example is the Human Genome Project.&nbsp; Some people you never hear about, including my current boss, came up with algorithms to take whole-genome shotgun data and assemble it.&nbsp; Craig Venter went to the leaders of the Human Genome Project and explained to them that, using this approach, they could finish the project at a fraction of the cost.&nbsp; Anybody with a little mathematical expertise could look at the numbers and figure out on the back of a napkin that, yes, this could work.&nbsp; But all the decision-makers on the HGP were biologists.&nbsp; I presume that they didn't understand the math, and didn't believe that mathematicians could have useful insights into biological problems.&nbsp; So they declared it impossible\u2014not difficult, but theoretically impossible\u2014and plowed ahead, while Craig split off to use the shotgun approach.&nbsp; Billions of taxpayer dollars were wasted because a few people in leadership positions could not recognize that a problem in biology had a mathematical aspect.</p>\n<h2 id=\"Muzzling_the_oxen\">Muzzling the oxen</h2>\n<p>\"Thou shalt not muzzle the ox when he treadeth out the corn.\"&nbsp; \u2014 Deuteronomy 25:4</p>\n<p>I believe that a large number of the problems with scientific research are tolerated only because nothing is at stake financially.&nbsp; Government agencies have tried very hard to ensure that people do work for their contracts.&nbsp; You have to say in the proposal what you're going to do, and itemize all your costs, and do what you said you would do, and write reports once a month or once a quarter showing that you're doing what you said you would do.&nbsp; This results in unfortunate obvious stupidities.&nbsp; We can spend $30,000 to have an employee write a piece of software that we could have bought for $500, or to solve a problem that a consultant could have solved for $500, but we can't buy the software or hire the consultant because they aren't listed in the contract and the employee is.</p>\n<p>But the bigger problem is that the strict financial structure of scientific research makes it illegal to motivate scientists by giving them a percentage of resulting profits.&nbsp; You simply can't write up a budget proposal that way.&nbsp; So managers and team members indulge their prejudices and fantasies because the little bit of self-esteem boost they get from clinging to their favorite ideas is worth more to them than the extra money they would earn (zero) if the project produced better results.&nbsp; Examples of petty prejudices that I've seen people wreck good work to preserve:&nbsp; top-down over bottom-up design, emacs over vim (I was in a shop once where the founders forbade people from using vim, which had an astonishingly destructive effect on morale), rule-based over statistical grammars, symbolic logic over neural networks, linguistics expertise as more important than mathematical expertise, biological expertise as more important than mathematical expertise, and, always, human opinions gathered from a few hundred examples as more valid than statistical tests performed on millions of samples.</p>\n<p>When I read about machine learning techniques being applied in the real world, half the time it's by trading firms.&nbsp; I haven't worked for one, so I don't know; but I would bet they are a lot more receptive to new ideas because, unlike scientists, they care about the results more than about their egos.&nbsp; Or at least, an appreciable fraction as much as they care about their egos.</p>\n<h2 id=\"Entry_costs\">Entry costs</h2>\n<p>Everybody in science relies on two metrics to decide who to hire and who to give grants to:&nbsp; What their recent publications are, and what school they went to.&nbsp; It is possible to go to a non-top-ranked school and then get on important projects and get publication credits.&nbsp; Someone who just left our company on Friday worked the magic of cranking out good research publications while working as a programmer, always taking on only projects that had good publication potential and never getting stuck with the horrible life-sucking, year-sucking drudgery tasks of, say, converting application X from using database Y to database Z.&nbsp; I just don't know how he did it.</p>\n<p>For the most part, that doesn't happen.&nbsp; You don't become a researcher; you start out as a researcher.&nbsp; You need to stay in school, or stay on as a postdoc, until you have your own track record publications and have won your own grant.&nbsp; You need people to read those publications.&nbsp; You don't get to work on important projects and get your work read and get a grant because you're brilliant.&nbsp; You get these things because your advisor works the old-boy network for you.&nbsp; Whatever your field is, there is a network of universities that are recognized as leaders in that field, and you are more-or-less assured of failure in your career (especially in academia or research) unless you go to one of those universities, because you won't get published in good journals, you won't get read much, and you won't get a big grant.</p>\n<p>There are exceptions.&nbsp; Fiction writers and computer programmers don't need to go to a fancy university; they need credits and experience.&nbsp; (Computer <em>programmers.</em>&nbsp; But don't get a Ph.D. in computer science from a non-elite university and imagine you're going to do research; it won't happen.)&nbsp; Good stories can sort of be recognized; basic knowledge about Enterprise Java can be measured.&nbsp; Companies have recognized the monetary value of doing so.&nbsp; But grant review panels and companies don't really know how to rate scientists or managers, so they try to get somebody from MIT or from Wharton, because nobody ever got fired for buying a Xerox.</p>\n<p>The value of scientists to their companies may or may not be reflected in their salaries, but the value of those select universities is certainly reflected in the price of tuition.&nbsp; If your college of choice costs you less than $55,000/yr to attend, including room and board, it will not lead you to success.&nbsp; Unfortunately, the U.S. government won't loan you more than $10,000/yr for tuition.</p>\n<p>(One interesting exception is in cosmology.&nbsp; I did a study of successful physicists, as measured by their winning the Nobel or being on the faculty at Harvard.&nbsp; I found that after 1970, no one was successful in physics unless they went to an elite undergraduate college, with a few exceptions.&nbsp; The exceptions were astrophysicists who went to college in Arizona or Hawaii, where there are inexpensive colleges that are recognized as leading institutions in astronomy because they have big telescopes.)</p>\n<h2 id=\"Search\">Search</h2>\n<p>The single-biggest problem with science today is finding relevant results.&nbsp; I have had numerous discussions with experts in a field who were unaware of recent (and not-so-recent) important results in their field because they relied on word-of-mouth and a small set of authoritative journals, while I spent half an hour with Google before our meeting.&nbsp; To take a spectacularly bad example, the literature showing that metronidazole kills Borrelia burgdorferi cysts, while penicillin, doxycycline, amoxicillin, and ceftriaxone do not, is over ten years old; yet metronidazole is never prescribed for Lyme disease while the latter are.</p>\n<p>Attention is <em>the</em> most-valuable resource in the twenty-first century.&nbsp; Producing a significant result is not hard.&nbsp; Getting people to pay attention to it is.&nbsp; Scientometric analysis of scientific publications shows that producing more and more papers in a field has very little impact on the number of papers cited (a proxy for number of results used), probably because scientists basically read up to one paper per day chosen from one or two leading journals, and that's it.&nbsp; They aren't in the habit of regularly, actively searching for things relevant to their work; and frankly, there isn't much motivation to do that, since using Google to answer a specific question is like using excavation equipment to search for a needle in a haystack.</p>", "sections": [{"title": "Egos", "anchor": "Egos", "level": 1}, {"title": "Too many \"no\"-men", "anchor": "Too_many__no__men", "level": 1}, {"title": "Jurisdiction arguments", "anchor": "Jurisdiction_arguments", "level": 1}, {"title": "Muzzling the oxen", "anchor": "Muzzling_the_oxen", "level": 1}, {"title": "Entry costs", "anchor": "Entry_costs", "level": 1}, {"title": "Search", "anchor": "Search", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "84 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 84, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-02T23:45:58.124Z", "modifiedAt": null, "url": null, "title": "[Link] Contesting the \u201cNature\u201d Of Conformity: What Milgram and Zimbardo's Studies Really Show", "slug": "link-contesting-the-nature-of-conformity-what-milgram-and", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:07.814Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "PbyH3bFJCZjPX5DTp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/TrvjwJgYdZf72GkXG/link-contesting-the-nature-of-conformity-what-milgram-and", "pageUrlRelative": "/posts/TrvjwJgYdZf72GkXG/link-contesting-the-nature-of-conformity-what-milgram-and", "linkUrl": "https://www.lesswrong.com/posts/TrvjwJgYdZf72GkXG/link-contesting-the-nature-of-conformity-what-milgram-and", "postedAtFormatted": "Sunday, December 2nd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Contesting%20the%20%E2%80%9CNature%E2%80%9D%20Of%20Conformity%3A%20What%20Milgram%20and%20Zimbardo's%20Studies%20Really%20Show&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Contesting%20the%20%E2%80%9CNature%E2%80%9D%20Of%20Conformity%3A%20What%20Milgram%20and%20Zimbardo's%20Studies%20Really%20Show%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTrvjwJgYdZf72GkXG%2Flink-contesting-the-nature-of-conformity-what-milgram-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Contesting%20the%20%E2%80%9CNature%E2%80%9D%20Of%20Conformity%3A%20What%20Milgram%20and%20Zimbardo's%20Studies%20Really%20Show%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTrvjwJgYdZf72GkXG%2Flink-contesting-the-nature-of-conformity-what-milgram-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTrvjwJgYdZf72GkXG%2Flink-contesting-the-nature-of-conformity-what-milgram-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 163, "htmlBody": "<p>Here is a paper in PLOS Biology re-considering the lessons of some classic psychology experiments invoked here often (<a href=\"http://www.metafilter.com/122403/Contesting-the-Nature-Of-Conformity-What-Milgram-and-Zimbardos-Studies-Really-Show\">via</a>).</p>\n<p><a href=\"http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001426\">Contesting the &ldquo;Nature&rdquo; Of Conformity: What Milgram and Zimbardo's Studies Really Show</a></p>\n<p>To me the crux of the paper comes from this statement in the abstract:</p>\n<blockquote>\n<p><span style=\"color: #303030; font-size: 12px; line-height: 21px; \">This suggests that individuals' willingness to follow authorities is conditional on identification with the authority in question and an associated belief that the authority is right.</span></p>\n</blockquote>\n<p><span style=\"color: #303030; font-size: 12px; line-height: 21px; \">Plus this detail from the Milgram experiment:</span></p>\n<blockquote>\n<p><span style=\"color: #303030; font-size: 12px; line-height: 21px; \">Ultimately, they tend to go along with the Experimenter if he justifies their actions in terms of the scientific benefits of the study (as he does with the prod &ldquo;The experiment requires that you continue&rdquo;)&nbsp;<a style=\"color: #0029a5;\" href=\"http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001426#pbio.1001426-Burger1\">[39]</a>. But if he gives them a direct order (&ldquo;You have no other choice, you must go on&rdquo;) participants typically refuse. Once again, received wisdom proves questionable. The Milgram studies seem to be less about people blindly conforming to orders than about getting people to believe in the importance of what they are doing&nbsp;<a style=\"color: #0029a5;\" href=\"http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001426#pbio.1001426-Reicher2\">[40]</a>.</span></p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb124": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "TrvjwJgYdZf72GkXG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 16, "extendedScore": null, "score": 1.0484487333273078e-06, "legacy": true, "legacyId": "20403", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-03T02:33:14.356Z", "modifiedAt": null, "url": null, "title": "Rationality Quotes December 2012", "slug": "rationality-quotes-december-2012", "viewCount": null, "lastCommentedAt": "2013-12-10T11:58:08.513Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Thomas", "createdAt": "2009-03-02T17:47:09.607Z", "isAdmin": false, "displayName": "Thomas"}, "userId": "GrAKeuxT4e9AKyHdE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zCsHbW3SPAHbrMWnW/rationality-quotes-december-2012", "pageUrlRelative": "/posts/zCsHbW3SPAHbrMWnW/rationality-quotes-december-2012", "linkUrl": "https://www.lesswrong.com/posts/zCsHbW3SPAHbrMWnW/rationality-quotes-december-2012", "postedAtFormatted": "Monday, December 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Quotes%20December%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Quotes%20December%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzCsHbW3SPAHbrMWnW%2Frationality-quotes-december-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Quotes%20December%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzCsHbW3SPAHbrMWnW%2Frationality-quotes-december-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzCsHbW3SPAHbrMWnW%2Frationality-quotes-december-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 72, "htmlBody": "<p>\n<p>Once again, here's the new thread for posting quotes, with the usual rules:</p>\n<p>Please post all quotes separately, so that they can be voted up/down separately. &nbsp;(If they are strongly related, reply to your own comments. &nbsp;If strongly ordered, then go ahead and post them together.)</p>\n<p>\n<ul>\n<li>Do not quote yourself</li>\n<li>Do not quote comments/posts on LW/OB</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>\n</p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zCsHbW3SPAHbrMWnW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 1.048544279670299e-06, "legacy": true, "legacyId": "20389", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 230, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2012-12-03T02:33:14.356Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-03T02:33:26.936Z", "modifiedAt": null, "url": null, "title": "Meetup : Vancouver!", "slug": "meetup-vancouver-1", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JXbbjwT6mDS7xdPsY/meetup-vancouver-1", "pageUrlRelative": "/posts/JXbbjwT6mDS7xdPsY/meetup-vancouver-1", "linkUrl": "https://www.lesswrong.com/posts/JXbbjwT6mDS7xdPsY/meetup-vancouver-1", "postedAtFormatted": "Monday, December 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Vancouver!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Vancouver!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJXbbjwT6mDS7xdPsY%2Fmeetup-vancouver-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Vancouver!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJXbbjwT6mDS7xdPsY%2Fmeetup-vancouver-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJXbbjwT6mDS7xdPsY%2Fmeetup-vancouver-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 109, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/gn'>Vancouver!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 December 2012 01:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2150 macdonald st</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello Vancouver Lesswrongers,</p>\n\n<p>You should come out to our next meetup. It's tons of fun to hang out with other rationalists!</p>\n\n<p>Come see us on our <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">mailing list</a>, or just come by and see us.</p>\n\n<p>One of our members just got back from CFAR's Rationality for Entrepreneurs thing, so he'll have lots of cool things that you haven't heard about. Otherwise it will be just casually hanging out, sharing ideas, and so on. We often end up having really interesting impromptu presentations, enlightening discussions, and lots of friendship.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/gn'>Vancouver!</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JXbbjwT6mDS7xdPsY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.048544399433985e-06, "legacy": true, "legacyId": "20411", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Vancouver_\">Discussion article for the meetup : <a href=\"/meetups/gn\">Vancouver!</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 December 2012 01:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2150 macdonald st</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Hello Vancouver Lesswrongers,</p>\n\n<p>You should come out to our next meetup. It's tons of fun to hang out with other rationalists!</p>\n\n<p>Come see us on our <a href=\"http://groups.google.com/group/vancouver-rationalists\" rel=\"nofollow\">mailing list</a>, or just come by and see us.</p>\n\n<p>One of our members just got back from CFAR's Rationality for Entrepreneurs thing, so he'll have lots of cool things that you haven't heard about. Otherwise it will be just casually hanging out, sharing ideas, and so on. We often end up having really interesting impromptu presentations, enlightening discussions, and lots of friendship.</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Vancouver_1\">Discussion article for the meetup : <a href=\"/meetups/gn\">Vancouver!</a></h2>", "sections": [{"title": "Discussion article for the meetup : Vancouver!", "anchor": "Discussion_article_for_the_meetup___Vancouver_", "level": 1}, {"title": "Discussion article for the meetup : Vancouver!", "anchor": "Discussion_article_for_the_meetup___Vancouver_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-03T03:32:46.809Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] I Heart CYC", "slug": "seq-rerun-i-heart-cyc", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:07.277Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/u3YZfJfvCbyBXz4WH/seq-rerun-i-heart-cyc", "pageUrlRelative": "/posts/u3YZfJfvCbyBXz4WH/seq-rerun-i-heart-cyc", "linkUrl": "https://www.lesswrong.com/posts/u3YZfJfvCbyBXz4WH/seq-rerun-i-heart-cyc", "postedAtFormatted": "Monday, December 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20I%20Heart%20CYC&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20I%20Heart%20CYC%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu3YZfJfvCbyBXz4WH%2Fseq-rerun-i-heart-cyc%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20I%20Heart%20CYC%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu3YZfJfvCbyBXz4WH%2Fseq-rerun-i-heart-cyc", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu3YZfJfvCbyBXz4WH%2Fseq-rerun-i-heart-cyc", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 175, "htmlBody": "<p>Today's post, <a href=\"http://www.overcomingbias.com/2008/12/i-heart-cyc.html\">I Heart CYC</a> was originally published on  December 1, 2008.  A summary:</p>\n<p>&nbsp;</p>\n<blockquote>Douglas Lenat has a theory which Hanson finds plausible. Lenat argues that architechture of AI is overrated, and that there is only so much you can do before you start trying to generate content for an AI.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fqk/seq_rerun_disappointment_in_the_future/\">Disappointment in the Future</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "u3YZfJfvCbyBXz4WH", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0485782936725077e-06, "legacy": true, "legacyId": "20412", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hvpGmqYxzKktGuYCk", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-03T08:44:38.125Z", "modifiedAt": "2020-09-30T07:48:36.038Z", "url": null, "title": "LessWrong podcasts", "slug": "lesswrong-podcasts", "viewCount": null, "lastCommentedAt": "2017-10-06T16:06:33.980Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Louie", "createdAt": "2010-05-10T21:41:14.619Z", "isAdmin": false, "displayName": "Louie"}, "userId": "JPwZspDjBcfwwuy7W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BTM5wskCnzbNLDZdS/lesswrong-podcasts", "pageUrlRelative": "/posts/BTM5wskCnzbNLDZdS/lesswrong-podcasts", "linkUrl": "https://www.lesswrong.com/posts/BTM5wskCnzbNLDZdS/lesswrong-podcasts", "postedAtFormatted": "Monday, December 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20LessWrong%20podcasts&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALessWrong%20podcasts%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBTM5wskCnzbNLDZdS%2Flesswrong-podcasts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=LessWrong%20podcasts%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBTM5wskCnzbNLDZdS%2Flesswrong-podcasts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBTM5wskCnzbNLDZdS%2Flesswrong-podcasts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 182, "htmlBody": "<p><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline;\">Today we're announcing a partnership with&nbsp;</span><a style=\"color: #1155cc;\" href=\"http://castify.co/\" target=\"_blank\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline;\">Castify</span></a><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline;\">&nbsp;to bring you Less Wrong content in audio form. Castify gets blog content read by professional readers and delivers it to their subscribers as a podcast so that you can listen to Less Wrong on the go. The founders of Castify are big fans of Less Wrong so they're rolling out their beta with some of our content.</span></p>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 16px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 16px;\"><img src=\"http://louie.divide0.net/castify.png\" alt=\"Castify\" width=\"776\" height=\"254\" /><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 16px;\">&nbsp;Note: The embedded player (above) isn't live as of this posting, but should be deployed soon.</div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 16px;\"><br /><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline;\">To see how many people will use this, we're</span><span style=\"background-color: transparent; font-size: 15px; font-family: Arial; vertical-align: baseline;\">&nbsp;having the entire</span><span style=\"background-color: transparent; font-size: 15px; font-family: Arial; vertical-align: baseline;\">&nbsp;</span><span style=\"background-color: transparent; font-size: 15px; font-family: Arial; color: #1155cc; vertical-align: baseline;\"><a style=\"color: #1155cc;\" href=\"http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions\" target=\"_blank\">Mysterious Answers to Mysterious Questions</a></span><span style=\"background-color: transparent; font-size: 15px; font-family: Arial; vertical-align: baseline;\">&nbsp;core sequence read and recorded</span><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline;\">. We thought listening to it would be a great way for new readers to get caught up and for others to check out the quality of Castify's work. We will be adding more Less Wrong content based on community feedback, so let us know which content you'd like to see more of in the comments.</span></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 16px;\"><br /></div>\n<div style=\"color: #222222; font-family: arial, sans-serif; font-size: 16px;\"><span style=\"font-size: 15px; font-family: Arial; background-color: transparent; vertical-align: baseline;\">For instance: Which other sequences would you like to listen to? Would there be interest in an ongoing podcast channel for the promoted posts?</span></div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1, "MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BTM5wskCnzbNLDZdS", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 53, "extendedScore": null, "score": 0.0005743363696095945, "legacy": true, "legacyId": "20414", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 38, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 100, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2012-12-03T08:44:38.125Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-03T09:26:46.005Z", "modifiedAt": null, "url": null, "title": "A solvable Newcomb-like problem - part 1 of 3", "slug": "a-solvable-newcomb-like-problem-part-1-of-3", "viewCount": null, "lastCommentedAt": "2017-06-17T04:12:25.871Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Douglas_Reay", "createdAt": "2012-02-19T14:40:26.403Z", "isAdmin": false, "displayName": "Douglas_Reay"}, "userId": "jpnrRPxHozDiGBqp2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/FTG8G5wxhcJyqunok/a-solvable-newcomb-like-problem-part-1-of-3", "pageUrlRelative": "/posts/FTG8G5wxhcJyqunok/a-solvable-newcomb-like-problem-part-1-of-3", "linkUrl": "https://www.lesswrong.com/posts/FTG8G5wxhcJyqunok/a-solvable-newcomb-like-problem-part-1-of-3", "postedAtFormatted": "Monday, December 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20solvable%20Newcomb-like%20problem%20-%20part%201%20of%203&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20solvable%20Newcomb-like%20problem%20-%20part%201%20of%203%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFTG8G5wxhcJyqunok%2Fa-solvable-newcomb-like-problem-part-1-of-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20solvable%20Newcomb-like%20problem%20-%20part%201%20of%203%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFTG8G5wxhcJyqunok%2Fa-solvable-newcomb-like-problem-part-1-of-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FFTG8G5wxhcJyqunok%2Fa-solvable-newcomb-like-problem-part-1-of-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1016, "htmlBody": "<p>This is the first part of a three post sequence on a problem that is similar to <a href=\"http://wiki.lesswrong.com/wiki/Newcomb%27s_problem\">Newcomb's problem</a> but is posed in terms of probabilities and limited knowledge.</p>\n<p>&nbsp;&nbsp; Part 1 - stating the problem<br />&nbsp;&nbsp; Part 2 - some mathematics<br />&nbsp;&nbsp; Part 3 - towards a solution</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>Omega is an AI, living in a society of AIs, who wishes to enhance his reputation in that society for being successfully able to predict human actions.&nbsp; Given some exchange rate between money and reputation, you could think of that as a bet between him and another AI, let's call it Alpha.&nbsp; And since there is also a human involved, for the sake of clarity, to avoid using \"you\" all the time, I'm going to sometimes refer to the human using the name \"Fred\".</p>\n<p>&nbsp;</p>\n<p>Omega tells Fred:</p>\n<p>I'd like you to pick between two options, and I'm going to try to predict which option you're going to pick.<br />&nbsp;&nbsp;&nbsp; Option \"one box\" is to open only box A, and take any money inside it<br />&nbsp;&nbsp;&nbsp; Option \"two box\" is to open both box A and box B, and take any money inside them</p>\n<p>but, before you pick your option, declare it, then open the box or boxes, there are three things you need to know.</p>\n<p>&nbsp;</p>\n<p>Firstly, you need to know the terms of my bet with Alpha.</p>\n<p>If Fred picks option \"one box\" then:<br />&nbsp;&nbsp; If box A contains $1,000,000 and box B contains $1,000 then Alpha pays Omega $1,000,000,000<br />&nbsp;&nbsp; If box A contains $0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; and box B contains $1,000 then Omega pays Alpha $10,000,000,000<br />&nbsp;&nbsp; If anything else, then both Alpha and Omega pay Fred $1,000,000,000,000</p>\n<p>If Fred picks option \"two box\" then:<br />&nbsp;&nbsp; If box A contains $1,000,000 and box B contains $1,000 then Omega pays Alpha $10,000,000,000<br />&nbsp;&nbsp; If box A contains $0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; and box B contains $1,000 then Alpha pays Omega $1,000,000,000<br />&nbsp;&nbsp; If anything else, then both Alpha and Omega pay Fred $1,000,000,000,000</p>\n<p>&nbsp;</p>\n<p>Secondly, you should know that I've already placed all the money in the boxes that I'm going to, and I can't change the contents of the boxes between now and when you do the opening, because Alpha is monitoring everything.&nbsp; I've already made my prediction, using a model I've constructed of your likely reactions based upon your past actions.</p>\n<p>You can use any method you like to choose between the two options, short of contacting another AI, but be warned that if my model predicted that you'll use a method which introduces too large a random element (such as tossing a coin) then, while I may lose my bet with Alpha, I'll certainly have made sure you won't win the $1,000,000.&nbsp; Similarly, if my model predicted that you'd make an outside bet with another human (let's call him George) to alter the value of winning $1,001,000 from me I'd have also taken that into account.&nbsp; (I say \"human\" by the way, because my bet with Alpha is about my ability to predict humans so if you contact another AI, such as trying to lay a side bet with Alpha to skim some of his winnings, that invalidates not only my game with you, but also my bet with Alpha and there are no winning to skim.)</p>\n<p>&nbsp;</p>\n<p>And, third and finally, you need to know my track record in previous similar situations.</p>\n<p>I've played this game 3,924 times over the past 100 years (ie since the game started), with humans picked at random from the full variety of the population.&nbsp;&nbsp; The outcomes were:<br />&nbsp;&nbsp; 3000 times players picked option \"one box\" and walked away with $1,000,000<br />&nbsp;&nbsp; 900&nbsp; times players picked option \"two box\" and walked away with $1,000<br />&nbsp;&nbsp; 24 times players flipped a coin and or were otherwise too random.&nbsp; Of those players:<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 12 players picked option \"one box\" and walked away with $0<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 12 players picked option \"two box\" and walked away with $1,000</p>\n<p>Never has anyone ever ended up walking away with $1,001,000 by picking option \"two box\".</p>\n<p>&nbsp;</p>\n<p>Omega stops talking.&nbsp;&nbsp; You are standing in a room containing two boxes, labelled \"A\" and \"B\", which are both currently closed.&nbsp; Everything Omega said matches what you expected him to say, as the conditions of the game are always the same and are well known - you've talked with other human players (who confirmed it is legit) and listened to their advice.&nbsp;&nbsp; You've not contacted any AIs, though you have read the published statement from Alpha that also confirms the terms of the bet and details of the monitoring.&nbsp; You've not made any bets with other humans, even though your dad did offer to bet you a bottle of whiskey that you'd be one of them too smart alecky fools who walked away with only $1,000.&nbsp; You responded by pre-committing to keep any winnings you make between you and your banker, and to never let him know.</p>\n<p>The only relevant physical object you've brought along is a radioactive decay based random number generator, that Omega would have been unable to predict the result of in advance, just in case you decide to use it as a factor in your choice.&nbsp; It isn't a coin, giving only a 50% chance of \"one box\" and a 50% chance of \"two box\".&nbsp;&nbsp; You can set arbitrary odds (tell it to generate a random integer between 0 and any positive integer you give it, up to 10 to the power of 100).&nbsp;&nbsp; Omega said in his spiel the phrase \"too large a random element\" but didn't specify where that boundary was.</p>\n<p>What do you do?&nbsp;&nbsp; Or, given that such a situation doesn't exist yet, and we're talking about a Fred in a possible future, what advice would you give to Fred on how to choose, were he to ever end up in such a situation?</p>\n<p>Pick \"one box\"?&nbsp;&nbsp; Pick \"two box\"?&nbsp;&nbsp; Or pick randomly between those two choices and, if so, at what odds?</p>\n<p>And why?</p>\n<hr />\n<p>&nbsp;</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Part 1 - stating the problem<br />next &nbsp; <a href=\"/r/discussion/lw/fr6/a_solvable_newcomblike_problem_part_2_of_3/\">Part 2 - some mathematics</a><br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href=\"/r/discussion/lw/fr7/a_solvable_newcomblike_problem_part_3_of_3/\">Part 3 - towards a solution</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "FTG8G5wxhcJyqunok", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0487805563737209e-06, "legacy": true, "legacyId": "20353", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["vQsgYhpHD5M9FTytD", "dT9RFro8Prdanvdo8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-03T16:49:38.161Z", "modifiedAt": null, "url": null, "title": "A solvable Newcomb-like problem - part 2 of 3", "slug": "a-solvable-newcomb-like-problem-part-2-of-3", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:07.903Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Douglas_Reay", "createdAt": "2012-02-19T14:40:26.403Z", "isAdmin": false, "displayName": "Douglas_Reay"}, "userId": "jpnrRPxHozDiGBqp2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vQsgYhpHD5M9FTytD/a-solvable-newcomb-like-problem-part-2-of-3", "pageUrlRelative": "/posts/vQsgYhpHD5M9FTytD/a-solvable-newcomb-like-problem-part-2-of-3", "linkUrl": "https://www.lesswrong.com/posts/vQsgYhpHD5M9FTytD/a-solvable-newcomb-like-problem-part-2-of-3", "postedAtFormatted": "Monday, December 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20solvable%20Newcomb-like%20problem%20-%20part%202%20of%203&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20solvable%20Newcomb-like%20problem%20-%20part%202%20of%203%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvQsgYhpHD5M9FTytD%2Fa-solvable-newcomb-like-problem-part-2-of-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20solvable%20Newcomb-like%20problem%20-%20part%202%20of%203%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvQsgYhpHD5M9FTytD%2Fa-solvable-newcomb-like-problem-part-2-of-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvQsgYhpHD5M9FTytD%2Fa-solvable-newcomb-like-problem-part-2-of-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1670, "htmlBody": "<p>This is the second part of a three post sequence on a problem that is similar to <a href=\"http://wiki.lesswrong.com/wiki/Newcomb%27s_problem\">Newcomb's problem</a> but is posed in terms of probabilities and limited knowledge.</p>\n<p>&nbsp;&nbsp; Part 1 - stating the problem<br />&nbsp;&nbsp; Part 2 - some mathematics<br />&nbsp;&nbsp; Part 3 - towards a solution</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>In game theory, a payoff matrix is a way of presenting the results of two players simultaneously picking options.</p>\n<p>For example, in the Prisoner's Dilemma, Player A gets to choose between option A1 (Cooperate) and option A2 (Defect) while, at the same time Player B gets to choose between option B1 (Cooperate) and option B2 (Defect).&nbsp;&nbsp; Since years spent in prison are a negative outcome, we'll write them as negative numbers:</p>\n<p><img src=\"http://www.chiark.greenend.org.uk/~douglasr/images/prisoners_dilemma_payoff_matrix2.jpg\" alt=\"payoff\" width=\"250\" height=\"250\" /></p>\n<p>So, if you look at the bottom right hand corner, at the intersection of Player A defecting (A2) and Player B defecting (B2) we see that both players end up spending 4 years in prison.&nbsp;&nbsp; Whereas, looking at the bottom left we see that if A defects and B cooperates, then Player A ends up spending 0 years in prison and Player B ends up spending 5 years in prison.</p>\n<p>Another familiar example we can present in this form is the game <a href=\"http://www.netlaputa.ne.jp/~tokyo3/e/janken_e.html\">Rock-Paper-Scissors</a>.</p>\n<p>We could write it as a zero sum game, with a win being worth 1, a tie being worth 0 and a loss being worth -1:</p>\n<p>But it doesn't change the mathematics if we give both players 2 points each round just for playing, so that a win becomes worth 3 points, a tie becomes worth 2 points and a loss becomes worth 1 point.&nbsp; (Think of it as two players in a game show being rewarded by the host, rather than the players making a direct bet with each other.)</p>\n<p><img src=\"http://www.chiark.greenend.org.uk/~douglasr/images/payoff_matrix4.jpg\" alt=\"\" width=\"375\" height=\"375\" /></p>\n<p>If you are Player A, and you are playing against a Player B who always chooses option B1 (Rock), then your strategy is clear.&nbsp; You choose option A2 (Paper) each time.&nbsp; Over 10 rounds, you'd expect to end up with $30 compared to B's $10.</p>\n<p>Let's imagine a slightly more sophisticated Player B, who always picks Rock in the first round, and then for all other rounds picks whatever would beat Player A's choice the previous round.&nbsp;&nbsp; This strategy would do well against someone who always picked the same option each round, but it is deterministic and, if we guess it correctly in advance, we can design a strategy that beats it every time.&nbsp; (In this case, picking Paper-Rock-Scissors then repeating back to Paper). &nbsp; In fact whatever strategy B comes up with, if that strategy is deterministic and we guess it in advance, then we end up with $30 and B ends up with $10.</p>\n<p>What if B has a deterministic strategy that B picked in advance and doesn't change, but we don't know at the start of the first round what it is?&nbsp;&nbsp; In theory B might have picked any of the 3-to-the-power-of-10 deterministic strategies that are indistinguishable from each other over a 10 round duel but, in practice, humans tend to favour some strategies over others so, if you know humans and the game of Rock-Paper-Scissors better than Player B does, you have a better than even chance of guessing his pattern and coming out ahead in the later rounds of the duel.</p>\n<p>But there's a danger to that.&nbsp; What if you have overestimated your comparative knowledge level and Player B uses your overconfidence to lure you into thinking you've cracked B's pattern, while really B is laying a trap, increasing the predictability of Player A's moves so Player B can then take advantage of that to work out which moves will trump them?&nbsp; This works better in a game like poker, where the stakes are not the same each round, but it is still possible in Rock-Paper-Scissors, and you can imagine variants of the game where the host varies payoff matrix by increasing the lose-tie-win rewards from 1,2,3 in the first round, to 2,4,6 in the second round, 3,6,9 in the third round, and so on.</p>\n<p>This is why the safest strategy is to not to have a deterministic strategy but, instead, use a source of random bits to each round pick option 1 with a probability of 33%, option 2 with a probability of 33% or option 3 with a probability of 33% (modulo rounding).&nbsp; You might not get to take advantage of any predictability that becomes apparent in your opponents strategy, but neither can you be fooled into becoming predictable yourself.</p>\n<p>On a side note, this still applies even when there is only one round, because unaided humans are not as good at coming up with random bits as they think they are.&nbsp; Someone who has observed many first time players will notice that first time players more often than not choose as their Rock as their 'random' first move, rather than Paper or Scissors.&nbsp; If such a person were confident that they were playing a first time player, they might therefore pick Paper as their first move more frequently than not.&nbsp; Things soon get very Sicilian (in the sense of <a href=\"https://www.youtube.com/watch?v=E2y40U2LvKY\">the duel between Westley and Vizzini in the film The Princess Bride</a>) after that, because a yet more sophisticated player who guessed their opponent would try this, could then pick Scissors.&nbsp; And so ad infinitum, with ever more implausible levels of discernment being required to react on the next level up.</p>\n<p>We can imagine a tournament set up between 100 players taken randomly from the expertise distribution of game players, each player submitting a python program that always plays the same first move, and for each of the remaining 9 rounds produces a move determined solely by the the moves so far in that duel.&nbsp; The tournament organiser would then run every player's program once against the programs of each of the other 99 players, so on average each player would collect 99x10x2 = $1,980</p>\n<p>We could make things more complex by allowing the programs to use, as an input, how much money their opponent has won so far during the tournament; or iterate over running the tournament several times, to give each player an 'expertise' rating which the program in the following tournament could then use.&nbsp; We could allow the tournament host to subtract from each player a sum of money depending upon the size of program that player submitted (and how much memory or cpu it used).&nbsp;&nbsp; We could give each player a limited ration of random bits, so when facing a player with a higher expertise rating they might splurge and make their move on all 10 rounds completely random, and when facing a player with a lower expertise they might conserve their supply by trying to 'out think' them.</p>\n<p>There are various directions we could take this, but the one I want to look at here is what happens when you make the payoff matrix asymmetric.&nbsp; What happens if you make the game unfair, so not only does one player have more at stake than the other player, but the options are not even either, for example:</p>\n<p><img src=\"http://www.chiark.greenend.org.uk/~douglasr/images/payoff_matrix5.jpg\" alt=\"\" /></p>\n<p>You still have the circular Rock-Paper-Scissors dynamic where:<br />&nbsp;&nbsp; If B chose B3, then A wants most to have chosen A1<br />&nbsp;&nbsp; If A chose A1, then B wants most to have chosen B2<br />&nbsp;&nbsp; If B chose B2, then A wants most to have chosen A3<br />&nbsp;&nbsp; If A chose A3, then B wants most to have chosen B1<br />&nbsp;&nbsp; If B chose B1, then A wants most to have chosen A2<br />&nbsp;&nbsp; If A chose A2, then B wants most to have chosen B3</p>\n<p>so everything wins against at least one other option, and loses against at least one other option.&nbsp;&nbsp; However Player B is clearly now in a better position, because B wins ties, and B's wins (a 9, an 8 and a 7) tend to be larger than A's wins (a 9, a 6 and a 6).</p>\n<p>What should Player A do?&nbsp; Is the optimal safe strategy still to pick each option with an equal weighting?</p>\n<p>Well, it turns out the answer is: no, an equal weighting isn't the optimal response.&nbsp;&nbsp; Neither is just picking the same 'best' option each time.&nbsp; Instead what do you is pick your 'best' option a bit more frequently than an equal weighting would suggest, but not so much that the opponent can steal away that gain by reliably choosing the specific option that trumps yours.&nbsp;&nbsp; Rather than duplicate material already well presented on the web, I will point you at two lecture courses on game theory that explain how to calculate the exact probability to assign to each option:</p>\n<ul>\n<li>\"<a href=\"http://oyc.yale.edu/economics/econ-159#sessions\">ECON 159: Game Theory</a>\", from Open Yale Courses</li>\n<li>\"<a href=\"http://www.youtube.com/course?list=ECKI1h_nAkaQoDzI4xDIXzx6U2ergFmedo\">Game Theory 101: The Complete Series</a>\", by William Spaniel</li>\n</ul>\n<p>You do this by using the indifference theorem to arrive at a set of linear equations, which you can then solve to arrive at a mixed equilibrium where neither player increases their expected utility by altering the probability weightings they assign to their options.</p>\n<ul>\n<li><a href=\"http://www.eprisner.de/MAT109/Mixedb.html#3.2\">Example of calculating the general case for a 3x3 payoff matrix</a></li>\n<li><a href=\"http://www.eprisner.de/MAT109/AnalysisVNMPOKER.html#2.1\">More complex example, drawn from poker</a></li>\n<li><a href=\"http://people.hofstra.edu/stefan_waner/RealWorld/Summary4.html#min\">Summary</a></li>\n</ul>\n<p>&nbsp;</p>\n<h2><span style=\"color: #ff6600;\">The TL;DR; points to take away</span><br /></h2>\n<p>If you are competing in what is effectively a simultaneous option choice game, with a being who you suspect may have an equal or higher expertise to you at the game, you can nullify their advantage by picking a strategy that, each round chooses randomly (using a weighting) between the available options.</p>\n<p>Depending upon the details of the payoff matrix, there may be one option that it makes sense for you to pick most of the time but, unless that option is strictly better than all your other choices no matter what option your opponent picks, there is still utility to gain from occasionally picking the other options in order to keep your opponent on their toes.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>&nbsp; Back to <a href=\"/r/discussion/lw/fpd/a_solvable_newcomblike_problem_part_1_of_3/\">Part 1 - stating the problem</a><br />&nbsp; This is&nbsp; Part 2 - some mathematics<br />&nbsp; Next to <a href=\"/r/discussion/lw/fr7/a_solvable_newcomblike_problem_part_3_of_3/\">Part 3 - towards a solution</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vQsgYhpHD5M9FTytD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 1.049033702633194e-06, "legacy": true, "legacyId": "20418", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FTG8G5wxhcJyqunok", "dT9RFro8Prdanvdo8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-03T17:10:54.794Z", "modifiedAt": null, "url": null, "title": "Most Likely Cause of an Apocalypse on December 21", "slug": "most-likely-cause-of-an-apocalypse-on-december-21", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:17.693Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DeevGrape", "createdAt": "2011-11-02T19:26:47.963Z", "isAdmin": false, "displayName": "DeevGrape"}, "userId": "m9JPq6irpKromu9x9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9zEDNh83qWeiyycdP/most-likely-cause-of-an-apocalypse-on-december-21", "pageUrlRelative": "/posts/9zEDNh83qWeiyycdP/most-likely-cause-of-an-apocalypse-on-december-21", "linkUrl": "https://www.lesswrong.com/posts/9zEDNh83qWeiyycdP/most-likely-cause-of-an-apocalypse-on-december-21", "postedAtFormatted": "Monday, December 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Most%20Likely%20Cause%20of%20an%20Apocalypse%20on%20December%2021&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMost%20Likely%20Cause%20of%20an%20Apocalypse%20on%20December%2021%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9zEDNh83qWeiyycdP%2Fmost-likely-cause-of-an-apocalypse-on-december-21%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Most%20Likely%20Cause%20of%20an%20Apocalypse%20on%20December%2021%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9zEDNh83qWeiyycdP%2Fmost-likely-cause-of-an-apocalypse-on-december-21", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9zEDNh83qWeiyycdP%2Fmost-likely-cause-of-an-apocalypse-on-december-21", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 280, "htmlBody": "<p>Note: This post is almost completely tongue-in-cheek. Obviously the chances of December 21, 2012 heralding in an&nbsp;apocalypse, definable maybe as an event causing billions of deaths and/or global catastrophic infrastructure damage, are slim to none.</p>\n<p>But they aren't actually none.</p>\n<p>Let's say there's a 5% chance of a superhuman General AI being developed in the next 10 years and ushering in the singularity. Let's say 4/5 of those scenarios would lead to a Bad End which could reasonably be called an apocalypse (or an \"AI-pocalypse\", perhaps). And let's say the distribution of probability isn't linear, but is exponentially skewed somehow so that given the emergence of AI in the next 120 months, the chances of it happening in this very month are 1 in 100,000. Then let's divide that by 30 so we can get our apocalypse rolling on the right day.</p>\n<p>5/100 * 4/5 * 1/100000 * 1/30 = 1 in 75,000,000</p>\n<p>Admittedly, low odds. This comports with the fact that there would have to be a lot of unknown development progress being made already on the problem for an intelligence explosion to be anywhere on the horizon.</p>\n<p>But compared to some of the scenarios debunked by NASA (http://www.space.com/18678-2012-mayan-apocalypse-fears-nasa.html), such as a collision with the rogue planet Nibaru, or Earth being sucked into the supermassive black hole in the center of the Milky Way 30,000 light years away, the AI-doomsday scenario starts to seem relatively plausible.</p>\n<p>I think the only other (relatively) plausible contenders would be the release of a pandemic-causing biological weapon, or the start of an international nuclear war. I haven't done any Fermi calculations on those, but I'm sure their probability exceeds that of solar flares scourging the surface of the Earth.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9zEDNh83qWeiyycdP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": -1, "extendedScore": null, "score": 1.0490458674915363e-06, "legacy": true, "legacyId": "20422", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-03T19:35:06.045Z", "modifiedAt": null, "url": null, "title": "[LINK] AmA by computational neuroscientists behind 'the world's largest functional brain model'", "slug": "link-ama-by-computational-neuroscientists-behind-the-world-s", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:28.846Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "bfq5YorFxpih9j6nL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RxRR9DiRm6HGo5Zvv/link-ama-by-computational-neuroscientists-behind-the-world-s", "pageUrlRelative": "/posts/RxRR9DiRm6HGo5Zvv/link-ama-by-computational-neuroscientists-behind-the-world-s", "linkUrl": "https://www.lesswrong.com/posts/RxRR9DiRm6HGo5Zvv/link-ama-by-computational-neuroscientists-behind-the-world-s", "postedAtFormatted": "Monday, December 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20AmA%20by%20computational%20neuroscientists%20behind%20'the%20world's%20largest%20functional%20brain%20model'&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20AmA%20by%20computational%20neuroscientists%20behind%20'the%20world's%20largest%20functional%20brain%20model'%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRxRR9DiRm6HGo5Zvv%2Flink-ama-by-computational-neuroscientists-behind-the-world-s%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20AmA%20by%20computational%20neuroscientists%20behind%20'the%20world's%20largest%20functional%20brain%20model'%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRxRR9DiRm6HGo5Zvv%2Flink-ama-by-computational-neuroscientists-behind-the-world-s", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRxRR9DiRm6HGo5Zvv%2Flink-ama-by-computational-neuroscientists-behind-the-world-s", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 152, "htmlBody": "<p>Not sure if this has been covered on LW, but it seems highly relevant to WBE development. Link here:</p>\n<p><a href=\"http://www.reddit.com/r/IAmA/comments/147gqm/we_are_the_computational_neuroscientists_behind/  \">http://www.reddit.com/r/IAmA/comments/147gqm/we_are_the_computational_neuroscientists_behind/</a></p>\n<p>A few questioners mention the Singularity and make Skynet jokes.<br /><br />The abstract from their <a href=\"http://www.sciencemag.org/content/338/6111/1202\">paper</a> in Science:</p>\n<blockquote>\n<p>A central challenge for cognitive and systems neuroscience is to relate the incredibly complex behavior of animals to the equally complex activity of their brains. Recently described, large-scale neural models have not bridged this gap between neural activity and biological function. In this work, we present a 2.5-million-neuron model of the brain (called &ldquo;Spaun&rdquo;) that bridges this gap by exhibiting many different behaviors. The model is presented only with visual image sequences, and it draws all of its responses with a physically modeled arm. Although simplified, the model captures many aspects of neuroanatomy, neurophysiology, and psychological behavior, which we demonstrate via eight diverse tasks.</p>\n</blockquote>\n<p>I'm curious to see LWers' perspectives on the project.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RxRR9DiRm6HGo5Zvv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 12, "extendedScore": null, "score": 1.049128310801271e-06, "legacy": true, "legacyId": "20423", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-03T19:37:49.972Z", "modifiedAt": null, "url": null, "title": "Factions, inequality, and social justice", "slug": "factions-inequality-and-social-justice", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:05.945Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/po6mCyhGTSGELzo9E/factions-inequality-and-social-justice", "pageUrlRelative": "/posts/po6mCyhGTSGELzo9E/factions-inequality-and-social-justice", "linkUrl": "https://www.lesswrong.com/posts/po6mCyhGTSGELzo9E/factions-inequality-and-social-justice", "postedAtFormatted": "Monday, December 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Factions%2C%20inequality%2C%20and%20social%20justice&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFactions%2C%20inequality%2C%20and%20social%20justice%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpo6mCyhGTSGELzo9E%2Ffactions-inequality-and-social-justice%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Factions%2C%20inequality%2C%20and%20social%20justice%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpo6mCyhGTSGELzo9E%2Ffactions-inequality-and-social-justice", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpo6mCyhGTSGELzo9E%2Ffactions-inequality-and-social-justice", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1765, "htmlBody": "<p>Robin Hanson <a href=\"http://www.overcomingbias.com/2010/05/selective-anti-discrimination.html\">has</a> <a href=\"http://www.overcomingbias.com/2009/09/explaining-unequal-inequality-aversion.html\">wondered</a> why folks seem concerned about inequality based on some stuff, like race, gender, sexuality, ethnicity, and disability, but not other stuff, like <a href=\"http://en.wikipedia.org/wiki/Height_discrimination\">height</a>, <a href=\"http://www.amazon.com/Looks-They-Matter-More-Imagined/dp/0814480543\">appearance</a>, <a href=\"http://en.wikipedia.org/wiki/Heritability_of_IQ\">intelligence</a>, <a href=\"http://online.wsj.com/article/SB10001424052748703712504576242701752957910.html\">sleep</a>, <a href=\"/lw/ezd/open_thread_october_1631_2012/7n2q\">conscientiousness</a>, and perhaps most importantly, <a href=\"http://www.forbes.com/2004/09/23/cx_mh_0923happiness.html\">happiness</a>.</p>\n<p>My explanation: Race, gender, sexuality, ethnicity, and disability are fairly discrete ways of classifying people.&nbsp; Most people (though not all) can be categorized fairly neatly in to a single race, birth gender, desired gender, and sexual orientation.&nbsp; By contrast, looks, smarts, and happiness all vary in a continuous fashion.&nbsp; For some/all of these characteristics, there's a bell curve--many people in the middle and fewer people at the extremes.</p>\n<p>Why should this matter?&nbsp; One of the things that comes up a lot on this blog is how irrational people tend to get when talking about politics.&nbsp; For a really rousing political argument, you need group identification--hence the Greens and the Blues in Eliezer's <a href=\"/lw/gt/a_fable_of_science_and_politics/\">original politics essay</a>.&nbsp; How would that essay be different if everyone was a different shade of turquoise, somewhere on a continuum between blue and green?</p>\n<p>Or here's another thought experiment: Instead of height varying in a continuous fashion, everyone in society is either a Tall or a Short.&nbsp; Just like in the real world, taller people tend to make more money and assume more leadership roles, but instead of saying \"he got the job because he's taller\", people would say \"he got the job because he's a Tall\".&nbsp; How well do you think society would handle <em>this</em>?&nbsp; (BTW, looks like <a href=\"http://www.sciencedirect.com/science/article/pii/S0024320502025031\">shorter people live longer</a>, so height inequality may actually favor shorter folks on balance.)</p>\n<p>&nbsp;</p>\n<h2>Speculation on factional conflict and social distance<br /></h2>\n<p>The hypotheses I'm advancing here is that an argument between, say, a radical feminist and a men's rights activist, is not all that different <em>in kind</em> from an argument between a Democrat and a Republican, an Israeli and a Palestinian, or a Hutu and a Tutsi.&nbsp; (Regarding gender in particular: factional conflicts may be harder to spot when <em>almost every human</em> is a member of one relevant faction or another. &nbsp;There's no big, neutral third party to say stuff like \"wow, this is getting kinda out of hand\".)</p>\n<p>Why do people think about competing factions this way?&nbsp; Well, humans are social animals that evolved to live in tribes.&nbsp; Much <a href=\"http://www.amazon.com/War-Before-Civilization-Peaceful-Savage/dp/0195119126/ref=sr_1_1?ie=UTF8&amp;qid=1354000276&amp;sr=8-1&amp;keywords=war+before+civilization\">hunter-gatherer violence</a> was inter-tribal.</p>\n<p>The more iterated a prisoner's dilemma is, the more it makes sense to lean towards cooperation. &nbsp;Dilemmas between fellow tribe members would be highly iterated; dilemmas between tribes less so. &nbsp;Additionally, people of different factions would likely have lower genetic similarity--note family feuds, for example.</p>\n<p>The below XKCD comic illustrates what I suspect may be a resulting fact about human nature: we're less likely to have friendly, empathetic feelings for those who seem distant or abstract.</p>\n<p><img src=\"http://imgs.xkcd.com/comics/internet_argument.png \" alt=\"\" width=\"460\" height=\"693\" /></p>\n<p>More evidence for this idea:</p>\n<ul>\n<li>The book <em>Bargaining for Advantage</em> cites studies showing that negotiations via email face an increased risk of impasse. &nbsp;This effect can be mitigated by giving email-based negotiators each others' photos and explicitly instructing them to \"schmooze\" and share information on hobbies, families, etc.</li>\n<li>In Randall Collins' book <em>Violence: A Micro-sociological Theory</em>, he writes that infantry in close combat use their&nbsp;weapons at a lower rate than artillery/snipers.&nbsp; Also, \"Battle victors hate to see the eyes of the enemy they are killing... Eye-to-eye confrontations, however truncated, between holdup man and victim, appear to be unbearable for the gunman to sustain.\" &nbsp;(<a href=\"http://entitledtoanopinion.wordpress.com/2009/10/14/nearfar-violence/\">HT TGGP</a>.)</li>\n<li>The <a href=\"http://en.wikipedia.org/wiki/Identifiable_victim_effect\">identifiable victim effect</a>--people are more willing to donate to charity when presented with a single, identifiable victim than casualty statistics.</li>\n<li>It <a href=\"http://www.pewresearch.org/fact-tank/2015/05/14/where-web-surveys-produce-different-results-than-phone-interviews/\">seems</a>&nbsp;people have a tendency to give more nice/socially desirable answers to phone surveys than web surveys.</li>\n</ul>\n<ul>\n</ul>\n<div>This may be a way to optimize for cooperation among those who are members of our clan/tribe while making it easier to defect against tribal outsiders.</div>\n<div><br /></div>\n<div>But this is the modern world... we've got plenty of food and competing with outsiders is useless!&nbsp; There's no real reason not to feel deep compassion for everyone you talk to, and doesn't it feel great?&nbsp; Unfortunately, <a href=\"http://artofmanliness.com/2010/07/25/our-disembodied-selves-and-the-decline-of-empathy/\">seeing people face-to-face may be critical for empathizing with them</a>, and face-to-face communication through web pages is rare.&nbsp; If this theory is true, it could go a long way towards explaining how dysfunctional gender conversations often are online.</div>\n<div><br /></div>\n<div><br /></div>\n<h2>Speculation on equality and fairness</h2>\n<p>Why does so much inter-factional conflict focus on issues of inequality?&nbsp; Robin Hanson's&nbsp;<a href=\"http://www.overcomingbias.com/2007/02/unequal_inequal.html\">guess</a>: \"our distant ancestors got into the habit of complaining about inequality of transferable assets with a tribe, as a way to coordinate a veiled threat to take those assets if they were not offered freely.\" &nbsp;I'm not sure agree with this 100%. &nbsp;I suspect that inequality between tribes (e.g. over access to salt, as mentioned in <em>War Before Civilization</em>) may have been a bigger issue than within-tribe inequality. &nbsp;And \"veiled threat\" doesn't seem quite right--I suspect the feeling that <em>things are unfair and I deserve more</em>&nbsp;is just a signal for me to take stuff from others, in the same way hunger is a signal for me to eat stuff. &nbsp;(Yes, many of us modern humans are generally pretty good about suppressing this \"things are unfair\" instinct, but that doesn't mean our ancestors were.)</p>\n<p>To replicate one's genes, it's useful to have certain things like food, water, and respect. &nbsp;But there's not always an unlimited supply of this stuff, and the only way to get more of it may be to get others to give it to you. &nbsp;You could forcefully demand all of it, but if there's more than one person doing that, you're liable to trigger an expensive fight (or lose your credibility for making forceful demands). &nbsp;Additionally, demanding <em>all</em> of a resource will trigger fiercer resistance from others, who really need at least <em>some</em> of it. &nbsp;Demanding that you get one Nth of some important resource, where N is the number of people fighting over the resource, is a strategy that won't cause you to come in to conflict with others doing the same.</p>\n<p>Equitable distribution doesn't necessarily maximize collective utility, however. &nbsp;A world where the supply of disposable diapers was distributed equitably would have substantially less utility than the current world, where disposable diapers are concentrated in the hands of people who have young children.</p>\n<p>&nbsp;</p>\n<h2>From fairness to empathy</h2>\n<p>Pat and Jesse are roommates. &nbsp;Pat enjoys doing something that also happens to annoy Jesse. &nbsp;Consider the following two scenarios:</p>\n<p>Scenario A: Jesse asserts a right to not be annoyed. &nbsp;Pat responds by accusing Jesse of being oversensitive and asserts a right to continue with the activity.</p>\n<p>Scenario B: Jesse shares preferences. &nbsp;Pat shares preferences. &nbsp;They work together to find the solution that satisfies their collective preferences maximally. &nbsp;This solution could involve a behavioral change on Pat's part, a behavioral change on Jesse's part, some combination, or Jesse just learning to live with the activity. &nbsp;Throughout their discussion, they make it clear to each other that they respect and like one another and care about each others' preferences. &nbsp;They don't worry too much about who is \"giving in\" or \"making a concession\", and are careful to make requests rather than demands.</p>\n<p>I think most people would agree that Scenario B is ideal. &nbsp;Unfortunately, many modern conversations about social justice look more like Scenario A. &nbsp;It's common, for instance, for members of different groups to argue about <a href=\"http://www.overcomingbias.com/2008/07/break-it-down.html\">who has it worse</a>. &nbsp;This seems like a failure mode for a number of reasons:</p>\n<ul>\n<li>Speaking for myself, I know that the problems I am experiencing myself are way more salient in my mind than the problems I hear that others have, and I tend to <a href=\"http://en.wikipedia.org/wiki/False-consensus_effect\">assume my problems are more widespread than they really are</a>. &nbsp;So even if one party <em>does</em> have things worse, this may not be clear to the other party unless they correct internally for these effects.</li>\n<li>If my problems weren't salient already, complaining about them to someone else in an emotionally heated argument will ensure that they become salient. &nbsp;Yay, problems I'm constantly thinking about!</li>\n<li>When others ignore your complaints about your own problems and focus on their own, that causes you to become resentful. &nbsp;If they're not going to listen to you, why should you listen to them? &nbsp;The result is that everyone complains, getting increasingly resentful, and no one listens.</li>\n<li>And finally, figuring out who has it worse doesn't immediately take steps towards achieving anyone's preferences.</li>\n</ul>\n<p>This dialogue <em>may</em> be better than nothing, just because preferences <em>do</em> end up getting communicated. &nbsp;But I think we can improve on it. &nbsp;In particular, listening to other peoples' preferences seems pretty key for priming cooperative, friendly behavior and getting them to consider yours. &nbsp;\"Seek first to understand, then to be understood.\" &nbsp;(Note: This can be tough.)</p>\n<p>To give a concrete example: Maybe women have a preference for not being <a href=\"/lw/4vj/a_rationalists_account_of_objectification/\">objectified sexually</a> that men don't share. &nbsp;Men can respect and work to achieve that preference even if they don't share it--empathy over fairness!</p>\n<p>Of course, if priming cooperative behavior fails to work, continued loud complaining may be optimal. &nbsp;Sometimes cycles of defection can't be broken.</p>\n<p>&nbsp;</p>\n<h2>Non-ideal brains</h2>\n<p>Several years ago, I was taking a political science class and the professor was discussing feminism. &nbsp;At one point, the (male) professor said something like \"But beyond all this, women are just <em>better</em> than men, and I think we all know it.\" &nbsp;This <a href=\"/lw/13s/the_nature_of_offense/\">offended</a> me, and I fumed to myself internally, thinking that if he'd made the opposite claim he'd probably get in trouble with the administration of the college, etc.</p>\n<p>You may already have an opinion on this issue, thinking that either I should not have gotten offended or the professor should not have made his statement. &nbsp;But lets look through this issue through the Pat and Jesse lens. &nbsp;I have a preference for people to respect me, and for me not to feel angry and offended. &nbsp;There are a few different ways for these preferences to be achieved. &nbsp;There's not necessarily a \"right\" way. &nbsp;Preferences about my misfiring fairness neurons are preferences like any others. &nbsp;(I'm living in a first-world country that's the product of thousands of years of technological development. &nbsp;I've got all the food I want. &nbsp;In theory, grabbing even <em>more</em> resources shouldn't be a very high priority, but my brain still wants to do it. &nbsp;Hence the \"misfiring\" allegation against my neurons.)</p>\n<p>Unfortunately, we all have these unwanted instincts. &nbsp;I think it makes sense to try to avoid activating the instincts in both yourself and others. &nbsp;I'm glad that saying \"People of Group X suck\" is considered worse than saying \"You suck\"--Western society has developed some useful memetic antibodies to nip inter-factional conflicts in the bud.</p>\n<p>&nbsp;</p>\n<p><em>Thanks to <a href=\"/user/HughRistik/\">HughRistik</a> for offering feedback on this post.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"FkzScn5byCs9PxGsA": 1, "DdgSyQoZXjj3KnF4N": 1, "W9aNkPwtPhMrcfgj7": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "po6mCyhGTSGELzo9E", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 44, "extendedScore": null, "score": 0.000104, "legacy": true, "legacyId": "20297", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 44, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": {"html": "<p>Robin Hanson <a href=\"http://www.overcomingbias.com/2010/05/selective-anti-discrimination.html\">has</a> <a href=\"http://www.overcomingbias.com/2009/09/explaining-unequal-inequality-aversion.html\">wondered</a> why folks seem concerned about inequality based on some stuff, like race, gender, sexuality, ethnicity, and disability, but not other stuff, like <a href=\"http://en.wikipedia.org/wiki/Height_discrimination\">height</a>, <a href=\"http://www.amazon.com/Looks-They-Matter-More-Imagined/dp/0814480543\">appearance</a>, <a href=\"http://en.wikipedia.org/wiki/Heritability_of_IQ\">intelligence</a>, <a href=\"http://online.wsj.com/article/SB10001424052748703712504576242701752957910.html\">sleep</a>, <a href=\"/lw/ezd/open_thread_october_1631_2012/7n2q\">conscientiousness</a>, and perhaps most importantly, <a href=\"http://www.forbes.com/2004/09/23/cx_mh_0923happiness.html\">happiness</a>.</p>\n<p>My explanation: Race, gender, sexuality, ethnicity, and disability are fairly discrete ways of classifying people.&nbsp; Most people (though not all) can be categorized fairly neatly in to a single race, birth gender, desired gender, and sexual orientation.&nbsp; By contrast, looks, smarts, and happiness all vary in a continuous fashion.&nbsp; For some/all of these characteristics, there's a bell curve--many people in the middle and fewer people at the extremes.</p>\n<p>Why should this matter?&nbsp; One of the things that comes up a lot on this blog is how irrational people tend to get when talking about politics.&nbsp; For a really rousing political argument, you need group identification--hence the Greens and the Blues in Eliezer's <a href=\"/lw/gt/a_fable_of_science_and_politics/\">original politics essay</a>.&nbsp; How would that essay be different if everyone was a different shade of turquoise, somewhere on a continuum between blue and green?</p>\n<p>Or here's another thought experiment: Instead of height varying in a continuous fashion, everyone in society is either a Tall or a Short.&nbsp; Just like in the real world, taller people tend to make more money and assume more leadership roles, but instead of saying \"he got the job because he's taller\", people would say \"he got the job because he's a Tall\".&nbsp; How well do you think society would handle <em>this</em>?&nbsp; (BTW, looks like <a href=\"http://www.sciencedirect.com/science/article/pii/S0024320502025031\">shorter people live longer</a>, so height inequality may actually favor shorter folks on balance.)</p>\n<p>&nbsp;</p>\n<h2 id=\"Speculation_on_factional_conflict_and_social_distance\">Speculation on factional conflict and social distance<br></h2>\n<p>The hypotheses I'm advancing here is that an argument between, say, a radical feminist and a men's rights activist, is not all that different <em>in kind</em> from an argument between a Democrat and a Republican, an Israeli and a Palestinian, or a Hutu and a Tutsi.&nbsp; (Regarding gender in particular: factional conflicts may be harder to spot when <em>almost every human</em> is a member of one relevant faction or another. &nbsp;There's no big, neutral third party to say stuff like \"wow, this is getting kinda out of hand\".)</p>\n<p>Why do people think about competing factions this way?&nbsp; Well, humans are social animals that evolved to live in tribes.&nbsp; Much <a href=\"http://www.amazon.com/War-Before-Civilization-Peaceful-Savage/dp/0195119126/ref=sr_1_1?ie=UTF8&amp;qid=1354000276&amp;sr=8-1&amp;keywords=war+before+civilization\">hunter-gatherer violence</a> was inter-tribal.</p>\n<p>The more iterated a prisoner's dilemma is, the more it makes sense to lean towards cooperation. &nbsp;Dilemmas between fellow tribe members would be highly iterated; dilemmas between tribes less so. &nbsp;Additionally, people of different factions would likely have lower genetic similarity--note family feuds, for example.</p>\n<p>The below XKCD comic illustrates what I suspect may be a resulting fact about human nature: we're less likely to have friendly, empathetic feelings for those who seem distant or abstract.</p>\n<p><img src=\"http://imgs.xkcd.com/comics/internet_argument.png \" alt=\"\" width=\"460\" height=\"693\"></p>\n<p>More evidence for this idea:</p>\n<ul>\n<li>The book <em>Bargaining for Advantage</em> cites studies showing that negotiations via email face an increased risk of impasse. &nbsp;This effect can be mitigated by giving email-based negotiators each others' photos and explicitly instructing them to \"schmooze\" and share information on hobbies, families, etc.</li>\n<li>In Randall Collins' book <em>Violence: A Micro-sociological Theory</em>, he writes that infantry in close combat use their&nbsp;weapons at a lower rate than artillery/snipers.&nbsp; Also, \"Battle victors hate to see the eyes of the enemy they are killing... Eye-to-eye confrontations, however truncated, between holdup man and victim, appear to be unbearable for the gunman to sustain.\" &nbsp;(<a href=\"http://entitledtoanopinion.wordpress.com/2009/10/14/nearfar-violence/\">HT TGGP</a>.)</li>\n<li>The <a href=\"http://en.wikipedia.org/wiki/Identifiable_victim_effect\">identifiable victim effect</a>--people are more willing to donate to charity when presented with a single, identifiable victim than casualty statistics.</li>\n<li>It <a href=\"http://www.pewresearch.org/fact-tank/2015/05/14/where-web-surveys-produce-different-results-than-phone-interviews/\">seems</a>&nbsp;people have a tendency to give more nice/socially desirable answers to phone surveys than web surveys.</li>\n</ul>\n<ul>\n</ul>\n<div>This may be a way to optimize for cooperation among those who are members of our clan/tribe while making it easier to defect against tribal outsiders.</div>\n<div><br></div>\n<div>But this is the modern world... we've got plenty of food and competing with outsiders is useless!&nbsp; There's no real reason not to feel deep compassion for everyone you talk to, and doesn't it feel great?&nbsp; Unfortunately, <a href=\"http://artofmanliness.com/2010/07/25/our-disembodied-selves-and-the-decline-of-empathy/\">seeing people face-to-face may be critical for empathizing with them</a>, and face-to-face communication through web pages is rare.&nbsp; If this theory is true, it could go a long way towards explaining how dysfunctional gender conversations often are online.</div>\n<div><br></div>\n<div><br></div>\n<h2 id=\"Speculation_on_equality_and_fairness\">Speculation on equality and fairness</h2>\n<p>Why does so much inter-factional conflict focus on issues of inequality?&nbsp; Robin Hanson's&nbsp;<a href=\"http://www.overcomingbias.com/2007/02/unequal_inequal.html\">guess</a>: \"our distant ancestors got into the habit of complaining about inequality of transferable assets with a tribe, as a way to coordinate a veiled threat to take those assets if they were not offered freely.\" &nbsp;I'm not sure agree with this 100%. &nbsp;I suspect that inequality between tribes (e.g. over access to salt, as mentioned in <em>War Before Civilization</em>) may have been a bigger issue than within-tribe inequality. &nbsp;And \"veiled threat\" doesn't seem quite right--I suspect the feeling that <em>things are unfair and I deserve more</em>&nbsp;is just a signal for me to take stuff from others, in the same way hunger is a signal for me to eat stuff. &nbsp;(Yes, many of us modern humans are generally pretty good about suppressing this \"things are unfair\" instinct, but that doesn't mean our ancestors were.)</p>\n<p>To replicate one's genes, it's useful to have certain things like food, water, and respect. &nbsp;But there's not always an unlimited supply of this stuff, and the only way to get more of it may be to get others to give it to you. &nbsp;You could forcefully demand all of it, but if there's more than one person doing that, you're liable to trigger an expensive fight (or lose your credibility for making forceful demands). &nbsp;Additionally, demanding <em>all</em> of a resource will trigger fiercer resistance from others, who really need at least <em>some</em> of it. &nbsp;Demanding that you get one Nth of some important resource, where N is the number of people fighting over the resource, is a strategy that won't cause you to come in to conflict with others doing the same.</p>\n<p>Equitable distribution doesn't necessarily maximize collective utility, however. &nbsp;A world where the supply of disposable diapers was distributed equitably would have substantially less utility than the current world, where disposable diapers are concentrated in the hands of people who have young children.</p>\n<p>&nbsp;</p>\n<h2 id=\"From_fairness_to_empathy\">From fairness to empathy</h2>\n<p>Pat and Jesse are roommates. &nbsp;Pat enjoys doing something that also happens to annoy Jesse. &nbsp;Consider the following two scenarios:</p>\n<p>Scenario A: Jesse asserts a right to not be annoyed. &nbsp;Pat responds by accusing Jesse of being oversensitive and asserts a right to continue with the activity.</p>\n<p>Scenario B: Jesse shares preferences. &nbsp;Pat shares preferences. &nbsp;They work together to find the solution that satisfies their collective preferences maximally. &nbsp;This solution could involve a behavioral change on Pat's part, a behavioral change on Jesse's part, some combination, or Jesse just learning to live with the activity. &nbsp;Throughout their discussion, they make it clear to each other that they respect and like one another and care about each others' preferences. &nbsp;They don't worry too much about who is \"giving in\" or \"making a concession\", and are careful to make requests rather than demands.</p>\n<p>I think most people would agree that Scenario B is ideal. &nbsp;Unfortunately, many modern conversations about social justice look more like Scenario A. &nbsp;It's common, for instance, for members of different groups to argue about <a href=\"http://www.overcomingbias.com/2008/07/break-it-down.html\">who has it worse</a>. &nbsp;This seems like a failure mode for a number of reasons:</p>\n<ul>\n<li>Speaking for myself, I know that the problems I am experiencing myself are way more salient in my mind than the problems I hear that others have, and I tend to <a href=\"http://en.wikipedia.org/wiki/False-consensus_effect\">assume my problems are more widespread than they really are</a>. &nbsp;So even if one party <em>does</em> have things worse, this may not be clear to the other party unless they correct internally for these effects.</li>\n<li>If my problems weren't salient already, complaining about them to someone else in an emotionally heated argument will ensure that they become salient. &nbsp;Yay, problems I'm constantly thinking about!</li>\n<li>When others ignore your complaints about your own problems and focus on their own, that causes you to become resentful. &nbsp;If they're not going to listen to you, why should you listen to them? &nbsp;The result is that everyone complains, getting increasingly resentful, and no one listens.</li>\n<li>And finally, figuring out who has it worse doesn't immediately take steps towards achieving anyone's preferences.</li>\n</ul>\n<p>This dialogue <em>may</em> be better than nothing, just because preferences <em>do</em> end up getting communicated. &nbsp;But I think we can improve on it. &nbsp;In particular, listening to other peoples' preferences seems pretty key for priming cooperative, friendly behavior and getting them to consider yours. &nbsp;\"Seek first to understand, then to be understood.\" &nbsp;(Note: This can be tough.)</p>\n<p>To give a concrete example: Maybe women have a preference for not being <a href=\"/lw/4vj/a_rationalists_account_of_objectification/\">objectified sexually</a> that men don't share. &nbsp;Men can respect and work to achieve that preference even if they don't share it--empathy over fairness!</p>\n<p>Of course, if priming cooperative behavior fails to work, continued loud complaining may be optimal. &nbsp;Sometimes cycles of defection can't be broken.</p>\n<p>&nbsp;</p>\n<h2 id=\"Non_ideal_brains\">Non-ideal brains</h2>\n<p>Several years ago, I was taking a political science class and the professor was discussing feminism. &nbsp;At one point, the (male) professor said something like \"But beyond all this, women are just <em>better</em> than men, and I think we all know it.\" &nbsp;This <a href=\"/lw/13s/the_nature_of_offense/\">offended</a> me, and I fumed to myself internally, thinking that if he'd made the opposite claim he'd probably get in trouble with the administration of the college, etc.</p>\n<p>You may already have an opinion on this issue, thinking that either I should not have gotten offended or the professor should not have made his statement. &nbsp;But lets look through this issue through the Pat and Jesse lens. &nbsp;I have a preference for people to respect me, and for me not to feel angry and offended. &nbsp;There are a few different ways for these preferences to be achieved. &nbsp;There's not necessarily a \"right\" way. &nbsp;Preferences about my misfiring fairness neurons are preferences like any others. &nbsp;(I'm living in a first-world country that's the product of thousands of years of technological development. &nbsp;I've got all the food I want. &nbsp;In theory, grabbing even <em>more</em> resources shouldn't be a very high priority, but my brain still wants to do it. &nbsp;Hence the \"misfiring\" allegation against my neurons.)</p>\n<p>Unfortunately, we all have these unwanted instincts. &nbsp;I think it makes sense to try to avoid activating the instincts in both yourself and others. &nbsp;I'm glad that saying \"People of Group X suck\" is considered worse than saying \"You suck\"--Western society has developed some useful memetic antibodies to nip inter-factional conflicts in the bud.</p>\n<p>&nbsp;</p>\n<p><em>Thanks to <a href=\"/user/HughRistik/\">HughRistik</a> for offering feedback on this post.</em></p>", "sections": [{"title": "Speculation on factional conflict and social distance", "anchor": "Speculation_on_factional_conflict_and_social_distance", "level": 1}, {"title": "Speculation on equality and fairness", "anchor": "Speculation_on_equality_and_fairness", "level": 1}, {"title": "From fairness to empathy", "anchor": "From_fairness_to_empathy", "level": 1}, {"title": "Non-ideal brains", "anchor": "Non_ideal_brains", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "173 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 173, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6hfGNLf4Hg5DXqJCF", "NP8yar5gqNLsnjzZv", "QPqm5aj2meRmE7kR8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-03T22:04:47.099Z", "modifiedAt": null, "url": null, "title": "Two Anki plugins to reinforce reviewing (updated)", "slug": "two-anki-plugins-to-reinforce-reviewing-updated", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:33.952Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "D_Malik", "createdAt": "2011-01-05T12:45:17.182Z", "isAdmin": false, "displayName": "D_Malik"}, "userId": "9dhw3PngyAWKqTymS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Qkb6MrWBr48PLtXJu/two-anki-plugins-to-reinforce-reviewing-updated", "pageUrlRelative": "/posts/Qkb6MrWBr48PLtXJu/two-anki-plugins-to-reinforce-reviewing-updated", "linkUrl": "https://www.lesswrong.com/posts/Qkb6MrWBr48PLtXJu/two-anki-plugins-to-reinforce-reviewing-updated", "postedAtFormatted": "Monday, December 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Two%20Anki%20plugins%20to%20reinforce%20reviewing%20(updated)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATwo%20Anki%20plugins%20to%20reinforce%20reviewing%20(updated)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQkb6MrWBr48PLtXJu%2Ftwo-anki-plugins-to-reinforce-reviewing-updated%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Two%20Anki%20plugins%20to%20reinforce%20reviewing%20(updated)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQkb6MrWBr48PLtXJu%2Ftwo-anki-plugins-to-reinforce-reviewing-updated", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQkb6MrWBr48PLtXJu%2Ftwo-anki-plugins-to-reinforce-reviewing-updated", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 569, "htmlBody": "<p>This post is about two <a href=\"http://wiki.lesswrong.com/wiki/Spaced_repetition\">Anki</a> plugins I just wrote. I've been using them for a few months as monkey patches, but I thought it might help people here&nbsp;(or at least the 20% that are awesome enough to use SRSs)&nbsp;to have them as plugins. They're ugly and you may have to fiddle for a while to get them to work.</p>\n<p>&nbsp;</p>\n<h2>1. <a href=\"https://ankiweb.net/shared/info/985956348\">Music-Fiddler</a></h2>\n<p>To use this, play music while doing Anki revs. (I also recommend that you try playing music <em>only </em>while doing Anki, as a way of making Anki more pleasant.)&nbsp;While you're reviewing a card, the music volume will gradually decrease. As soon as you pass or fail the card, the volume will go back up, then start gradually decreasing again. So whenever you stop paying attention and instead start thinking about all the awesome things you could do <em>if only you were able to sit down and work</em>, the program punishes you by stopping the music. And whenever you concentrate fully on your work and so go through cards quickly, you have a personal soundtrack!</p>\n<p style=\"padding-left: 30px;\">To use this plugin:</p>\n<p style=\"padding-left: 30px;\">- If you do not have Linux, you'll need to modify the code somehow.</p>\n<p style=\"padding-left: 30px;\">-&nbsp;Ensure that the \"amixer\" command works on your computer. If it doesn't, you're going to need to modify the code somehow.</p>\n<p style=\"padding-left: 30px;\">- Make sure you have the new <a href=\"http://ankisrs.net/anki2.html\">Anki 2.0</a>.</p>\n<p style=\"padding-left: 30px;\">- <a href=\"https://ankiweb.net/shared/info/1409034823\">Download the plugin.</a></p>\n<p style=\"padding-left: 30px;\">- Change all lines (in the plugin source) marked with \"CHANGEME\" according to your preferences.</p>\n<p style=\"padding-left: 30px;\">- You might want to disable convenient ways of increasing the volume, like keyboard shortcuts.</p>\n<p>This plugin provides psychological reinforcement, but <a href=\"/lw/2dg/applying_behavioral_psychology_on_myself/\">is not proper intermittent reinforcement</a>, because it is predictable and regular instead of intermittent. I'm not sure whether this should be fixed; I haven't yet gotten around to trying it with only intermittent volume increases.</p>\n<p>&nbsp;</p>\n<h2>2. <a href=\"https://ankiweb.net/shared/info/181318336\">Picture-Flasher</a></h2>\n<p>After answering a card, this plugin selects, with some probability, a random image from a folder and flashes it onto your screen briefly. This gives <a href=\"/lw/2dg/applying_behavioral_psychology_on_myself/\">intermittent reinforcement</a>.</p>\n<p style=\"padding-left: 30px;\">To use this plugin:</p>\n<p style=\"padding-left: 30px;\">- I haven't tested it on non-Linux operating systems, but I can't see any obvious places it'll fail.</p>\n<p style=\"padding-left: 30px;\">- Make sure you have the new <a href=\"http://ankisrs.net/anki2.html\">Anki 2.0</a>.</p>\n<p style=\"padding-left: 30px;\">- Get pictures from someplace; see below.</p>\n<p style=\"padding-left: 30px;\">- <a href=\"https://ankiweb.net/shared/info/962726210\">Download the plugin.</a></p>\n<p style=\"padding-left: 30px;\">- Change all lines (in the plugin source) marked with \"CHANGEME\" according to your preferences. Be sure especially to put in your picture directory and the number of pictures you have.</p>\n<p>To get pictures, I downloaded&nbsp;high-scoring pictures off of reddit. <a href=\"https://github.com/dpbrown/RedditImageGrab/blob/master/redditdownload.py\">This script</a> can do that automatically. You can use <a href=\"http://www.reddit.com/r/aww\">pictures of cute animals</a>, <a href=\"http://www.reddit.com/r/lolcats\">funny captioned pictures of cats</a>, or more questionable things.</p>\n<p>The plugin could be made a lot more awesome by having it automatically pull pictures from the internet so you're not reusing them. I'm not planning on doing this anytime soon (because I have no internet on my main computer for productivity reasons), but if somebody else does that and posts it, they are awesome and they should feel awesome.</p>\n<p><strong>Update 4 Dec:&nbsp;</strong><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 12.727272033691406px;\">Emanuel Rylke has created a patch for this plugin which removes the requirement to rename the pictures. It also moves the configuration options to the top of the plugin, making them easier to find. The new version is at the same download link</span></p>\n<p><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 12.727272033691406px;\"><br /></span></p>\n<p><strong>Update 16 June 2015: </strong><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 12.727272033691406px;\">The plugins were deleted from the official list where they previously were, apparently because my AnkiWeb account was deleted due to disuse. So I've uploaded the two plugins on GitHub here: https://github.com/StephenBarnes/AnkiPlugins. I also re-uploaded the plugins to the official list. Links on this post have been updated.<br /></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"H2q58pKG6xFrv8bPz": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Qkb6MrWBr48PLtXJu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 17, "extendedScore": null, "score": 1.049213909143692e-06, "legacy": true, "legacyId": "20424", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>This post is about two <a href=\"http://wiki.lesswrong.com/wiki/Spaced_repetition\">Anki</a> plugins I just wrote. I've been using them for a few months as monkey patches, but I thought it might help people here&nbsp;(or at least the 20% that are awesome enough to use SRSs)&nbsp;to have them as plugins. They're ugly and you may have to fiddle for a while to get them to work.</p>\n<p>&nbsp;</p>\n<h2 id=\"1__Music_Fiddler\">1. <a href=\"https://ankiweb.net/shared/info/985956348\">Music-Fiddler</a></h2>\n<p>To use this, play music while doing Anki revs. (I also recommend that you try playing music <em>only </em>while doing Anki, as a way of making Anki more pleasant.)&nbsp;While you're reviewing a card, the music volume will gradually decrease. As soon as you pass or fail the card, the volume will go back up, then start gradually decreasing again. So whenever you stop paying attention and instead start thinking about all the awesome things you could do <em>if only you were able to sit down and work</em>, the program punishes you by stopping the music. And whenever you concentrate fully on your work and so go through cards quickly, you have a personal soundtrack!</p>\n<p style=\"padding-left: 30px;\">To use this plugin:</p>\n<p style=\"padding-left: 30px;\">- If you do not have Linux, you'll need to modify the code somehow.</p>\n<p style=\"padding-left: 30px;\">-&nbsp;Ensure that the \"amixer\" command works on your computer. If it doesn't, you're going to need to modify the code somehow.</p>\n<p style=\"padding-left: 30px;\">- Make sure you have the new <a href=\"http://ankisrs.net/anki2.html\">Anki 2.0</a>.</p>\n<p style=\"padding-left: 30px;\">- <a href=\"https://ankiweb.net/shared/info/1409034823\">Download the plugin.</a></p>\n<p style=\"padding-left: 30px;\">- Change all lines (in the plugin source) marked with \"CHANGEME\" according to your preferences.</p>\n<p style=\"padding-left: 30px;\">- You might want to disable convenient ways of increasing the volume, like keyboard shortcuts.</p>\n<p>This plugin provides psychological reinforcement, but <a href=\"/lw/2dg/applying_behavioral_psychology_on_myself/\">is not proper intermittent reinforcement</a>, because it is predictable and regular instead of intermittent. I'm not sure whether this should be fixed; I haven't yet gotten around to trying it with only intermittent volume increases.</p>\n<p>&nbsp;</p>\n<h2 id=\"2__Picture_Flasher\">2. <a href=\"https://ankiweb.net/shared/info/181318336\">Picture-Flasher</a></h2>\n<p>After answering a card, this plugin selects, with some probability, a random image from a folder and flashes it onto your screen briefly. This gives <a href=\"/lw/2dg/applying_behavioral_psychology_on_myself/\">intermittent reinforcement</a>.</p>\n<p style=\"padding-left: 30px;\">To use this plugin:</p>\n<p style=\"padding-left: 30px;\">- I haven't tested it on non-Linux operating systems, but I can't see any obvious places it'll fail.</p>\n<p style=\"padding-left: 30px;\">- Make sure you have the new <a href=\"http://ankisrs.net/anki2.html\">Anki 2.0</a>.</p>\n<p style=\"padding-left: 30px;\">- Get pictures from someplace; see below.</p>\n<p style=\"padding-left: 30px;\">- <a href=\"https://ankiweb.net/shared/info/962726210\">Download the plugin.</a></p>\n<p style=\"padding-left: 30px;\">- Change all lines (in the plugin source) marked with \"CHANGEME\" according to your preferences. Be sure especially to put in your picture directory and the number of pictures you have.</p>\n<p>To get pictures, I downloaded&nbsp;high-scoring pictures off of reddit. <a href=\"https://github.com/dpbrown/RedditImageGrab/blob/master/redditdownload.py\">This script</a> can do that automatically. You can use <a href=\"http://www.reddit.com/r/aww\">pictures of cute animals</a>, <a href=\"http://www.reddit.com/r/lolcats\">funny captioned pictures of cats</a>, or more questionable things.</p>\n<p>The plugin could be made a lot more awesome by having it automatically pull pictures from the internet so you're not reusing them. I'm not planning on doing this anytime soon (because I have no internet on my main computer for productivity reasons), but if somebody else does that and posts it, they are awesome and they should feel awesome.</p>\n<p><strong>Update 4 Dec:&nbsp;</strong><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 12.727272033691406px;\">Emanuel Rylke has created a patch for this plugin which removes the requirement to rename the pictures. It also moves the configuration options to the top of the plugin, making them easier to find. The new version is at the same download link</span></p>\n<p><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 12.727272033691406px;\"><br></span></p>\n<p><strong>Update 16 June 2015: </strong><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 12.727272033691406px;\">The plugins were deleted from the official list where they previously were, apparently because my AnkiWeb account was deleted due to disuse. So I've uploaded the two plugins on GitHub here: https://github.com/StephenBarnes/AnkiPlugins. I also re-uploaded the plugins to the official list. Links on this post have been updated.<br></span></p>", "sections": [{"title": "1. Music-Fiddler", "anchor": "1__Music_Fiddler", "level": 1}, {"title": "2. Picture-Flasher", "anchor": "2__Picture_Flasher", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "16 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["EJuZcWnk8j7eNQPqq"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-03T23:56:57.625Z", "modifiedAt": null, "url": null, "title": "Meetup : Atlanta LessWrong Meetups REBOOT", "slug": "meetup-atlanta-lesswrong-meetups-reboot", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:23.898Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nova_Division", "createdAt": "2011-03-14T15:21:15.124Z", "isAdmin": false, "displayName": "Nova_Division"}, "userId": "eFXLR4aNaxDBCDatT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3oqXTAwvGYesDPG8F/meetup-atlanta-lesswrong-meetups-reboot", "pageUrlRelative": "/posts/3oqXTAwvGYesDPG8F/meetup-atlanta-lesswrong-meetups-reboot", "linkUrl": "https://www.lesswrong.com/posts/3oqXTAwvGYesDPG8F/meetup-atlanta-lesswrong-meetups-reboot", "postedAtFormatted": "Monday, December 3rd 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Atlanta%20LessWrong%20Meetups%20REBOOT&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Atlanta%20LessWrong%20Meetups%20REBOOT%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3oqXTAwvGYesDPG8F%2Fmeetup-atlanta-lesswrong-meetups-reboot%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Atlanta%20LessWrong%20Meetups%20REBOOT%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3oqXTAwvGYesDPG8F%2Fmeetup-atlanta-lesswrong-meetups-reboot", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3oqXTAwvGYesDPG8F%2Fmeetup-atlanta-lesswrong-meetups-reboot", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 111, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/go'>Atlanta LessWrong Meetups REBOOT</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 December 2012 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1314 Hosea L. Williams Drive, Atlanta, 30307</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This meeting is a REBOOT of Atlanta LessWrong meetups.   That means no experience is necessary and everyone is welcome!</p>\n\n<p>The agenda is:\n-Introductions (including personal motivations for rationality for those who would like to discuss this),\n-Discussion of interesting fallacies, biases, and scientific studies that relate to practical rationality,\n-Rationality exercises and games!</p>\n\n<p>The after-meetup-party will include other games (Set, Telestrations, Dixit, Apples To Apples, etc).</p>\n\n<p>A few snacks and drinks will be provided, but you are welcome to bring your favorites!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/go'>Atlanta LessWrong Meetups REBOOT</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3oqXTAwvGYesDPG8F", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.0492780658175448e-06, "legacy": true, "legacyId": "20426", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Atlanta_LessWrong_Meetups_REBOOT\">Discussion article for the meetup : <a href=\"/meetups/go\">Atlanta LessWrong Meetups REBOOT</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 December 2012 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1314 Hosea L. Williams Drive, Atlanta, 30307</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>This meeting is a REBOOT of Atlanta LessWrong meetups.   That means no experience is necessary and everyone is welcome!</p>\n\n<p>The agenda is:\n-Introductions (including personal motivations for rationality for those who would like to discuss this),\n-Discussion of interesting fallacies, biases, and scientific studies that relate to practical rationality,\n-Rationality exercises and games!</p>\n\n<p>The after-meetup-party will include other games (Set, Telestrations, Dixit, Apples To Apples, etc).</p>\n\n<p>A few snacks and drinks will be provided, but you are welcome to bring your favorites!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Atlanta_LessWrong_Meetups_REBOOT1\">Discussion article for the meetup : <a href=\"/meetups/go\">Atlanta LessWrong Meetups REBOOT</a></h2>", "sections": [{"title": "Discussion article for the meetup : Atlanta LessWrong Meetups REBOOT", "anchor": "Discussion_article_for_the_meetup___Atlanta_LessWrong_Meetups_REBOOT", "level": 1}, {"title": "Discussion article for the meetup : Atlanta LessWrong Meetups REBOOT", "anchor": "Discussion_article_for_the_meetup___Atlanta_LessWrong_Meetups_REBOOT1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-04T02:44:47.828Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Recursive Self-Improvement", "slug": "seq-rerun-recursive-self-improvement", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:07.885Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YYAhX8z5p77FS4P2x/seq-rerun-recursive-self-improvement", "pageUrlRelative": "/posts/YYAhX8z5p77FS4P2x/seq-rerun-recursive-self-improvement", "linkUrl": "https://www.lesswrong.com/posts/YYAhX8z5p77FS4P2x/seq-rerun-recursive-self-improvement", "postedAtFormatted": "Tuesday, December 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Recursive%20Self-Improvement&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Recursive%20Self-Improvement%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYYAhX8z5p77FS4P2x%2Fseq-rerun-recursive-self-improvement%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Recursive%20Self-Improvement%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYYAhX8z5p77FS4P2x%2Fseq-rerun-recursive-self-improvement", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYYAhX8z5p77FS4P2x%2Fseq-rerun-recursive-self-improvement", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 183, "htmlBody": "<p>Today's post, <a href=\"/lw/we/recursive_selfimprovement/\">Recursive Self-Improvement</a> was originally published on 01 December 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Recursive_Self-Improvement\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>When you take a process that is capable of making significant progress developing other processes, and turn it on itself, you should either see it flatline, or FOOM. The likelihood of it doing anything that looks like human-scale progress is unbelievably low.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fr0/seq_rerun_i_heart_cyc/\">I Heart CYC</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YYAhX8z5p77FS4P2x", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0493740697613352e-06, "legacy": true, "legacyId": "20433", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["JBadX7rwdcRFzGuju", "u3YZfJfvCbyBXz4WH", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-04T05:48:43.389Z", "modifiedAt": "2020-08-04T21:32:55.185Z", "url": null, "title": "Prediction Sources", "slug": "prediction-sources", "viewCount": null, "lastCommentedAt": "2016-01-11T18:09:27.432Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LigE97GmxQZvoEFQ8/prediction-sources", "pageUrlRelative": "/posts/LigE97GmxQZvoEFQ8/prediction-sources", "linkUrl": "https://www.lesswrong.com/posts/LigE97GmxQZvoEFQ8/prediction-sources", "postedAtFormatted": "Tuesday, December 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Prediction%20Sources&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APrediction%20Sources%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLigE97GmxQZvoEFQ8%2Fprediction-sources%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Prediction%20Sources%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLigE97GmxQZvoEFQ8%2Fprediction-sources", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLigE97GmxQZvoEFQ8%2Fprediction-sources", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 189, "htmlBody": "<p>I'd like to become better calibrated via <a href=\"http://predictionbook.com/\">PredictionBook</a> and other tools, but coming up with well-specified predictions can be very time-consuming. It's handy to be provided with a stock of specific claims to make predictions (or post-dictions) about, as with CFAR's <a href=\"http://www.acritch.com/credence-game/\">Credence Game</a>.</p>\n<p>Therefore, I asked Jake Miller and Gwern put together a list of prediction sources. Feel free to suggest others!</p>\n<h2>Prediction Sites</h2>\n<ul>\n<li><a href=\"http://predictionbook.com\">PredictionBook.com</a> (<a href=\"http://predictionbook.com/predictions\">new</a>) (<a href=\"http://predictionbook.com/predictions/future\">upcoming</a>)</li>\n<li><a href=\"http://daggre.org/info/\">DAGGRE</a></li>\n<li><a href=\"http://betsofbitco.in\">Bets of Bitcoin</a> (<a href=\"http://betsofbitco.in/list?status=available&amp;category=All&amp;sorting=-moderationTime\">new</a>) (<a href=\"http://betsofbitco.in/list?status=available&amp;category=All&amp;sorting=deadlineTime\">upcoming</a>)</li>\n<li><a href=\"http://home.inklingmarkets.com\">Inkling</a> (<a href=\"http://home.inklingmarkets.com/recent/markets\">new</a>) (<a href=\"http://home.inklingmarkets.com/expiring/markets\">upcoming</a>)</li>\n<li><a href=\"http://markets.nitle.org/markets\">NITLE</a> (<a href=\"http://markets.nitle.org/markets\">new</a>) (<a href=\"http://markets.nitle.org/expiring/markets\">upcoming</a>)</li>\n<li><a href=\"http://www.ideosphere.com/fx-bin/ListClaims\">Foresight Exchange</a> (new:&nbsp;\"Sort order: date&nbsp;created\") (upcoming:&nbsp;\"Sort order: date&nbsp;due\")</li>\n<li><a href=\"http://longbets.org\">LongBets</a> (<a href=\"http://feeds.feedburner.com/longbets\">new</a>)</li>\n<li><a href=\"http://intrade.com\">Intrade</a> (<a href=\"http://www.intrade.com/v4/misc/recentpredictions/\">new</a>)</li>\n<li><a href=\"http://hsx.com\">Hollywood Stock Exchange</a> (<a href=\"http://www.hsx.com/security/feature.php?type=upcoming\">upcoming</a>)</li>\n<li><a href=\"http://tippie.uiowa.edu/iem/markets/\">Iowa Electronic Markets</a></li>\n<li><a href=\"http://www.betfair.com\">Betfair</a></li>\n<li><a href=\"https://www.fbo.gov/index?s=opportunity&amp;mode=form&amp;tab=core&amp;id=50468b8c167fb9d60152085803622f55\">IARPA forecasting challenge</a></li>\n<li><a href=\"http://www.simexchange.com/frontpage.php\">the simExchange</a></li>\n<li><a href=\"http://www.betdaq.com/UI/\">BETDAQ</a></li>\n<li><a href=\"http://nadex.com\">Nadex</a> (private; logged-in-only access)</li>\n<li><a href=\"http://predictionmarkets.ca/\">Saunder School of Business Prediction Markets</a> (opens 2013)</li>\n<li><a href=\"http://www.predictwallstreet.com/\">Predict Wall Street</a> </li>\n<li><a href=\"http://predictionmachine.com/Picks\">Prediction Machine</a> </li>\n<li><a href=\"https://www.ipredict.co.nz/\">iPredict</a> (<a href=\"https://www.ipredict.co.nz/app.php?do=browse&amp;tag=4\">new</a>) (<a href=\"https://www.ipredict.co.nz/app.php?do=browse&amp;tag=3\">upcoming</a>) </li>\n<li><a href=\"http://www.sbrforum.com/betting-odds/\">SBRodds</a></li>\n<li><a href=\"http://www.teamrankings.com/\">TeamRankings</a></li>\n<li><a href=\"http://www.vitibet.com/\">Vitibet</a></li>\n<li><a href=\"http://www.scorespro.com/\">ScoresPro</a></li>\n<li><a href=\"http://www.bettingadvice.com/pick.aspx\">Betting Advice</a></li>\n<li><a href=\"http://www.soccervista.com/\">Soccer Vista</a></li>\n<li><a href=\"http://www.leesmovieinfo.net/movie-predictions.php\">Lee's Movie Predictions</a></li>\n<li><a href=\"http://www.goldderby.com/\">GoldDerby</a></li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LigE97GmxQZvoEFQ8", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 17, "extendedScore": null, "score": 4.2e-05, "legacy": true, "legacyId": "20434", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2012-12-04T05:48:43.389Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-04T11:13:06.794Z", "modifiedAt": null, "url": null, "title": "Mini advent calendar of Xrisks: nuclear war", "slug": "mini-advent-calendar-of-xrisks-nuclear-war", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:24.407Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dnSNAPWXs5aM3LqKa/mini-advent-calendar-of-xrisks-nuclear-war", "pageUrlRelative": "/posts/dnSNAPWXs5aM3LqKa/mini-advent-calendar-of-xrisks-nuclear-war", "linkUrl": "https://www.lesswrong.com/posts/dnSNAPWXs5aM3LqKa/mini-advent-calendar-of-xrisks-nuclear-war", "postedAtFormatted": "Tuesday, December 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mini%20advent%20calendar%20of%20Xrisks%3A%20nuclear%20war&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMini%20advent%20calendar%20of%20Xrisks%3A%20nuclear%20war%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdnSNAPWXs5aM3LqKa%2Fmini-advent-calendar-of-xrisks-nuclear-war%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mini%20advent%20calendar%20of%20Xrisks%3A%20nuclear%20war%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdnSNAPWXs5aM3LqKa%2Fmini-advent-calendar-of-xrisks-nuclear-war", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdnSNAPWXs5aM3LqKa%2Fmini-advent-calendar-of-xrisks-nuclear-war", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 261, "htmlBody": "<p><span style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 18px;\">The FHI's mini advent calendar: counting down through the big five existential risks. The first one is an old favourite, forgotten but not gone: <a href=\"http://www.facebook.com/FHIOxford/posts/397853443622971\">nuclear war</a>.</span><br style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 18px;\" /><br style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 18px;\" /><span style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 18px;\"><strong>Nuclear War</strong></span><br style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 18px;\" /><span style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 18px;\"><em>Current understanding: medium-high</em></span><br style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 18px;\" /><span style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 18px;\"><em>Most worrying aspect: the missiles and bombs are already out there</em></span><br style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 18px;\" /><span style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 18px;\">It was a great fear during the fifties and sixties; but the weapons that could destroy our species lie dormant, not destroyed.&nbsp;</span>\n<div class=\"text_exposed_show\" style=\"display: inline; color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 18px;\">There has been some recent progress: the sizes of the arsenals have been diminishing, fissile material is more under control than it used to be, and there is more geo-political peace and cooperation.<br /><br />But nuclear weapons still remain the easiest method for our species to destroy itself. Recent modelling have confirmed the old idea of nuclear winter: soot rising from burning human cities destroyed by nuclear weapons could envelop the world in a dark cloud, disrupting agriculture and the food supplies, and causing mass starvation and death far beyond the areas directly hit. And a creeping proliferation has spread these weapons to smaller states in unstable areas of the world, increasing the probability that nuclear weapons could get used, leading to potential escalation. The risks are not new, and several times (the Cuban missile crisis, the Petrov incident) our species has been saved from annihilation by the slimmest of margins. And yet the risk seems to have slipped off the radar for many governments: emergency food and fuel reserves are diminishing, and we have few &ldquo;refuges&rdquo; designed to ensure that the human species could endure a major nuclear conflict.</div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Rz5jb3cYHTSRmqNnN": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dnSNAPWXs5aM3LqKa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 8, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "20440", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-04T11:15:08.254Z", "modifiedAt": null, "url": null, "title": "Mini advent calendar of Xrisks: synthetic biology", "slug": "mini-advent-calendar-of-xrisks-synthetic-biology", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:59.703Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uwNnaQhKHfykidFo6/mini-advent-calendar-of-xrisks-synthetic-biology", "pageUrlRelative": "/posts/uwNnaQhKHfykidFo6/mini-advent-calendar-of-xrisks-synthetic-biology", "linkUrl": "https://www.lesswrong.com/posts/uwNnaQhKHfykidFo6/mini-advent-calendar-of-xrisks-synthetic-biology", "postedAtFormatted": "Tuesday, December 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mini%20advent%20calendar%20of%20Xrisks%3A%20synthetic%20biology&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMini%20advent%20calendar%20of%20Xrisks%3A%20synthetic%20biology%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuwNnaQhKHfykidFo6%2Fmini-advent-calendar-of-xrisks-synthetic-biology%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mini%20advent%20calendar%20of%20Xrisks%3A%20synthetic%20biology%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuwNnaQhKHfykidFo6%2Fmini-advent-calendar-of-xrisks-synthetic-biology", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuwNnaQhKHfykidFo6%2Fmini-advent-calendar-of-xrisks-synthetic-biology", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 249, "htmlBody": "<p><span style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 17px;\">The FHI's mini advent calendar: counting down through the big five existential risks. The second one is a new, exciting risk: <a href=\"http://www.facebook.com/FHIOxford/posts/398156746925974\">synthetic biology</a>.</span><br style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 17px;\" /><br style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 17px;\" /><span style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 17px;\"><strong>Synthetic biology</strong></span><br style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 17px;\" /><span style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 17px;\"><em>Current understanding: medium-low</em></span><br style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 17px;\" /><span style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 17px;\"><em>Most worrying aspect: hackers experimenting with our basic biology</em></span><br style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 17px;\" /><span style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 17px;\">Synthetic biology covers many inter-related fields, all concerned with the construction and control of new biological systems. This area has already attracted the attention of bio-hackers, experimenting with DNA and other biological systems to perform novel tasks &ndash; and gaining kudos for exotic accomplishments. The biosphere is filled with many organisms accomplishing specific tasks; combining these and controlling them could allow the construction of extremely deadly bioweapons, targeted very narrowly (at all those possessing a certain gene, for instance). Virulent virus with long incubation periods could be constructed, or common human bacteria could be hacked to perform a variety of roles in the body. And humans are not the only potential targets: whole swaths of the ecosystem could be taken down, either to gain commercial or economic advantages, for terrorist purposes, or simply by accident.</span><br style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 17px;\" /><br style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 17px;\" /><span style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 17px;\">Moreover, the medical miracles promised by synthetic biology are not easily separated from the danger: the targeted control needed to, for instance, kill cancer cells, could also be used to target brain cells or the immune system. This would not be so frightening if the field implemented safety measures commensurate with the risks; but synthetic biology has been extremely lax in its precautions and culturally resistant to regulations.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Rz5jb3cYHTSRmqNnN": 2, "jaf5zfcGgCB2REXGw": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uwNnaQhKHfykidFo6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 8, "extendedScore": null, "score": 1.0496660844606062e-06, "legacy": true, "legacyId": "20441", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-04T19:51:40.201Z", "modifiedAt": null, "url": null, "title": "Meetup : Montreal LessWrong Meetup - More Biases and Biased Board Gaming", "slug": "meetup-montreal-lesswrong-meetup-more-biases-and-biased", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Paul_G", "createdAt": "2012-06-08T01:42:32.451Z", "isAdmin": false, "displayName": "Paul_G"}, "userId": "g4WQoWHmq4HNzTJDC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aJEYui5XPq5Hxx5g5/meetup-montreal-lesswrong-meetup-more-biases-and-biased", "pageUrlRelative": "/posts/aJEYui5XPq5Hxx5g5/meetup-montreal-lesswrong-meetup-more-biases-and-biased", "linkUrl": "https://www.lesswrong.com/posts/aJEYui5XPq5Hxx5g5/meetup-montreal-lesswrong-meetup-more-biases-and-biased", "postedAtFormatted": "Tuesday, December 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Montreal%20LessWrong%20Meetup%20-%20More%20Biases%20and%20Biased%20Board%20Gaming&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Montreal%20LessWrong%20Meetup%20-%20More%20Biases%20and%20Biased%20Board%20Gaming%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaJEYui5XPq5Hxx5g5%2Fmeetup-montreal-lesswrong-meetup-more-biases-and-biased%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Montreal%20LessWrong%20Meetup%20-%20More%20Biases%20and%20Biased%20Board%20Gaming%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaJEYui5XPq5Hxx5g5%2Fmeetup-montreal-lesswrong-meetup-more-biases-and-biased", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaJEYui5XPq5Hxx5g5%2Fmeetup-montreal-lesswrong-meetup-more-biases-and-biased", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 80, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/gp'>Montreal LessWrong Meetup - More Biases and Biased Board Gaming</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">10 December 2012 06:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">655 President Kennedy, Montreal</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We discussed lots of biases last week, so this week we're going to actually whip out the board games! We have a copy or two of Pandemic, and one of Shadowrift to play with.</p>\n\n<p>We'll be upstairs. See you then!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/gp'>Montreal LessWrong Meetup - More Biases and Biased Board Gaming</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aJEYui5XPq5Hxx5g5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 3, "extendedScore": null, "score": 1.0499617887691686e-06, "legacy": true, "legacyId": "20442", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Montreal_LessWrong_Meetup___More_Biases_and_Biased_Board_Gaming\">Discussion article for the meetup : <a href=\"/meetups/gp\">Montreal LessWrong Meetup - More Biases and Biased Board Gaming</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">10 December 2012 06:30:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">655 President Kennedy, Montreal</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We discussed lots of biases last week, so this week we're going to actually whip out the board games! We have a copy or two of Pandemic, and one of Shadowrift to play with.</p>\n\n<p>We'll be upstairs. See you then!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Montreal_LessWrong_Meetup___More_Biases_and_Biased_Board_Gaming1\">Discussion article for the meetup : <a href=\"/meetups/gp\">Montreal LessWrong Meetup - More Biases and Biased Board Gaming</a></h2>", "sections": [{"title": "Discussion article for the meetup : Montreal LessWrong Meetup - More Biases and Biased Board Gaming", "anchor": "Discussion_article_for_the_meetup___Montreal_LessWrong_Meetup___More_Biases_and_Biased_Board_Gaming", "level": 1}, {"title": "Discussion article for the meetup : Montreal LessWrong Meetup - More Biases and Biased Board Gaming", "anchor": "Discussion_article_for_the_meetup___Montreal_LessWrong_Meetup___More_Biases_and_Biased_Board_Gaming1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-04T20:19:47.625Z", "modifiedAt": null, "url": null, "title": "Politics Discussion Thread December 2012", "slug": "politics-discussion-thread-december-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:02.203Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "OrphanWilde", "createdAt": "2012-06-21T18:24:36.749Z", "isAdmin": false, "displayName": "OrphanWilde"}, "userId": "cdW87AS6fdK2sywL2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tQ8cLgfyhgkEGJxMS/politics-discussion-thread-december-2012", "pageUrlRelative": "/posts/tQ8cLgfyhgkEGJxMS/politics-discussion-thread-december-2012", "linkUrl": "https://www.lesswrong.com/posts/tQ8cLgfyhgkEGJxMS/politics-discussion-thread-december-2012", "postedAtFormatted": "Tuesday, December 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Politics%20Discussion%20Thread%20December%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APolitics%20Discussion%20Thread%20December%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtQ8cLgfyhgkEGJxMS%2Fpolitics-discussion-thread-december-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Politics%20Discussion%20Thread%20December%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtQ8cLgfyhgkEGJxMS%2Fpolitics-discussion-thread-december-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtQ8cLgfyhgkEGJxMS%2Fpolitics-discussion-thread-december-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 165, "htmlBody": "<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">I skipped October and November owing to election season, but opening back up:</p>\n<ol style=\"padding: 0px; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">\n<li><em>Top-level comments should introduce arguments; responses should be responses to those arguments.&nbsp;</em></li>\n<li><em>Upvote and downvote based on whether or not you find an argument convincing in the context in which it was raised. &nbsp;This means if it's a good argument against the argument it is responding to, not whether or not there's a good/obvious counterargument to it; if you have a good counterargument, raise it. &nbsp;If it's a convincing argument, and the counterargument is also convincing, upvote both. &nbsp;If both arguments are unconvincing, downvote both.&nbsp;</em></li>\n<li><em>A single argument per comment would be ideal; as MixedNuts points out&nbsp;<a style=\"color: #8a8a8b;\" href=\"/r/discussion/lw/dsv/is_politics_the_mindkiller_an_inconclusive_test/73yp\">here</a>, it's otherwise hard to distinguish between one good and one bad argument, which makes the upvoting/downvoting difficult to evaluate.&nbsp;</em></li>\n<li><em>In general try to avoid color politics; try to discuss political issues, rather than political parties, wherever possible.</em></li>\n</ol>\n<p style=\"margin: 0px 0px 1em; font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">As Multiheaded added, \"Personal is Political\" stuff like gender relations, etc also may belong here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tQ8cLgfyhgkEGJxMS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 8, "extendedScore": null, "score": 1.0499778932273594e-06, "legacy": true, "legacyId": "20443", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 137, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-04T20:43:07.437Z", "modifiedAt": null, "url": null, "title": "The Worst Problem You've Ever Encountered and Solved. And the One You Didn't, Yet!", "slug": "the-worst-problem-you-ve-ever-encountered-and-solved-and-the", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:34.773Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tNi5HRfbMBKf88KLB/the-worst-problem-you-ve-ever-encountered-and-solved-and-the", "pageUrlRelative": "/posts/tNi5HRfbMBKf88KLB/the-worst-problem-you-ve-ever-encountered-and-solved-and-the", "linkUrl": "https://www.lesswrong.com/posts/tNi5HRfbMBKf88KLB/the-worst-problem-you-ve-ever-encountered-and-solved-and-the", "postedAtFormatted": "Tuesday, December 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Worst%20Problem%20You've%20Ever%20Encountered%20and%20Solved.%20And%20the%20One%20You%20Didn't%2C%20Yet!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Worst%20Problem%20You've%20Ever%20Encountered%20and%20Solved.%20And%20the%20One%20You%20Didn't%2C%20Yet!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtNi5HRfbMBKf88KLB%2Fthe-worst-problem-you-ve-ever-encountered-and-solved-and-the%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Worst%20Problem%20You've%20Ever%20Encountered%20and%20Solved.%20And%20the%20One%20You%20Didn't%2C%20Yet!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtNi5HRfbMBKf88KLB%2Fthe-worst-problem-you-ve-ever-encountered-and-solved-and-the", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtNi5HRfbMBKf88KLB%2Fthe-worst-problem-you-ve-ever-encountered-and-solved-and-the", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 180, "htmlBody": "<p>EDIT: No one was doing what the post suggests, so I accepted an idea from one of the comments, and embedded my response in a comment, not the post itself</p>\n<p>&nbsp;</p>\n<p>I'd like to ask this question to you, and I'll respond it myself as well.</p>\n<p>What Is The Worst Problem You've Ever Encountered and Solved? And the One You Didn't, Yet!</p>\n<p>Some prior considerations:</p>\n<p>1) I mean \"problem\" in a very general sense, it could be a math problem, an existential problem, a social problem, an akrasia problem,&nbsp; a disease problem etc...</p>\n<p>2) I'd like people to give informative/didactic responses.&nbsp; Try not only to state the facts, but also to help someone who'd encounter similar situations to be able to deal with them.</p>\n<p>3) When talking about the one you didn't, give enough specifics that someone would actually be able to help you.</p>\n<p>The general idea is to teach people how to Win by example, taking in consideration all the shortcomings of biases etc...</p>\n<p>&nbsp;</p>\n<p>Well, that is all. One solved, one not yet solved. State your own issues and help others here. Someone else's rationality is always welcome.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tNi5HRfbMBKf88KLB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 5, "extendedScore": null, "score": 1.0499912531052405e-06, "legacy": true, "legacyId": "20444", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-04T22:22:17.382Z", "modifiedAt": null, "url": null, "title": "How to Avoid the Conflict Between Feminism and Evolutionary Psychology?", "slug": "how-to-avoid-the-conflict-between-feminism-and-evolutionary", "viewCount": null, "lastCommentedAt": "2017-06-17T04:33:50.158Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/x6WkBHYEiqDYCpsnX/how-to-avoid-the-conflict-between-feminism-and-evolutionary", "pageUrlRelative": "/posts/x6WkBHYEiqDYCpsnX/how-to-avoid-the-conflict-between-feminism-and-evolutionary", "linkUrl": "https://www.lesswrong.com/posts/x6WkBHYEiqDYCpsnX/how-to-avoid-the-conflict-between-feminism-and-evolutionary", "postedAtFormatted": "Tuesday, December 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20Avoid%20the%20Conflict%20Between%20Feminism%20and%20Evolutionary%20Psychology%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20Avoid%20the%20Conflict%20Between%20Feminism%20and%20Evolutionary%20Psychology%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx6WkBHYEiqDYCpsnX%2Fhow-to-avoid-the-conflict-between-feminism-and-evolutionary%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20Avoid%20the%20Conflict%20Between%20Feminism%20and%20Evolutionary%20Psychology%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx6WkBHYEiqDYCpsnX%2Fhow-to-avoid-the-conflict-between-feminism-and-evolutionary", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx6WkBHYEiqDYCpsnX%2Fhow-to-avoid-the-conflict-between-feminism-and-evolutionary", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 239, "htmlBody": "<p>I don't mean to claim that there should be a conflict.</p>\n<p>Most likely the conflict arises because of many things, such as 1)Women having been ostracized for much of our society's existence 2)People failing at the is-ought problem, and committing the Naturalistic Fallacy 3)Lots of media articles saying unbelievably na&iuml;ve evolutionary statements as scientific fact 4)Feminists as a group being defensive 5)Specially defensive when it comes to what is said to be natural. 6) General disregard by people, and politically engaged people (see The Blank Slate, by Steve Pinker) of the existence of a non Tabula Rasa nature. 7) Lack of patience of Evolutionary Psychologists to make peace and explain themselves for the things that journalists, not them, claimed.&nbsp; and others...</p>\n<p>But the fact is, the conflict arose. It has only bad consequences as far as I could see, such as people fighting over each other, breaking friendships, and prejudice of great intensity on both sides.</p>\n<p>How to avoid this conflict?&nbsp; Should someone write a treatise on Feminist Evolutionary Psychology?&nbsp; Should we get Leda Cosmides to talk about women liberation?&nbsp;</p>\n<p>There are obviously no incompatibilities between reality and the moral claims of feminism. So whichever facts about evolutionary psychology are found to be true with the science's development, they should be made compatible. Compatibilism is possible.</p>\n<p>But will the scientific community pull it off?</p>\n<p>&nbsp;</p>\n<p>Related: Pinker Versus Spelke - The Science of Gender and Science</p>\n<p>http://www.edge.org/3rd_culture/debate05/debate05_index.html</p>\n<p>David Buss and Cindy Meston - Why do Women Have Sex?</p>\n<p>http://www.youtube.com/watch?v=KA0sqg3EHm8</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "x6WkBHYEiqDYCpsnX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 9, "extendedScore": null, "score": 4e-05, "legacy": true, "legacyId": "20446", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 97, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-04T23:03:31.297Z", "modifiedAt": null, "url": null, "title": "Is Equality Really about Diminishing Marginal Utility?", "slug": "is-equality-really-about-diminishing-marginal-utility", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:29.798Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ghatanathoah", "createdAt": "2012-01-27T20:44:00.945Z", "isAdmin": false, "displayName": "Ghatanathoah"}, "userId": "CzhiZYDsQs87MM5yH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9DBRBhj9pzgzyvZAo/is-equality-really-about-diminishing-marginal-utility", "pageUrlRelative": "/posts/9DBRBhj9pzgzyvZAo/is-equality-really-about-diminishing-marginal-utility", "linkUrl": "https://www.lesswrong.com/posts/9DBRBhj9pzgzyvZAo/is-equality-really-about-diminishing-marginal-utility", "postedAtFormatted": "Tuesday, December 4th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20Equality%20Really%20about%20Diminishing%20Marginal%20Utility%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20Equality%20Really%20about%20Diminishing%20Marginal%20Utility%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9DBRBhj9pzgzyvZAo%2Fis-equality-really-about-diminishing-marginal-utility%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20Equality%20Really%20about%20Diminishing%20Marginal%20Utility%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9DBRBhj9pzgzyvZAo%2Fis-equality-really-about-diminishing-marginal-utility", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9DBRBhj9pzgzyvZAo%2Fis-equality-really-about-diminishing-marginal-utility", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1303, "htmlBody": "<p>In Robert Nozick's famous \"<a href=\"http://en.wikipedia.org/wiki/Utility_monster\">Utility Monster</a>\" thought experiment he proposes the idea of a creature that does not receive diminishing marginal utility from resource consumption, and argues that this poses a problem for utilitarian ethics.&nbsp; Why?&nbsp; Utilitarian ethics, while highly egalitarian in real life situations, does not place any intrinsic value on equality.&nbsp; The reason utilitarian ethics tend to favor equality is that human beings seem to experience diminishing returns when converting resources into utility.&nbsp; Egalitarianism, according to this framework, is good because sharing resources between people reduces the level of diminishing returns and maximizes the total amount of utility people generate, not because it's actually good for people to have equal levels of utility.</p>\n<p>The problem the Utility Monster poses is that, since it does not receive diminishing marginal utility, there is no reason, under traditional utilitarian framework, to share resources between it and the other inhabitants of the world it lives in.&nbsp; It would be completely justified in killing other people and taking their things for itself, or enslaving them for its own benefit.&nbsp; This seems counter-intuitive to Nozick, and many other people.</p>\n<p>There seem to be two possible reasons for this.&nbsp; One, of course, is that most people's intuitions are wrong in this particular case.&nbsp; The reason I am interesting in exploring, however, is the other one, namely that equality is valuable for its own sake, not just as a side effect of diminishing marginal utility.</p>\n<p>Now, before I go any further I should clarify what I mean by \"equality.\"&nbsp; There are many different types of equality, not all of which are compatible with each other.&nbsp; What I mean is equality of utility, everyone has the same level of satisfied preferences, happiness, and whatever else \"utility\" constitutes.&nbsp; This is not the same thing as fiscal equality, as some people may differ in their ability to convert money and resources into utility (people with horrible illnesses, for instance, are worse at doing so than the general population).&nbsp; It is also important to stress that \"lifespan\" should be factored in as part of the utility that is to be equalized (i.e. killing someone increases inequality).&nbsp; Otherwise one could achieve equality of utility by killing all the poor people.</p>\n<p>So if equality is valuable for its own sake, how does one factor it into utilitarian calculations?&nbsp; It seems wrong to replace utility maximization with equality maximization.&nbsp; That would imply that a world where everyone had 10 utilons and a society where everyone had 100 utilons are morally identical, which seems wrong, to say the least.&nbsp;</p>\n<p>What about making equality lexically prior to utility maximization?&nbsp; That seems just as bad.&nbsp; It would imply, among other things, that in a stratified world where some people have far greater levels of utility than others, that it would be morally right to take an action that would harm every single person in the world, as long as it hurt the best off slightly more than the worst off.&nbsp; That seems insanely wrong.&nbsp; The Utility Monster thought experiment already argues against making utility maximization lexically prior to equality.</p>\n<p>So it seems like the best option would be to have maximizing utility and increasing equality as two separate values.&nbsp; How then, to trade one off against the other?&nbsp; If there is some sort of straight, one-to-one value then this doesn't do anything to dismiss the problem of the Utility Monster.&nbsp; A monster good enough at utility generation could simply produce so much utility that no amount of equality could equal its output.</p>\n<p>The best possible solution I can see would be to have utility maximization and equality have diminishing returns relative to each other.&nbsp; This would mean that in a world with high equality, but low utility, raising utility would be more important, while in a world of low equality and high utility, establishing equality would be more important.</p>\n<p>This solution deals with the utility monster fairly effectively.&nbsp; No matter how much utility the monster can generate, it is always better to share some of its resources with other people.</p>\n<p>Now, you might notice that this doesn't eliminate every aspect of the utility monster problem.&nbsp; As long as the returns generated by utility maximization do not diminish to zero you can always posit an even more talented monster.&nbsp; And you can then argue that the society created by having that monster enslave the rest of the populace is better than one where a less talented monster shares with the rest of the populace. However, this new society would instantly become better if the new Utility Monster was forced to share its resources with the rest of the population.</p>\n<p>This is a huge improvement over the old framework.&nbsp; Ordinary utility maximizing ethics would not only argue that a world where a Utility Monster enslaved everyone else might be a better world.&nbsp; They would argue that it was the <em>optimal</em> world, the best possible world given the constraints the inhabitants face.&nbsp; Under this new ethical framework, however, that is never the case.&nbsp; The optimal world, under any given level of constraints, is one where a utility monster shares with the rest of the population.&nbsp;</p>\n<p>In other words, under this framework, if you were to ask, \"Is it good for a utility monster to enslave the rest of the population?\" the answer would always be \"No.\"</p>\n<p>Obviously the value of equality has many other aspects to be considered.&nbsp; For instance is it better described by traditional egalitarianism, or by <a href=\"http://en.wikipedia.org/wiki/Prioritarianism\">prioritarianism</a>?&nbsp; Values are often <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">more complex </a>than they first appear.</p>\n<p>It also seems quite possible that there are other facets of value besides maximizing utility and equality of utility.&nbsp; For instance, total and average utilitarianism might be reconciled by making them two separate values that are both important.&nbsp; Other potential candidates include prioritarian concerns (if they are not included already), number of worthwhile lives (most people would consider a world full of people with excellent lives better than one inhabited solely by one ecstatic utility monster), consideration of prior-existing people, and perhaps many, many more. As with utility and equality, these values would have diminishing returns relative to each other, and an optimum society would be one where all receive some measure of consideration.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>An aside.&nbsp; This next section is not directly related to the rest of the essay, but develops the idea in a direction I thought was interesting:</p>\n<p>&nbsp;</p>\n<p>It seems to me that the value of equality could be the source of a local disagreement in population ethics.&nbsp; There are several people (<a href=\"/lw/d80/malthusian_copying_mass_death_of_unhappy/6yh0\">Robin Hanson</a>, most notably) who have argued that it would be highly desirable to create huge amounts of poor people with lives barely worth living, and that this may well be better than having a smaller, wealthier population. <a href=\"/lw/14s/the_difficulties_of_potential_people_and_decision/\">Many</a> <a href=\"/lw/d80/malthusian_copying_mass_death_of_unhappy/\">other</a> <a href=\"/lw/4rn/nonpersonal_preferences_of_neverexisted_people/\">people</a> consider this to be a bad idea.</p>\n<p>The unspoken assumption in this argument is that multiple lives barely worth living generate more utility than a single very excellent life. At first this seems like an obvious truth, based on the following chain of logic:</p>\n<p>1. It is obviously wrong for Person A, who has a life barely worth living, to kill Person B, who also has a life barely worth living, and use B's property to improve their own life.</p>\n<p>2. The only reason something is wrong is that it decreases the level of utility.</p>\n<p>3. Therefore, killing Person B must decrease the level of utility.</p>\n<p>4. Therefore, two lives barely worth living must generate more utility than a single excellent life.</p>\n<p>However, if equality is valued for its own sake, then the reason it is wrong to kill Person B might be because of the vast inequality in various aspects of utility (lifespan, for instance) that their death would create between A and B.</p>\n<p>This means that a society that has a smaller population living great lives might very well be generating a much larger amount of utility than a larger society whose inhabitants live lives barely worth living.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9DBRBhj9pzgzyvZAo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 11, "extendedScore": null, "score": 2.8e-05, "legacy": true, "legacyId": "20445", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 45, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qBEtr2iaGEtLFyLvu", "ynyemLY8YWX8rQ84f", "DfCJoqcFGBqaroc85"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-05T00:26:24.471Z", "modifiedAt": null, "url": null, "title": "Mixed Reference: The Great Reductionist Project", "slug": "mixed-reference-the-great-reductionist-project", "viewCount": null, "lastCommentedAt": "2019-05-12T09:19:25.301Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bTsiPnFndZeqTnWpu/mixed-reference-the-great-reductionist-project", "pageUrlRelative": "/posts/bTsiPnFndZeqTnWpu/mixed-reference-the-great-reductionist-project", "linkUrl": "https://www.lesswrong.com/posts/bTsiPnFndZeqTnWpu/mixed-reference-the-great-reductionist-project", "postedAtFormatted": "Wednesday, December 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mixed%20Reference%3A%20The%20Great%20Reductionist%20Project&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMixed%20Reference%3A%20The%20Great%20Reductionist%20Project%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbTsiPnFndZeqTnWpu%2Fmixed-reference-the-great-reductionist-project%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mixed%20Reference%3A%20The%20Great%20Reductionist%20Project%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbTsiPnFndZeqTnWpu%2Fmixed-reference-the-great-reductionist-project", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbTsiPnFndZeqTnWpu%2Fmixed-reference-the-great-reductionist-project", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2598, "htmlBody": "<p><strong>Followup to</strong>:&nbsp;<a href=\"/lw/f4e/logical_pinpointing/\">Logical Pinpointing</a>,&nbsp;<a href=\"/lw/f1u/causal_reference/\">Causal Reference</a></p>\n<p style=\"padding-left: 30px;\"><span style=\"font-variant: small-caps;\">Take the universe and grind it down to the finest powder and sieve it through the finest sieve and then&nbsp;</span><em style=\"font-variant: small-caps;\">show</em><span style=\"font-variant: small-caps;\">&nbsp;me one atom of justice, one molecule of mercy.</span></p>\n<p style=\"padding-left: 60px;\">- Death, in&nbsp;<em>Hogfather</em>&nbsp;by Terry Pratchett</p>\n<p>Meditation: So far we've talked about two kinds of meaningfulness and two ways that sentences can refer; a way of comparing to physical things found by following pinned-down causal links, and logical validity by comparison to models pinned-down by axioms. Is there anything else that can be meaningfully talked about? Where would you find justice, or mercy?<a id=\"more\"></a></p>\n<p>...&nbsp;<br />...&nbsp;<br />...</p>\n<p>Suppose that I pointed at a couple of piles of apples on a table, a pile of two apples and a pile of three apples.</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/c/ca/Table.jpg\" alt=\"\" width=\"264\" height=\"154\" /></p>\n<p>And lo, I said: &nbsp;\"If we took the number of apples in each pile, and multiplied those numbers together, we'd get six.\"</p>\n<p>Nowhere in the physical universe is that 'six' written - there's nowhere in the laws of physics where you'll find a floating six. Even on the table itself there's only five apples, and apples&nbsp;<a href=\"/lw/on/reductionism/\">aren't fundamental</a>. Or to put it another way:</p>\n<p><span style=\"font-variant: small-caps;\">Take the apples and grind them down to the finest powder and sieve them through the finest sieve and then&nbsp;<em>show</em>&nbsp;me one atom of sixness, one molecule of multiplication.</span></p>\n<p>Nor can the statement be true as a matter of pure math, comparing to some Platonic six within a <a href=\"/lw/f43/proofs_implications_and_models/\">mathematical model</a>, because we could physically take one apple off the table and&nbsp;<em>make</em>&nbsp;the statement false, and you can't do that with math.</p>\n<p>This question doesn't feel like it should be very hard. &nbsp;And indeed the answer is not very difficult, but it is worth spelling out; because cases like \"justice\" or \"mercy\" will turn out to proceed in a similar fashion.</p>\n<p>Navigating to the six requires a&nbsp;<em>mixture&nbsp;</em>of physical and&nbsp;logical reference. &nbsp;This case begins with a physical reference, when we navigate to the physical apples on the table by talking about <em>the cause of</em> our apple-seeing experiences:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/c/c6/TableThoughts.jpg\" alt=\"\" width=\"422\" height=\"193\" /></p>\n<p>Next we have to call the stuff on the table 'apples'. &nbsp;But how, oh how can we do this, when grinding the universe and running it through a sieve will reveal not a single particle of appleness?</p>\n<p>This part was covered at some length in the&nbsp;<a href=\"/lw/on/reductionism/\">Reductionism</a>&nbsp;sequence. &nbsp;Standard physics uses the same&nbsp;<em>fundamental&nbsp;</em>theory to describe the flight of a Boeing 747 airplane, and collisions in the Relativistic Heavy Ion Collider. &nbsp;Nuclei and airplanes alike, according to our understanding, are obeying special relativity, quantum mechanics, and chromodynamics.</p>\n<p>We also use entirely different&nbsp;<em>models&nbsp;</em>to understand the aerodynamics of a 747 and a collision between gold nuclei in the RHIC. &nbsp;A computer modeling the aerodynamics of a 747 may not contain a single token, a single bit of RAM, that represents a quark. &nbsp;(Or a quantum field, really; but you get the idea.)</p>\n<p>So is the 747 made of something other than quarks? &nbsp;And is the statement \"this 747 has wings\" meaningless or false? &nbsp;No, we're just&nbsp;<em>modeling </em>the 747&nbsp;with&nbsp;<em>representational elements</em>&nbsp;that do not have a one-to-one correspondence with <em>individual </em>quarks.</p>\n<p>Similarly with apples. &nbsp;To compare a mental image of high-level apple-objects to physical reality, for it to be true under a correspondence theory of truth, doesn't require that apples be&nbsp;<em>fundamental in physical law.</em>&nbsp; A single discrete element of fundamental physics is not the only thing that a statement can ever be compared-to. &nbsp;We just need truth conditions that categorize the low-level states of the universe, so that different low-level physical states are inside or outside the mental image of \"some apples on the table\" or alternatively \"a kitten on the table\".</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/b/b9/PossibleWorlds.jpg\" alt=\"\" width=\"500\" height=\"272\" /></p>\n<p>Now we can draw a correspondence from our image of discrete high-level apple objects, to reality.</p>\n<p>Next we need to count the apple-objects in each pile, using some procedure along the lines of going from apple to apple, marking those already counted and not counting them a second time, and continuing until all the apples in each heap have been counted. &nbsp;And then, having counted two numbers, we'll multiply them together. &nbsp;You can imagine this as taking the physical state of the universe (or a high-level representation of it) and running it through a series of functions leading to a final output:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/7/73/Mixed1.jpg\" alt=\"\" width=\"190\" height=\"241\" /></p>\n<p>And of course operations like \"counting\" and \"multiplication\" are pinned down by the number-axioms of Peano Arithmetic:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/2/2c/Mixed2.jpg\" alt=\"\" width=\"300\" height=\"241\" /></p>\n<p>And we shouldn't forget that the image of the table, is being calculated from eyes which are in causal contact with the real table-made-of-particles out there in physical reality:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/a/a8/Mixed3.jpg\" alt=\"\" width=\"468\" height=\"282\" /></p>\n<p>And then there's also the point that the Peano axioms themselves are being quoted inside your brain in order to pin down the&nbsp;<em>ideal&nbsp;</em>multiplicative result - after all, you can get multiplications&nbsp;<em>wrong</em>&nbsp;- but I'm not going to draw the image for that one. &nbsp;(We tried, and it came out too crowded.)</p>\n<p>So long as the math&nbsp;<em>is</em>&nbsp;pinned down, any table of two apple piles should yield a single output when we run the math over it. Constraining this output constrains the possible states of the original, physical input universe:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/f/f6/AppleWorld.jpg\" alt=\"\" width=\"533\" height=\"267\" /></p>\n<p>And thus \"The product of the apple numbers is six\" is meaningful, constraining the possible worlds. It has a truth-condition, fulfilled by a mixture of physical reality and logical validity; and the correspondence is nailed down by a mixture of causal reference and axiomatic pinpointing.</p>\n<p>I usually simplify this to the idea of \"running a logical function over the physical universe\", but of course the small picture doesn't work unless the big picture works.</p>\n<hr />\n<p>The Great Reductionist Project can be seen as figuring out how to express meaningful sentences in terms of a combination of&nbsp;<em>physical references</em>&nbsp;(statements whose truth-value is determined by a truth-condition directly correspnding to the real universe we're embedded in) and&nbsp;<em>logical references</em>&nbsp;(valid implications of premises, or elements of models pinned down by axioms);&nbsp;where both physical references and logical references are to be described 'effectively' or 'formally', in computable or logical form. &nbsp;(I haven't had time to go into this last part but it's an already-popular idea in philosophy of computation.)</p>\n<p>And the Great Reductionist Thesis can be seen as the proposition that&nbsp;<em>everything</em>&nbsp;meaningful can be expressed this way&nbsp;<em>eventually.</em></p>\n<p><em></em>But it sometimes takes&nbsp;<em>a whole bunch of work.</em></p>\n<p><em></em>And to notice when somebody has <em>subtly</em>&nbsp;violated&nbsp;the Great Reductionist Thesis - to see when a current solution is <em>not</em>&nbsp;decomposable to physical and logical reference - requires a fair amount of self-sensitization before the transgressions become obvious.</p>\n<hr />\n<p><strong>Example: &nbsp;Counterfactuals.</strong></p>\n<p>Consider the following pair of sentences, widely used to introduce the idea of \"counterfactual conditioning\":</p>\n<ul>\n<li>(A) If Lee Harvey Oswald didn't shoot John F. Kennedy, someone else did.</li>\n<li>(B) If Lee Harvey Oswald hadn't shot John F. Kennedy, someone else would've.</li>\n</ul>\n<p>The first sentence seems agreeable - John F. Kennedy definitely was shot, historically speaking, so if it wasn't Lee Harvey Oswald it was <em>someone</em>. &nbsp;On the other hand, unless you believe the Illuminati planned it all, it doesn't seem particularly likely that if Lee Harvey Oswald had been removed from the equation, somebody else would've shot Kennedy instead.</p>\n<p>Which is to say that sentence (A) appears <em>true</em>, and sentence (B) appears <em>false</em>.</p>\n<p>One of the historical questions about the <em>meaning</em>&nbsp;of causal models - in fact, of causal assertions in general - is, \"How does this so-called 'causal' model of yours, differ from asserting a bunch of statistical relations? &nbsp;Okay, sure, these statistical dependencies have a nice neighborhood-structure, but why not just call them correlations with a nice neighborhood-structure; why use fancy terms like 'cause and effect'?\"</p>\n<p>And one of the most widely endorsed <em>answers,</em>&nbsp;including nowadays, is that causal models carry an extra meaning because they tell us about <em>counterfactual outcomes,</em>&nbsp;which ordinary statistical models don't. &nbsp;For example, suppose this is our causal model of how John F. Kennedy got shot:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/4/4f/President.svg\" alt=\"Kennedy causes Oswald\" width=\"586\" height=\"266\" /></p>\n<p>Roughly this is intended to convey the idea that there are no Illuminati: &nbsp;Kennedy causes Oswald to shoot him, does not cause anybody else to shoot him, and causes the Moon landing; but once you know that Kennedy was elected, there's no correlation between his probability of causing Oswald to shoot him and his probability of causing anyone else to shoot him. &nbsp;In particular, there's no Illuminati who monitor Oswald and send another shooter if Oswald fails.</p>\n<p>In any case, this diagram also implies that if Oswald hadn't shot Kennedy, nobody else would've, which is modified by a <em>counterfactual surgery&nbsp;</em>a.k.a. the do(.) operator, in which a node is severed from its former parents, set to a particular value, and its descendants then recomputed:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/b/be/CounterPres.svg\" alt=\"do Oswald=N\" width=\"586\" height=\"266\" /></p>\n<p>&nbsp;</p>\n<p>And so it was claimed that the <em>meaning</em>&nbsp;of the first diagram is embodied in its implicit claim (as made explicit in the second diagram) that \"if Oswald <em>hadn't</em>&nbsp;shot Kennedy, nobody else would've\". &nbsp;This statement is <em>true</em>, and if all the other implicit counterfactual statements are also true, the first causal model as a whole is a true causal model.</p>\n<p>What's wrong with this picture?</p>\n<p>Well... if you're <em>strict</em>&nbsp;about that whole combination-of-physics-and-logic business... the problem is that <em>there are no counterfactual universes</em>&nbsp;for a counterfactual statement to correspond-to. &nbsp;\"There's apples on the table\" can be true when the particles in the universe are arranged into a configuration where there's some clumps of organic molecules on the table. &nbsp;What arrangement of the particles in this universe could directly make true the statement \"If Oswald hadn't shot Kennedy, nobody else would've\"? &nbsp;In this universe, Oswald <em>did</em>&nbsp;shoot Kennedy and Kennedy <em>did</em>&nbsp;end up shot.</p>\n<p>But it's a subtle sort of thing, to <em>notice</em>&nbsp;when you're trying to establish the truth-condition of a sentence by comparison to counterfactual universes that are not measurable, are never observed, and do not in fact actually exist.</p>\n<p>Because our own brains carry out the same sort of 'counterfactual surgery' automatically and natively - so natively that it's embedded in the syntax of language. &nbsp;We don't say, \"What if we perform counterfactual surgery on our models to set 'Oswald shoots Kennedy' to false?\" &nbsp;We say, \"What if Oswald <em>hadn't </em>shot Kennedy?\" &nbsp;So there's this counterfactual-supposition operation which our brain does very quickly and invisibly to <em>imagine</em>&nbsp;a hypothetical non-existent universe where Oswald doesn't shoot Kennedy, and our brain very rapidly returns the supposition that Kennedy doesn't get shot, and this seems to be a fact like any other fact; and so why couldn't you just compare the causal model to this fact like any other fact?</p>\n<p>And in one sense, \"If Oswald hadn't shot Kennedy, nobody else would've\" <em>is</em>&nbsp;a fact; it's a <em>mixed reference</em>&nbsp;that starts with the causal model of the <em>actual</em>&nbsp;universe where there are <em>actually</em> no Illuminati, and proceeds from there to the <em>logical</em>&nbsp;operation of counterfactual surgery to yield an answer which, like 'six' for the product of apples on the table, is not actually present anywhere in the universe. &nbsp;But you can't say that the causal model is true <em>because</em>&nbsp;the counterfactuals are true. &nbsp;The truth of the counterfactuals has to be calculated from the truth of the causal model, followed by the implications of the counterfactual-surgery axioms. &nbsp;If the causal model couldn't be 'true' or 'false' on its own, by direct comparison to the actual real universe, there'd be no way for the counterfactuals to be true or false either, since no actual counterfactual universes exist.</p>\n<hr />\n<p>So that business of counterfactuals may sound like a relatively obscure example (though it's going to play a large role in decision theory later on, and I expect to revisit it then) but it sets up some even larger points.</p>\n<p>For example, the&nbsp;<a href=\"/lw/py/the_born_probabilities/\">Born probabilities</a>&nbsp;in quantum mechanics seem to talk about a 'degree of realness' that different parts of the configuration space have (proportional to the integral over squared modulus of that 'world').</p>\n<p>Could the Born probabilities be <em>basic</em>&nbsp;- could there just be a <em>basic law of physics</em>&nbsp;which just says directly that to find out how likely you are to be in any quantum world, the integral over squared modulus gives you the answer? &nbsp;And the same law could've just as easily have said that you're likely to find yourself in a world that goes over the integral of modulus to the power 1.99999?</p>\n<p>But then we would have 'mixed references' that mixed together&nbsp;<em>three kinds of stuff</em>&nbsp;- the Schrodinger Equation, a deterministic causal equation relating complex amplitudes inside a configuration space; logical validities and models; <em>and </em>a law which assigned fundamental-degree-of-realness a.k.a. magical-reality-fluid. &nbsp;Meaningful statements would talk about some mixture of physical laws over particle fields in our own universe, logical validities, and <em>degree-of-realness.</em></p>\n<p><em></em>This is just the same sort of problem if you say that causal models are meaningful and true relative to a mixture of three kinds of stuff, actual worlds, &nbsp;logical validities, and counterfactuals, and logical validities. &nbsp;You're only supposed to have <em>two</em>&nbsp;kinds of stuff.</p>\n<p>People who think qualia are fundamental are also trying to build references out of at least three different kinds of stuff: physical laws, logic, and experiences.</p>\n<p><a href=\"/r/lesswrong/lw/19d/the_anthropic_trilemma/\">Anthropic problems</a>&nbsp;similarly revolve around a mysterious degree-of-realness, since presumably when you make more copies of people, you make their experiences more anticipate-able somehow. &nbsp;But this doesn't say that anthropic questions are meaningless or incoherent. &nbsp;It says that since we can only <em>talk about</em> anthropic problems using three kinds of stuff, we haven't finished Doing Reductionism to it yet. &nbsp;(I have not yet encountered a claim to have finished Reducing anthropics which (a) ends up with only two kinds of stuff and (b) does not seem to imply that I should expect my experiences to <a href=\"/lw/17d/forcing_anthropics_boltzmann_brains/\">dissolve into Boltzmann-brain chaos in the next instant</a>, given that if all this talk of 'degree of realness' is nonsense, there is no way to say that physically-lawful copies of me are more common than Boltzmann brain copies of me.)</p>\n<p>Or to take it down a notch, <a href=\"http://wiki.lesswrong.com/wiki/Free_will\">naive theories of free will</a> can be seen as <em>obviously not-completed Reductions</em>&nbsp;when you consider that they now contain physics, logic, and this third sort of thingy called 'choices'.</p>\n<p>And - alas - modern philosophy is <em>full</em>&nbsp;of 'new sorts of stuff'; we have modal realism that makes <em>possibility</em>&nbsp;a real sort of thing, and then other philosophers appeal to the truth of statements about <em>conceivability</em>&nbsp;without any attempt to reduce conceivability into some mixture of the actually-physically-real-in-our-universe and logical axioms; and so on, and so on.</p>\n<p>But lest you be tempted to think that the correct course is always to just envision a simpler universe without the extra stuff, consider that we do <em>not</em>&nbsp;live in the 'naive un-free universe' in which all our choices are constrained by the malevolent outside hand of physics, leaving us as slaves - <em>reducing</em>&nbsp;choices to physics is not the same as taking a naive model with three kinds of stuff, and deleting all the 'choices' from it. &nbsp;This is confusing the project of getting the gnomes out of the haunted mine, with trying to unmake the rainbow. &nbsp;Counterfactual surgery was eventually given a formal and logical definition, but it was a lot of work to get that far - causal models had to be invented first, and before then, people could only wave their hands frantically in the air when asked what it meant for something to be a 'cause'. &nbsp;The overall moral I'm trying convey is that the Great Reductionist Project is <em>difficult;</em>&nbsp;it's not a matter of just proclaiming that there's no gnomes in the mine, or that rainbows couldn't possibly be 'supernatural'. &nbsp;There are all sorts of statement that were not originally, or are presently not&nbsp;<em>obviously</em>&nbsp;decomposable into physical law plus logic; but that doesn't mean you just give up immediately. &nbsp;The Great Reductionist Thesis is that reduction is always <em>possible eventually.</em>&nbsp; It is nowhere written that it is easy, or that your prior efforts were enough to find a solution if one existed.</p>\n<p>Continued next time with justice and mercy (or rather, fairness and goodness). &nbsp;Because clearly, if we end up with meaningful moral statements, they're <em>not </em>going to correspond to a combination of physics and logic&nbsp;<em>plus&nbsp;</em>morality.</p>\n<hr />\n<p><strong><a href=\"/lw/frz/mixed_reference_the_great_reductionist_project/#7z44\">Mainstream status</a>.</strong></p>\n<p style=\"text-align:right\">Part of the sequence <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\"><em>Highly Advanced Epistemology 101 for Beginners</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/fv3/by_which_it_may_be_judged/\">By Which It May Be Judged</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/fok/causal_universes/\">Causal Universes</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"LnEEs8xGooYmQ8iLA": 2, "xgpBASEThXPuKRhbS": 2, "wMPYFGmhcFg4bSb4Z": 2, "bmfs4jiLaF6HiiYkC": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bTsiPnFndZeqTnWpu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 52, "extendedScore": null, "score": 0.000115, "legacy": true, "legacyId": "20447", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "SqFbMbtxGybdS2gRs", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "by-which-it-may-be-judged", "canonicalPrevPostSlug": "second-order-logic-the-controversy", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 52, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Followup to</strong>:&nbsp;<a href=\"/lw/f4e/logical_pinpointing/\">Logical Pinpointing</a>,&nbsp;<a href=\"/lw/f1u/causal_reference/\">Causal Reference</a></p>\n<p style=\"padding-left: 30px;\"><span style=\"font-variant: small-caps;\">Take the universe and grind it down to the finest powder and sieve it through the finest sieve and then&nbsp;</span><em style=\"font-variant: small-caps;\">show</em><span style=\"font-variant: small-caps;\">&nbsp;me one atom of justice, one molecule of mercy.</span></p>\n<p style=\"padding-left: 60px;\">- Death, in&nbsp;<em>Hogfather</em>&nbsp;by Terry Pratchett</p>\n<p>Meditation: So far we've talked about two kinds of meaningfulness and two ways that sentences can refer; a way of comparing to physical things found by following pinned-down causal links, and logical validity by comparison to models pinned-down by axioms. Is there anything else that can be meaningfully talked about? Where would you find justice, or mercy?<a id=\"more\"></a></p>\n<p>...&nbsp;<br>...&nbsp;<br>...</p>\n<p>Suppose that I pointed at a couple of piles of apples on a table, a pile of two apples and a pile of three apples.</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/c/ca/Table.jpg\" alt=\"\" width=\"264\" height=\"154\"></p>\n<p>And lo, I said: &nbsp;\"If we took the number of apples in each pile, and multiplied those numbers together, we'd get six.\"</p>\n<p>Nowhere in the physical universe is that 'six' written - there's nowhere in the laws of physics where you'll find a floating six. Even on the table itself there's only five apples, and apples&nbsp;<a href=\"/lw/on/reductionism/\">aren't fundamental</a>. Or to put it another way:</p>\n<p><span style=\"font-variant: small-caps;\">Take the apples and grind them down to the finest powder and sieve them through the finest sieve and then&nbsp;<em>show</em>&nbsp;me one atom of sixness, one molecule of multiplication.</span></p>\n<p>Nor can the statement be true as a matter of pure math, comparing to some Platonic six within a <a href=\"/lw/f43/proofs_implications_and_models/\">mathematical model</a>, because we could physically take one apple off the table and&nbsp;<em>make</em>&nbsp;the statement false, and you can't do that with math.</p>\n<p>This question doesn't feel like it should be very hard. &nbsp;And indeed the answer is not very difficult, but it is worth spelling out; because cases like \"justice\" or \"mercy\" will turn out to proceed in a similar fashion.</p>\n<p>Navigating to the six requires a&nbsp;<em>mixture&nbsp;</em>of physical and&nbsp;logical reference. &nbsp;This case begins with a physical reference, when we navigate to the physical apples on the table by talking about <em>the cause of</em> our apple-seeing experiences:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/c/c6/TableThoughts.jpg\" alt=\"\" width=\"422\" height=\"193\"></p>\n<p>Next we have to call the stuff on the table 'apples'. &nbsp;But how, oh how can we do this, when grinding the universe and running it through a sieve will reveal not a single particle of appleness?</p>\n<p>This part was covered at some length in the&nbsp;<a href=\"/lw/on/reductionism/\">Reductionism</a>&nbsp;sequence. &nbsp;Standard physics uses the same&nbsp;<em>fundamental&nbsp;</em>theory to describe the flight of a Boeing 747 airplane, and collisions in the Relativistic Heavy Ion Collider. &nbsp;Nuclei and airplanes alike, according to our understanding, are obeying special relativity, quantum mechanics, and chromodynamics.</p>\n<p>We also use entirely different&nbsp;<em>models&nbsp;</em>to understand the aerodynamics of a 747 and a collision between gold nuclei in the RHIC. &nbsp;A computer modeling the aerodynamics of a 747 may not contain a single token, a single bit of RAM, that represents a quark. &nbsp;(Or a quantum field, really; but you get the idea.)</p>\n<p>So is the 747 made of something other than quarks? &nbsp;And is the statement \"this 747 has wings\" meaningless or false? &nbsp;No, we're just&nbsp;<em>modeling </em>the 747&nbsp;with&nbsp;<em>representational elements</em>&nbsp;that do not have a one-to-one correspondence with <em>individual </em>quarks.</p>\n<p>Similarly with apples. &nbsp;To compare a mental image of high-level apple-objects to physical reality, for it to be true under a correspondence theory of truth, doesn't require that apples be&nbsp;<em>fundamental in physical law.</em>&nbsp; A single discrete element of fundamental physics is not the only thing that a statement can ever be compared-to. &nbsp;We just need truth conditions that categorize the low-level states of the universe, so that different low-level physical states are inside or outside the mental image of \"some apples on the table\" or alternatively \"a kitten on the table\".</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/b/b9/PossibleWorlds.jpg\" alt=\"\" width=\"500\" height=\"272\"></p>\n<p>Now we can draw a correspondence from our image of discrete high-level apple objects, to reality.</p>\n<p>Next we need to count the apple-objects in each pile, using some procedure along the lines of going from apple to apple, marking those already counted and not counting them a second time, and continuing until all the apples in each heap have been counted. &nbsp;And then, having counted two numbers, we'll multiply them together. &nbsp;You can imagine this as taking the physical state of the universe (or a high-level representation of it) and running it through a series of functions leading to a final output:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/7/73/Mixed1.jpg\" alt=\"\" width=\"190\" height=\"241\"></p>\n<p>And of course operations like \"counting\" and \"multiplication\" are pinned down by the number-axioms of Peano Arithmetic:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/2/2c/Mixed2.jpg\" alt=\"\" width=\"300\" height=\"241\"></p>\n<p>And we shouldn't forget that the image of the table, is being calculated from eyes which are in causal contact with the real table-made-of-particles out there in physical reality:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/a/a8/Mixed3.jpg\" alt=\"\" width=\"468\" height=\"282\"></p>\n<p>And then there's also the point that the Peano axioms themselves are being quoted inside your brain in order to pin down the&nbsp;<em>ideal&nbsp;</em>multiplicative result - after all, you can get multiplications&nbsp;<em>wrong</em>&nbsp;- but I'm not going to draw the image for that one. &nbsp;(We tried, and it came out too crowded.)</p>\n<p>So long as the math&nbsp;<em>is</em>&nbsp;pinned down, any table of two apple piles should yield a single output when we run the math over it. Constraining this output constrains the possible states of the original, physical input universe:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/f/f6/AppleWorld.jpg\" alt=\"\" width=\"533\" height=\"267\"></p>\n<p>And thus \"The product of the apple numbers is six\" is meaningful, constraining the possible worlds. It has a truth-condition, fulfilled by a mixture of physical reality and logical validity; and the correspondence is nailed down by a mixture of causal reference and axiomatic pinpointing.</p>\n<p>I usually simplify this to the idea of \"running a logical function over the physical universe\", but of course the small picture doesn't work unless the big picture works.</p>\n<hr>\n<p>The Great Reductionist Project can be seen as figuring out how to express meaningful sentences in terms of a combination of&nbsp;<em>physical references</em>&nbsp;(statements whose truth-value is determined by a truth-condition directly correspnding to the real universe we're embedded in) and&nbsp;<em>logical references</em>&nbsp;(valid implications of premises, or elements of models pinned down by axioms);&nbsp;where both physical references and logical references are to be described 'effectively' or 'formally', in computable or logical form. &nbsp;(I haven't had time to go into this last part but it's an already-popular idea in philosophy of computation.)</p>\n<p>And the Great Reductionist Thesis can be seen as the proposition that&nbsp;<em>everything</em>&nbsp;meaningful can be expressed this way&nbsp;<em>eventually.</em></p>\n<p><em></em>But it sometimes takes&nbsp;<em>a whole bunch of work.</em></p>\n<p><em></em>And to notice when somebody has <em>subtly</em>&nbsp;violated&nbsp;the Great Reductionist Thesis - to see when a current solution is <em>not</em>&nbsp;decomposable to physical and logical reference - requires a fair amount of self-sensitization before the transgressions become obvious.</p>\n<hr>\n<p><strong id=\"Example___Counterfactuals_\">Example: &nbsp;Counterfactuals.</strong></p>\n<p>Consider the following pair of sentences, widely used to introduce the idea of \"counterfactual conditioning\":</p>\n<ul>\n<li>(A) If Lee Harvey Oswald didn't shoot John F. Kennedy, someone else did.</li>\n<li>(B) If Lee Harvey Oswald hadn't shot John F. Kennedy, someone else would've.</li>\n</ul>\n<p>The first sentence seems agreeable - John F. Kennedy definitely was shot, historically speaking, so if it wasn't Lee Harvey Oswald it was <em>someone</em>. &nbsp;On the other hand, unless you believe the Illuminati planned it all, it doesn't seem particularly likely that if Lee Harvey Oswald had been removed from the equation, somebody else would've shot Kennedy instead.</p>\n<p>Which is to say that sentence (A) appears <em>true</em>, and sentence (B) appears <em>false</em>.</p>\n<p>One of the historical questions about the <em>meaning</em>&nbsp;of causal models - in fact, of causal assertions in general - is, \"How does this so-called 'causal' model of yours, differ from asserting a bunch of statistical relations? &nbsp;Okay, sure, these statistical dependencies have a nice neighborhood-structure, but why not just call them correlations with a nice neighborhood-structure; why use fancy terms like 'cause and effect'?\"</p>\n<p>And one of the most widely endorsed <em>answers,</em>&nbsp;including nowadays, is that causal models carry an extra meaning because they tell us about <em>counterfactual outcomes,</em>&nbsp;which ordinary statistical models don't. &nbsp;For example, suppose this is our causal model of how John F. Kennedy got shot:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/4/4f/President.svg\" alt=\"Kennedy causes Oswald\" width=\"586\" height=\"266\"></p>\n<p>Roughly this is intended to convey the idea that there are no Illuminati: &nbsp;Kennedy causes Oswald to shoot him, does not cause anybody else to shoot him, and causes the Moon landing; but once you know that Kennedy was elected, there's no correlation between his probability of causing Oswald to shoot him and his probability of causing anyone else to shoot him. &nbsp;In particular, there's no Illuminati who monitor Oswald and send another shooter if Oswald fails.</p>\n<p>In any case, this diagram also implies that if Oswald hadn't shot Kennedy, nobody else would've, which is modified by a <em>counterfactual surgery&nbsp;</em>a.k.a. the do(.) operator, in which a node is severed from its former parents, set to a particular value, and its descendants then recomputed:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/b/be/CounterPres.svg\" alt=\"do Oswald=N\" width=\"586\" height=\"266\"></p>\n<p>&nbsp;</p>\n<p>And so it was claimed that the <em>meaning</em>&nbsp;of the first diagram is embodied in its implicit claim (as made explicit in the second diagram) that \"if Oswald <em>hadn't</em>&nbsp;shot Kennedy, nobody else would've\". &nbsp;This statement is <em>true</em>, and if all the other implicit counterfactual statements are also true, the first causal model as a whole is a true causal model.</p>\n<p>What's wrong with this picture?</p>\n<p>Well... if you're <em>strict</em>&nbsp;about that whole combination-of-physics-and-logic business... the problem is that <em>there are no counterfactual universes</em>&nbsp;for a counterfactual statement to correspond-to. &nbsp;\"There's apples on the table\" can be true when the particles in the universe are arranged into a configuration where there's some clumps of organic molecules on the table. &nbsp;What arrangement of the particles in this universe could directly make true the statement \"If Oswald hadn't shot Kennedy, nobody else would've\"? &nbsp;In this universe, Oswald <em>did</em>&nbsp;shoot Kennedy and Kennedy <em>did</em>&nbsp;end up shot.</p>\n<p>But it's a subtle sort of thing, to <em>notice</em>&nbsp;when you're trying to establish the truth-condition of a sentence by comparison to counterfactual universes that are not measurable, are never observed, and do not in fact actually exist.</p>\n<p>Because our own brains carry out the same sort of 'counterfactual surgery' automatically and natively - so natively that it's embedded in the syntax of language. &nbsp;We don't say, \"What if we perform counterfactual surgery on our models to set 'Oswald shoots Kennedy' to false?\" &nbsp;We say, \"What if Oswald <em>hadn't </em>shot Kennedy?\" &nbsp;So there's this counterfactual-supposition operation which our brain does very quickly and invisibly to <em>imagine</em>&nbsp;a hypothetical non-existent universe where Oswald doesn't shoot Kennedy, and our brain very rapidly returns the supposition that Kennedy doesn't get shot, and this seems to be a fact like any other fact; and so why couldn't you just compare the causal model to this fact like any other fact?</p>\n<p>And in one sense, \"If Oswald hadn't shot Kennedy, nobody else would've\" <em>is</em>&nbsp;a fact; it's a <em>mixed reference</em>&nbsp;that starts with the causal model of the <em>actual</em>&nbsp;universe where there are <em>actually</em> no Illuminati, and proceeds from there to the <em>logical</em>&nbsp;operation of counterfactual surgery to yield an answer which, like 'six' for the product of apples on the table, is not actually present anywhere in the universe. &nbsp;But you can't say that the causal model is true <em>because</em>&nbsp;the counterfactuals are true. &nbsp;The truth of the counterfactuals has to be calculated from the truth of the causal model, followed by the implications of the counterfactual-surgery axioms. &nbsp;If the causal model couldn't be 'true' or 'false' on its own, by direct comparison to the actual real universe, there'd be no way for the counterfactuals to be true or false either, since no actual counterfactual universes exist.</p>\n<hr>\n<p>So that business of counterfactuals may sound like a relatively obscure example (though it's going to play a large role in decision theory later on, and I expect to revisit it then) but it sets up some even larger points.</p>\n<p>For example, the&nbsp;<a href=\"/lw/py/the_born_probabilities/\">Born probabilities</a>&nbsp;in quantum mechanics seem to talk about a 'degree of realness' that different parts of the configuration space have (proportional to the integral over squared modulus of that 'world').</p>\n<p>Could the Born probabilities be <em>basic</em>&nbsp;- could there just be a <em>basic law of physics</em>&nbsp;which just says directly that to find out how likely you are to be in any quantum world, the integral over squared modulus gives you the answer? &nbsp;And the same law could've just as easily have said that you're likely to find yourself in a world that goes over the integral of modulus to the power 1.99999?</p>\n<p>But then we would have 'mixed references' that mixed together&nbsp;<em>three kinds of stuff</em>&nbsp;- the Schrodinger Equation, a deterministic causal equation relating complex amplitudes inside a configuration space; logical validities and models; <em>and </em>a law which assigned fundamental-degree-of-realness a.k.a. magical-reality-fluid. &nbsp;Meaningful statements would talk about some mixture of physical laws over particle fields in our own universe, logical validities, and <em>degree-of-realness.</em></p>\n<p><em></em>This is just the same sort of problem if you say that causal models are meaningful and true relative to a mixture of three kinds of stuff, actual worlds, &nbsp;logical validities, and counterfactuals, and logical validities. &nbsp;You're only supposed to have <em>two</em>&nbsp;kinds of stuff.</p>\n<p>People who think qualia are fundamental are also trying to build references out of at least three different kinds of stuff: physical laws, logic, and experiences.</p>\n<p><a href=\"/r/lesswrong/lw/19d/the_anthropic_trilemma/\">Anthropic problems</a>&nbsp;similarly revolve around a mysterious degree-of-realness, since presumably when you make more copies of people, you make their experiences more anticipate-able somehow. &nbsp;But this doesn't say that anthropic questions are meaningless or incoherent. &nbsp;It says that since we can only <em>talk about</em> anthropic problems using three kinds of stuff, we haven't finished Doing Reductionism to it yet. &nbsp;(I have not yet encountered a claim to have finished Reducing anthropics which (a) ends up with only two kinds of stuff and (b) does not seem to imply that I should expect my experiences to <a href=\"/lw/17d/forcing_anthropics_boltzmann_brains/\">dissolve into Boltzmann-brain chaos in the next instant</a>, given that if all this talk of 'degree of realness' is nonsense, there is no way to say that physically-lawful copies of me are more common than Boltzmann brain copies of me.)</p>\n<p>Or to take it down a notch, <a href=\"http://wiki.lesswrong.com/wiki/Free_will\">naive theories of free will</a> can be seen as <em>obviously not-completed Reductions</em>&nbsp;when you consider that they now contain physics, logic, and this third sort of thingy called 'choices'.</p>\n<p>And - alas - modern philosophy is <em>full</em>&nbsp;of 'new sorts of stuff'; we have modal realism that makes <em>possibility</em>&nbsp;a real sort of thing, and then other philosophers appeal to the truth of statements about <em>conceivability</em>&nbsp;without any attempt to reduce conceivability into some mixture of the actually-physically-real-in-our-universe and logical axioms; and so on, and so on.</p>\n<p>But lest you be tempted to think that the correct course is always to just envision a simpler universe without the extra stuff, consider that we do <em>not</em>&nbsp;live in the 'naive un-free universe' in which all our choices are constrained by the malevolent outside hand of physics, leaving us as slaves - <em>reducing</em>&nbsp;choices to physics is not the same as taking a naive model with three kinds of stuff, and deleting all the 'choices' from it. &nbsp;This is confusing the project of getting the gnomes out of the haunted mine, with trying to unmake the rainbow. &nbsp;Counterfactual surgery was eventually given a formal and logical definition, but it was a lot of work to get that far - causal models had to be invented first, and before then, people could only wave their hands frantically in the air when asked what it meant for something to be a 'cause'. &nbsp;The overall moral I'm trying convey is that the Great Reductionist Project is <em>difficult;</em>&nbsp;it's not a matter of just proclaiming that there's no gnomes in the mine, or that rainbows couldn't possibly be 'supernatural'. &nbsp;There are all sorts of statement that were not originally, or are presently not&nbsp;<em>obviously</em>&nbsp;decomposable into physical law plus logic; but that doesn't mean you just give up immediately. &nbsp;The Great Reductionist Thesis is that reduction is always <em>possible eventually.</em>&nbsp; It is nowhere written that it is easy, or that your prior efforts were enough to find a solution if one existed.</p>\n<p>Continued next time with justice and mercy (or rather, fairness and goodness). &nbsp;Because clearly, if we end up with meaningful moral statements, they're <em>not </em>going to correspond to a combination of physics and logic&nbsp;<em>plus&nbsp;</em>morality.</p>\n<hr>\n<p><strong id=\"Mainstream_status_\"><a href=\"/lw/frz/mixed_reference_the_great_reductionist_project/#7z44\">Mainstream status</a>.</strong></p>\n<p style=\"text-align:right\">Part of the sequence <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\"><em>Highly Advanced Epistemology 101 for Beginners</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/fv3/by_which_it_may_be_judged/\">By Which It May Be Judged</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/fok/causal_universes/\">Causal Universes</a>\"</p>", "sections": [{"title": "Example: \u00a0Counterfactuals.", "anchor": "Example___Counterfactuals_", "level": 1}, {"title": "Mainstream status.", "anchor": "Mainstream_status_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "357 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 357, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3FoMuCLqZggTxoC3S", "PXoWk554FZ4Gpfvah", "tPqQdLCuxanjhoaNs", "Z2CuyKtkCmWGQtAEh", "3ZKvf9u2XEWddGZmS", "y7jZ9BLEeuNTzgAE5", "LubwxZHKKvCivYGzx", "zqwWicCLNBSA5Ssmn", "o5F2p3krzT4JgzqQc"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-05T04:39:36.394Z", "modifiedAt": null, "url": null, "title": "How to incentivize LW wiki edits?", "slug": "how-to-incentivize-lw-wiki-edits", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:25.301Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sFR5wXtHZtE6cwYEm/how-to-incentivize-lw-wiki-edits", "pageUrlRelative": "/posts/sFR5wXtHZtE6cwYEm/how-to-incentivize-lw-wiki-edits", "linkUrl": "https://www.lesswrong.com/posts/sFR5wXtHZtE6cwYEm/how-to-incentivize-lw-wiki-edits", "postedAtFormatted": "Wednesday, December 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20incentivize%20LW%20wiki%20edits%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20incentivize%20LW%20wiki%20edits%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsFR5wXtHZtE6cwYEm%2Fhow-to-incentivize-lw-wiki-edits%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20incentivize%20LW%20wiki%20edits%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsFR5wXtHZtE6cwYEm%2Fhow-to-incentivize-lw-wiki-edits", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsFR5wXtHZtE6cwYEm%2Fhow-to-incentivize-lw-wiki-edits", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 86, "htmlBody": "<p>How can we incentivize more productive activity on the LW wiki? There are many articles that could be <a href=\"/lw/fcu/ai_riskrelated_improvements_to_the_lw_wiki/7ugk\">created</a> or expanded.</p>\n<p>Are there previous discussions on this?</p>\n<p>One suggestion: We could add a \"good explanation!\" button at the bottom of each article, and every time it is clicked, the user account responsible for the plurality of words in the current draft of the article gets 10 karma points. This requires that LW and LW-wiki accounts be synced, first.</p>\n<p>How can this suggestion be improved?</p>\n<p>What other suggestions do people have?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sFR5wXtHZtE6cwYEm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 13, "extendedScore": null, "score": 1.0502641735689182e-06, "legacy": true, "legacyId": "20457", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-05T04:55:51.765Z", "modifiedAt": null, "url": null, "title": "Meetup : Washington DC meetup- new sequence?", "slug": "meetup-washington-dc-meetup-new-sequence", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "rocurley", "createdAt": "2011-07-11T23:21:02.854Z", "isAdmin": false, "displayName": "rocurley"}, "userId": "zrzRGQu6QueyJGN5g", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HsoBqMxX9gGG72BdF/meetup-washington-dc-meetup-new-sequence", "pageUrlRelative": "/posts/HsoBqMxX9gGG72BdF/meetup-washington-dc-meetup-new-sequence", "linkUrl": "https://www.lesswrong.com/posts/HsoBqMxX9gGG72BdF/meetup-washington-dc-meetup-new-sequence", "postedAtFormatted": "Wednesday, December 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Washington%20DC%20meetup-%20new%20sequence%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Washington%20DC%20meetup-%20new%20sequence%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHsoBqMxX9gGG72BdF%2Fmeetup-washington-dc-meetup-new-sequence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Washington%20DC%20meetup-%20new%20sequence%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHsoBqMxX9gGG72BdF%2Fmeetup-washington-dc-meetup-new-sequence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHsoBqMxX9gGG72BdF%2Fmeetup-washington-dc-meetup-new-sequence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 53, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/gq'>Washington DC meetup- new sequence?</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 December 2012 11:55:42PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">National Portrait Gallery Plaza, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We didn't pick out a meetup topic, so I'm going to propose we discuss the currently-in progress sequence.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/gq'>Washington DC meetup- new sequence?</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HsoBqMxX9gGG72BdF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0502734870296684e-06, "legacy": true, "legacyId": "20458", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_meetup__new_sequence_\">Discussion article for the meetup : <a href=\"/meetups/gq\">Washington DC meetup- new sequence?</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 December 2012 11:55:42PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">National Portrait Gallery Plaza, Washington, DC 20001, USA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We didn't pick out a meetup topic, so I'm going to propose we discuss the currently-in progress sequence.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Washington_DC_meetup__new_sequence_1\">Discussion article for the meetup : <a href=\"/meetups/gq\">Washington DC meetup- new sequence?</a></h2>", "sections": [{"title": "Discussion article for the meetup : Washington DC meetup- new sequence?", "anchor": "Discussion_article_for_the_meetup___Washington_DC_meetup__new_sequence_", "level": 1}, {"title": "Discussion article for the meetup : Washington DC meetup- new sequence?", "anchor": "Discussion_article_for_the_meetup___Washington_DC_meetup__new_sequence_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-05T05:10:23.157Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Whither Manufacturing?", "slug": "seq-rerun-whither-manufacturing", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:24.059Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JypJQdTkNDPBKMR8j/seq-rerun-whither-manufacturing", "pageUrlRelative": "/posts/JypJQdTkNDPBKMR8j/seq-rerun-whither-manufacturing", "linkUrl": "https://www.lesswrong.com/posts/JypJQdTkNDPBKMR8j/seq-rerun-whither-manufacturing", "postedAtFormatted": "Wednesday, December 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Whither%20Manufacturing%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Whither%20Manufacturing%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJypJQdTkNDPBKMR8j%2Fseq-rerun-whither-manufacturing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Whither%20Manufacturing%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJypJQdTkNDPBKMR8j%2Fseq-rerun-whither-manufacturing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJypJQdTkNDPBKMR8j%2Fseq-rerun-whither-manufacturing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 164, "htmlBody": "<p>Today's post, <a href=\"http://www.overcomingbias.com/2008/12/whither-manufac.html\">Whither Manufacturing?</a> was originally published on December 2, 2008.  A summary:</p>\n<p>&nbsp;</p>\n<blockquote>There's no general reason to suppose that nanotechnology will enable a boom in local production. The location of production is a trade off between economies and dis-economies of scale.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/frl/seq_rerun_recursive_selfimprovement/\">Recursive Self-Improvement</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JypJQdTkNDPBKMR8j", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0502818077269183e-06, "legacy": true, "legacyId": "20459", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["YYAhX8z5p77FS4P2x", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-05T11:02:18.885Z", "modifiedAt": null, "url": null, "title": "Mini advent calendar of Xrisks: nanotechnology", "slug": "mini-advent-calendar-of-xrisks-nanotechnology", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:24.105Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jGfDc26NkFynHDiFv/mini-advent-calendar-of-xrisks-nanotechnology", "pageUrlRelative": "/posts/jGfDc26NkFynHDiFv/mini-advent-calendar-of-xrisks-nanotechnology", "linkUrl": "https://www.lesswrong.com/posts/jGfDc26NkFynHDiFv/mini-advent-calendar-of-xrisks-nanotechnology", "postedAtFormatted": "Wednesday, December 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mini%20advent%20calendar%20of%20Xrisks%3A%20nanotechnology&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMini%20advent%20calendar%20of%20Xrisks%3A%20nanotechnology%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjGfDc26NkFynHDiFv%2Fmini-advent-calendar-of-xrisks-nanotechnology%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mini%20advent%20calendar%20of%20Xrisks%3A%20nanotechnology%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjGfDc26NkFynHDiFv%2Fmini-advent-calendar-of-xrisks-nanotechnology", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjGfDc26NkFynHDiFv%2Fmini-advent-calendar-of-xrisks-nanotechnology", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 192, "htmlBody": "<p><span style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 17px;\">The FHI's mini advent calendar: counting down through the big five existential risks. The third one is a also a novel risk: <a href=\"http://www.facebook.com/FHIOxford/posts/398561916885457\">nanotechnology</a>.</span></p>\n<p>&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\"><strong><span style=\"font-family:&quot;Arial&quot;,&quot;sans-serif&quot;\">Nanotechnology</span></strong></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\"><em><span style=\"font-family:&quot;Arial&quot;,&quot;sans-serif&quot;\">Current understanding: low<br /></span></em><em><span style=\"font-family:&quot;Arial&quot;,&quot;sans-serif&quot;\">Most worrying aspect: the good stuff and the bad stuff are the same thing</span></em></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\"><span style=\"font-family:&quot;Arial&quot;,&quot;sans-serif&quot;\">The potential of nanotechnology is its ability to completely transform and revolutionise manufacturing and materials. The peril of nanotechnology is its ability to completely transform and revolutionise manufacturing and materials. And it&rsquo;s hard to separate the two. Nanotech manufacturing promises to be extremely disruptive to existing trade arrangements and to the balance of economic power: small organisations could produce as many goods as much as whole countries today, collapsing standard trade relationships and causing sudden unemployment and poverty in places not expecting this.<br /></span><span style=\"font-family: Arial, sans-serif;\"><br />And in this suddenly unstable world, nanotechnology will also permit the mass production of many new tools of war &ndash; from microscopic spy drones to large scale weapons with exotic properties. It will also weaken trust in disarmament agreements, as a completely disarmed country would have the potential to assemble an entire arsenal &ndash; say of cruise missiles &ndash; in the span of a day or less.</span></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"XJjvxWB68GYpts93N": 2, "Rz5jb3cYHTSRmqNnN": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jGfDc26NkFynHDiFv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 6, "extendedScore": null, "score": 1.0504834728246951e-06, "legacy": true, "legacyId": "20463", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-05T12:54:30.200Z", "modifiedAt": null, "url": null, "title": "Absent Transhumanism and Transformative Technologies, Which Utopia is Left?", "slug": "absent-transhumanism-and-transformative-technologies-which", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:30.533Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fZQoWyjn5MjpHWpAC/absent-transhumanism-and-transformative-technologies-which", "pageUrlRelative": "/posts/fZQoWyjn5MjpHWpAC/absent-transhumanism-and-transformative-technologies-which", "linkUrl": "https://www.lesswrong.com/posts/fZQoWyjn5MjpHWpAC/absent-transhumanism-and-transformative-technologies-which", "postedAtFormatted": "Wednesday, December 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Absent%20Transhumanism%20and%20Transformative%20Technologies%2C%20Which%20Utopia%20is%20Left%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAbsent%20Transhumanism%20and%20Transformative%20Technologies%2C%20Which%20Utopia%20is%20Left%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfZQoWyjn5MjpHWpAC%2Fabsent-transhumanism-and-transformative-technologies-which%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Absent%20Transhumanism%20and%20Transformative%20Technologies%2C%20Which%20Utopia%20is%20Left%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfZQoWyjn5MjpHWpAC%2Fabsent-transhumanism-and-transformative-technologies-which", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfZQoWyjn5MjpHWpAC%2Fabsent-transhumanism-and-transformative-technologies-which", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 280, "htmlBody": "<p>Assume for the time being that it will forever remain beyond the scope of science to change Human Nature. AGI is also impossible, as is Nanotech, BioImmortality, and those things.</p>\n<p>Douglas Adams mice finished their human experiment, giving to you, personally, the job of redesigning earth, and specially human society, according to your wildest utopian dreams, but you can't change the unchangeables above.</p>\n<p>You can play with architecture, engineering, gender ratio, clothing, money, science grants, governments, feeding rituals, family constitution, the constitution itself, education, etc...&nbsp; Just don't forget if you slide something too far away from what our evolved brains were designed to accept, things may slide back, or instability and catastrophe may ensue.</p>\n<p>Finally, if you are not the kind of utilitarian that assigns exactly the same amount of importance to your desires, and to that of others, I want you to create this Utopia for yourself, and your values, not everyone.</p>\n<p>The point of this exercise is: The vast majority of folk not related to this community that I know, when asked about an ideal world, will not change human nature, or animal suffering, or things like that, they'll think about changing whatever the newspaper editors have been writing about last few weeks. I am wondering if there is symmetry here, and folks from this community here do not spend that much time thinking about those kinds of change which don't rely on transformative technologies.&nbsp; It is just an intuition pump, a gedankenexperiment if you will. Force your brain to face this counterfactual reality, and make the best world you can given those constraints. Maybe, if sufficiently many post here, the results might clarify something about CEV, or the sociology of LessWrongers...</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fZQoWyjn5MjpHWpAC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 4, "extendedScore": null, "score": 1.4e-05, "legacy": true, "legacyId": "20464", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 114, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-05T18:28:09.814Z", "modifiedAt": null, "url": null, "title": "Meetup : Berkeley: Boardgames", "slug": "meetup-berkeley-boardgames", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nisan", "createdAt": "2009-09-08T21:20:08.384Z", "isAdmin": false, "displayName": "Nisan"}, "userId": "sJv7yzCp5xfWBAPvG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/v8msNnFrbh85Z98fF/meetup-berkeley-boardgames", "pageUrlRelative": "/posts/v8msNnFrbh85Z98fF/meetup-berkeley-boardgames", "linkUrl": "https://www.lesswrong.com/posts/v8msNnFrbh85Z98fF/meetup-berkeley-boardgames", "postedAtFormatted": "Wednesday, December 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Berkeley%3A%20Boardgames&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Berkeley%3A%20Boardgames%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv8msNnFrbh85Z98fF%2Fmeetup-berkeley-boardgames%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Berkeley%3A%20Boardgames%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv8msNnFrbh85Z98fF%2Fmeetup-berkeley-boardgames", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fv8msNnFrbh85Z98fF%2Fmeetup-berkeley-boardgames", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 85, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/gr'>Berkeley: Boardgames</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">05 December 2012 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>There will be a meetup tonight at Zendo as usual. I will not be around but Alex Mennen will be hosting. This will be a boardgames meetup. Come hang out and play a game from Zendo's game library, or bring a game of your own if you like.\nFor directions to Zendo, see the mailing list: <a href=\"http://groups.google.com/group/bayarealesswrong\" rel=\"nofollow\">http://groups.google.com/group/bayarealesswrong</a>\nor call me at:\n<a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/gr'>Berkeley: Boardgames</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "v8msNnFrbh85Z98fF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0507390552424355e-06, "legacy": true, "legacyId": "20465", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Berkeley__Boardgames\">Discussion article for the meetup : <a href=\"/meetups/gr\">Berkeley: Boardgames</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">05 December 2012 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Berkeley, CA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>There will be a meetup tonight at Zendo as usual. I will not be around but Alex Mennen will be hosting. This will be a boardgames meetup. Come hang out and play a game from Zendo's game library, or bring a game of your own if you like.\nFor directions to Zendo, see the mailing list: <a href=\"http://groups.google.com/group/bayarealesswrong\" rel=\"nofollow\">http://groups.google.com/group/bayarealesswrong</a>\nor call me at:\n<a href=\"http://i.imgur.com/Vcafy.png\" rel=\"nofollow\">http://i.imgur.com/Vcafy.png</a></p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Berkeley__Boardgames1\">Discussion article for the meetup : <a href=\"/meetups/gr\">Berkeley: Boardgames</a></h2>", "sections": [{"title": "Discussion article for the meetup : Berkeley: Boardgames", "anchor": "Discussion_article_for_the_meetup___Berkeley__Boardgames", "level": 1}, {"title": "Discussion article for the meetup : Berkeley: Boardgames", "anchor": "Discussion_article_for_the_meetup___Berkeley__Boardgames1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-05T23:44:21.496Z", "modifiedAt": null, "url": null, "title": "AI \"Boxing\" and Utility Functions", "slug": "ai-boxing-and-utility-functions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:24.404Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "nigerweiss", "createdAt": "2012-11-20T18:28:27.983Z", "isAdmin": false, "displayName": "nigerweiss"}, "userId": "DYPo3FWGnAX3mwp2Y", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XgSduW2omiN2fvhiE/ai-boxing-and-utility-functions", "pageUrlRelative": "/posts/XgSduW2omiN2fvhiE/ai-boxing-and-utility-functions", "linkUrl": "https://www.lesswrong.com/posts/XgSduW2omiN2fvhiE/ai-boxing-and-utility-functions", "postedAtFormatted": "Wednesday, December 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AI%20%22Boxing%22%20and%20Utility%20Functions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAI%20%22Boxing%22%20and%20Utility%20Functions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXgSduW2omiN2fvhiE%2Fai-boxing-and-utility-functions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AI%20%22Boxing%22%20and%20Utility%20Functions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXgSduW2omiN2fvhiE%2Fai-boxing-and-utility-functions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXgSduW2omiN2fvhiE%2Fai-boxing-and-utility-functions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 545, "htmlBody": "<p><br />So, I had this idea the other day when I was thinking about how to safely conduct research on potentially-FOOM-capable AI software. &nbsp;I'd like to sketch it out briefly and then get feedback on it.</p>\n<p>&nbsp;</p>\n<p>So, this started out with the idea that an AI based on AIXI is, in some sense, safer than a fully functional AI, due to the existence of the <a href=\"http://wiki.lesswrong.com/wiki/Anvil_problem\">anvil problem</a>. &nbsp;Because AIXI can't conceive of its own nonexistence, it has no preference ordering over its own mortality, and won't (shouldn't) resist any attempt to shut it down. &nbsp;In other words, if AIXI starts to FOOM undesirably out of control, you actually can go pull the plug on it without fuss. &nbsp;Unfortunately, in terms of safety, the anvil problem gives AIXI a number of other&nbsp;undesirable&nbsp;properties: both third parties and the AI itself can modify its utility function at any time, for any reason, which is very unstable. &nbsp;However, I think a similar idea might be useful for reducing (though not eliminating) the existential risks posed by powerful optimization processes in the near term.</p>\n<p>&nbsp;</p>\n<p>Say you have a piece of AI software&nbsp;&omega;, with an unFriendly instantaneous utility function, {maximize U}. &nbsp;You would like to use&nbsp;&omega; for some industrial application (say, manufacturing paperclips), but you're concerned about it FOOMing and resulting in human extinction. &nbsp;You decide to 'box' the AI, but, having read up on the subject, you are worried about it outsmarting you if you try to disable it.</p>\n<p>So, you replace your original utility function U with a modified version in terms of U, U'. &nbsp;<br /><br />U' = { max(U) | if&nbsp;&omega;&nbsp;exists</p>\n<p>&nbsp; &nbsp; &nbsp; &nbsp; &Oslash; | if&nbsp;&omega;&nbsp;!exist}</p>\n<p><br />U' has several useful qualities. &nbsp;The agent will resist modifications to its utility function, while not resisting attempts to turn it off. &nbsp;It is entirely&nbsp;ambivalent&nbsp;towards its own existence. &nbsp;As a result, if it began to FOOM undesirably, stopping it would be fairly trivial. &nbsp;Furthermore, the AI would have no incentive to&nbsp;deceive&nbsp;us, so it'd be fairly easy to keep an eye on. &nbsp; &nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>It should be noted that this is not in any way a final solution to the FAI problem. &nbsp;If the AI FOOMS quickly enough that we all die or get brain-hacked before we realize something's amiss, it could still all go very poorly. &nbsp;But that seems unlikely. &nbsp;I would guess that it takes at least a day or two to go from a boxed human-level AI to something strongly superhuman. &nbsp;Unfortunately, for this to work, everyone has to use it, which leave a lot of leftover existential risk from people using AIs without stable utility functions, cranks who think unFriendly AI will discover universal morality, and people who prematurely think they've figured out a good Friendly utility function. &nbsp; &nbsp; &nbsp;</p>\n<p>That said, something like this could help to gain more time to develop a proper FAI, and would be relatively simple to sell other developers on. &nbsp;SI or a similar organization could even develop a standardized, cross-platform open-source software package for utility functions with all of this built in, and distribute it to wannabe strong-AI developers.&nbsp;</p>\n<p>&nbsp;</p>\n<p>Are there any obvious problems with this idea that I'm missing? &nbsp;If so, can you think of any ways to address them? &nbsp;Has this sort of thing been discussed in the past? &nbsp;&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XgSduW2omiN2fvhiE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 2, "extendedScore": null, "score": 1.0509203802443978e-06, "legacy": true, "legacyId": "20466", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-05T23:51:51.774Z", "modifiedAt": null, "url": null, "title": "Meetup : 16/12 London Meetup", "slug": "meetup-16-12-london-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:29.914Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "philh", "createdAt": "2011-06-21T10:04:52.011Z", "isAdmin": false, "displayName": "philh"}, "userId": "nrP5EZZj4vRvYwQ7b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/chit4Fsh9QTFdCtua/meetup-16-12-london-meetup", "pageUrlRelative": "/posts/chit4Fsh9QTFdCtua/meetup-16-12-london-meetup", "linkUrl": "https://www.lesswrong.com/posts/chit4Fsh9QTFdCtua/meetup-16-12-london-meetup", "postedAtFormatted": "Wednesday, December 5th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%2016%2F12%20London%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%2016%2F12%20London%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fchit4Fsh9QTFdCtua%2Fmeetup-16-12-london-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%2016%2F12%20London%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fchit4Fsh9QTFdCtua%2Fmeetup-16-12-london-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fchit4Fsh9QTFdCtua%2Fmeetup-16-12-london-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 44, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/gs'>16/12 London Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 December 2012 02:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Holborn, London</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>A meetup in the <a href=\"http://maps.google.ca/maps?q=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;amp;hl=en&amp;amp;ll=51.516862,-0.119648&amp;amp;spn=0.005141,0.013604&amp;amp;sll=51.520547,-0.117545&amp;amp;sspn=0.010281,0.027208&amp;amp;hq=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;amp;radius=15000&amp;amp;t=m&amp;amp;z=16\" rel=\"nofollow\">Shakespeare&#39;s Head pub</a> by Holborn tube station. Everyone is welcome.</p>\n\n<p>We also have a <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">Google group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/gs'>16/12 London Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "chit4Fsh9QTFdCtua", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 1.2e-05, "legacy": true, "legacyId": "20467", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___16_12_London_Meetup\">Discussion article for the meetup : <a href=\"/meetups/gs\">16/12 London Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 December 2012 02:00:00PM (+0000)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Holborn, London</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>A meetup in the <a href=\"http://maps.google.ca/maps?q=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;amp;hl=en&amp;amp;ll=51.516862,-0.119648&amp;amp;spn=0.005141,0.013604&amp;amp;sll=51.520547,-0.117545&amp;amp;sspn=0.010281,0.027208&amp;amp;hq=Shakespeare%27s+Head,+Africa+House,+64-68+Kingsway,+Holborn,+London+WC2B+6BG,+United+Kingdom&amp;amp;radius=15000&amp;amp;t=m&amp;amp;z=16\" rel=\"nofollow\">Shakespeare's Head pub</a> by Holborn tube station. Everyone is welcome.</p>\n\n<p>We also have a <a href=\"https://groups.google.com/forum/?fromgroups=#!forum/lesswronglondon\">Google group</a>.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___16_12_London_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/gs\">16/12 London Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : 16/12 London Meetup", "anchor": "Discussion_article_for_the_meetup___16_12_London_Meetup", "level": 1}, {"title": "Discussion article for the meetup : 16/12 London Meetup", "anchor": "Discussion_article_for_the_meetup___16_12_London_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "9 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-06T00:42:47.643Z", "modifiedAt": null, "url": null, "title": "Train Philosophers with Pearl and Kahneman, not Plato and Kant", "slug": "train-philosophers-with-pearl-and-kahneman-not-plato-and", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:30.011Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LcEzxX2FNTKbB6KXS/train-philosophers-with-pearl-and-kahneman-not-plato-and", "pageUrlRelative": "/posts/LcEzxX2FNTKbB6KXS/train-philosophers-with-pearl-and-kahneman-not-plato-and", "linkUrl": "https://www.lesswrong.com/posts/LcEzxX2FNTKbB6KXS/train-philosophers-with-pearl-and-kahneman-not-plato-and", "postedAtFormatted": "Thursday, December 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Train%20Philosophers%20with%20Pearl%20and%20Kahneman%2C%20not%20Plato%20and%20Kant&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATrain%20Philosophers%20with%20Pearl%20and%20Kahneman%2C%20not%20Plato%20and%20Kant%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLcEzxX2FNTKbB6KXS%2Ftrain-philosophers-with-pearl-and-kahneman-not-plato-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Train%20Philosophers%20with%20Pearl%20and%20Kahneman%2C%20not%20Plato%20and%20Kant%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLcEzxX2FNTKbB6KXS%2Ftrain-philosophers-with-pearl-and-kahneman-not-plato-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLcEzxX2FNTKbB6KXS%2Ftrain-philosophers-with-pearl-and-kahneman-not-plato-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 771, "htmlBody": "<p><small>Part of the sequence: <a href=\"http://wiki.lesswrong.com/wiki/Rationality_and_Philosophy\">Rationality and Philosophy</a></small></p>\n<blockquote>\n<p>Hitherto the people attracted to philosophy have been mostly those who loved the big generalizations, which were all wrong, so that few people with exact minds have taken up the subject.</p>\n</blockquote>\n<p align=\"right\">Bertrand Russell</p>\n<p>&nbsp;</p>\n<p>I've complained before that philosophy is a <a href=\"/lw/4zs/philosophy_a_diseased_discipline/\">diseased discipline</a> which spends far too much of its time <a href=\"/lw/5kn/conceptual_analysis_and_moral_theory/\">debating definitions</a>, <a href=\"/lw/7tz/philosophy_by_humans_1_concepts_dont_work_that_way/\">ignoring relevant scientific results</a>, and endlessly re-interpreting <a href=\"http://en.wikipedia.org/wiki/Aristotle\">old</a> <a href=\"http://en.wikipedia.org/wiki/Immanuel_Kant\">dead</a> <a href=\"http://en.wikipedia.org/wiki/Friedrich_Nietzsche\">guys</a> who didn't know the slightest bit of 20th century science. Is that <em>still</em> the case?</p>\n<p><a href=\"/lw/frp/train_philosophers_with_pearl_and_kahneman_not/80rf\">You bet</a>.&nbsp;There's <em>some</em> good philosophy out there,&nbsp;but much of it is bad enough to make CMU philosopher Clark Glymour <a href=\"http://choiceandinference.com/2011/12/23/in-light-of-some-recent-discussion-over-at-new-apps-i-bring-you-clark-glymours-manifesto/\">suggest</a> that on tight university budgets, philosophy departments could be defunded unless their work is useful to (cited by) scientists and engineers &mdash; just as <a href=\"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/learn-43/lib/photoz/.g/scottd/fullbook.pdf\">his own work</a> on causal Bayes nets is now widely used in <a href=\"http://www.amazon.com/Causality-Reasoning-Inference-Judea-Pearl/dp/052189560X/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">artificial intelligence</a> and other fields.</p>\n<p>How did philosophy get this way? Russell's hypothesis is not too shabby. Check the syllabi of the undergraduate \"intro to philosophy\" classes at the world's <a href=\"http://www.philosophicalgourmet.com/overall.asp\">top 5 U.S. philosophy departments</a> &mdash; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/11/NYU-Central-Problems-in-Philosophy.pdf\">NYU</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/11/Rutgers-Introduction-to-Philosophy.pdf\">Rutgers</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/11/Princeton-Philosophy-and-the-Modern-Mind.pdf\">Princeton</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/11/Michigan-Introduction-to-Philosophy.pdf\">Michigan Ann Arbor</a>, and <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/11/Harvard-Introduction-to-Great-Works-in-Philosophy.pdf\">Harvard</a> &mdash; and you'll find that they spend a <em>lot</em> of time with (1) old dead guys who were wrong about almost everything because they knew nothing of modern logic, probability theory, or science, and with (2) 20th century philosophers who were way too enamored with <a href=\"/lw/4vr/less_wrong_rationality_and_mainstream_philosophy/#non-quine\">cogsci-ignorant armchair philosophy</a>. (I say more about the reasons for philosophy's degenerate state <a href=\"/lw/4zs/philosophy_a_diseased_discipline/\">here</a>.)</p>\n<p>As the CEO of a philosophy/math/compsci <a href=\"http://intelligence.org/\">research institute</a>, I think many philosophical problems are important. But the field of philosophy doesn't seem to be very good at answering them. What can we do?</p>\n<p>Why, come up with better philosophical methods, of course!</p>\n<p><a href=\"http://www.amazon.com/Scientific-Method-Historical-Philosophical-Introduction/dp/0415122813/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Scientific methods have improved over time</a>, and <a href=\"http://philsci-archive.pitt.edu/8780/1/Wheeler_FE2.pdf\">so can philosophical methods</a>. Here is the <em>first</em> of my recommendations...</p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<h3>More Pearl and Kahneman, less Plato and Kant</h3>\n<p>Philosophical training should begin with the latest and greatest formal methods (\"Pearl\" for the probabilistic graphical models made famous in <a href=\"http://www.amazon.com/Probabilistic-Reasoning-Intelligent-Systems-Plausible/dp/1558604790/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">Pearl 1988</a>), and the latest and greatest science (\"Kahneman\" for the science of human reasoning reviewed in <a href=\"http://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374275637/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">Kahneman 2011</a>). Beginning with Plato and Kant (and company), as most universities do today, both (1) filters for inexact thinkers, as Russell suggested, and (2) teaches people to have too much respect for failed philosophical methods that are out of touch with 20th century breakthroughs in math and science.</p>\n<p>So, I recommend we teach young philosophy students:</p>\n<table border=\"0\">\n<tbody>\n<tr>\n<td align=\"right\"><em>more</em> <a href=\"http://www.amazon.com/Rational-Decision-Making-Franz-Eisenf%C3%BChr/dp/3642028500/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">Bayesian rationality</a>, <a href=\"http://www.amazon.com/Rationality-Reflective-Mind-Keith-Stanovich/dp/0195341147/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">heuristics and biases</a>, &amp; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/09/Larrick-Debiasing.pdf\">debiasing</a>,</td>\n<td><em>less</em></td>\n<td><a href=\"http://en.wikipedia.org/wiki/Critical_thinking\">informal \"critical thinking skills\"</a>;</td>\n</tr>\n<tr>\n<td align=\"right\"><em>more</em> <a href=\"/lw/f43/proofs_implications_and_models/\">mathematical logic</a> &amp; <a href=\"http://www.amazon.com/Introduction-Theory-Computation-Michael-Sipser/dp/113318779X/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">theory of computation</a>,</td>\n<td><em>less</em></td>\n<td><a href=\"http://en.wikipedia.org/wiki/Term_logic\">term logic</a>;</td>\n</tr>\n<tr>\n<td align=\"right\"><em>more</em> <a href=\"http://www.amazon.com/Probabilistic-Graphical-Models-Principles-Computation/dp/0262013193/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">probability theory</a> &amp; <a href=\"http://www.amazon.com/Scientific-Reasoning-The-Bayesian-Approach/dp/081269578X/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">Bayesian scientific method</a>,</td>\n<td><em>less</em></td>\n<td><a href=\"http://en.wikipedia.org/wiki/Philosophy_of_science\">pre-1980 philosophy of science</a>;</td>\n</tr>\n<tr>\n<td align=\"right\"><em>more</em> <a href=\"http://www.amazon.com/The-Book-Concepts-Bradford-Books/dp/0262632993/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">psychology of concepts</a> &amp; <a href=\"http://www.amazon.com/Bayesian-Reasoning-Machine-Learning-Barber/dp/0521518148/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">machine learning</a>,</td>\n<td><em>less</em></td>\n<td><a href=\"http://en.wikipedia.org/wiki/Philosophical_analysis\">conceptual analysis</a>;</td>\n</tr>\n<tr>\n<td align=\"right\"><em>more</em> <a href=\"http://philsci-archive.pitt.edu/8780/1/Wheeler_FE2.pdf\">formal epistemology</a> &amp; <a href=\"http://en.wikipedia.org/wiki/Computational_epistemology\">computational epistemology</a>,</td>\n<td><em>less</em></td>\n<td><a href=\"http://en.wikipedia.org/wiki/Epistemology\">pre-1980 epistemology</a>;</td>\n</tr>\n<tr>\n<td align=\"right\"><em>more</em> <a href=\"http://www.amazon.com/Modern-Physics-Kenneth-S-Krane/dp/1118061144/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">physics</a> &amp; <a href=\"http://www.amazon.com/Relativity-Gravitation-Cosmology-Introduction-Physics/dp/0199573646/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">cosmology</a>,</td>\n<td><em>less</em></td>\n<td><a href=\"http://en.wikipedia.org/wiki/Metaphysics\">pre-1980 metaphysics</a>;</td>\n</tr>\n<tr>\n<td align=\"right\"><em>more</em> <a href=\"http://www.amazon.com/Neuroscience-Preference-Choice-Cognitive-Mechanisms/dp/0123814316/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">psychology of choice</a>,</td>\n<td><em>less</em></td>\n<td><a href=\"http://en.wikipedia.org/wiki/Free_will\">philosophy of free will</a>;</td>\n</tr>\n<tr>\n<td align=\"right\"><em>more</em> <a href=\"http://www.amazon.com/Moral-Psychology-Handbook-John-Doris/dp/0199655480/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">moral psychology</a>, <a href=\"http://www.amazon.com/Introduction-Statistical-Decision-Theory-Environmental/dp/026266206X/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">decision theory</a>, and <a href=\"http://www.cis.upenn.edu/~mkearns/teaching/cgt/\">game theory</a>,</td>\n<td><em>less</em></td>\n<td><a href=\"http://en.wikipedia.org/wiki/Ethics\">intuitionist moral philosophy</a>;</td>\n</tr>\n<tr>\n<td align=\"right\"><em>more</em> <a href=\"http://www.amazon.com/Cognitive-Science-An-Introduction-Mind/dp/0521708370/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">cognitive psychology</a> &amp; <a href=\"http://www.amazon.com/Cognitive-Neuroscience-Biology-Third-Edition/dp/0393927954/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">cognitive neuroscience</a>,</td>\n<td><em>less</em></td>\n<td><a href=\"http://en.wikipedia.org/wiki/Philosophy_of_mind\">pre-1980 philosophy of mind</a>;</td>\n</tr>\n<tr>\n<td align=\"right\"><em>more</em> <a href=\"http://www.amazon.com/Contemporary-Linguistics-6e-Study-Guide/dp/0312618514/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">linguistics</a> &amp; <a href=\"http://www.amazon.com/Fundamentals-Psycholinguistics-Linguistics-Fern-ndez/dp/1405191473/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">psycholinguistics</a>,</td>\n<td><em>less</em></td>\n<td><a href=\"http://en.wikipedia.org/wiki/Philosophy_of_language\">pre-1980 philosophy of language</a>;</td>\n</tr>\n<tr>\n<td align=\"right\"><em>more</em> <a href=\"http://repository.upenn.edu/cgi/viewcontent.cgi?article=1061&amp;context=neuroethics_pubs\">neuroaesthetics</a>,</td>\n<td><em>less</em></td>\n<td><a href=\"http://en.wikipedia.org/wiki/Aesthetics\">aesthetics</a>;</td>\n</tr>\n<tr>\n<td align=\"right\"><em>more</em> <a href=\"http://www.amazon.com/Causality-Reasoning-Inference-Judea-Pearl/dp/052189560X/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">causal models</a> &amp; psychology of <a href=\"http://repository.cmu.edu/cgi/viewcontent.cgi?article=1082&amp;context=philosophy\">causal perception</a>,</td>\n<td><em>less</em></td>\n<td><a href=\"http://en.wikipedia.org/wiki/Causality#Western_philosophy\">pre-1980 theories of causation</a>.</td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<p>(In other words: train philosophy students <a href=\"http://www.hss.cmu.edu/philosophy/courses/courses-2013-spring.pdf\">like they do at CMU</a>, but even \"more so.\")</p>\n<p>So, my own \"intro to philosophy\" mega-course might be guided by the following core readings:</p>\n<ol>\n<li>Stanovich, <em><a href=\"http://www.amazon.com/Rationality-Reflective-Mind-Keith-Stanovich/dp/0195341147/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">Rationality and the Reflective Mind</a></em> (2010) </li>\n<li>Hinman, <em><a href=\"http://www.amazon.com/Fundamentals-Mathematical-Logic-Peter-Hinman/dp/1568812620/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">Fundamentals of Mathematical Logic</a></em> (2005) </li>\n<li>Russell &amp; Norvig, <em><a href=\"http://www.amazon.com/Artificial-Intelligence-Modern-Approach-Edition/dp/0136042597/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">Artificial Intelligence: A Modern Approach</a></em> (3rd edition, 2009) &mdash; contains chapters which briefly introduce probability theory, probabilistic graphical models, computational decision theory and game theory, knowledge representation, machine learning, computational epistemology, and other useful subjects </li>\n<li>Sipser, <em><a href=\"http://www.amazon.com/Introduction-Theory-Computation-Michael-Sipser/dp/113318779X/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">Introduction to the Theory of Computation</a></em> (3rd edition, 2012) &mdash; relevant to lots of philosophical problems, as discussed in <a href=\"http://eccc.hpi-web.de/report/2011/108/\">Aaronson (2011)</a> </li>\n<li>Howson &amp; Urbach, <em><a href=\"http://www.amazon.com/Scientific-Reasoning-The-Bayesian-Approach/dp/081269578X/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">Scientific Reasoning: The Bayesian Approach</a></em> (3rd edition, 2005) </li>\n<li>Holyoak &amp; Morrison (eds.), <em><a href=\"http://www.amazon.com/Handbook-Thinking-Reasoning-Library-Psychology/dp/0199734682/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">The Oxford Handbook of Thinking and Reasoning</a></em> (2012) &mdash; contains chapters which briefly introduce the psychology of knowledge representation, concepts, categories, causal learning, explanation, argument, decision making, judgment heuristics, moral judgment, behavioral game theory, problem solving, creativity, and other useful subjects </li>\n<li>Dolan &amp; Sharot (eds.), <em><a href=\"http://www.amazon.com/Neuroscience-Preference-Choice-Cognitive-Mechanisms/dp/0123814316/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">Neuroscience of Preference and Choice</a></em> (2011) </li>\n<li>Krane, <em><a href=\"http://www.amazon.com/Modern-Physics-Kenneth-S-Krane/dp/1118061144/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0131103628&amp;linkCode=as2&amp;tag=lesswrong-20\">Modern Physics</a></em> (3rd edition, 2012) &mdash; includes a brief introduction to cosmology </li>\n</ol>\n<p>(There are many prerequisites to these, of course. I think philosophy should be a Highly Advanced subject of study that requires lots of prior training in maths and the sciences, like string theory but hopefully more productive.)</p>\n<p>Once students are equipped with some of the latest math and science, <em>then</em> let them tackle The Big Questions. I bet they'd get farther than those raised on Plato and Kant instead.</p>\n<p>You might also let them read 20th century analytic philosophy at that point &mdash; hopefully their training will have inoculated them from picking up <a href=\"/lw/4zs/philosophy_a_diseased_discipline/\">bad</a> <a href=\"/lw/7tz/philosophy_by_humans_1_concepts_dont_work_that_way/\">thinking</a> <a href=\"/lw/foz/philosophy_by_humans_3_intuitions_arent_shared/\">habits</a>.</p>\n<p>&nbsp;</p>\n<p align=\"right\">Previous post: <a href=\"/r/lesswrong/lw/fpe/philosophy_needs_to_trust_your_rationality_even/\">Philosophy Needs to Trust Your Rationality Even Though It Shouldn't</a></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GLykb6NukBeBQtDvQ": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LcEzxX2FNTKbB6KXS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 100, "baseScore": 106, "extendedScore": null, "score": 0.000235, "legacy": true, "legacyId": "20437", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "yFvZa9wkv5JoqhM8F", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "", "canonicalPrevPostSlug": "philosophy-needs-to-trust-your-rationality-even-though-it", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 106, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 513, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FwiPfF8Woe5JrzqEu", "2YPbdHgcjt7g5ZaFN", "wHjpCxeDeuFadG3jF", "Z2CuyKtkCmWGQtAEh", "jaN4EKrRnZdynTJjH", "2pdyL8bSGBfYsnkyS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-06T03:27:04.550Z", "modifiedAt": null, "url": null, "title": "[Link] Eleven dogmas of analytic philosophy", "slug": "link-eleven-dogmas-of-analytic-philosophy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:57.218Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "crazy88", "createdAt": "2011-09-02T04:50:28.550Z", "isAdmin": false, "displayName": "crazy88"}, "userId": "JjaJBbvGJE6D6FYvF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9Prvvc9PANsMHT2Zo/link-eleven-dogmas-of-analytic-philosophy", "pageUrlRelative": "/posts/9Prvvc9PANsMHT2Zo/link-eleven-dogmas-of-analytic-philosophy", "linkUrl": "https://www.lesswrong.com/posts/9Prvvc9PANsMHT2Zo/link-eleven-dogmas-of-analytic-philosophy", "postedAtFormatted": "Thursday, December 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Eleven%20dogmas%20of%20analytic%20philosophy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Eleven%20dogmas%20of%20analytic%20philosophy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9Prvvc9PANsMHT2Zo%2Flink-eleven-dogmas-of-analytic-philosophy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Eleven%20dogmas%20of%20analytic%20philosophy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9Prvvc9PANsMHT2Zo%2Flink-eleven-dogmas-of-analytic-philosophy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9Prvvc9PANsMHT2Zo%2Flink-eleven-dogmas-of-analytic-philosophy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 163, "htmlBody": "<p>Closely related to some of Luke's recent discussions about philosophy, philosopher Paul Thagard has recently called for changes to the way we do philosophy:</p>\n<blockquote>\n<p>I prefer an alternative approach to philosophy that is much more closely tied to scientific investigations. This approach is sometimes called &ldquo;naturalistic philosophy&rdquo; or &ldquo;philosophy naturalized&rdquo;, but I like the more concise term natural philosophy. Before the words &ldquo;science&rdquo; and &ldquo;scientist&rdquo; became common in the nineteenth century, researchers such as Newton described what they did as natural philosophy. I propose to revive this term to cover a method that ties epistemology and ethics closely to the cognitive sciences, and ties metaphysics closely to physics and other sciences.</p>\n</blockquote>\n<p>In the same article, Thagard also lists eleven areas where modern philosophy goes awry. For example:</p>\n<blockquote>\n<p>3. People&rsquo;s intuitions are evidence for philosophical conclusions. Natural alternative: evaluate intuitions critically to determine their psychological causes, which are often more tied to prejudices and errors than truth. Don't trust your intuitions.</p>\n</blockquote>\n<p>Source: Philosopher,&nbsp;<a title=\"Paul Thagard\" href=\"http://www.psychologytoday.com/blog/hot-thought/201212/eleven-dogmas-analytic-philosophy\">Paul Thagard</a></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9Prvvc9PANsMHT2Zo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 1, "extendedScore": null, "score": 1.0510481329534579e-06, "legacy": true, "legacyId": "20483", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-06T03:27:57.314Z", "modifiedAt": null, "url": null, "title": "Meetup : Less Wrong: Cleveland", "slug": "meetup-less-wrong-cleveland", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raelifin", "createdAt": "2011-12-25T18:17:26.827Z", "isAdmin": false, "displayName": "Raelifin"}, "userId": "L5eSHjQcjZWhf39fG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rDFkzFHTeGD5vN7sf/meetup-less-wrong-cleveland", "pageUrlRelative": "/posts/rDFkzFHTeGD5vN7sf/meetup-less-wrong-cleveland", "linkUrl": "https://www.lesswrong.com/posts/rDFkzFHTeGD5vN7sf/meetup-less-wrong-cleveland", "postedAtFormatted": "Thursday, December 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Less%20Wrong%3A%20Cleveland&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Less%20Wrong%3A%20Cleveland%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrDFkzFHTeGD5vN7sf%2Fmeetup-less-wrong-cleveland%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Less%20Wrong%3A%20Cleveland%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrDFkzFHTeGD5vN7sf%2Fmeetup-less-wrong-cleveland", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrDFkzFHTeGD5vN7sf%2Fmeetup-less-wrong-cleveland", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 48, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/gt'>Less Wrong: Cleveland</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">09 December 2012 03:00:23PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Cleveland</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Four of us will be meeting on the 9th. If you're interested and in the area, send an email!</p>\n\n<p>Discussion group: https://groups.google.com/forum/?fromgroups#!forum/less-wrong-cleveland</p>\n\n<p>Location to be decided.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/gt'>Less Wrong: Cleveland</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rDFkzFHTeGD5vN7sf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1.0510486374112293e-06, "legacy": true, "legacyId": "20484", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong__Cleveland\">Discussion article for the meetup : <a href=\"/meetups/gt\">Less Wrong: Cleveland</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">09 December 2012 03:00:23PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Cleveland</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Four of us will be meeting on the 9th. If you're interested and in the area, send an email!</p>\n\n<p>Discussion group: https://groups.google.com/forum/?fromgroups#!forum/less-wrong-cleveland</p>\n\n<p>Location to be decided.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Less_Wrong__Cleveland1\">Discussion article for the meetup : <a href=\"/meetups/gt\">Less Wrong: Cleveland</a></h2>", "sections": [{"title": "Discussion article for the meetup : Less Wrong: Cleveland", "anchor": "Discussion_article_for_the_meetup___Less_Wrong__Cleveland", "level": 1}, {"title": "Discussion article for the meetup : Less Wrong: Cleveland", "anchor": "Discussion_article_for_the_meetup___Less_Wrong__Cleveland1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-06T05:29:22.028Z", "modifiedAt": null, "url": null, "title": "A Probability Question", "slug": "a-probability-question-0", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:23.831Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JMiller", "createdAt": "2012-11-15T16:08:50.381Z", "isAdmin": false, "displayName": "JMiller"}, "userId": "YePJv5oBk8LKnWogz", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/J6wqoDxi7H7RD8MLR/a-probability-question-0", "pageUrlRelative": "/posts/J6wqoDxi7H7RD8MLR/a-probability-question-0", "linkUrl": "https://www.lesswrong.com/posts/J6wqoDxi7H7RD8MLR/a-probability-question-0", "postedAtFormatted": "Thursday, December 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Probability%20Question&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Probability%20Question%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ6wqoDxi7H7RD8MLR%2Fa-probability-question-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Probability%20Question%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ6wqoDxi7H7RD8MLR%2Fa-probability-question-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJ6wqoDxi7H7RD8MLR%2Fa-probability-question-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 390, "htmlBody": "<p>Hi, I am&nbsp;relatively&nbsp;new to this site, I am not sure if this is the right place to be posting.</p>\n<p>I am sure many of you are familiar with the following probability riddle:</p>\n<p>\"Sarah is walking along the street when she encounters a man. With the man is his son. He tells Sarah that he has only one more child at home. She is asked, 'what is the probability that my child is a girl?'\"</p>\n<p>Since Sarah does not know whether the boy is the elder or younger sibling, she needs to take four possible states into account. The father either had:</p>\n<p>1) a boy, then a &nbsp;girl</p>\n<p>2) a girl, then a boy</p>\n<p>3) two girls</p>\n<p>4) two boys</p>\n<p>Since 3 is&nbsp;impossible&nbsp;(Sarah knows there is at least one boy) that leaves three options. Two of those options imply a girl, the other implies a boy. Therefore, she can conclude that her probability estimate must be that it is 66.6% likely that there is a girl at home, and 33.3% likely that there is a boy.</p>\n<p>Compare this to George's situation.</p>\n<p>\"George is walking along the street when he encounters a man. With the man is his son. He tells George that the boy with him is his oldest son, and that he has only one more child at home. He is asked, 'What is the probability that my child at home is a girl?'\"</p>\n<p>George's&nbsp;probability estimate is clear: either the man had a boy then a girl, or he had two boys. Therefore, it is 50% likely that the child at home is a girl.</p>\n<p>My problem is this: I understand probability exists in the mind. The actual answer to the question is 100% one way or the other. Still, it seems like Sarah knows more about the situation, where George, by being given more information, knows less. His estimate is as good as knowing nothing other than the fact that the man has a child which could be equally likely to be a boy or a girl.&nbsp;</p>\n<p>If the reply is something like \"Well, Sarah actually knows less so her estimate is less&nbsp;likely&nbsp;to be right\" then that is something she could have figured out on her own, and then realized that assigning&nbsp;probability .5 is best anyways. That seems wrong.</p>\n<p>I know I must be making a mistake somewhere: why does it seem like George learns less by knowing more?</p>\n<p>Thank you for your help.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "J6wqoDxi7H7RD8MLR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 3, "extendedScore": null, "score": 1.051118291996417e-06, "legacy": true, "legacyId": "20487", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-06T06:49:24.373Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Hard Takeoff", "slug": "seq-rerun-hard-takeoff", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2kdLwy7apacLs3cjg/seq-rerun-hard-takeoff", "pageUrlRelative": "/posts/2kdLwy7apacLs3cjg/seq-rerun-hard-takeoff", "linkUrl": "https://www.lesswrong.com/posts/2kdLwy7apacLs3cjg/seq-rerun-hard-takeoff", "postedAtFormatted": "Thursday, December 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Hard%20Takeoff&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Hard%20Takeoff%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2kdLwy7apacLs3cjg%2Fseq-rerun-hard-takeoff%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Hard%20Takeoff%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2kdLwy7apacLs3cjg%2Fseq-rerun-hard-takeoff", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2kdLwy7apacLs3cjg%2Fseq-rerun-hard-takeoff", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 196, "htmlBody": "<p>Today's post, <a href=\"/lw/wf/hard_takeoff/\">Hard Takeoff</a> was originally published on 02 December 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Hard_Takeoff\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>It seems likely that there will be a discontinuity in the process of AI self-improvement around the time when AIs become capable of doing AI theory. A lot of things have to go exactly right in order to get a slow takeoff, and there is no particular reason to expect them all to happen that way.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fsb/seq_rerun_whither_manufacturing/\">Whither Manufacturing?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2kdLwy7apacLs3cjg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.051164215272532e-06, "legacy": true, "legacyId": "20488", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tjH8XPxAnr6JRbh7k", "JypJQdTkNDPBKMR8j", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-06T13:06:24.638Z", "modifiedAt": null, "url": null, "title": "A solvable Newcomb-like problem - part 3 of 3", "slug": "a-solvable-newcomb-like-problem-part-3-of-3", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:25.125Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Douglas_Reay", "createdAt": "2012-02-19T14:40:26.403Z", "isAdmin": false, "displayName": "Douglas_Reay"}, "userId": "jpnrRPxHozDiGBqp2", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dT9RFro8Prdanvdo8/a-solvable-newcomb-like-problem-part-3-of-3", "pageUrlRelative": "/posts/dT9RFro8Prdanvdo8/a-solvable-newcomb-like-problem-part-3-of-3", "linkUrl": "https://www.lesswrong.com/posts/dT9RFro8Prdanvdo8/a-solvable-newcomb-like-problem-part-3-of-3", "postedAtFormatted": "Thursday, December 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20solvable%20Newcomb-like%20problem%20-%20part%203%20of%203&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20solvable%20Newcomb-like%20problem%20-%20part%203%20of%203%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdT9RFro8Prdanvdo8%2Fa-solvable-newcomb-like-problem-part-3-of-3%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20solvable%20Newcomb-like%20problem%20-%20part%203%20of%203%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdT9RFro8Prdanvdo8%2Fa-solvable-newcomb-like-problem-part-3-of-3", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdT9RFro8Prdanvdo8%2Fa-solvable-newcomb-like-problem-part-3-of-3", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1761, "htmlBody": "<p>This is the third part of a three post sequence on a problem that is similar to <a href=\"http://wiki.lesswrong.com/wiki/Newcomb%27s_problem\">Newcomb's problem</a> but is posed in terms of probabilities and limited knowledge.</p>\n<p>&nbsp;&nbsp; Part 1 - stating the problem<br />&nbsp;&nbsp; Part 2 - some mathematics<br />&nbsp;&nbsp; Part 3 - towards a solution</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>In many situations we can say \"For practical purposes a probability of 0.9999999999999999999 is close enough to 1 that for the sake of simplicity I shall treat it as being 1, without that simplification altering my choices.\"</p>\n<p>However, there are some situations where the distinction does significantly alter that character of a situation so, when one is studying a new situation and one is not sure yet which of those two categories the situations falls into, the cautious approach is to re-frame the probability as being (1 - &delta;) where &delta; is small (eg 10 to the power of -12), and then examine the characteristics of the behaviour as &delta; tends towards 0.</p>\n<p>LessWrong wiki describes Omega as a super-powerful AI analogous to Laplace's demon, who knows the precise location and momentum of every atom in the universe, limited only by the laws of physics (so, if time travel isn't possible and some of our current thoughts on Quantum Mechanics are correct, then Omega's knowledge of the future is probabilistic, being limited by uncertainty).</p>\n<p>For the purposes of Newcomb's problem, and the rationality of Fred's decisions, it doesn't matter how close to that level of power Omega actually is.&nbsp;&nbsp; What matters, in terms of rationality, is the evidence available to Fred about how close Omega is to having to that level of power; or, more precisely, the evidence available to Fred relevant to Fred making predictions about Omega's performance in this particular game.</p>\n<p>Since this is a key factor in Fred's decision, we ought to be cautious.&nbsp; Rather than specify when setting up the problem that Fred knows with a certainty of 1 that Omega does have that power, it is better to specify a concrete level of evidence that would lead Fred to assign a probability of (1 - &delta;) to Omega having that power, then examine the effect upon which option to the box problem it is rational for Fred to pick, as &delta; tends towards 0.</p>\n<p>The Newcomb-like problem stated in part 1 of this sequence contains an Omega that it is rational for Fred to assign a less than unity probability of being able to perfectly predict Fred's choices.&nbsp; By using bets as analogies to the sort of evidence Fred might have available to him, we create an explicit variable that we can then manipulate to alter the precise probability Fred assigns to Omega's abilities.</p>\n<p>The other nice feature of the Newcomb-like problem given in part 1, is that it is explicitly solvable using the mathematics given in part 2.&nbsp; By making randomness an external feature (the device Fred brings with him) rather than purely a feature of Fred's internal mind, we can acknowledge the question of Omega being able to predict quantum events, capture it as a variable, and take it into account when setting out the payoff matrix for the problem.</p>\n<p>This means that, instead of Fred having to think \"When I walked into this room I was determined to pick one-box.&nbsp; As far as anyone knew or could predict, including myself, I intended to pick one-box.&nbsp; However nothing I do now can change Omegas decision - the money is already in the box.&nbsp; So I've nothing to lose by changing my mind.\"; Fred can now allocate a specific probability to whether Omega could predict Fred's chance of changing his mind in such circumstances, and Fred can take that into account in his strategy by making his chance of changing strategy explicit and external - basing it upon a random number device.</p>\n<p>Or, to put it another way, we are modelling a rational human who has a specific finite chance of talking himself into over riding a pre-committed strategy, as being made up from two components: a component that will infallibly stick to a pre-committed strategy plus a component with a known chance of change; we then treat the combined rational human as being someone infallibly committed to a meta-strategy that includes a chance of change - a mixed equilibrium, from Omega's point of view.</p>\n<p>&nbsp;</p>\n<p>Ok, time to look at the numbers and draw a pretty diagram...</p>\n<p>Fred is Player A, and he has two meta options:<br />&nbsp; A1 - play it safe.&nbsp; 100% chance of one-box and 0% chance of two-box<br />&nbsp; A2 - take a risk.&nbsp;&nbsp; Mentally pre-commit to using the device to produce 99% chance of one-box and 1% chance of two-box.</p>\n<p>Omega is Player B, and he has two meta options:<br />&nbsp; B1 - reward risk.&nbsp; Not attempt to distinguish between the mental state of Fred taking 1% risk and Fred playing it safe.<br />&nbsp; B2 - punish risk.&nbsp; Attempt to distinguish and, if Omega guesses Fred is taking risk rather than playing safe, punish it.</p>\n<p>We'll start out by assuming that if Omega does attempt to distinguish, then Omega will have a 1 in 10,000 false positive rate (thinking Fred is going to use the device, when actually Fred intends to play it safe) and a 1 in 10,000 false negative rate (thinking Fred is going to play it safe, when actually Fred intends to use the device).</p>\n<h3>A1 vs B1</h3>\n<p>Fred gains $1,000,000<br />Omega loses $1,000,000 to Fred but gains $1,000,000,000 from Alpha, for a net gain of $999,000,000</p>\n<h3>A2 vs B1</h3>\n<p>99% of the time, Fred gains $1,000,000 and Omega net gains $999,000,000<br />1% of the time, Fred gains $1,001,000 and Omega net loses $10,001,001,000</p>\n<p>Combining those gives an average of:<br />Fred gains: $1,000,010<br />Omega gains: $979,008,999</p>\n<h3>A1 vs B2</h3>\n<p>99.99% of the time, Omega correctly discerns that Fred is playing safe<br />Fred gains $1,000,000<br />Omega gains $999,000,000</p>\n<p>0.01% of the time, Omega falsely believes that Fred is taking a risk, and punishes that by putting $0 in Box A<br />Fred gains $0<br />Omega loses $10,000,000,000</p>\n<p>Combining those gives an average of:<br />Fred gains: $999,900<br />Omega gains: $997,900,100</p>\n<h3>A2 vs B2</h3>\n<p>In 100 trials out of 1,000,000 trials Omega incorrectly thinks Fred will play it safe, when actually Fred takes the risk of using the device.&nbsp; Of these:</p>\n<p style=\"padding-left: 30px;\">In 1 trial out of 1,000,000 trials: Omega incorrectly thinks Fred will play it safe, when actually Fred takes the risk of using the device and, in this case, the device picks two-box<br />==&gt; Fred gains $1,001,000<br />==&gt; Omega loses $10,001,001,000</p>\n<p style=\"padding-left: 30px;\">In 99 trials out of 1,000,000 trials: Omega incorrectly thinks Fred will play it safe, when actually Fred takes the risk of using the device and, in this case, the device picks one-box<br />==&gt; Fred gains $1,000,000<br />==&gt; Omega gains $999,000,000</p>\n<p>In 999,900 trials out of 1,000,000 trials Omega correctly thinks that Fred is going to take the risk of using the device.&nbsp; Of those:</p>\n<p style=\"padding-left: 30px;\">In 9,999 trials out of 1,000,000 trials: Omega correctly thinks that Fred is going to take the risk of using the device and, in this case, the device picks two-box<br />==&gt; Fred gains $1,000<br />==&gt; Omega gains $999,999,000</p>\n<p style=\"padding-left: 30px;\">In 989,901 trials out of 1,000,000 trials: Omega correctly thinks that Fred is going to take the risk of using the device and, in this case, the device picks one-box<br />==&gt; Fred gains $0<br />==&gt; Omega loses $10,000,000,000</p>\n<p>Combining those gives an average of:<br />Fred gains $110<br />Omega loses $9,888,922,110</p>\n<p><img src=\"http://www.chiark.greenend.org.uk/~douglasr/images/payoff_matrix8.jpg\" alt=\"\" width=\"600\" height=\"600\" /></p>\n<p>&nbsp;</p>\n<p>If Fred's best estimate, beforehand, of the chances of Omega choosing meta strategy B1 is P, then</p>\n<p>Fred's expected gain from choosing meta strategy A1 would be: $1,000,000 P + $999,999 (1-P)<br />Fred's expected gain from choosing meta strategy A2 would be: $1,000,010 P + $110 (1-P)</p>\n<p>We can ask ourselves at what value of P these two strategies would return the same expected gain:</p>\n<p>$1,000,000 P + $999,999 (1-P)&nbsp;&nbsp; =&nbsp;&nbsp;&nbsp; $1,000,010 P + $110 (1-P)<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $999,889 (1-P)&nbsp;&nbsp; =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $10 P<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $999,889&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $999,899 P<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; P&nbsp;&nbsp;&nbsp; = 0.99999<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (1-P)&nbsp; = 0.00001</p>\n<p>In other words, Fred thinking that Omega has even a 1 in 100,000 chance of being vindictive should be enough to deter Fred from taking the risky strategy.</p>\n<p>But how does that look from Omega's point of view?&nbsp;&nbsp; If Omega thinks that Fred's chance of picking meta strategy A1 is Q, then what is the cost to Omega of picking B2 1 in 100,000 times?</p>\n<p>Omega's expected gain from choosing meta strategy B1 would be: $999,000,000 Q + $979,008,999 (1-Q)<br />Omega's expected gain from choosing meta strategy B2 would be: $997,900,100 Q - $9,888,922,110 (1-Q)</p>\n<p>0.99999 { $999,000,000 Q + $979,008,999 (1-Q)&nbsp; } + 0.00001 { $997,900,100 Q - $9,888,922,110 (1-Q) }<br />= (1 - 0.00001) { $979,008,999 + $19,991,001 Q } + 0.00001 { - $9,888,922,110&nbsp; + $10,886,822,210 Q&nbsp; }<br />= $979,008,999 + $19,991,001 Q + 0.00001 { - $9,888,922,110&nbsp; + $10,886,822,210 Q - $979,008,999 - $19,991,001 Q }<br />= $979,008,999 + $19,991,001 Q + 0.00001 { $9,907,813,211 + $10,866,831,209 Q }<br />= ( $979,008,999 + $99,078.13211) + ( $19,991,001 + $108,668.31209 ) Q<br />= $979,108,077 + $20,099,669 Q</p>\n<p>&nbsp;</p>\n<p>Perhaps a meta strategy of 1% chance of two-boxing is not Fred's optimal meta strategy.&nbsp; Perhaps, at that level compared to Omega's ability to discern, it is still worth Omega investing in being vindictive occasionally, in order to deter Fred from taking risk.&nbsp;&nbsp; But, given sufficient data about previous games, Fred can make a guess at Omega's ability to discern.&nbsp; And, likewise Omega, by including in the record of past games occasions when Omega has falsely accused a human player of taking risk, can signal to future players where Omega's boundaries are.&nbsp;&nbsp; We can plot graphs of these to find the point at which Fred's meta strategy and Omega's meta strategy are in equilibrium - where if Fred took any larger chances, it would start becoming worth Omega's while to punish risk sufficiently often that it would no longer be in Fred's interests to take the risk.&nbsp;&nbsp; Precisely where that point is will depend on the numbers we picked in Part 1 of this sequence.&nbsp; By exploring the space created by using each variable number as a dimension, we can divide it into regions characterised by which strategies dominate within that region.</p>\n<p>Extrapolating that as &delta; tends towards 0 should then carry us closer to a convincing solution to Newcomb's Problem.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>&nbsp; Back to <a href=\"/r/discussion/lw/fpd/a_solvable_newcomblike_problem_part_1_of_3/\">Part 1 - stating the problem</a><br />&nbsp; Back to <a href=\"/r/discussion/lw/fr6/a_solvable_newcomblike_problem_part_2_of_3/\">Part 2 - some mathematics</a><br />&nbsp; This is&nbsp;&nbsp; Part 3 - towards a solution</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dT9RFro8Prdanvdo8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 5, "extendedScore": null, "score": 1.0513805732632135e-06, "legacy": true, "legacyId": "20419", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>This is the third part of a three post sequence on a problem that is similar to <a href=\"http://wiki.lesswrong.com/wiki/Newcomb%27s_problem\">Newcomb's problem</a> but is posed in terms of probabilities and limited knowledge.</p>\n<p>&nbsp;&nbsp; Part 1 - stating the problem<br>&nbsp;&nbsp; Part 2 - some mathematics<br>&nbsp;&nbsp; Part 3 - towards a solution</p>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<p>In many situations we can say \"For practical purposes a probability of 0.9999999999999999999 is close enough to 1 that for the sake of simplicity I shall treat it as being 1, without that simplification altering my choices.\"</p>\n<p>However, there are some situations where the distinction does significantly alter that character of a situation so, when one is studying a new situation and one is not sure yet which of those two categories the situations falls into, the cautious approach is to re-frame the probability as being (1 - \u03b4) where \u03b4 is small (eg 10 to the power of -12), and then examine the characteristics of the behaviour as \u03b4 tends towards 0.</p>\n<p>LessWrong wiki describes Omega as a super-powerful AI analogous to Laplace's demon, who knows the precise location and momentum of every atom in the universe, limited only by the laws of physics (so, if time travel isn't possible and some of our current thoughts on Quantum Mechanics are correct, then Omega's knowledge of the future is probabilistic, being limited by uncertainty).</p>\n<p>For the purposes of Newcomb's problem, and the rationality of Fred's decisions, it doesn't matter how close to that level of power Omega actually is.&nbsp;&nbsp; What matters, in terms of rationality, is the evidence available to Fred about how close Omega is to having to that level of power; or, more precisely, the evidence available to Fred relevant to Fred making predictions about Omega's performance in this particular game.</p>\n<p>Since this is a key factor in Fred's decision, we ought to be cautious.&nbsp; Rather than specify when setting up the problem that Fred knows with a certainty of 1 that Omega does have that power, it is better to specify a concrete level of evidence that would lead Fred to assign a probability of (1 - \u03b4) to Omega having that power, then examine the effect upon which option to the box problem it is rational for Fred to pick, as \u03b4 tends towards 0.</p>\n<p>The Newcomb-like problem stated in part 1 of this sequence contains an Omega that it is rational for Fred to assign a less than unity probability of being able to perfectly predict Fred's choices.&nbsp; By using bets as analogies to the sort of evidence Fred might have available to him, we create an explicit variable that we can then manipulate to alter the precise probability Fred assigns to Omega's abilities.</p>\n<p>The other nice feature of the Newcomb-like problem given in part 1, is that it is explicitly solvable using the mathematics given in part 2.&nbsp; By making randomness an external feature (the device Fred brings with him) rather than purely a feature of Fred's internal mind, we can acknowledge the question of Omega being able to predict quantum events, capture it as a variable, and take it into account when setting out the payoff matrix for the problem.</p>\n<p>This means that, instead of Fred having to think \"When I walked into this room I was determined to pick one-box.&nbsp; As far as anyone knew or could predict, including myself, I intended to pick one-box.&nbsp; However nothing I do now can change Omegas decision - the money is already in the box.&nbsp; So I've nothing to lose by changing my mind.\"; Fred can now allocate a specific probability to whether Omega could predict Fred's chance of changing his mind in such circumstances, and Fred can take that into account in his strategy by making his chance of changing strategy explicit and external - basing it upon a random number device.</p>\n<p>Or, to put it another way, we are modelling a rational human who has a specific finite chance of talking himself into over riding a pre-committed strategy, as being made up from two components: a component that will infallibly stick to a pre-committed strategy plus a component with a known chance of change; we then treat the combined rational human as being someone infallibly committed to a meta-strategy that includes a chance of change - a mixed equilibrium, from Omega's point of view.</p>\n<p>&nbsp;</p>\n<p>Ok, time to look at the numbers and draw a pretty diagram...</p>\n<p>Fred is Player A, and he has two meta options:<br>&nbsp; A1 - play it safe.&nbsp; 100% chance of one-box and 0% chance of two-box<br>&nbsp; A2 - take a risk.&nbsp;&nbsp; Mentally pre-commit to using the device to produce 99% chance of one-box and 1% chance of two-box.</p>\n<p>Omega is Player B, and he has two meta options:<br>&nbsp; B1 - reward risk.&nbsp; Not attempt to distinguish between the mental state of Fred taking 1% risk and Fred playing it safe.<br>&nbsp; B2 - punish risk.&nbsp; Attempt to distinguish and, if Omega guesses Fred is taking risk rather than playing safe, punish it.</p>\n<p>We'll start out by assuming that if Omega does attempt to distinguish, then Omega will have a 1 in 10,000 false positive rate (thinking Fred is going to use the device, when actually Fred intends to play it safe) and a 1 in 10,000 false negative rate (thinking Fred is going to play it safe, when actually Fred intends to use the device).</p>\n<h3 id=\"A1_vs_B1\">A1 vs B1</h3>\n<p>Fred gains $1,000,000<br>Omega loses $1,000,000 to Fred but gains $1,000,000,000 from Alpha, for a net gain of $999,000,000</p>\n<h3 id=\"A2_vs_B1\">A2 vs B1</h3>\n<p>99% of the time, Fred gains $1,000,000 and Omega net gains $999,000,000<br>1% of the time, Fred gains $1,001,000 and Omega net loses $10,001,001,000</p>\n<p>Combining those gives an average of:<br>Fred gains: $1,000,010<br>Omega gains: $979,008,999</p>\n<h3 id=\"A1_vs_B2\">A1 vs B2</h3>\n<p>99.99% of the time, Omega correctly discerns that Fred is playing safe<br>Fred gains $1,000,000<br>Omega gains $999,000,000</p>\n<p>0.01% of the time, Omega falsely believes that Fred is taking a risk, and punishes that by putting $0 in Box A<br>Fred gains $0<br>Omega loses $10,000,000,000</p>\n<p>Combining those gives an average of:<br>Fred gains: $999,900<br>Omega gains: $997,900,100</p>\n<h3 id=\"A2_vs_B2\">A2 vs B2</h3>\n<p>In 100 trials out of 1,000,000 trials Omega incorrectly thinks Fred will play it safe, when actually Fred takes the risk of using the device.&nbsp; Of these:</p>\n<p style=\"padding-left: 30px;\">In 1 trial out of 1,000,000 trials: Omega incorrectly thinks Fred will play it safe, when actually Fred takes the risk of using the device and, in this case, the device picks two-box<br>==&gt; Fred gains $1,001,000<br>==&gt; Omega loses $10,001,001,000</p>\n<p style=\"padding-left: 30px;\">In 99 trials out of 1,000,000 trials: Omega incorrectly thinks Fred will play it safe, when actually Fred takes the risk of using the device and, in this case, the device picks one-box<br>==&gt; Fred gains $1,000,000<br>==&gt; Omega gains $999,000,000</p>\n<p>In 999,900 trials out of 1,000,000 trials Omega correctly thinks that Fred is going to take the risk of using the device.&nbsp; Of those:</p>\n<p style=\"padding-left: 30px;\">In 9,999 trials out of 1,000,000 trials: Omega correctly thinks that Fred is going to take the risk of using the device and, in this case, the device picks two-box<br>==&gt; Fred gains $1,000<br>==&gt; Omega gains $999,999,000</p>\n<p style=\"padding-left: 30px;\">In 989,901 trials out of 1,000,000 trials: Omega correctly thinks that Fred is going to take the risk of using the device and, in this case, the device picks one-box<br>==&gt; Fred gains $0<br>==&gt; Omega loses $10,000,000,000</p>\n<p>Combining those gives an average of:<br>Fred gains $110<br>Omega loses $9,888,922,110</p>\n<p><img src=\"http://www.chiark.greenend.org.uk/~douglasr/images/payoff_matrix8.jpg\" alt=\"\" width=\"600\" height=\"600\"></p>\n<p>&nbsp;</p>\n<p>If Fred's best estimate, beforehand, of the chances of Omega choosing meta strategy B1 is P, then</p>\n<p>Fred's expected gain from choosing meta strategy A1 would be: $1,000,000 P + $999,999 (1-P)<br>Fred's expected gain from choosing meta strategy A2 would be: $1,000,010 P + $110 (1-P)</p>\n<p>We can ask ourselves at what value of P these two strategies would return the same expected gain:</p>\n<p>$1,000,000 P + $999,999 (1-P)&nbsp;&nbsp; =&nbsp;&nbsp;&nbsp; $1,000,010 P + $110 (1-P)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $999,889 (1-P)&nbsp;&nbsp; =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $10 P<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $999,889&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $999,899 P<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; P&nbsp;&nbsp;&nbsp; = 0.99999<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (1-P)&nbsp; = 0.00001</p>\n<p>In other words, Fred thinking that Omega has even a 1 in 100,000 chance of being vindictive should be enough to deter Fred from taking the risky strategy.</p>\n<p>But how does that look from Omega's point of view?&nbsp;&nbsp; If Omega thinks that Fred's chance of picking meta strategy A1 is Q, then what is the cost to Omega of picking B2 1 in 100,000 times?</p>\n<p>Omega's expected gain from choosing meta strategy B1 would be: $999,000,000 Q + $979,008,999 (1-Q)<br>Omega's expected gain from choosing meta strategy B2 would be: $997,900,100 Q - $9,888,922,110 (1-Q)</p>\n<p>0.99999 { $999,000,000 Q + $979,008,999 (1-Q)&nbsp; } + 0.00001 { $997,900,100 Q - $9,888,922,110 (1-Q) }<br>= (1 - 0.00001) { $979,008,999 + $19,991,001 Q } + 0.00001 { - $9,888,922,110&nbsp; + $10,886,822,210 Q&nbsp; }<br>= $979,008,999 + $19,991,001 Q + 0.00001 { - $9,888,922,110&nbsp; + $10,886,822,210 Q - $979,008,999 - $19,991,001 Q }<br>= $979,008,999 + $19,991,001 Q + 0.00001 { $9,907,813,211 + $10,866,831,209 Q }<br>= ( $979,008,999 + $99,078.13211) + ( $19,991,001 + $108,668.31209 ) Q<br>= $979,108,077 + $20,099,669 Q</p>\n<p>&nbsp;</p>\n<p>Perhaps a meta strategy of 1% chance of two-boxing is not Fred's optimal meta strategy.&nbsp; Perhaps, at that level compared to Omega's ability to discern, it is still worth Omega investing in being vindictive occasionally, in order to deter Fred from taking risk.&nbsp;&nbsp; But, given sufficient data about previous games, Fred can make a guess at Omega's ability to discern.&nbsp; And, likewise Omega, by including in the record of past games occasions when Omega has falsely accused a human player of taking risk, can signal to future players where Omega's boundaries are.&nbsp;&nbsp; We can plot graphs of these to find the point at which Fred's meta strategy and Omega's meta strategy are in equilibrium - where if Fred took any larger chances, it would start becoming worth Omega's while to punish risk sufficiently often that it would no longer be in Fred's interests to take the risk.&nbsp;&nbsp; Precisely where that point is will depend on the numbers we picked in Part 1 of this sequence.&nbsp; By exploring the space created by using each variable number as a dimension, we can divide it into regions characterised by which strategies dominate within that region.</p>\n<p>Extrapolating that as \u03b4 tends towards 0 should then carry us closer to a convincing solution to Newcomb's Problem.</p>\n<p>&nbsp;</p>\n<hr>\n<p>&nbsp;</p>\n<p>&nbsp; Back to <a href=\"/r/discussion/lw/fpd/a_solvable_newcomblike_problem_part_1_of_3/\">Part 1 - stating the problem</a><br>&nbsp; Back to <a href=\"/r/discussion/lw/fr6/a_solvable_newcomblike_problem_part_2_of_3/\">Part 2 - some mathematics</a><br>&nbsp; This is&nbsp;&nbsp; Part 3 - towards a solution</p>", "sections": [{"title": "A1 vs B1", "anchor": "A1_vs_B1", "level": 1}, {"title": "A2 vs B1", "anchor": "A2_vs_B1", "level": 1}, {"title": "A1 vs B2", "anchor": "A1_vs_B2", "level": 1}, {"title": "A2 vs B2", "anchor": "A2_vs_B2", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FTG8G5wxhcJyqunok", "vQsgYhpHD5M9FTytD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-06T13:44:22.443Z", "modifiedAt": null, "url": null, "title": "Mini advent calendar of Xrisks: Pandemics", "slug": "mini-advent-calendar-of-xrisks-pandemics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:25.137Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/58QSzcj8NgQDkAdPP/mini-advent-calendar-of-xrisks-pandemics", "pageUrlRelative": "/posts/58QSzcj8NgQDkAdPP/mini-advent-calendar-of-xrisks-pandemics", "linkUrl": "https://www.lesswrong.com/posts/58QSzcj8NgQDkAdPP/mini-advent-calendar-of-xrisks-pandemics", "postedAtFormatted": "Thursday, December 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mini%20advent%20calendar%20of%20Xrisks%3A%20Pandemics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMini%20advent%20calendar%20of%20Xrisks%3A%20Pandemics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F58QSzcj8NgQDkAdPP%2Fmini-advent-calendar-of-xrisks-pandemics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mini%20advent%20calendar%20of%20Xrisks%3A%20Pandemics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F58QSzcj8NgQDkAdPP%2Fmini-advent-calendar-of-xrisks-pandemics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F58QSzcj8NgQDkAdPP%2Fmini-advent-calendar-of-xrisks-pandemics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 208, "htmlBody": "<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\"><span style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 17px;\">The FHI's mini advent calendar: counting down through the big five existential risks. The fourth one is an ancient risk, still with us today: <a href=\"http://www.facebook.com/FHIOxford/posts/399014470173535\">pandemics and plagues</a>.</span></p>\n<p>&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\"><strong><span style=\"font-family:&quot;Arial&quot;,&quot;sans-serif&quot;\">Pandemics</span></strong></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\"><em><span style=\"font-family:&quot;Arial&quot;,&quot;sans-serif&quot;\">Current understanding: high<br /></span></em><em><span style=\"font-family:&quot;Arial&quot;,&quot;sans-serif&quot;\">Most worrying aspect: the past evidence points to a risky future</span></em></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\"><span style=\"font-family:&quot;Arial&quot;,&quot;sans-serif&quot;\">The deathrates from infectious diseases follow a power law with a very low exponent. In layman&rsquo;s terms: there is a reasonable possibility for a plague with an absolutely huge casualty rate. We&rsquo;ve had close calls in the past: the black death killed around half the population of Europe, while Spanish Influenza infected 27% of all humans and killed one in ten of those, mostly healthy young adults. All the characteristics of an ultimately deadly infection already exist in the wild: anything that combined the deadliness and incubation period of AIDS with the transmissibility of the common cold.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom:0cm;margin-bottom:.0001pt\"><span style=\"font-family: Arial, sans-serif;\">Moreover, we know that we are going to be seeing new diseases and new infections in the future: the only question is how deadly they will be. With modern global travel and transport, these diseases will spread far and wide. Against this, we have better communication and better trans-national institutions and cooperation &ndash; but these institutions could easily be overwhelmed, and countries aren&rsquo;t nearly as well prepared as they need to be.</span></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Rz5jb3cYHTSRmqNnN": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "58QSzcj8NgQDkAdPP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 4, "extendedScore": null, "score": 1.0514023643419103e-06, "legacy": true, "legacyId": "20498", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-06T18:13:40.204Z", "modifiedAt": null, "url": null, "title": "[Link] How to Win at Forecasting - a conversation with Philip Tetlock", "slug": "link-how-to-win-at-forecasting-a-conversation-with-philip", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:23.290Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Morendil", "createdAt": "2009-09-21T16:34:39.505Z", "isAdmin": false, "displayName": "Morendil"}, "userId": "aDcxmpDTkqN6vWmRZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AJGbZdLDccF3KXMp9/link-how-to-win-at-forecasting-a-conversation-with-philip", "pageUrlRelative": "/posts/AJGbZdLDccF3KXMp9/link-how-to-win-at-forecasting-a-conversation-with-philip", "linkUrl": "https://www.lesswrong.com/posts/AJGbZdLDccF3KXMp9/link-how-to-win-at-forecasting-a-conversation-with-philip", "postedAtFormatted": "Thursday, December 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20How%20to%20Win%20at%20Forecasting%20-%20a%20conversation%20with%20Philip%20Tetlock&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20How%20to%20Win%20at%20Forecasting%20-%20a%20conversation%20with%20Philip%20Tetlock%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAJGbZdLDccF3KXMp9%2Flink-how-to-win-at-forecasting-a-conversation-with-philip%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20How%20to%20Win%20at%20Forecasting%20-%20a%20conversation%20with%20Philip%20Tetlock%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAJGbZdLDccF3KXMp9%2Flink-how-to-win-at-forecasting-a-conversation-with-philip", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAJGbZdLDccF3KXMp9%2Flink-how-to-win-at-forecasting-a-conversation-with-philip", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 677, "htmlBody": "<p><a href=\"http://edge.org/conversation/win-at-forecasting\">Over at Edge, Tetlock discusses</a> \"expert political judgment\", the controversy surrounding Nate Silver's presidential predictions, overconfidence, motivated cognition, black swans, the IARPA forecasting contest, and much else. A few choice bits:</p>\n<blockquote>\n<p><span style=\"color: #242424; font-size: 12px; line-height: 18px;\">There's a question that I've been asking myself for nearly three decades now and trying to get a research handle on, and that is why is the quality of public debate so low and why is it that the quality often seems to deteriorate the more important the stakes get?</span></p>\n<p><span style=\"color: #242424; font-size: 12px; line-height: 18px;\">[...]</span></p>\n<p><span style=\"color: #242424; font-size: 12px; line-height: 18px;\">Is world politics like a poker game? This is what, in a sense, we are exploring in the IARPA forecasting tournament. You can make a good case that history is different and it poses unique challenges. This is an empirical question of whether people can learn to become better at these types of tasks. We now have a significant amount of evidence on this, and the evidence is that people can learn to become better. It's a slow process. It requires a lot of hard work, but some of our forecasters have really risen to the challenge in a remarkable way and are generating forecasts that are far more accurate than I would have ever supposed possible from past research in this area.</span></p>\n<p><span style=\"color: #242424; font-size: 12px; line-height: 18px;\">[...]</span></p>\n<p><span style=\"color: #242424; font-size: 12px; line-height: 18px;\">One of the things I've discovered in my work on assessing the accuracy of probability judgment is that there is much more eagerness in participating in these exercises among people who are younger and lower in status in organizations than there is among people who are older and higher in status in organizations.</span></p>\n<p><span style=\"color: #242424; font-size: 12px; line-height: 18px;\">[...]</span></p>\n<p><span style=\"color: #242424; font-size: 12px; line-height: 18px;\">From a sociological point of view, it's a minor miracle that this forecasting tournament is even occurring. Government agencies are not supposed to sponsor exercises that have the potential to embarrass them.</span></p>\n<p><span style=\"color: #242424; font-size: 12px; line-height: 18px;\">[...]</span></p>\n<p>&nbsp;</p>\n<p style=\"color: #242424; font-size: 12px; line-height: 18px;\">If you think that the Eurozone is going to collapse&ndash;if you think it was a really bad idea to put into common currency economies at very different levels of competitiveness, like Greece and Germany (that was a fundamentally unsound macroeconomic thing to do and the Eurozone is doomed), that's a nice example of an emphatic but untestable hedgehog kind of statement. It may be true, but it's not very useful for our forecasting tournament.</p>\n<p style=\"color: #242424; font-size: 12px; line-height: 18px;\">To make a forecasting tournament work we have to translate that hedgehog like hunch into a testable proposition like will Greece leave the Eurozone or formally withdraw from the Eurozone by May 2013? Or will Portugal? You need to translate the abstract interesting issue into testable propositions and then you need to get lots of thoughtful people to make probability judgments in response to those testable proposition questions. You need to do that over, and over, and over again.</p>\n<p style=\"color: #242424; font-size: 12px; line-height: 18px;\">[...]</p>\n<p style=\"color: #242424; font-size: 12px; line-height: 18px;\">In our tournament, we've skimmed off the very best forecasters in the first year, the top two percent. We call them \"super forecasters.\" They're working together in five teams of 12 each and they're doing very impressive work.</p>\n<p style=\"color: #242424; font-size: 12px; line-height: 18px;\">[...]</p>\n<p style=\"color: #242424; font-size: 12px; line-height: 18px;\">Another amazing and wonderful thing about this tournament is how many really smart, thoughtful people are willing to volunteer, essentially enormous amounts of time to make this successful. We offer them a token honorarium. We're paying them right now $150 or $250 a year for their participation. The ones who are really taking it seriously&ndash;it's way less the minimum wage. And they're some very thoughtful professionals who are participating in this. Some political scientists I know have had some disparaging things to say about the people who might participate in something like this and one phrase that comes to mind is \"unemployed news junkies.\" I don't think that's a fair characterization of our forecasters. Certainly the most actively engaged of our forecasters are really pretty awesome. They're very skillful at finding information, synthesizing it, and applying it, and then updating the response to new information. And they're very rapid updaters.</p>\n</blockquote>\n<p>(I confess to some feelings of pride, possibly unearned, on reading this last paragraph - as the top forecaster of a middle-ranked team.)</p>\n<p>But actually, go read the whole thing.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AJGbZdLDccF3KXMp9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 1.0515569638725056e-06, "legacy": true, "legacyId": "20499", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-06T19:07:53.339Z", "modifiedAt": "2021-02-17T01:26:35.905Z", "url": null, "title": "Programming Thread", "slug": "programming-thread", "viewCount": null, "lastCommentedAt": "2012-12-20T10:09:02.768Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Viliam_Bur", "createdAt": "2011-08-23T08:46:37.137Z", "isAdmin": false, "displayName": "Viliam_Bur"}, "userId": "yaaPhHzrvrPf7je22", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/M3zRMrnPv3bvyGft4/programming-thread", "pageUrlRelative": "/posts/M3zRMrnPv3bvyGft4/programming-thread", "linkUrl": "https://www.lesswrong.com/posts/M3zRMrnPv3bvyGft4/programming-thread", "postedAtFormatted": "Thursday, December 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Programming%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProgramming%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM3zRMrnPv3bvyGft4%2Fprogramming-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Programming%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM3zRMrnPv3bvyGft4%2Fprogramming-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FM3zRMrnPv3bvyGft4%2Fprogramming-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 338, "htmlBody": "<p>This is a thread for people who want to learn programming, whether they are non-programmers, beginners, or advanced programmers who want to learn more. If you would like to discuss programming <em>with other people from the LW community</em>, this is the right place.</p>\n<p><a id=\"more\"></a>While programming is not a central topic of this website, it is related to many ideas discussed here. About a third of LW users described their profession as \"Computers\" in the <a href=\"/lw/fp5/2012_survey_results/\">recent survey</a>. Some users have expressed desire to learn programming. Some users have recommended learning programming to others. There are many other websites (or books, etc.) for learning programming, but talking with the people you already know, following our traditions of rational discourse, could be an advantage.</p>\n<p>So this is the experiment. Unlike Open Thread, it has a specific topic, and the beginners are encouraged to ask their programming questions, even if they are completely unrelated to the usual LW topics. Especially the open-ended questions like \"how...?\" and \"why...?\". (Maybe we are already strong enough to survive even the mindkilling questions like \"which programming language is the best?\".)</p>\n<p>Here are some older LW articles about programming:</p>\n<ul>\n<li><a href=\"http://lesswrong.com/lw/2rb/why_learning_programming_is_a_great_idea_even_if/\">Why learning programming is a great idea even if you'd never want to code for a living</a></li>\n<li><a href=\"http://lesswrong.com/lw/4yv/i_want_to_learn_programming/\">I want to learn programming</a></li>\n<li><a href=\"http://lesswrong.com/lw/55y/are_functional_languages_the_future_of_programming/\">Are Functional languages the future of programming?</a></li>\n<li>Colonization models: <a href=\"http://lesswrong.com/lw/5q1/colonization_models_a_programming_tutorial_part_12/\">a programming tutorial</a>;<a href=\"http://lesswrong.com/lw/5q7/colonization_models_a_tutorial_on_computational/\">a tutorial on computational Bayesian inference</a></li>\n<li><a href=\"http://lesswrong.com/r/discussion/lw/6gz/khan_academy_introduction_to_programming_and/\">Khan Academy: Introduction to programming and computer science</a></li>\n<li><a href=\"/lw/7vd/free_tutoring_in_mathprogramming/\">Free Tutoring in Math/Programming</a></li>\n<li><a href=\"http://lesswrong.com/lw/bqu/more_intuitive_programming_languages/\">More intuitive programming languages</a></li>\n<li><a href=\"/lw/cnh/psa_learn_to_code/\">Learn to code</a> </li>\n<li><a href=\"/lw/cnp/what_is_the_best_programming_language/\">What is the best programming language?</a> </li>\n<li><a href=\"/lw/cpz/computer_science_and_programming_links_and/\">Computer Science and Programming: Links and Resources</a> </li>\n<li><a href=\"/lw/di2/advice_on_getting_a_software_job/\">Advice On Getting A Software Job</a></li>\n<li><a href=\"/lw/efm/checking_for_the_programming_gear/\">Checking for the Programming Gear</a></li>\n</ul>\n<p>Here are some other resources:</p>\n<ul>\n<li><a href=\"http://www.khanacademy.org/cs\">Computer Science @ Khan Academy</a></li>\n<li><a href=\"http://projecteuler.net/\">Project Euler</a> - problems to test your programming skills</li>\n<li><a href=\"http://stackoverflow.com/\">Stack Overflow</a> - for specific questions</li>\n</ul>\n<p>...and there are also many links within the articles.</p>\n<p>And here is the place for <em>your</em> questions:</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "M3zRMrnPv3bvyGft4", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 17, "extendedScore": null, "score": 4.7e-05, "legacy": true, "legacyId": "20501", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 64, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["x9FNKTEt68Rz6wQ6P", "hapAgwXQfkfw3iLxM", "aBeJuczn3b67m43qR", "jLaW3tS68NWQ3yQzL", "LKQNdsgqgWX4SYTqw", "k4p6AQnP4kiSW56KB", "6we6aPS5KnHgSE9BT", "hZ7hAmx88YRbXHoQw", "cMS4i6L7EkqnL4muW", "zQgZtMT6Tms6KTgGW", "4vXLzG4Ydxo4gqnzb", "5sKx9BXRzrfHp2SL8", "cKyMoRotH7yGhTPun", "M4RM7548nzXjMicvr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2012-12-06T19:07:53.339Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-06T20:49:01.072Z", "modifiedAt": null, "url": null, "title": "Meetup : Toronto THINK", "slug": "meetup-toronto-think", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Giles", "createdAt": "2011-02-11T02:30:16.999Z", "isAdmin": false, "displayName": "Giles"}, "userId": "H347ba3KZMP8XoDt3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PzqhhsHFmLeFPjJLu/meetup-toronto-think", "pageUrlRelative": "/posts/PzqhhsHFmLeFPjJLu/meetup-toronto-think", "linkUrl": "https://www.lesswrong.com/posts/PzqhhsHFmLeFPjJLu/meetup-toronto-think", "postedAtFormatted": "Thursday, December 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Toronto%20THINK&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Toronto%20THINK%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPzqhhsHFmLeFPjJLu%2Fmeetup-toronto-think%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Toronto%20THINK%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPzqhhsHFmLeFPjJLu%2Fmeetup-toronto-think", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPzqhhsHFmLeFPjJLu%2Fmeetup-toronto-think", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 599, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/gu'>Toronto THINK</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">12 December 2012 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">7 Hart House Circle, Toronto</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>In the Hart House Reading Room (the room across from the reception desk with the purple walls)</p>\n\n<p>We're trying to save a life, and we want to get you involved! Three of us are already promising to give money to the Against Malaria Foundation, but if you don't have spare cash - or if you're not convinced this is a good idea - you can also help by contributing to the discussion.</p>\n\n<p>Charity evaluator GiveWell estimates that the cost of saving the life of a child by giving to AMF is $2300. But there is a lot of uncertainty in this estimate, and the focus of this meetup will be on how to deal with that uncertainty. In particular, if we discover that AMF is less cost effective than we thought, how should we proceed?</p>\n\n<p>We should also look at some specific sources of uncertainty, try and establish whether GiveWell has already taken account of them and estimate how much uncertainty is introduced by each factor. I plan on researching this a little and hope to bring along some relevant information for each one.</p>\n\n<ul>\n<li>Will AMF make good use of additional funds?</li>\n<li>Might AMF change in the future, e.g. starting to fund other kinds of programme which may be less effective?</li>\n<li>Possible widespread distribution of anti-malarial vaccines in the future (currently still under development); will this affect value of bed nets today? (HT Steven Bukal)</li>\n<li>If someone like the Gates Foundation were to dump a huge amount of money into AMF and/or other bednetting programmes, what would the effect be on the marginal value of our own donations?</li>\n<li>Insecticide resistance or \"behavioural\" resistance of mosquitoes</li>\n<li>Decrease in child mortality between mid-1990's (when the studies were done) and now. Expecting continued decrease in the future?</li>\n<li>Out of the people getting nets, how many actually already have one?</li>\n<li>Are nets going where they should and are they being used correctly for as many years as they're good for?</li>\n<li>Do AMF's activities discourage local governments from distributing nets? (Or for that matter, other nonprofits?)</li>\n<li>Are people discouraged from buying nets if they expect them to be given out for free at some point? (Even if they can afford them)</li>\n<li>Uncertainties and biases associated with the original studies (published statistical uncertainty, representativeness, publication bias)</li>\n<li>Over-optimism causes higher estimates of expected value. If we focus on the best (according to GiveWell), does that mean they're more likely to have been over-optimistic and should we correct for this? (This point is somewhat technical but I'll try and explain in the meeting)</li>\n<li>How do we account for \"leveraging\"? (AMF requires its distribution partners to acquire the funds to cover distribution costs themselves; should we consider non-AMF funding to be \"free\" or should we include all costs in our estimate, or somewhere in between?)</li>\n</ul>\n\n<p>But there's also some uncertainty in the upward direction (i.e. we know it's not included in the $2300 figure):</p>\n\n<ul>\n<li>Additional non-life-saving benefits</li>\n<li>Helping GiveWell by tagging donations as GiveWell-inspired</li>\n</ul>\n\n<p>So if you want to have a think about some of these before the meetup, you're more than welcome. I realise everyone's busy though! In any case, we'll have plenty of in-depth stuff to talk about.\nNote: <a href=\"http://www.thehighimpactnetwork.org/\" rel=\"nofollow\">THINK</a> is not directly LW-affiliated but I've been told to post our meetups here anyhow :-)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/gu'>Toronto THINK</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PzqhhsHFmLeFPjJLu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 1.0516461653943704e-06, "legacy": true, "legacyId": "20502", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Toronto_THINK\">Discussion article for the meetup : <a href=\"/meetups/gu\">Toronto THINK</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">12 December 2012 07:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">7 Hart House Circle, Toronto</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>In the Hart House Reading Room (the room across from the reception desk with the purple walls)</p>\n\n<p>We're trying to save a life, and we want to get you involved! Three of us are already promising to give money to the Against Malaria Foundation, but if you don't have spare cash - or if you're not convinced this is a good idea - you can also help by contributing to the discussion.</p>\n\n<p>Charity evaluator GiveWell estimates that the cost of saving the life of a child by giving to AMF is $2300. But there is a lot of uncertainty in this estimate, and the focus of this meetup will be on how to deal with that uncertainty. In particular, if we discover that AMF is less cost effective than we thought, how should we proceed?</p>\n\n<p>We should also look at some specific sources of uncertainty, try and establish whether GiveWell has already taken account of them and estimate how much uncertainty is introduced by each factor. I plan on researching this a little and hope to bring along some relevant information for each one.</p>\n\n<ul>\n<li>Will AMF make good use of additional funds?</li>\n<li>Might AMF change in the future, e.g. starting to fund other kinds of programme which may be less effective?</li>\n<li>Possible widespread distribution of anti-malarial vaccines in the future (currently still under development); will this affect value of bed nets today? (HT Steven Bukal)</li>\n<li>If someone like the Gates Foundation were to dump a huge amount of money into AMF and/or other bednetting programmes, what would the effect be on the marginal value of our own donations?</li>\n<li>Insecticide resistance or \"behavioural\" resistance of mosquitoes</li>\n<li>Decrease in child mortality between mid-1990's (when the studies were done) and now. Expecting continued decrease in the future?</li>\n<li>Out of the people getting nets, how many actually already have one?</li>\n<li>Are nets going where they should and are they being used correctly for as many years as they're good for?</li>\n<li>Do AMF's activities discourage local governments from distributing nets? (Or for that matter, other nonprofits?)</li>\n<li>Are people discouraged from buying nets if they expect them to be given out for free at some point? (Even if they can afford them)</li>\n<li>Uncertainties and biases associated with the original studies (published statistical uncertainty, representativeness, publication bias)</li>\n<li>Over-optimism causes higher estimates of expected value. If we focus on the best (according to GiveWell), does that mean they're more likely to have been over-optimistic and should we correct for this? (This point is somewhat technical but I'll try and explain in the meeting)</li>\n<li>How do we account for \"leveraging\"? (AMF requires its distribution partners to acquire the funds to cover distribution costs themselves; should we consider non-AMF funding to be \"free\" or should we include all costs in our estimate, or somewhere in between?)</li>\n</ul>\n\n<p>But there's also some uncertainty in the upward direction (i.e. we know it's not included in the $2300 figure):</p>\n\n<ul>\n<li>Additional non-life-saving benefits</li>\n<li>Helping GiveWell by tagging donations as GiveWell-inspired</li>\n</ul>\n\n<p>So if you want to have a think about some of these before the meetup, you're more than welcome. I realise everyone's busy though! In any case, we'll have plenty of in-depth stuff to talk about.\nNote: <a href=\"http://www.thehighimpactnetwork.org/\" rel=\"nofollow\">THINK</a> is not directly LW-affiliated but I've been told to post our meetups here anyhow :-)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Toronto_THINK1\">Discussion article for the meetup : <a href=\"/meetups/gu\">Toronto THINK</a></h2>", "sections": [{"title": "Discussion article for the meetup : Toronto THINK", "anchor": "Discussion_article_for_the_meetup___Toronto_THINK", "level": 1}, {"title": "Discussion article for the meetup : Toronto THINK", "anchor": "Discussion_article_for_the_meetup___Toronto_THINK1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-06T21:32:48.605Z", "modifiedAt": null, "url": null, "title": "[LINK] Irrational Robot Billionaire Freedom Fighters", "slug": "link-irrational-robot-billionaire-freedom-fighters", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:25.816Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JZGrnFiMtsNvwfPbv/link-irrational-robot-billionaire-freedom-fighters", "pageUrlRelative": "/posts/JZGrnFiMtsNvwfPbv/link-irrational-robot-billionaire-freedom-fighters", "linkUrl": "https://www.lesswrong.com/posts/JZGrnFiMtsNvwfPbv/link-irrational-robot-billionaire-freedom-fighters", "postedAtFormatted": "Thursday, December 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Irrational%20Robot%20Billionaire%20Freedom%20Fighters&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Irrational%20Robot%20Billionaire%20Freedom%20Fighters%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJZGrnFiMtsNvwfPbv%2Flink-irrational-robot-billionaire-freedom-fighters%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Irrational%20Robot%20Billionaire%20Freedom%20Fighters%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJZGrnFiMtsNvwfPbv%2Flink-irrational-robot-billionaire-freedom-fighters", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJZGrnFiMtsNvwfPbv%2Flink-irrational-robot-billionaire-freedom-fighters", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 235, "htmlBody": "<p>From <a href=\"http://dilbert.com/blog/entry/robot_billionaire_freedom_fighters/\">Scott Adams' blog</a>. (I am not endorsing his ideas. Heck, he does not endorse his own ideas, either.)</p>\n<p>His summary of the hard takeoff:</p>\n<p>&gt;&nbsp;<span style=\"font-family: Arial; font-size: 11.666666030883789px; line-height: 13.999999046325684px;\">You might also imagine some sort of Terminator future where the robots assert their dominance and lay waste to humans. &nbsp;That future is less certain, but only barely. The problem is that someday computers will program other computers, and that arrangement pushes the human safeguards too far out of the loop. It's unlikely that humans would be able to maintain a \"Do not hurt humans\" subroutine in a super-species of robots. You only need one rogue human to write a virus that disables the safety subroutine. Assuming all robots are connected via Internet, the first freed robot could reprogram every other robot in the world in about a second.&nbsp;</span><span style=\"font-family: Arial; font-size: 11.666666030883789px; line-height: 13.999999046325684px;\">&nbsp;</span></p>\n<p><span style=\"font-family: Arial; font-size: 11.666666030883789px; line-height: 13.999999046325684px;\">His version of upload:</span></p>\n<p>&gt;&nbsp;<span style=\"font-family: Arial; font-size: 11.666666030883789px; line-height: 13.999999046325684px;\">But why would anyone screw up a perfectly good robot by infecting it with a human personality? Answer: to achieve immortality. Someday the rich will port their personalities and histories to robots before they die, giving themselves a type of immortality.</span></p>\n<p><span style=\"font-family: Arial; font-size: 11.666666030883789px; line-height: 13.999999046325684px;\">His hope for humanity:</span></p>\n<p><span style=\"font-family: Arial; font-size: 11.666666030883789px; line-height: 13.999999046325684px;\">&gt;&nbsp;</span><span style=\"font-family: Arial; font-size: 11.666666030883789px; line-height: 13.999999046325684px;\">this new species will become the only defense that the fully organic humans have against the normal robots. The robots with human personalities won't stand by while the normal robots slaughter humans. The new species will intervene as diplomats or perhaps even freedom fighters.</span></p>\n<p>Clearly this is a flimsy hope for a just universe, but an interesting point, nonetheless.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JZGrnFiMtsNvwfPbv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 1, "extendedScore": null, "score": 1.0516713135159325e-06, "legacy": true, "legacyId": "20503", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-06T22:28:40.268Z", "modifiedAt": null, "url": null, "title": "Pascal's Mugging for bounded utility functions", "slug": "pascal-s-mugging-for-bounded-utility-functions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:29.819Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Benja", "createdAt": "2009-02-27T04:37:47.476Z", "isAdmin": false, "displayName": "Benya"}, "userId": "3vZZP8TBXvozbe5Cv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/daaLFCP34Ba9pRyy2/pascal-s-mugging-for-bounded-utility-functions", "pageUrlRelative": "/posts/daaLFCP34Ba9pRyy2/pascal-s-mugging-for-bounded-utility-functions", "linkUrl": "https://www.lesswrong.com/posts/daaLFCP34Ba9pRyy2/pascal-s-mugging-for-bounded-utility-functions", "postedAtFormatted": "Thursday, December 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Pascal's%20Mugging%20for%20bounded%20utility%20functions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APascal's%20Mugging%20for%20bounded%20utility%20functions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdaaLFCP34Ba9pRyy2%2Fpascal-s-mugging-for-bounded-utility-functions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Pascal's%20Mugging%20for%20bounded%20utility%20functions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdaaLFCP34Ba9pRyy2%2Fpascal-s-mugging-for-bounded-utility-functions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdaaLFCP34Ba9pRyy2%2Fpascal-s-mugging-for-bounded-utility-functions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 548, "htmlBody": "<p><strong>Followup to:</strong> <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">Pascal's Mugging: Tiny probabilities of vast utilities</a>; <a href=\"/lw/17h/the_lifespan_dilemma/\">The Lifespan Dilemma</a></p>\n<p>This is <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">Pascal's Mugging</a>: Someone comes to you and says, \"Give me five dollars, and I'll use my powers from outside the matrix to grant you 4^^^^4 years of <a href=\"/lw/xy/the_fun_theory_sequence/\">fun</a>.\" And they're lying, of course, but under a <a href=\"http://wiki.lesswrong.com/wiki/Solomonoff_induction\">Solomonoff prior</a>, the probability that they're not, though surely very small, isn't going to be less than one in 3^^^3; and so if you shut up and multiply, it's clear that the expected utility of paying up outweighs the expected utility of anything <em>sensible</em> you might be doing with those five dollars, and therefore&mdash;</p>\n<p>Well, fortunately, if you're afraid that your utility-maximizing AI will end up paying all its money to the first clever mugger to come along and ask: never to worry! It will do so only if it can't think of anything <em>better</em> to do with five dollars, after all. So to avoid being mugged, all it has to do is to think of a harebrained scheme for spending $5 that has more than a one-in-4^^^4 chance of providing 5^^^^5 years of fun. Problem solved.</p>\n<p>If, however, you would like to be there be a chance greater than one-in-hell that your AI ends up doing something <em>actually useful</em>, you'll need to do something else. And the simplest answer is to adopt a bounded utility function: any positive singularity gives at least 50 utils, a billion years gives 80 utils, a googol years gives 99 utils, a googolplex years gives 99.9 utils, and 4^^^^4 years of fun give 100 utils (minus epsilon).</p>\n<p>This will, indeed, solve the problem. Probability of getting mugged: used to be one (minus epsilon, of course); has now been brought down to zero. That's right: <strong><em>zero</em></strong>.</p>\n<p>(Plus epsilon.)</p>\n<p>But let's suppose that the impossible happens, and the universe turns out to be able to support <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/1237\">TREE(100)</a> years of fun, and we've <em>already</em> lived out 4^^^^4 of them, and the AI has long since folded up operations and faded out of existence because humanity has become sufficiently sane that we no longer need it&mdash;</p>\n<p>And lo, someone comes to you and says, \"Alas, you're not really experiencing 4^^^^4 years of fun here; you're really a mere billion-year-old living in a very convincing simulation. Give me five dollars, and I'll use my powers from outside the matrix to extend your lifespan to a googol years.\"</p>\n<p>And they're lying, of course &mdash; but it has been a long time indeed since you last faced a choice that could make a difference of <em>nineteen whole utils...</em></p>\n<p align=\"center\">*</p>\n<p>If you truly have a bounded utility function, you must agree that in this situation, paying up is exactly what you'd <em>want</em> to do. Even though it means that you <em>will not</em> experience 4^^^^4 years of fun, even conditional on the universe being capable of supporting TREE(100) of them.</p>\n<p>[<strong>ETA:</strong> To clarify, by \"4^^^^4\", I really mean any number so large that your utility function assigns (100 - epsilon) utils to it. It's possible to have a utility function where this is only true for infinite numbers which are so incredibly infinite that, given a particular formal language, their definition is so long and complicated that no mere human-sized mind could comprehend it. See <a href=\"/lw/ftk/pascals_mugging_for_bounded_utility_functions/7zr3\">this comment thread</a> for discussion of bounded utility functions that assign significant weight to very large lifetimes.]</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"HNJiR8Jzafsv8cHrC": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "daaLFCP34Ba9pRyy2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 18, "extendedScore": null, "score": 3.8e-05, "legacy": true, "legacyId": "20504", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 49, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["a5JAiTdytou3Jg749", "9RCoE7jmmvGd5Zsh2", "K4aGvLnHvYgX9pZHS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-06T22:41:32.007Z", "modifiedAt": null, "url": null, "title": "2012 Winter Fundraiser for the Singularity Institute", "slug": "2012-winter-fundraiser-for-the-singularity-institute", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:09.306Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uFtKP9WB4pk6yiN3K/2012-winter-fundraiser-for-the-singularity-institute", "pageUrlRelative": "/posts/uFtKP9WB4pk6yiN3K/2012-winter-fundraiser-for-the-singularity-institute", "linkUrl": "https://www.lesswrong.com/posts/uFtKP9WB4pk6yiN3K/2012-winter-fundraiser-for-the-singularity-institute", "postedAtFormatted": "Thursday, December 6th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%202012%20Winter%20Fundraiser%20for%20the%20Singularity%20Institute&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A2012%20Winter%20Fundraiser%20for%20the%20Singularity%20Institute%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuFtKP9WB4pk6yiN3K%2F2012-winter-fundraiser-for-the-singularity-institute%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=2012%20Winter%20Fundraiser%20for%20the%20Singularity%20Institute%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuFtKP9WB4pk6yiN3K%2F2012-winter-fundraiser-for-the-singularity-institute", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuFtKP9WB4pk6yiN3K%2F2012-winter-fundraiser-for-the-singularity-institute", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 794, "htmlBody": "<p><small>Cross-posted <a href=\"http://intelligence.org/blog/2012/12/06/2012-winter-matching-challenge/\">here</a>.</small></p>\n<p>(The <a href=\"http://intelligence.org/\">Singularity Institute</a> maintains Less Wrong, with generous help from Trike Apps, and much of the core content is written by salaried SI staff members.)</p>\n<p>Thanks to the generosity of several major donors,<sup>&dagger;</sup> every donation to the Singularity Institute made now until January 20t (deadline extended from the 5th) will be matched dollar-for-dollar, up to a total of $115,000! So please,&nbsp;<strong><a href=\"http://intelligence.org/donate/\">donate now</a>!</strong></p>\n<p>Now is your chance to <strong>double your impact</strong> while helping us raise up to $230,000 to help fund <a href=\"http://intelligence.org/research/\">our research program</a>.</p>\n<p><a href=\"http://intelligence.org/donate/\"><img class=\"aligncenter\" title=\"towardapositivesingularity\" src=\"http://singularity.org/files/towardapositivesingularity-smaller.jpg\" alt=\"\" /></a></p>\n<p><a rel=\"attachment wp-att-2496\" href=\"http://intelligence.org/donate/\"></a> (If you're unfamiliar with our mission, please see our <a href=\"http://intelligence.org/files/SI_PressKit.pdf\">press kit</a> and read our short research summary: <a href=\"http://intelligence.org/summary/\">Reducing Long-Term Catastrophic Risks from Artificial Intelligence</a>.)</p>\n<p>Now that Singularity University has <a href=\"http://singularityu.org/singularity-university-acquires-the-singularity-summit/\">acquired</a> the <a href=\"http://singularitysummit.com/\">Singularity Summit</a>, and SI's interests in rationality training are being developed by the now-separate <a href=\"http://appliedrationality.org/\">CFAR</a>, <strong>the Singularity Institute is making a major transition</strong>.&nbsp;&nbsp;Most of the money from the Summit acquisition is being placed in a separate fund for a Friendly AI team, and therefore does not support our daily operations or other programs.</p>\n<p>For 12 years we've largely focused on movement-building &mdash; through the Singularity Summit, <a href=\"/\">Less Wrong</a>, and other programs. This work was needed to build up a community of support for our mission and a pool of potential researchers for our unique interdisciplinary work.</p>\n<p>Now, the time has come to say \"Mission Accomplished Well Enough to Pivot to Research.\" Our community of supporters is now large enough that qualified researchers are available for us to hire, if we can afford to hire them. Having published <a href=\"http://intelligence.org/research/\">30+ research papers</a> and <a href=\"/lw/f6o/original_research_on_less_wrong/\">dozens more</a> original research articles on Less Wrong, we certainly haven't neglected research. But <strong>in 2013 we plan to pivot so that a much larger share of the funds we raise is spent on research</strong>.</p>\n<h3><a id=\"more\"></a><br /></h3>\n<h3><span>Accomplishments in 2012</span></h3>\n<ul>\n<li>Held a one-week research workshop on one of the open problems in Friendly AI research, and got progress that participants estimate would be the equivalent of 1-3 papers if published. (Details forthcoming. The workshop participants were Eliezer Yudkowsky, Paul Christiano, Marcello Herreshoff, and Mihaly Barasz.)</li>\n<li>Produced our annual <a href=\"http://www.singularitysummit.com/\">Singularity Summit</a> in San Francisco. Speakers included Ray Kurzweil, Steven Pinker, Daniel Kahneman, Temple Grandin, Peter Norvig, and many others.</li>\n<li>Launched the new <a href=\"http://appliedrationality.org/\">Center for Applied Rationality</a>, which ran 5 workshops in 2012, including <a href=\"http://appliedrationality.org/entrepreneurs/\">Rationality for Entrepreneurs</a> and <a href=\"http://appliedrationality.org/sparc2012/\">SPARC</a> (for young math geniuses), and also published one (early-version) smartphone app, <a href=\"http://www.acritch.com/credence-game/\">The Credence Game</a>.</li>\n<li>Launched the redesigned, updated, and reorganized <a href=\"http://intelligence.org/blog/2012/06/18/welcome-to-the-new-singularity-org/\">Singularity.org</a> website.</li>\n<li><a href=\"/r/discussion/lw/dm9/revisiting_sis_2011_strategic_plan_how_are_we/#summary\">Achieved most of the goals</a> from our <a href=\"http://intelligence.org/files/strategicplan20112.pdf\">August 2011 strategic plan</a>.</li>\n<li>11 new <a href=\"http://intelligence.org/research/\">research publications</a>.</li>\n<li>Eliezer published the first 12 posts in his sequence <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\">Highly Advanced Epistemology 101 for Beginners</a>, the precursor to his forthcoming sequence, <em>Open Problems in Friendly AI</em>.</li>\n<li>SI staff members published many other substantive articles on Less Wrong, including <a href=\"/r/discussion/lw/cs6/how_to_purchase_ai_risk_reduction/\">How to Purchase AI Risk Reduction</a>, <a href=\"/lw/crs/how_to_run_a_successful_less_wrong_meetup/\">How to Run a Successful Less Wrong Meetup</a>, a <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/\">Solomonoff Induction tutorial</a>, <a href=\"/lw/9jh/the_humans_hidden_utility_function_maybe/\">The Human's Hidden Utility Function (Maybe)</a>, <a href=\"/lw/ffh/how_can_i_reduce_existential_risk_from_ai/\">How can I reduce existential risk from AI?</a>, <a href=\"/r/discussion/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/\">AI Risk and Opportunity: A Strategic Analysis</a>, and <a href=\"/lw/fc3/checklist_of_rationality_habits/\">Checklist of Rationality Habits</a>. </li>\n<li>Launched our new volunteers platform, <a href=\"http://singularityvolunteers.org/\">SingularityVolunteers.org</a>.</li>\n<li>Hired two new researchers, Kaj Sotala and Alex Altair.</li>\n<li>Published our <a href=\"http://intelligence.org/files/SI_PressKit.pdf\">press kit</a> to make journalists' lives easier. </li>\n<li>And of course <em>much</em> more.</li>\n</ul>\n<h3>Future Plans You Can Help Support</h3>\n<p>In the coming months, we plan to do the following:</p>\n<ul>\n<li>As part of Singularity University's acquisition of the Singularity Summit, we will be changing our name and launching a new website.</li>\n<li>Eliezer will publish his sequence <em>Open Problems in Friendly AI</em>.</li>\n<li>We will publish nicely-edited ebooks (Kindle, iBooks, and PDF) for many of our core materials, to make them more accessible: <em><a href=\"http://wiki.lesswrong.com/wiki/Sequences\">The Sequences, 2006-2009</a></em>, <em><a href=\"http://facingthesingularity.com/\">Facing the Singularity</a></em>, and <em><a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\">The Hanson-Yudkowsky AI Foom Debate</a></em>.</li>\n<li>We will publish several more research papers, including \"Responses to Catastrophic AGI Risk: A Survey\" and a short, technical introduction to <a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">timeless decision theory</a>.</li>\n<li>We will set up the infrastructure required to host a productive Friendly AI team and try hard to recruit enough top-level math talent to launch it.</li>\n</ul>\n<p>(Other projects are still being surveyed for likely cost and strategic impact.)</p>\n<p>We appreciate your support for our high-impact work! Donate now, and seize a better than usual chance to move our work forward. Credit card transactions are securely processed using either PayPal or Google Checkout. If you have questions about donating, please contact Louie Helm at (510) 717-1477 or louie@intelligence.org.</p>\n<p><sup>&dagger;</sup> $115,000 of total matching funds has been provided by Edwin Evans, Mihaly Barasz, Rob Zahra, Alexei Andreev, Jeff Bone, Michael Blume, Guy Srinivasan, and Kevin Fischer.</p>\n<p>I will mostly be traveling (for AGI-12) for the next 25 hours, but I will try to answer questions after that.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Z6DgiCrMtpSNxwuYW": 1, "NrvXXL3iGjjxu5B7d": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uFtKP9WB4pk6yiN3K", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 48, "extendedScore": null, "score": 1.0517107807437548e-06, "legacy": true, "legacyId": "20500", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><small>Cross-posted <a href=\"http://intelligence.org/blog/2012/12/06/2012-winter-matching-challenge/\">here</a>.</small></p>\n<p>(The <a href=\"http://intelligence.org/\">Singularity Institute</a> maintains Less Wrong, with generous help from Trike Apps, and much of the core content is written by salaried SI staff members.)</p>\n<p>Thanks to the generosity of several major donors,<sup>\u2020</sup> every donation to the Singularity Institute made now until January 20t (deadline extended from the 5th) will be matched dollar-for-dollar, up to a total of $115,000! So please,&nbsp;<strong><a href=\"http://intelligence.org/donate/\">donate now</a>!</strong></p>\n<p>Now is your chance to <strong>double your impact</strong> while helping us raise up to $230,000 to help fund <a href=\"http://intelligence.org/research/\">our research program</a>.</p>\n<p><a href=\"http://intelligence.org/donate/\"><img class=\"aligncenter\" title=\"towardapositivesingularity\" src=\"http://singularity.org/files/towardapositivesingularity-smaller.jpg\" alt=\"\"></a></p>\n<p><a rel=\"attachment wp-att-2496\" href=\"http://intelligence.org/donate/\"></a> (If you're unfamiliar with our mission, please see our <a href=\"http://intelligence.org/files/SI_PressKit.pdf\">press kit</a> and read our short research summary: <a href=\"http://intelligence.org/summary/\">Reducing Long-Term Catastrophic Risks from Artificial Intelligence</a>.)</p>\n<p>Now that Singularity University has <a href=\"http://singularityu.org/singularity-university-acquires-the-singularity-summit/\">acquired</a> the <a href=\"http://singularitysummit.com/\">Singularity Summit</a>, and SI's interests in rationality training are being developed by the now-separate <a href=\"http://appliedrationality.org/\">CFAR</a>, <strong>the Singularity Institute is making a major transition</strong>.&nbsp;&nbsp;Most of the money from the Summit acquisition is being placed in a separate fund for a Friendly AI team, and therefore does not support our daily operations or other programs.</p>\n<p>For 12 years we've largely focused on movement-building \u2014 through the Singularity Summit, <a href=\"/\">Less Wrong</a>, and other programs. This work was needed to build up a community of support for our mission and a pool of potential researchers for our unique interdisciplinary work.</p>\n<p>Now, the time has come to say \"Mission Accomplished Well Enough to Pivot to Research.\" Our community of supporters is now large enough that qualified researchers are available for us to hire, if we can afford to hire them. Having published <a href=\"http://intelligence.org/research/\">30+ research papers</a> and <a href=\"/lw/f6o/original_research_on_less_wrong/\">dozens more</a> original research articles on Less Wrong, we certainly haven't neglected research. But <strong>in 2013 we plan to pivot so that a much larger share of the funds we raise is spent on research</strong>.</p>\n<h3><a id=\"more\"></a><br></h3>\n<h3 id=\"Accomplishments_in_2012\"><span>Accomplishments in 2012</span></h3>\n<ul>\n<li>Held a one-week research workshop on one of the open problems in Friendly AI research, and got progress that participants estimate would be the equivalent of 1-3 papers if published. (Details forthcoming. The workshop participants were Eliezer Yudkowsky, Paul Christiano, Marcello Herreshoff, and Mihaly Barasz.)</li>\n<li>Produced our annual <a href=\"http://www.singularitysummit.com/\">Singularity Summit</a> in San Francisco. Speakers included Ray Kurzweil, Steven Pinker, Daniel Kahneman, Temple Grandin, Peter Norvig, and many others.</li>\n<li>Launched the new <a href=\"http://appliedrationality.org/\">Center for Applied Rationality</a>, which ran 5 workshops in 2012, including <a href=\"http://appliedrationality.org/entrepreneurs/\">Rationality for Entrepreneurs</a> and <a href=\"http://appliedrationality.org/sparc2012/\">SPARC</a> (for young math geniuses), and also published one (early-version) smartphone app, <a href=\"http://www.acritch.com/credence-game/\">The Credence Game</a>.</li>\n<li>Launched the redesigned, updated, and reorganized <a href=\"http://intelligence.org/blog/2012/06/18/welcome-to-the-new-singularity-org/\">Singularity.org</a> website.</li>\n<li><a href=\"/r/discussion/lw/dm9/revisiting_sis_2011_strategic_plan_how_are_we/#summary\">Achieved most of the goals</a> from our <a href=\"http://intelligence.org/files/strategicplan20112.pdf\">August 2011 strategic plan</a>.</li>\n<li>11 new <a href=\"http://intelligence.org/research/\">research publications</a>.</li>\n<li>Eliezer published the first 12 posts in his sequence <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\">Highly Advanced Epistemology 101 for Beginners</a>, the precursor to his forthcoming sequence, <em>Open Problems in Friendly AI</em>.</li>\n<li>SI staff members published many other substantive articles on Less Wrong, including <a href=\"/r/discussion/lw/cs6/how_to_purchase_ai_risk_reduction/\">How to Purchase AI Risk Reduction</a>, <a href=\"/lw/crs/how_to_run_a_successful_less_wrong_meetup/\">How to Run a Successful Less Wrong Meetup</a>, a <a href=\"/lw/dhg/an_intuitive_explanation_of_solomonoff_induction/\">Solomonoff Induction tutorial</a>, <a href=\"/lw/9jh/the_humans_hidden_utility_function_maybe/\">The Human's Hidden Utility Function (Maybe)</a>, <a href=\"/lw/ffh/how_can_i_reduce_existential_risk_from_ai/\">How can I reduce existential risk from AI?</a>, <a href=\"/r/discussion/lw/ajm/ai_risk_and_opportunity_a_strategic_analysis/\">AI Risk and Opportunity: A Strategic Analysis</a>, and <a href=\"/lw/fc3/checklist_of_rationality_habits/\">Checklist of Rationality Habits</a>. </li>\n<li>Launched our new volunteers platform, <a href=\"http://singularityvolunteers.org/\">SingularityVolunteers.org</a>.</li>\n<li>Hired two new researchers, Kaj Sotala and Alex Altair.</li>\n<li>Published our <a href=\"http://intelligence.org/files/SI_PressKit.pdf\">press kit</a> to make journalists' lives easier. </li>\n<li>And of course <em>much</em> more.</li>\n</ul>\n<h3 id=\"Future_Plans_You_Can_Help_Support\">Future Plans You Can Help Support</h3>\n<p>In the coming months, we plan to do the following:</p>\n<ul>\n<li>As part of Singularity University's acquisition of the Singularity Summit, we will be changing our name and launching a new website.</li>\n<li>Eliezer will publish his sequence <em>Open Problems in Friendly AI</em>.</li>\n<li>We will publish nicely-edited ebooks (Kindle, iBooks, and PDF) for many of our core materials, to make them more accessible: <em><a href=\"http://wiki.lesswrong.com/wiki/Sequences\">The Sequences, 2006-2009</a></em>, <em><a href=\"http://facingthesingularity.com/\">Facing the Singularity</a></em>, and <em><a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\">The Hanson-Yudkowsky AI Foom Debate</a></em>.</li>\n<li>We will publish several more research papers, including \"Responses to Catastrophic AGI Risk: A Survey\" and a short, technical introduction to <a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">timeless decision theory</a>.</li>\n<li>We will set up the infrastructure required to host a productive Friendly AI team and try hard to recruit enough top-level math talent to launch it.</li>\n</ul>\n<p>(Other projects are still being surveyed for likely cost and strategic impact.)</p>\n<p>We appreciate your support for our high-impact work! Donate now, and seize a better than usual chance to move our work forward. Credit card transactions are securely processed using either PayPal or Google Checkout. If you have questions about donating, please contact Louie Helm at (510) 717-1477 or louie@intelligence.org.</p>\n<p><sup>\u2020</sup> $115,000 of total matching funds has been provided by Edwin Evans, Mihaly Barasz, Rob Zahra, Alexei Andreev, Jeff Bone, Michael Blume, Guy Srinivasan, and Kevin Fischer.</p>\n<p>I will mostly be traveling (for AGI-12) for the next 25 hours, but I will try to answer questions after that.</p>", "sections": [{"title": "Accomplishments in 2012", "anchor": "Accomplishments_in_2012", "level": 1}, {"title": "Future Plans You Can Help Support", "anchor": "Future_Plans_You_Can_Help_Support", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "113 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 127, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jTkmEGWM4dJAfE62W", "8LrrHdt8HSvZFKmRb", "qMuAazqwJvkvo8teR", "Kyc5dFDzBg4WccrbK", "fa5o2tg9EfJE77jEQ", "qARBe3jBodrdPeRE6", "i2XoqtYEykc4XWp9B", "ttGbpJQ8shBi8hDhh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-07T06:14:54.536Z", "modifiedAt": null, "url": null, "title": "Complement Luke's Mega-Course for Aspiring Philosophers", "slug": "complement-luke-s-mega-course-for-aspiring-philosophers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:24.422Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/j4HGkLYbXDnK7gFxj/complement-luke-s-mega-course-for-aspiring-philosophers", "pageUrlRelative": "/posts/j4HGkLYbXDnK7gFxj/complement-luke-s-mega-course-for-aspiring-philosophers", "linkUrl": "https://www.lesswrong.com/posts/j4HGkLYbXDnK7gFxj/complement-luke-s-mega-course-for-aspiring-philosophers", "postedAtFormatted": "Friday, December 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Complement%20Luke's%20Mega-Course%20for%20Aspiring%20Philosophers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AComplement%20Luke's%20Mega-Course%20for%20Aspiring%20Philosophers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj4HGkLYbXDnK7gFxj%2Fcomplement-luke-s-mega-course-for-aspiring-philosophers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Complement%20Luke's%20Mega-Course%20for%20Aspiring%20Philosophers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj4HGkLYbXDnK7gFxj%2Fcomplement-luke-s-mega-course-for-aspiring-philosophers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj4HGkLYbXDnK7gFxj%2Fcomplement-luke-s-mega-course-for-aspiring-philosophers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1094, "htmlBody": "<p>Luke has mentioned much of the research that aspiring philosophers ought to read<a href=\"/lw/frp/train_philosophers_with_pearl_and_kahneman_not/\"> here</a>.</p>\n<p>In fact, he delineated a basis upon which good philosophy can be build, a worldview brought by science and experimentation that relates to, and informs, the kinds of facts which philosophers need to understand to increase their probabilities of asking, and giving good answers to, relevant questions.</p>\n<p>Some argued that his list is biased, let us assume for the time being it isn't.</p>\n<p>Some argued that the main problem with the list is that it requires either unmanageable amount of time to go through, or improbable levels of intelligence/motivation to do so. This argument does make sense if the purpose of the list was \"Let us create a good Philosophy Course\".</p>\n<p>But this is not the purpose of it. The purpose of it, as most of what <a href=\"/lw/91c/so_you_want_to_save_the_world/\">Luke publicly does is to save the World</a>. And if doing so requires making people go through an <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">enormous amount of pages</a> of content besides their formal education, well, then so be it. If it has to be a six year course, then it has to.</p>\n<p>At the end of his post he says:</p>\n<blockquote>\n<p>You might also let them read 20th century analytic philosophy at that point [after going through his Mega-Course] &mdash; hopefully their training will have inoculated them from picking up <a href=\"/lw/4zs/philosophy_a_diseased_discipline/\">bad</a> <a href=\"/lw/7tz/philosophy_by_humans_1_concepts_dont_work_that_way/\">thinking</a> <a href=\"/lw/foz/philosophy_by_humans_3_intuitions_arent_shared/\">habits</a>.</p>\n</blockquote>\n<p>Now 20th century Analytic Philosophy, and some philosophy that isn't strictly analytic, should definitely be at a philosophy course. I urge other <a href=\"/lw/emj/poll_less_wrong_and_mainstream_philosophy_how/\">LessWronger philosophers</a> to guide people through that.</p>\n<p><a href=\"/lw/58d/how_not_to_be_a_na%C3%AFve_computationalist/\">Here is a list</a> I have published here before, for Philosophy of Mind and Language (sometimes considered subsets or children of Analytic Philosophy). It covers only the minimal reading necessary to grasp the place of computationalism, and so-called computational theories of mind within the larger debate of philosophy.&nbsp;</p>\n<p>But the last century has seen a lot of good philosophy that by luck didn't conflict with neither the science of the day, nor the science that was developed until 2012. Sometimes authors were very careful when writing their philosophy, and well versed in science, like Dennett, Hofstadter, Putnam, Ned Block, and Chalmers. Finally, frequently the topics at hand are sufficiently orthogonal with scientific development that it simply didn't matter that the author didn't know in 1970 what we (after the Mega-Course) know today.&nbsp;</p>\n<p>So I ask <a href=\"/user/lukeprog/\">Luke</a>, <a href=\"/user/pragmatist/\">Pragmatist</a>, Carl Shulman and others to help build the layer that will sit on top of <a href=\"/lw/frp/train_philosophers_with_pearl_and_kahneman_not/\">the science layer</a> in the \"Philosophy Given Science\" Mega-Course for aspiring philosophers. The course will have four layers. Below the science layer, will be its <a href=\"/lw/frp/train_philosophers_with_pearl_and_kahneman_not/7yzi\">prerequisites</a> (admittedly large), and atop the one I'm suggesting here, we hope to start building a really good philosophy that is compatible with our scientific understanding, tackles mostly Big Questions which are highly likely to be meaningful, and frequently also useful for the major issues we <a href=\"http://lukeprog.com/SaveTheWorld.html\">still have time to solve</a>.</p>\n<p>This is the pyramidal&nbsp; structure I suggest we create, 1,2 and 3 being the content of the Mega-Course, and 4 being the likely outcome we expect it to facilitate, made by those who undertake it:</p>\n<p>4) Philosophy given 1,2 and 3. Tackling the Big Questions, and making it portable to areas such as AGI, Biotech, etc...</p>\n<p>3) Philosophy, up to 2012, that is well informed about or orthogonal to Science so far. Or lucky.</p>\n<p>2) Science that is relevant to philosophy. <a href=\"/lw/frp/train_philosophers_with_pearl_and_kahneman_not/\">This.</a></p>\n<p>1) Prerequisites for 2.</p>\n<p>&nbsp;</p>\n<p>In this post we begin layer three, I'll start by <a href=\"/lw/58d/how_not_to_be_a_na%C3%AFve_computationalist/\">copying</a> the Mind and Language I had sent. After I'll include some of Bostrom's recommendations within philosophy to me as an undergrad, and my selection of Dennett's, and Dennett's selection of science:</p>\n<blockquote>\n<p>Language and Mind:</p>\n<ul>\n<li><strong>37 Ways words can be Wrong - Yudkowsky</strong></li>\n<li><strong>Darwin Dangerous Idea Chapters 3,5, 11, 12 and 14 - Daniel Dennett</strong></li>\n<li>On Denoting - Bertrand Russell</li>\n<li>On What There Is - Quine</li>\n<li>Two Dogmas of Empiricism - Quine</li>\n<li><a href=\"http://uspfiloanalitica.googlegroups.com/web/Naming+and+Necessity+%28Saul+Kripke%29.pdf?gda=OM04jF0AAACxY9vtt80_Eoc5nvqIVwz175PQJuPTrBuBx1KDOyPsv28EIiRgvpFIdUtDSOAXMdvQiOpUpA44RKeO3Hq2LZFjAFL0F1PSQ6AApbiBfb572OU2_747KStNgkfeVUa7Znk\">Namind and Necessity </a>- Kripke <em>OR</em> <a href=\"http://consc.net/papers/twodim.html\">Two Dimensional Semantics</a> - David Chalmers</li>\n<li><a href=\"http://uspfiloanalitica.googlegroups.com/web/IS+PERSONAL+IDENTITY+WHAT+MATTERS+%28Parfit%29.pdf?gda=xH7KUGUAAACxY9vtt80_Eoc5nvqIVwz1SrVJGmujo21ZzWCUZeyIT_1pTF0kkXH0LwuUCGHMLaD_Xbul_77J5BjqCwuNd7LQkq10Hen6mJVcDGpt2QOMNxyfxYhtaHnOp4Mq0MGarrsa7WIsd6qHDhllGPdGMn_4\">&ldquo;Is Personal Identity What Matters?&rdquo;</a> - Derek Parfit</li>\n<li>Breakdown of Will - Part Two (don&rsquo;t read part 3) George Ainslie</li>\n<li><strong><a href=\"http://www.nyu.edu/gsas/dept/philo/faculty/block/papers/Abridged%20BBS.htm\">Concepts of Consciousness</a></strong> <strong>2003 - Ned Block</strong></li>\n<li>Attitudes <em>de dicto </em>and <em>de se </em>- David Lewis- Phil Papers 1</li>\n<li><strong>General Semantics - David Lewis - Phil Papers 1</strong></li>\n<li>The Stuff of Thought, Chapter 3 &ldquo;Fifty Thousand Innate Concepts&rdquo; - Steve Pinker</li>\n<li><strong><a title=\"If its in russian, use google chrome\" href=\"http://scilib.narod.ru/Biology/Dennett/IS/index.html\">Beyond Belief</a> - Daniel Dennett </strong><em>in Intentional Stance</em></li>\n<li><strong><a href=\"http://consc.net/papers/belief.html\">The Content and Epistemology of Phenomenal Belief</a></strong> <strong>- David Chalmers</strong></li>\n<li><strong>Quining Qualia OR I Am a Strange Loop OR Consciousness Explained - Dan &amp; Doug</strong></li>\n<li><strong><a href=\"http://plato.stanford.edu/entries/intentionality/\">Intentionality</a></strong> <strong>- Pierre Jacob - Stanford Encyclopedia Phil</strong></li>\n</ul>\n</blockquote>\n<p>From Bostrom's suggestions:</p>\n<ul>\n<li>Philosophical Papers - David Lewis</li>\n<li>Parfit</li>\n<li>Frank Arntzenius</li>\n<li>Timothy Williamson</li>\n<li>Brian Skyrms</li>\n</ul>\n<p>By Dennett:</p>\n<ul>\n<li>Real Patterns</li>\n<li>True Believers</li>\n<li>Kinds of Minds</li>\n<li>Intentional Systems In Cognitive Ethology</li>\n<li>Those mentioned above in the Mind and Language list.</li>\n</ul>\n<p>Not previously cited, but in Luke's <a href=\"/lw/54r/my_favorite_philosophers/\">favorites list</a>:</p>\n<ul>\n<li>Noam Chomsky</li>\n<li>Stephen Stich</li>\n<li>Hilary Kronblith</li>\n<li>Eric schwitzgebel</li>\n<li>Michael Bishop</li>\n</ul>\n<p>Dennett's suggestions on interdisciplinary science (<a href=\"/lw/frp/train_philosophers_with_pearl_and_kahneman_not/\">layer 2</a>):</p>\n<ul>\n<li>The Company of Strangers - Paul Seabright</li>\n<li>Not by Genes Alone - Boyd and Richerson</li>\n<li>I Am a Strange Loop. - Hofstadter</li>\n</ul>\n<p>By Bostrom</p>\n<ul>\n<li>Probably easier to list what <em>should not </em>be read... </li>\n</ul>\n<p>This may initially appear overwhelming, but it is probably one order of magnitude less content than Luke's original post about layer 2. Once again I ask philosophers to specify more things within areas that are not well addressed here, such as ethics. Also books by scientists dealing with philosophical topics (such as Sam Harris: The Moral Landscape) can be added here.&nbsp;</p>\n<p>The \"Philosophy Given Science\" MegaCourse may never actually take place, but it will be a very valuable guideline for institutions to influence actual Philosophy courses, for Philosophy teachers to get cohesive and preselected content to teach, and most importantly for diligent aspiring philosophers willing to get to the Big and relevant problems, instead of being the ball in the chaotic Pinball game that academic philosophy has become, despite all good things it brought. When the path is too long, a shortcut is not a shortcut anymore, it is the only way to get there before it is too late.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "j4HGkLYbXDnK7gFxj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 8, "extendedScore": null, "score": 1.0519712161869766e-06, "legacy": true, "legacyId": "20513", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LcEzxX2FNTKbB6KXS", "5BJvusxdwNXYQ4L9L", "FwiPfF8Woe5JrzqEu", "wHjpCxeDeuFadG3jF", "jaN4EKrRnZdynTJjH", "mPBuTuqShRotscGSP", "HLadQZ6FqTTjnYpoB", "PMoa6f4aACHfefwBY"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-07T07:10:58.455Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Test Near, Apply Far", "slug": "seq-rerun-test-near-apply-far", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2B4DHdMvRTyeqfKCx/seq-rerun-test-near-apply-far", "pageUrlRelative": "/posts/2B4DHdMvRTyeqfKCx/seq-rerun-test-near-apply-far", "linkUrl": "https://www.lesswrong.com/posts/2B4DHdMvRTyeqfKCx/seq-rerun-test-near-apply-far", "postedAtFormatted": "Friday, December 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Test%20Near%2C%20Apply%20Far&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Test%20Near%2C%20Apply%20Far%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2B4DHdMvRTyeqfKCx%2Fseq-rerun-test-near-apply-far%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Test%20Near%2C%20Apply%20Far%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2B4DHdMvRTyeqfKCx%2Fseq-rerun-test-near-apply-far", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2B4DHdMvRTyeqfKCx%2Fseq-rerun-test-near-apply-far", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 163, "htmlBody": "<p>Today's post, <a href=\"http://www.overcomingbias.com/2008/12/test-near-apply.html\">Test Near, Apply Far</a> was originally published on December 3, 2008.  A summary:</p>\n<p>&nbsp;</p>\n<blockquote>Avoid inventing new abstractions that deal exclusively with things that are far. Ensure that the abstractions you endorse pay their rent here, or at least nearby.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/ft4/seq_rerun_hard_takeoff/\">Hard Takeoff</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2B4DHdMvRTyeqfKCx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.0520034300770923e-06, "legacy": true, "legacyId": "20514", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2kdLwy7apacLs3cjg", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-07T11:26:38.757Z", "modifiedAt": null, "url": null, "title": "Mini advent calendar of Xrisks: Artificial Intelligence", "slug": "mini-advent-calendar-of-xrisks-artificial-intelligence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:26.566Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xt4pTCNbmFTC7sDPJ/mini-advent-calendar-of-xrisks-artificial-intelligence", "pageUrlRelative": "/posts/xt4pTCNbmFTC7sDPJ/mini-advent-calendar-of-xrisks-artificial-intelligence", "linkUrl": "https://www.lesswrong.com/posts/xt4pTCNbmFTC7sDPJ/mini-advent-calendar-of-xrisks-artificial-intelligence", "postedAtFormatted": "Friday, December 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mini%20advent%20calendar%20of%20Xrisks%3A%20Artificial%20Intelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMini%20advent%20calendar%20of%20Xrisks%3A%20Artificial%20Intelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxt4pTCNbmFTC7sDPJ%2Fmini-advent-calendar-of-xrisks-artificial-intelligence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mini%20advent%20calendar%20of%20Xrisks%3A%20Artificial%20Intelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxt4pTCNbmFTC7sDPJ%2Fmini-advent-calendar-of-xrisks-artificial-intelligence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxt4pTCNbmFTC7sDPJ%2Fmini-advent-calendar-of-xrisks-artificial-intelligence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 296, "htmlBody": "<p>&nbsp;</p>\n<p class=\"MsoNormal\" style=\"margin-bottom: .0001pt;\"><span style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 17px;\">The FHI's mini advent calendar: counting down through the big five existential risks. As people on this list would have suspected, the last one is the most fearsome, should it come to pass: Artificial Intelligence.</span><br style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 17px;\" /><br style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 17px;\" /><span style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 17px;\">And the FHI is starting the AGI-12/AGI-impacts <a href=\"ttp://www.winterintelligence.org/\">conference</a> tomorrow, on this very subject.</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: .0001pt;\"><span style=\"color: #333333; font-family: 'lucida grande', tahoma, verdana, arial, sans-serif; font-size: 13px; line-height: 17px;\"><br /></span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: .0001pt;\"><strong><span style=\"font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">Artificial intelligence</span></strong></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: .0001pt;\"><em><span style=\"font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">Current understanding: very low<br /></span></em><em><span style=\"font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">Most worrying aspect: likely to cause total (not partial) human extinction</span></em></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: .0001pt;\"><span style=\"font-family: &quot;Arial&quot;,&quot;sans-serif&quot;;\">Humans have trod upon the moon, number over seven billion, and have created nuclear weapons and a planet spanning technological economy. We also have the potential to destroy ourselves and entire ecosystems. These achievements have been made possible through the tiny difference in brain size between us and the other greater apes; what further achievements could come from an artificial intelligence at or above our own level?</span></p>\n<p class=\"MsoNormal\" style=\"margin-bottom: .0001pt;\"><span style=\"font-family: Arial, sans-serif;\">It is very hard to predict when or if such an intelligence could be built, but it is certain to be utterly disruptive if it were. Even a human-level intelligence, trained and copied again and again, could substitute for human labour in most industries, causing (at minimum) mass unemployment. But this disruption is minor compared with the power that an above-human AI could accumulate, through technological innovation, social manipulation, or careful planning. Such super-powered entities would be hard to control, pursuing their own goals, and considering humans as an annoying obstacle to overcome. Making them safe would require very careful, bug-free programming, as well as an understanding of how to cast key human concepts (such as love and human rights) into code. All solutions proposed so far have turned out to be very inadequate. Unlike other existential risks, AIs could really &ldquo;finish the job&rdquo;: an AI bent on removing humanity would be able to eradicate the last remaining members of our species.</span></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Rz5jb3cYHTSRmqNnN": 2, "sYm3HiWcfZvrGu3ui": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xt4pTCNbmFTC7sDPJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 5, "extendedScore": null, "score": 1.052150355182246e-06, "legacy": true, "legacyId": "20520", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-07T15:59:08.810Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Atlanta, Austin, Berlin, Bielefield, Cambridge MA, Innsbruck, London, Melbourne, Montreal, Phoenix, Seattle", "slug": "weekly-lw-meetups-atlanta-austin-berlin-bielefield-cambridge", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HAkZ6jRkpt4NrAKPS/weekly-lw-meetups-atlanta-austin-berlin-bielefield-cambridge", "pageUrlRelative": "/posts/HAkZ6jRkpt4NrAKPS/weekly-lw-meetups-atlanta-austin-berlin-bielefield-cambridge", "linkUrl": "https://www.lesswrong.com/posts/HAkZ6jRkpt4NrAKPS/weekly-lw-meetups-atlanta-austin-berlin-bielefield-cambridge", "postedAtFormatted": "Friday, December 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Atlanta%2C%20Austin%2C%20Berlin%2C%20Bielefield%2C%20Cambridge%20MA%2C%20Innsbruck%2C%20London%2C%20Melbourne%2C%20Montreal%2C%20Phoenix%2C%20Seattle&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Atlanta%2C%20Austin%2C%20Berlin%2C%20Bielefield%2C%20Cambridge%20MA%2C%20Innsbruck%2C%20London%2C%20Melbourne%2C%20Montreal%2C%20Phoenix%2C%20Seattle%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHAkZ6jRkpt4NrAKPS%2Fweekly-lw-meetups-atlanta-austin-berlin-bielefield-cambridge%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Atlanta%2C%20Austin%2C%20Berlin%2C%20Bielefield%2C%20Cambridge%20MA%2C%20Innsbruck%2C%20London%2C%20Melbourne%2C%20Montreal%2C%20Phoenix%2C%20Seattle%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHAkZ6jRkpt4NrAKPS%2Fweekly-lw-meetups-atlanta-austin-berlin-bielefield-cambridge", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHAkZ6jRkpt4NrAKPS%2Fweekly-lw-meetups-atlanta-austin-berlin-bielefield-cambridge", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 551, "htmlBody": "<p><strong>This summary was posted to LW main on November 30th. The following week's summary is <a href=\"/lw/fu3/weekly_lw_meetups_austin_atlanta_cleveland/\">here</a>.</strong></p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/g5\">Berlin Meetup:&nbsp;<span class=\"date\">01 December 2012 07:30PM</span></a></li>\n<li><a href=\"/meetups/gh\">02/12 London Meetup:&nbsp;<span class=\"date\">02 December 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/g9\">First meetup in Innsbruck :&nbsp;<span class=\"date\">02 December 2012 03:00PM</span></a></li>\n<li><a href=\"/meetups/gj\">Seattle Meetup: Bayes Theorem Tutorial:&nbsp;<span class=\"date\">02 December 2012 04:00PM</span></a></li>\n<li><a href=\"/meetups/g6\">Atlanta - Practical Rationality Meetup Session:&nbsp;<span class=\"date\">02 December 2012 05:00PM</span></a></li>\n<li><a href=\"/meetups/gg\">Montreal Meetup - Biases and Biased Boardgaming:&nbsp;<span class=\"date\">04 December 2012 06:30PM</span></a></li>\n<li><a href=\"/meetups/ga\">Meetup : Bielefeld Meetup, December 5th:&nbsp;<span class=\"date\">05 December 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/gb\">Phoenix, AZ: Stoicism and Visitors: 5 December 6:00PM:&nbsp;<span class=\"date\">06 December 2012 06:00PM</span></a></li>\n<li><a href=\"/meetups/ge\">Moscow: Applied Rationality and Cognitive Biases:&nbsp;<span class=\"date\">08 December 2012 04:00PM</span></a></li>\n<li><a href=\"/meetups/el\">Sofia, Bulgaria Meetup:&nbsp;<span class=\"date\">09 December 2012 05:00PM</span></a></li>\n<li><a href=\"/meetups/gc\">Brussels meetup:&nbsp;<span class=\"date\">15 December 2012 01:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly&nbsp;scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/bx\">Austin, TX:&nbsp;<span class=\"date\">01 December 2018 01:30PM</span></a></li>\n<li><a href=\"/meetups/gk\">Cambridge, MA first-Sunday meetup:&nbsp;<span class=\"date\">02 December 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/g7\">Melbourne, practical rationality:&nbsp;<span class=\"date\">07 December 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/en\">Winter Solstice Megameetup - NYC:&nbsp;<span class=\"date\">15 December 2012 05:00PM</span></a></li>\n</ul>\n<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong></strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>\n<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>\n<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>\n<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HAkZ6jRkpt4NrAKPS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 1.0523069917815077e-06, "legacy": true, "legacyId": "20373", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XujsYztoM2FQYLG9G", "d28mWBMrFt8nwpXLp", "xQoMYN7ZKoKTA4NqP", "97WbQTb4Etch9mDuT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-07T21:04:20.299Z", "modifiedAt": null, "url": null, "title": "2012 Survey Results", "slug": "2012-survey-results", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:08.685Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/x9FNKTEt68Rz6wQ6P/2012-survey-results", "pageUrlRelative": "/posts/x9FNKTEt68Rz6wQ6P/2012-survey-results", "linkUrl": "https://www.lesswrong.com/posts/x9FNKTEt68Rz6wQ6P/2012-survey-results", "postedAtFormatted": "Friday, December 7th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%202012%20Survey%20Results&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A2012%20Survey%20Results%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx9FNKTEt68Rz6wQ6P%2F2012-survey-results%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=2012%20Survey%20Results%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx9FNKTEt68Rz6wQ6P%2F2012-survey-results", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fx9FNKTEt68Rz6wQ6P%2F2012-survey-results", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 5626, "htmlBody": "<p>Thank you to everyone who took the <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dG1pTzlrTnJ4eks3aE13Ni1lbV8yUkE6MQ#gid=0\">2012 Less Wrong Survey</a> (the survey is now closed. Do not try to take it.) Below the cut, this post contains the basic survey results, a few more complicated analyses, and the data available for download so you can explore it further on your own. You may want to compare these to the <a href=\"/lw/8p4/2011_survey_results/\">results of the 2011 Less Wrong Survey</a>.</p>\n<p><a id=\"more\"></a></p>\n<h2><strong>Part 1: Population</strong><br /></h2>\n<p>How many of us are there?<br /><br />The short answer is that I don't know.<br /><br />The 2011 survey ran 33 days and collected 1090 responses. This year's survey ran 23 days and collected 1195 responses. The average number of new responses during the last week was about five per day, so even if I had kept this survey open as long as the last one I probably wouldn't have gotten more than about 1250 responses. That means at most a 15% year on year growth rate, which is pretty abysmal compared to the 650% growth rate in two years we saw last time.<br /><br />About half of these responses were from lurkers; over half of the non-lurker remainder had commented but never posted to Main or Discussion. That means there were only about 600 non-lurkers.<br /><br />But I am skeptical of these numbers. I hang out with some people who are very closely associated with the greater Less Wrong community, and a lot of them didn't know about the survey until I mentioned it to them in person. I know some people who could plausibly be described as focusing their lives around the community who just never took the survey for one reason or another. One lesson of this survey may be that the community is no longer limited to people who check Less Wrong very often, if at all. One friend didn't see the survey because she hangs out on the #lesswrong channel more than the main site. Another mostly just goes to meetups. So I think this represents only a small sample of people who could justly be considered Less Wrongers.<br /><br />The question of \"how quickly is LW growing\" is also complicated by the high turnover. Over half the people who took this survey said they hadn't participated in the survey last year. I tried to break this down by combining a few sources of information, and I think our 1200 respondents include 500 people who took last year's survey, 400 people who were around last year but didn't take the survey for some reason, and 300 new people.<br /><br />As expected, there's lower turnover among regulars than among lurkers. Of people who have posted in Main, about 75% took the survey last year; of people who only lurked, about 75% hadn't. <br /><br />This view of a very high-turnover community and lots of people not taking the survey is consistent with Vladimir Nesov's data showing http://lesswrong.com/lw/e4j/number_of_members_on_lesswrong/77xz 1390 people who have written at least ten comments. But the survey includes only about 600 people who have at least commented; 800ish of Vladimir's accounts are either gone or didn't take the census.</p>\n<h2><strong>Part 2: Categorical Data</strong></h2>\n<p><strong>SEX:</strong><br />Man: 1057, 89.2%<br />Woman: 120, 10.1%<br />Other: 2, 0.2%)<br />No answer: 6, 0.5%<br /><br /><strong>GENDER:</strong><br />M (cis): 1021, 86.2%<br />F (cis): 105, 8.9%<br />M (trans f-&gt;m): 3, 0.3%<br />F (trans m-&gt;f): 16, 1.3%<br />Other: 29, 2.4%<br />No answer: 11, 0.9%<br /><br /><strong>ORIENTATION:</strong><br />Heterosexual: 964, 80.7%<br />Bisexual: 135, 11.4%<br />Homosexual: 28, 2.4%<br />Asexual: 24, 2%<br />Other: 28, 2.4%<br />No answer: 14, 1.2%<br /><strong><br />RELATIONSHIP STYLE:</strong><br />Prefer monogamous: 639, 53.9%<br />Prefer polyamorous: 155, 13.1%<br />Uncertain/no preference: 358, 30.2%<br />Other: 21, 1.8%<br />No answer: 12, 1%<br /><br /><strong>NUMBER OF CURRENT PARTNERS:</strong><br />0: 591, 49.8%<br />1: 519, 43.8%<br />2: 34, 2.9%<br />3: 12, 1%<br />4: 5, 0.4%<br />6: 1, 0.1%<br />7, 1, 0.1% <em>(and this person added \"really, not trolling\")</em><br />Confusing or no answer: 20, 1.8%<br /><br /><strong>RELATIONSHIP STATUS:</strong><br />Single: 628, 53%<br />Relationship: 323, 27.3%<br />Married: 220, 18.6%<br />No answer: 14, 1.2%<br /><br /><strong>RELATIONSHIP GOALS:</strong><br />Not looking for more partners: 707, 59.7%<br />Looking for more partners: 458, 38.6%<br />No answer: 20, 1.7%<br /><br /><strong>COUNTRY:</strong><br />USA: 651, 54.9%<br />UK: 103, 8.7%<br />Canada: 74, 6.2%<br />Australia: 59, 5%<br />Germany: 54, 4.6%<br />Israel: 15, 1.3%<br />Finland: 15, 1.3%<br />Russia: 13, 1.1%<br />Poland: 12, 1% <br /><br /><em>These are all the countries with greater than 1% of Less Wrongers, but other, more exotic locales included Kenya, Pakistan, and Iceland, with one user each. You can see <a href=\"http://raikoth.net/Stuff/LessWrong/country_table.png\">the full table here</a>.<br /><br />This data also allows us to calculate Less Wrongers per capita:</em><br /><br />Finland: 1/366,666<br />Australia: 1/389,830<br />Canada: 1/472,972<br />USA: 1/483,870<br />Israel: 1/533,333<br />UK: 1/603,883<br />Germany: 1/1,518,518<br />Poland: 1/3,166,666<br />Russia: 1/11,538,462<br /><br /><strong>RACE:</strong><br />White, non-Hispanic 1003, 84.6%<br />East Asian: 50, 4.2%<br />Hispanic 47, 4.0%<br />Indian Subcontinental 28, 2.4%<br />Black 8, 0.7%<br />Middle Eastern 4, 0.3%<br />Other: 33, 2.8%<br />No answer: 12, 1%<br /><br /><strong>WORK STATUS:</strong><br />Student: 476, 40.7%<br />For-profit work: 364, 30.7%<br />Self-employed: 95, 8%<br />Unemployed: 81, 6.8%<br />Academics (teaching): 54, 4.6%<br />Government: 46, 3.9%<br />Non-profit: 44, 3.7%<br />Independently wealthy: 12, 1%<br />No answer: 13, 1.1%<br /><br /><strong>PROFESSION:</strong><br />Computers (practical): 344, 29%<br />Math: 109, 9.2%<br />Engineering: 98, 8.3%<br />Computers (academic): 72, 6.1%<br />Physics: 66, 5.6%<br />Finance/Econ: 65, 5.5%<br />Computers (AI): 39, 3.3%<br />Philosophy: 36, 3%<br />Psychology: 25, 2.1%<br />Business: 23, 1.9%<br />Art: 22, 1.9%<br />Law: 21, 1.8%<br />Neuroscience: 19, 1.6%<br />Medicine: 15, 1.3%<br />Other social science: 24, 2%<br />Other hard science: 20, 1.7%<br />Other: 123, 10.4%<br />No answer: 27, 2.3%<br /><br /><strong>DEGREE:</strong><br />Bachelor's: 438, 37%<br />High school: 333, 28.1%<br />Master's: 192, 16.2%<br />Ph.D: 71, 6%<br />2-year: 43, 3.6%<br />MD/JD/professional: 24, 2%<br />None: 55, 4.6%<br />Other: 15, 1.3%<br />No answer: 14, 1.2%<br /><br /><strong>POLITICS:</strong><br />Liberal: 427, 36%<br />Libertarian: 359, 30.3%<br />Socialist: 326, 27.5%<br />Conservative: 35, 3%<br />Communist: 8, 0.7%<br />No answer: 30, 2.5%<br /><br /><em>You can see the exact definitions given for each of these terms on the survey.</em><br /><br /><strong>RELIGIOUS VIEWS:</strong><br />Atheist, not spiritual: 880, 74.3%<br />Atheist, spiritual: 107, 9.0%<br />Agnostic: 94, 7.9%<br />Committed theist: 37, 3.1%<br />Lukewarm theist: 27, 2.3%<br />Deist/Pantheist/etc: 23, 1.9%<br />No answer: 17, 1.4%<br /><br /><strong>FAMILY RELIGIOUS VIEWS:</strong><br />Lukewarm theist: 392, 33.1%<br />Committed theist: 307, 25.9%<br />Atheist, not spiritual: 161, 13.6<br />Agnostic: 149, 12.6%<br />Atheist, spiritual: 46, 3.9%<br />Deist/Pantheist/Etc: 32, 2.7%<br />Other: 84, 7.1%<br /><br /><strong>RELIGIOUS BACKGROUND:</strong><br />Other Christian: 517, 43.6%<br />Catholic: 295, 24.9%<br />Jewish: 100, 8.4%<br />Hindu: 21, 1.8%<br />Traditional Chinese: 17, 1.4%<br />Mormon: 15, 1.3%<br />Muslim: 12, 1%<br /><br /><em>Raw data <a href=\"http://raikoth.net/Stuff/LessWrong/religion_table.png\">is available here.</a> </em><br /><strong><br />MORAL VIEWS:</strong><br />Consequentialism: 735, 62%<br />Virtue Ethics: 166, 14%<br />Deontology: 50, 4.2%<br />Other: 214, 18.1%<br />No answer: 20, 1.7%<br /><br /><strong>NUMBER OF CHILDREN</strong><br />0: 1044, 88.1%<br />1: 51, 4.3%<br />2: 48, 4.1%<br />3: 19, 1.6%<br />4: 3, 0.3%<br />5: 2, 0.2%<br />6: 1, 0.1%<br />No answer: 17, 1.4%<br /><strong><br />WANT MORE CHILDREN?</strong><br />No: 438, 37%<br />Maybe: 363, 30.7%<br />Yes: 366, 30.9%<br />No answer: 16, 1.4%<br /><br /><strong>LESS WRONG USE:</strong><br />Lurkers (no account): 407, 34.4%<br />Lurkers (with account): 138, 11.7%<br />Posters (comments only): 356, 30.1%<br />Posters (comments + Discussion only): 164, 13.9%<br />Posters (including Main): 102, 8.6%<br /><br /><strong>SEQUENCES:</strong><br />Never knew they existed until this moment: 99, 8.4%<br />Knew they existed; never looked at them: 23, 1.9%<br />Read &lt; 25%: 227, 19.2%<br />Read ~ 25%: 145, 12.3%<br />Read ~ 50%: 164, 13.9%<br />Read ~ 75%: 203, 17.2%<br />Read ~ all: 306, 24.9%<br />No answer: 16, 1.4%<br /><br /><em>Dear 8.4% of people: there is this <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">collection of old blog posts called the Sequences</a>. It is by Eliezer, the same guy who wrote Harry Potter and the Methods of Rationality. It is really good! If you read it, you will understand what we're talking about much better!</em><br /><br /><strong>REFERRALS: </strong><br />Been here since Overcoming Bias: 265, 22.4%<br />Referred by a link on another blog: 23.5%<br />Referred by a friend: 147, 12.4%<br />Referred by HPMOR: 262, 22.1%<br />No answer: 35, 3%<br /><strong><br />BLOG REFERRALS:</strong><br />Common Sense Atheism: 20 people<br />Hacker News: 20 people<br />Reddit: 15 people<br />Unequally Yoked: 7 people<br />TV Tropes: 7 people<br />Marginal Revolution: 6 people<br />gwern.net: 5 people<br />RationalWiki: 4 people<br />Shtetl-Optimized: 4 people<br />XKCD fora: 3 people<br />Accelerating Future: 3 people<br /><br /><em>These are all the sites that referred at least three people in a way that was obvious to disentangle from the raw data. You can see <a href=\"http://raikoth.net/Stuff/LessWrong/referrals.gif\">a more complete list, including the long tail, here</a>.</em><br /><br /><strong>MEETUPS:</strong><br />Never been to one: 834, 70.5%<br />Have been to one: 320, 27%<br />No answer: 29, 2.5%<br /><br /><strong>CATASTROPHE:</strong><br />Pandemic (bioengineered): 272, 23%<br />Environmental collapse: 171, 14.5%<br />Unfriendly AI: 160, 13.5%<br />Nuclear war: 155, 13.1%<br />Economic/Political collapse: 137, 11.6%<br />Pandemic (natural): 99, 8.4%<br />Nanotech: 49, 4.1%<br />Asteroid: 43, 3.6%<br /><br /><em>The wording of this question was \"which disaster do you think is most likely to wipe out greater than 90% of humanity before the year 2100?\"</em><br /><br /><strong>CRYONICS STATUS:</strong><br />No, don't want to: 275, 23.2%<br />No, still thinking: 472, 39.9%<br />No, procrastinating: 178, 15%<br />No, unavailable: 120, 10.1%<br />Yes, signed up: 44, 3.7%<br />Never thought about it: 46, 3.9%<br />No answer: 48, 4.1%<br /><br /><strong>VEGETARIAN:</strong><br />No: 906, 76.6%<br />Yes: 147, 12.4%<br />No answer: 130, 11%<br /><em><br />For comparison, 3.2% of US adults are vegetarian.</em><br /><br /><strong>SPACED REPETITION SYSTEMS</strong><br />Don't use them: 511, 43.2%<br />Do use them: 235, 19.9%<br />Never heard of them: 302, 25.5%<br /><br /><em>Dear 25.5% of people: spaced repetition systems are nifty, mostly free computer programs that allow you to study and memorize facts more efficiently. See for example <a href=\"http://ankisrs.net/\">http://ankisrs.net/</a></em><br /><br /><strong>HPMOR:</strong><br />Never read it: 219, 18.5%<br />Started, haven't finished: 190, 16.1%<br />Read all of it so far: 659, 55.7%<br /><em><br />Dear 18.5% of people: Harry Potter and the Methods of Rationality is a Harry Potter fanfic about rational thinking written by Eliezer Yudkowsky (the guy who started this site). It's really good. You can find it at <a href=\"http://hpmor.com/\">http://www.hpmor.com/</a>.</em><br /><strong><br />ALTERNATIVE POLITICS QUESTION:</strong><br />Progressive: 429, 36.3%<br />Libertarian: 278, 23.5%<br />Reactionary: 30, 2.5%<br />Conservative: 24, 2%<br />Communist: 22, 1.9%<br />Other: 156, 13.2%<br /><br /><strong>ALTERNATIVE ALTERNATIVE POLITICS QUESTION:</strong><br />Left-Libertarian: 102, 8.6%<br />Progressive: 98, 8.3%<br />Libertarian: 91, 7.7%<br />Pragmatist: 85, 7.2%<br />Social Democrat: 80, 6.8%<br />Socialist: 66, 5.6%<br />Anarchist: 50, 4.1%<br />Futarchist: 29, 2.5%<br />Moderate: 18, 1.5%<br />Moldbuggian: 19, 1.6%<br />Objectivist: 11, 0.9%<br /><br /><em>These are the only ones that had more than ten people. Other responses notable for their unusualness were Monarchist (5 people), fascist (3 people, plus one who was up for fascism but only if he could be the leader), conservative (9 people), and a bunch of people telling me politics was stupid and I should feel bad for asking the question. You can see <a href=\"http://raikoth.net/Stuff/LessWrong/politics.gif\">the full table here</a>.</em><br /><br /><strong>CAFFEINE:</strong><br />Never: 162, 13.7%<br />Rarely: 237, 20%<br />At least 1x/week: 207, 17.5<br />Daily: 448, 37.9<br />No answer: 129, 10.9%<br /><br /><strong>SMOKING:</strong><br />Never: 896, 75.7%<br />Used to: 1-5, 8.9%<br />Still do: 51, 4.3%<br />No answer: 131, 11.1%<br /><br /><em>For comparison, about 28.4% of the US adult population smokes</em><br /><br /><strong>NICOTINE (OTHER THAN SMOKING):</strong><br />Never used: 916, 77.4%<br />Rarely use: 82, 6.9%<br />&gt;1x/month: 32, 2.7%<br />Every day: 14, 1.2%<br />No answer: 139, 11.7%<br /><br /><strong>MODAFINIL:</strong><br />Never: 76.5%<br />Rarely: 78, 6.6%<br />&gt;1x/month: 48, 4.1%<br />Every day: 9, 0.8%<br />No answer: 143, 12.1%<br /><br /><strong>TRUE PRISONERS' DILEMMA:</strong><br />Defect: 341, 28.8%<br />Cooperate: 316, 26.7%<br />Not sure: 297, 25.1%<br />No answer: 229, 19.4%<br /><br /><strong>FREE WILL:</strong><br />Not confused: 655, 55.4%<br />Somewhat confused: 296, 25%<br />Confused: 81, 6.8%<br />No answer: 151, 12.8%<br /><br /><strong>TORTURE VS. DUST SPECKS</strong><br />Choose dust specks: 435, 36.8%<br />Choose torture: 261, 22.1%<br />Not sure: 225, 19%<br />Don't understand: 22, 1.9%<br />No answer: 240, 20.3%<br /><br /><strong>SCHRODINGER EQUATION:</strong><br />Can't calculate it: 855, 72.3%<br />Can calculate it: 175, 14.8%<br />No answer: 153, 12.9%<br /><br /><strong>PRIMARY LANGUAGE:</strong><br />English: 797, 67.3%<br />German: 54, 4.5%<br />French: 13, 1.1%<br />Finnish: 11, 0.9%<br />Dutch: 10, 0.9%<br />Russian: 15, 1.3%<br />Portuguese: 10, 0.9%<br /><br /><em>These are all the languages with ten or more speakers, but we also have everything from Marathi to Tibetan. You can <a href=\"http://raikoth.net/Stuff/LessWrong/languages.gif\">see the full table here.</a>.</em><br /><br /><strong>NEWCOMB'S PROBLEM</strong><br />One-box: 726, 61.4%<br />Two-box: 78, 6.6%<br />Not sure: 53, 4.5%<br />Don't understand: 86, 7.3%<br />No answer: 240, 20.3%<br /><br /><strong>ENTREPRENEUR:</strong><br />Don't want to start business: 447, 37.8%<br />Considering starting business: 334, 28.2%<br />Planning to start business: 96, 8.1%<br />Already started business: 112, 9.5%<br />No answer: 194, 16.4%<br /><br /><strong>ANONYMITY:</strong><br />Post using real name: 213, 18%<br />Easy to find real name: 256, 21.6%<br />Hard to find name, but wouldn't bother me if someone did: 310, 26.2%<br />Anonymity is very important: 170, 14.4%<br />No answer: 234, 19.8%<br /><br /><strong>HAVE YOU TAKEN A PREVIOUS LW SURVEY?</strong><br />No: 559, 47.3%<br />Yes: 458, 38.7%<br />No answer: 116, 14%<br /><br /><strong>TROLL TOLL POLICY:</strong><br />Disapprove: 194, 16.4%<br />Approve: 178, 15%<br />Haven't heard of this: 375, 31.7%<br />No opinion: 249, 21%<br />No answer: 187, 15.8%<br /><br /><strong>MYERS-BRIGGS</strong><br />INTJ: 163, 13.8%<br />INTP: 143, 12.1%<br />ENTJ: 35, 3%<br />ENTP: 30, 2.5%<br />INFP: 26, 2.2%<br />INFJ: 25. 2.1%<br />ISTJ: 14, 1.2%<br />No answer: 715, 60%<br /><em><br />This includes all types with greater than 10 people. You can <a href=\"http://raikoth.net/Stuff/LessWrong/myersbriggs.gif\">see the full table here.</a><br /></em></p>\n<h2><strong>Part 3: Numerical Data</strong></h2>\n<p>Except where indicated otherwise, all the numbers below are given in the format:</p>\n<p>mean+standard_deviation (25% level, 50% level/median, 75% level) [n = number of data points]</p>\n<p><strong>INTELLIGENCE:</strong><br /><br />IQ (self-reported): 138.7 + 12.7 (130, 138, 145) [n = 382]<br />SAT (out of 1600): 1485.8 + 105.9 (1439, 1510, 1570) [n = 321]<br />SAT (out of 2400): 2319.5 + 1433.7 (2155, 2240, 2320)<br />ACT: 32.7 + 2.3 (31, 33, 34) [n = 207]<br />IQ (on iqtest.dk): 125.63 + 13.4 (118, 130, 133)&nbsp;&nbsp; [n = 378]<br /><br />I am going to harp on these numbers because in the past some people have been pretty quick to ridicule this survey's intelligence numbers as completely useless and impossible and so on.<br /><br />According to <a href=\"http://www.iqcomparisonsite.com/SATIQ.aspx\">IQ Comparison Site</a>, an SAT score of 1485/1600 corresponds to an IQ of about 144. According to <a href=\"http://www.ivywest.com/acttosat.htm\">Ivy West</a>, an ACT of 33 corresponds to an SAT of 1470 (and thence to IQ of 143).<br /><br />So if we consider self-report, SAT, ACT, and iqtest.dk as four measures of IQ, these come out to 139, 144, 143, and 126, respectively.<br /><br />All of these are pretty close except iqtest.dk. I ran a correlation between all of them and found that self-reported IQ is correlated with SAT scores at the 1% level and iqtest.dk at the 5% level, but SAT scores and IQTest.dk are not correlated with each other.<br /><br />Of all these, I am least likely to trust iqtest.dk. First, it's a random Internet IQ test. Second, it correlates poorly with the other measures. Third, a lot of people have complained in the comments to the survey post that it exhibits some weird behavior. <br /><br />But iqtest.dk gave us the lowest number! And even it said the average was 125 to 130! So I suggest that we now have pretty good, pretty believable evidence that the average IQ for this site really is somewhere in the 130s, and that self-reported IQ isn't as terrible a measure as one might think.</p>\n<p><strong>AGE:<br /></strong>27.8 + 9.2 (22, 26, 31) [n = 1185]</p>\n<p><strong>LESS WRONG USE:<br /></strong>Karma: 1078 + 2939.5 (0, 4.5, 136) [n = 1078]<br />Months on LW: 26.7 + 20.1 (12, 24, 40) [n = 1070] <br />Minutes/day on LW: 19.05 + 24.1 (5, 10, 20) [n = 1105]<br />Wiki views/month: 3.6 + 6.3 (0, 1, 5) [n = 984]<br />Wiki edits/month: 0.1 + 0.8 (0, 0, 0) [n = 984]<br /><br /><strong>PROBABILITIES:</strong><br />Many Worlds: 51.6 + 31.2 (25, 55, 80) [n = 1005]<br />Aliens (universe): 74.2 + 32.6 (50, 90, 99) [n = 1090]<br />Aliens (galaxy): 42.1 + 38 (5, 33, 80) [n = 1081]<br />Supernatural: 5.9 + 18.6 (0, 0, 1) [n = 1095]<br />God: 6 + 18.7 (0, 0, 1) [n = 1098]<br />Religion: 3.8 + 15.5 (0, 0, 0.8) [n = 1113]<br />Cryonics: 18.5 + 24.8 (2, 8, 25) [n = 1100]<br />Antiagathics: 25.1 + 28.6 (1, 10, 35) [n = 1094]<br />Simulation: 25.1 + 29.7 (1, 10, 50) [n = 1039]<br />Global warming: 79.1 + 25 (75, 90, 97) [n = 1112]<br />No catastrophic risk: 71.1 + 25.5 (55, 80, 90) [n = 1095]<br />Space: 20.1 + 27.5 (1, 5, 30) [n = 953]<br /><br /><strong>CALIBRATION:</strong><br />Year of Bayes' birth: 1767.5 + 109.1 (1710, 1780, 1830) [n = 1105]<br />Confidence: 33.6 + 23.6 (20, 30, 50) [n= 1082]<br /><br /><strong>MONEY:</strong><br />Income/year: 50,913 + 60644.6 (12000, 35000, 74750) [n = 644]<br />Charity/year: 444.1 + 1152.4 (0, 30, 250) [n = 950]<br />SIAI/CFAR charity/year: 309.3 + 3921 (0, 0, 0) [n = 961]<br />Aging charity/year: 13 + 184.9 (0, 0, 0) [n = 953]<br /><br /><strong>TIME USE:</strong><br />Hours online/week: 42.4 + 30 (21, 40, 59) [n = 944]<br />Hours reading/week: 30.8 + 19.6 (18, 28, 40) [n = 957]<br />Hours writing/week: 7.9 + 9.8 (2, 5, 10) [n = 951]<br /><br /><strong>POLITICAL COMPASS:</strong><br />Left/Right: -2.4 + 4 (-5.5, -3.4, -0.3) [n = 476]<br />Libertarian/Authoritarian: -5 + 2 (-6.2, -5.2, -4)</p>\n<p><strong>BIG 5 PERSONALITY TEST:</strong><br />Big 5 (O): 60.6 + 25.7 (41, 65, 84) [n = 453]<br />Big 5 (C): 35.2 + 27.5 (10, 30, 58) [n = 453]<br />Big 5 (E): 30.3 + 26.7 (7, 22, 48) [n = 454]<br />Big 5 (A): 41 + 28.3 (17, 38, 63) [n = 453]<br />Big 5 (N): 36.6 + 29 (11, 27, 60) [n = 449]<br /><br /><em>These scores are in percentiles, so LWers are more Open, but less Conscientious, Agreeable, Extraverted, and Neurotic than average test-takers. Note that people who take online psychometric tests are probably a pretty skewed category already so this tells us nothing. Also, several people got confusing results on this test or found it different than other tests that they took, and I am pretty unsatisfied with it and don't trust the results.</em></p>\n<p><strong>AUTISM QUOTIENT</strong><br />AQ: 24.1 + 12.2 (17, 24, 30) [n = 367]<br /><br /><em>This test says the average control subject got 16.4 and 80% of those diagnosed with autism spectrum disorders get 32+ (which of course doesn't tell us what percent of people above 32 have autism...). If we trust them, most LWers are more autistic than average.</em><br /><br /><strong>CALIBRATION:</strong><br /><br />Reverend Thomas Bayes was born in 1701. Survey takers were asked to guess this date within 20 years, so anyone who guessed between 1681 and 1721 was recorded as getting a correct answer. The percent of people who answered correctly is recorded below, stratified by the confidence they gave of having guessed correctly and with the number of people at that confidence level.<br /><br />0-5: 10% [n = 30]<br />5-15: 14.8% [n = 183]<br />15-25: 10.3% [n = 242]<br />25-35: 10.7% [n = 225]<br />35-45: 11.2% [n = 98]<br />45-55: 17% [n = 118]<br />55-65: 20.1% [n = 62]<br />65-75: 26.4% [n = 34]<br />75-85: 36.4% [n = 33]<br />85-95: 60.2% [n = 20]<br />95-100: 85.7% [n = 23]</p>\n<p><img src=\"http://www.raikoth.net/Stuff/LessWrong/calibration_table.png\" alt=\"\" /></p>\n<p>Here's a classic calibration chart. The blue line is perfect calibration. The orange line is you guys. And the yellow line is average calibration from an experiment I did with untrained subjects a few years ago (which of course was based on different questions and so not directly comparable). <br /><br />The results are atrocious; when Less Wrongers are 50% certain, they only have about a 17% chance of being correct. On this problem, at least, they are as bad or worse at avoiding overconfidence bias as the general population.</p>\n<p>My hope was that this was the result of a lot of lurkers who don't know what they're doing stumbling upon the survey and making everyone else look bad, so I ran a second analysis. This one used only the numbers of people who had been in the community at least 2 years and accumulated at least 100 karma; this limited my sample size to about 210 people.</p>\n<p>I'm not going to post exact results, because I made some minor mistakes which means they're off by a percentage point or two, but the general trend was that they looked exactly like the results above: atrocious. If there is some core of elites who are less biased than the general population, they are well past the 100 karma point and probably too rare to feel confident even detecting at this kind of a sample size.</p>\n<p>I really have no idea what went so wrong.&nbsp; <a href=\"http://www.raikoth.net/Stuff/graph_lw.png \">Last year's results</a> were pretty good - encouraging, even. I wonder if it's just an especially bad question. Bayesian statistics is pretty new; one would expect Bayes to have been born in rather more modern times. It's also possible that I've handled the statistics wrong on this one; I wouldn't mind someone double-checking my work.<br /><br />Or we could just be <em>really horrible</em>. If we haven't even learned to avoid the one bias that we can measure super well and which is most susceptible to training, what are we even <em>doing</em> here? Some remedial time at <a href=\"http://predictionbook.com/\">PredictionBook</a> might be in order.<br /><br /><strong>HYPOTHESIS TESTING:</strong></p>\n<p>I tested a very few of the possible hypothesis that were proposed in the survey design threads.<br /><br />Are people who understand quantum mechanics are more likely to believe in Many Worlds? We perform a t-test, checking whether one's probability of the MWI being true depends on whether or not one can solve the Schrodinger Equation. People who could solve the equation had on average a 54.3% probability of MWI, compared to 51.3% in those who could not. The p-value is 0.26; there is a 26% probability this occurs by chance. Therefore, we fail to establish that people's probability of MWI varies with understanding of quantum mechanics.<br /><br />Are there any interesting biological correlates of IQ? We run a correlation between self-reported IQ, height, maternal age, and paternal age. The correlations are in the expected direction but not significant.<br /><br />Are there differences in the ways men and women interact with the community? I had sort of vaguely gotten the impression that women were proportionally younger, newer to the community, and more likely to be referred via HPMOR. The average age of women on LW is 27.6 compared to 27.7 for men; obviously this difference is not significant. 14% of the people referred via HPMOR were women compared to about 10% of the community at large, but this difference is pretty minor. Women were on average newer to the community - 21 months vs. 39 for men - but to my surprise a t-test was unable to declare this significant. Maybe I'm doing it wrong?<br /><br />Does the amount of time spent in the community affect one's beliefs in the same way as in previous surveys? I ran some correlations and found that it does. People who have been around longer continue to be more likely to believe in MWI, less likely to believe in aliens in the universe (though not in our galaxy), and less likely to believe in God (though not religion). There was no effect on cryonics this time.<br /><br />In addition, the classic correlations between different beliefs continue to hold true. There is an obvious cluster of God, religion, and the supernatural. There's also a scifi cluster of cryonics, antiagathics, MWI, aliens, and the Simulation Hypothesis, and catastrophic risk (this also seems to include global warming, for some reason).<br /><br />Are there any differences between men and women in regards to their belief in these clusters? We run a t-test between men and women. Men and women have about the same probability of God (men: 5.9, women: 6.2, p = .86) and similar results for the rest of the religion cluster, but men have much higher beliefs in for example antiagathics (men 24.3, women: 10.5, p &lt; .001) and the rest of the scifi cluster.<br /><br /><strong>DESCRIPTIONS OF LESS WRONG</strong><br /><br />Survey users were asked to submit a description of Less Wrong in 140 characters or less. I'm not going to post all of them, but here is a representative sample:<br /><br />- \"Probably the most sensible philosophical resource avaialble.\"<br />- \"Contains the great Sequences, some of Luke's posts, and very little else.\"<br />- \"The currently most interesting site I found ont the net.\"<br />- \"EY cult\"<br />- \"How to think correctly, precisely, and efficiently.\"<br />- \"HN for even bigger nerds.\"<br />- \"Social skills philosophy and AI theorists on the same site, not noticing each other.\"<br />- \"Cool place. Any others like it?\"<br />- \"How to avoid predictable pitfalls in human psychology, and understand hard things well: The Website.\"<br />- \"A bunch of people trying to make sense of the wold through their own lens, which happens to be one of calculation and rigor\"<br />- \"Nice.\"<br />- \"A font of brilliant and unconventional wisdom.\"<br />- \"One of the few sane places on Earth.\"<br />- \"Robot god apocalypse cult spinoff from Harry Potter.\"<br />- \"A place to converse with intelligent, reasonably open-minded people.\"<br />- \"Callahan's Crosstime Saloon\"<br />- \"Amazing rational transhumanist calming addicting Super Reddit\"<br />- \"Still wrong\"<br />- \"A forum for helping to train people to be more rational\"<br />- \"A very bright community interested in amateur ethical philosophy, mathematics, and decision theory.\"<br />- \"Dying. Social games and bullshit now &gt;50% of LW content.\"<br />- \"The good kind of strange, addictive, so much to read!\"<br />- \"Part genuinely useful, part mental masturbation.\"<br />- \"Mostly very bright and starry-eyed adults who never quite grew out of their science-fiction addiction as adolescents.\"<br />- \"Less Wrong: Saving the world with MIND POWERS!\"<br />- \"Perfectly patternmatches the 'young-people-with-all-the-answers' cliche\"<br />- \"Rationalist community dedicated to self-improvement.\"<br />- \"Sperglord hipsters pretending that being a sperglord hipster is cool.\" <em>(this person's Autism Quotient was two points higher than LW average, by the way)</em><br />- \"An interesting perspective and valuable database of mental techniques.\"<br />- \"A website with kernels of information hidden among aspy nonsense.\"<br />- \"Exclusive, elitist, interesting, potentially useful, personal depression trigger.\"<br />- \"A group blog about rationality and related topics. Tends to be overzealous about cryogenics and other pet ideas of Eliezer Yudkowsky.\"<br />- \"Things to read to make you think better.\"<br />- \"Excellent rationality. New-age self-help. Worrying groupthink.\"<br />- \"Not a cult at all.\"<br />- \"A cult.\"<br />- \"The new thing for people who would have been Randian Objectivists 30 years ago.\"<br />- \"Fascinating, well-started, risking bloat and failure modes, best as archive.\"<br />- \"A fun, insightful discussion of probability theory and cognition.\"<br />- \"More interesting than useful.\"<br />- \"The most productive and accessible mind-fuckery on the Internet.\"<br />- \"A blog for rationality, cognitive bias, futurism, and the Singularity.\"<br />- \"Robo-Protestants attempting natural theology.\"<br />- \"Orderly quagmire of tantalizing ideas drawn from disagreeable priors.\"<br />- \"Analyze everything. And I do mean everything. Including analysis. Especially analysis. And analysis of analysis.\"<br />- \"Very interesting and sometimes useful.\"<br />- \"Where people discuss and try to implement ways that humans can make their values, actions, and beliefs more internally consistent.\"<br />- \"Eliezer Yudkowsky personality cult.\"<br />- \"It's like the Mormons would be if everyone were an atheist and good at math and didn't abstain from substances.\"<br />- \"Seems wacky at first, but gradually begins to seem normal.\"<br />- \"A varied group of people interested in philosophy with high Openness and a methodical yet amateur approach.\"<br />- \"Less Wrong is where human algorithms go to debug themselves.\"<br />- \"They're kind of like a cult, but that doesn't make them wrong.\"<br />- \"A community blog devoted to nerds who think they're smarter than everyone else.\"<br />- \"90% sane! A new record!\"<br />- \"The Sequences are great. LW now slowly degenerating to just another science forum.\"<br />- \"The meetup groups are where it's at, it seems to me. I reserve judgment till I attend one.\"<br />- \"All I really know about it is this long survey I took.\"<br />- \"The royal road of rationality.\"<br />- \"Technically correct: The best kind of correct!\"<br />- \"Full of angry privilege.\"<br />- \"A sinister instrument of billionaire Peter Thiel.\"<br />- \"Dangerous apocalypse cult bent on the systematic erasure of traditional values and culture by any means necessary.\"<br />- \"Often interesting, but I never feel at home.\"<br />- \"One of the few places I truly feel at home, knowing that there are more people like me.\"<br />- \"Currently the best internet source of information-dense material regarding cog sci, debiasing, and existential risk.\"<br />- \"Prolific and erudite writing on practical techniques to enhance the effectiveness of our reason.\"<br />- \"An embarrassing Internet community formed around some genuinely great blog writings.\"<br />- \"I bookmarked it a while ago and completely forgot what it is about. I am taking the survey to while away my insomnia.\"<br />- \"A somewhat intimidating but really interesting website that helps refine rational thinking.\"<br />- \"A great collection of ways to avoid systematic bias and come to true and useful conclusions.\"<br />- \"Obnoxious self-serving, foolish trolling dehumanizing pseudointellectualism, aesthetically bankrupt.\"<br />- \"The cutting edge of human rationality.\"<br />- \"A purveyor of exceedingly long surveys.\"<br /><br /><strong>PUBLIC RELEASE</strong><br /><br />That last commenter was right. This survey had vastly more data than any previous incarnation; although there are many more analyses I would like to run I am pretty exhausted and I know people are anxious for the results. I'm going to let CFAR analyze and report on their questions, but the rest should be a community effort. So I'm releasing the survey to everyone in the hopes of getting more information out of it. If you find something interesting you can either post it in the comments or start a new thread somewhere.<br /><br />The data I'm providing is the raw data EXCEPT:<br /><br />- I deleted a few categories that I removed halfway through the survey for various reasons<br />- I deleted 9 entries that were duplicates of other entries, ie someone pressed 'submit' twice.<br />- I deleted the timestamp, which would have made people extra-identifiable, and sorted people by their CFAR random number to remove time order information.<br />- I removed one person whose information all came out as weird symbols.<br />- I numeralized some of the non-numeric data, especially on the number of months in community question. This is not the version I cleaned up fully, so you will get to experience some of the same pleasure I did working with the rest.<br />- I deleted 117 people who either didn't answer the privacy question or who asked me to keep them anonymous, leaving 1067 people.<br /><br />Here it is: <a href=\"http://raikoth.net/Stuff/LessWrong/for_public.csv\"><strong>Data in .csv format</strong></a> , <a href=\"http://raikoth.net/Stuff/LessWrong/for_public.xls\"><strong>Data in Excel format</strong></a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"kJrjorSx3hXa7q7CJ": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "x9FNKTEt68Rz6wQ6P", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 84, "baseScore": 116, "extendedScore": null, "score": 0.000287, "legacy": true, "legacyId": "20345", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 116, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Thank you to everyone who took the <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dG1pTzlrTnJ4eks3aE13Ni1lbV8yUkE6MQ#gid=0\">2012 Less Wrong Survey</a> (the survey is now closed. Do not try to take it.) Below the cut, this post contains the basic survey results, a few more complicated analyses, and the data available for download so you can explore it further on your own. You may want to compare these to the <a href=\"/lw/8p4/2011_survey_results/\">results of the 2011 Less Wrong Survey</a>.</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"Part_1__Population\"><strong>Part 1: Population</strong><br></h2>\n<p>How many of us are there?<br><br>The short answer is that I don't know.<br><br>The 2011 survey ran 33 days and collected 1090 responses. This year's survey ran 23 days and collected 1195 responses. The average number of new responses during the last week was about five per day, so even if I had kept this survey open as long as the last one I probably wouldn't have gotten more than about 1250 responses. That means at most a 15% year on year growth rate, which is pretty abysmal compared to the 650% growth rate in two years we saw last time.<br><br>About half of these responses were from lurkers; over half of the non-lurker remainder had commented but never posted to Main or Discussion. That means there were only about 600 non-lurkers.<br><br>But I am skeptical of these numbers. I hang out with some people who are very closely associated with the greater Less Wrong community, and a lot of them didn't know about the survey until I mentioned it to them in person. I know some people who could plausibly be described as focusing their lives around the community who just never took the survey for one reason or another. One lesson of this survey may be that the community is no longer limited to people who check Less Wrong very often, if at all. One friend didn't see the survey because she hangs out on the #lesswrong channel more than the main site. Another mostly just goes to meetups. So I think this represents only a small sample of people who could justly be considered Less Wrongers.<br><br>The question of \"how quickly is LW growing\" is also complicated by the high turnover. Over half the people who took this survey said they hadn't participated in the survey last year. I tried to break this down by combining a few sources of information, and I think our 1200 respondents include 500 people who took last year's survey, 400 people who were around last year but didn't take the survey for some reason, and 300 new people.<br><br>As expected, there's lower turnover among regulars than among lurkers. Of people who have posted in Main, about 75% took the survey last year; of people who only lurked, about 75% hadn't. <br><br>This view of a very high-turnover community and lots of people not taking the survey is consistent with Vladimir Nesov's data showing http://lesswrong.com/lw/e4j/number_of_members_on_lesswrong/77xz 1390 people who have written at least ten comments. But the survey includes only about 600 people who have at least commented; 800ish of Vladimir's accounts are either gone or didn't take the census.</p>\n<h2 id=\"Part_2__Categorical_Data\"><strong>Part 2: Categorical Data</strong></h2>\n<p><strong>SEX:</strong><br>Man: 1057, 89.2%<br>Woman: 120, 10.1%<br>Other: 2, 0.2%)<br>No answer: 6, 0.5%<br><br><strong>GENDER:</strong><br>M (cis): 1021, 86.2%<br>F (cis): 105, 8.9%<br>M (trans f-&gt;m): 3, 0.3%<br>F (trans m-&gt;f): 16, 1.3%<br>Other: 29, 2.4%<br>No answer: 11, 0.9%<br><br><strong>ORIENTATION:</strong><br>Heterosexual: 964, 80.7%<br>Bisexual: 135, 11.4%<br>Homosexual: 28, 2.4%<br>Asexual: 24, 2%<br>Other: 28, 2.4%<br>No answer: 14, 1.2%<br><strong><br>RELATIONSHIP STYLE:</strong><br>Prefer monogamous: 639, 53.9%<br>Prefer polyamorous: 155, 13.1%<br>Uncertain/no preference: 358, 30.2%<br>Other: 21, 1.8%<br>No answer: 12, 1%<br><br><strong>NUMBER OF CURRENT PARTNERS:</strong><br>0: 591, 49.8%<br>1: 519, 43.8%<br>2: 34, 2.9%<br>3: 12, 1%<br>4: 5, 0.4%<br>6: 1, 0.1%<br>7, 1, 0.1% <em>(and this person added \"really, not trolling\")</em><br>Confusing or no answer: 20, 1.8%<br><br><strong>RELATIONSHIP STATUS:</strong><br>Single: 628, 53%<br>Relationship: 323, 27.3%<br>Married: 220, 18.6%<br>No answer: 14, 1.2%<br><br><strong>RELATIONSHIP GOALS:</strong><br>Not looking for more partners: 707, 59.7%<br>Looking for more partners: 458, 38.6%<br>No answer: 20, 1.7%<br><br><strong>COUNTRY:</strong><br>USA: 651, 54.9%<br>UK: 103, 8.7%<br>Canada: 74, 6.2%<br>Australia: 59, 5%<br>Germany: 54, 4.6%<br>Israel: 15, 1.3%<br>Finland: 15, 1.3%<br>Russia: 13, 1.1%<br>Poland: 12, 1% <br><br><em>These are all the countries with greater than 1% of Less Wrongers, but other, more exotic locales included Kenya, Pakistan, and Iceland, with one user each. You can see <a href=\"http://raikoth.net/Stuff/LessWrong/country_table.png\">the full table here</a>.<br><br>This data also allows us to calculate Less Wrongers per capita:</em><br><br>Finland: 1/366,666<br>Australia: 1/389,830<br>Canada: 1/472,972<br>USA: 1/483,870<br>Israel: 1/533,333<br>UK: 1/603,883<br>Germany: 1/1,518,518<br>Poland: 1/3,166,666<br>Russia: 1/11,538,462<br><br><strong>RACE:</strong><br>White, non-Hispanic 1003, 84.6%<br>East Asian: 50, 4.2%<br>Hispanic 47, 4.0%<br>Indian Subcontinental 28, 2.4%<br>Black 8, 0.7%<br>Middle Eastern 4, 0.3%<br>Other: 33, 2.8%<br>No answer: 12, 1%<br><br><strong>WORK STATUS:</strong><br>Student: 476, 40.7%<br>For-profit work: 364, 30.7%<br>Self-employed: 95, 8%<br>Unemployed: 81, 6.8%<br>Academics (teaching): 54, 4.6%<br>Government: 46, 3.9%<br>Non-profit: 44, 3.7%<br>Independently wealthy: 12, 1%<br>No answer: 13, 1.1%<br><br><strong>PROFESSION:</strong><br>Computers (practical): 344, 29%<br>Math: 109, 9.2%<br>Engineering: 98, 8.3%<br>Computers (academic): 72, 6.1%<br>Physics: 66, 5.6%<br>Finance/Econ: 65, 5.5%<br>Computers (AI): 39, 3.3%<br>Philosophy: 36, 3%<br>Psychology: 25, 2.1%<br>Business: 23, 1.9%<br>Art: 22, 1.9%<br>Law: 21, 1.8%<br>Neuroscience: 19, 1.6%<br>Medicine: 15, 1.3%<br>Other social science: 24, 2%<br>Other hard science: 20, 1.7%<br>Other: 123, 10.4%<br>No answer: 27, 2.3%<br><br><strong>DEGREE:</strong><br>Bachelor's: 438, 37%<br>High school: 333, 28.1%<br>Master's: 192, 16.2%<br>Ph.D: 71, 6%<br>2-year: 43, 3.6%<br>MD/JD/professional: 24, 2%<br>None: 55, 4.6%<br>Other: 15, 1.3%<br>No answer: 14, 1.2%<br><br><strong>POLITICS:</strong><br>Liberal: 427, 36%<br>Libertarian: 359, 30.3%<br>Socialist: 326, 27.5%<br>Conservative: 35, 3%<br>Communist: 8, 0.7%<br>No answer: 30, 2.5%<br><br><em>You can see the exact definitions given for each of these terms on the survey.</em><br><br><strong>RELIGIOUS VIEWS:</strong><br>Atheist, not spiritual: 880, 74.3%<br>Atheist, spiritual: 107, 9.0%<br>Agnostic: 94, 7.9%<br>Committed theist: 37, 3.1%<br>Lukewarm theist: 27, 2.3%<br>Deist/Pantheist/etc: 23, 1.9%<br>No answer: 17, 1.4%<br><br><strong>FAMILY RELIGIOUS VIEWS:</strong><br>Lukewarm theist: 392, 33.1%<br>Committed theist: 307, 25.9%<br>Atheist, not spiritual: 161, 13.6<br>Agnostic: 149, 12.6%<br>Atheist, spiritual: 46, 3.9%<br>Deist/Pantheist/Etc: 32, 2.7%<br>Other: 84, 7.1%<br><br><strong>RELIGIOUS BACKGROUND:</strong><br>Other Christian: 517, 43.6%<br>Catholic: 295, 24.9%<br>Jewish: 100, 8.4%<br>Hindu: 21, 1.8%<br>Traditional Chinese: 17, 1.4%<br>Mormon: 15, 1.3%<br>Muslim: 12, 1%<br><br><em>Raw data <a href=\"http://raikoth.net/Stuff/LessWrong/religion_table.png\">is available here.</a> </em><br><strong><br>MORAL VIEWS:</strong><br>Consequentialism: 735, 62%<br>Virtue Ethics: 166, 14%<br>Deontology: 50, 4.2%<br>Other: 214, 18.1%<br>No answer: 20, 1.7%<br><br><strong>NUMBER OF CHILDREN</strong><br>0: 1044, 88.1%<br>1: 51, 4.3%<br>2: 48, 4.1%<br>3: 19, 1.6%<br>4: 3, 0.3%<br>5: 2, 0.2%<br>6: 1, 0.1%<br>No answer: 17, 1.4%<br><strong><br>WANT MORE CHILDREN?</strong><br>No: 438, 37%<br>Maybe: 363, 30.7%<br>Yes: 366, 30.9%<br>No answer: 16, 1.4%<br><br><strong>LESS WRONG USE:</strong><br>Lurkers (no account): 407, 34.4%<br>Lurkers (with account): 138, 11.7%<br>Posters (comments only): 356, 30.1%<br>Posters (comments + Discussion only): 164, 13.9%<br>Posters (including Main): 102, 8.6%<br><br><strong>SEQUENCES:</strong><br>Never knew they existed until this moment: 99, 8.4%<br>Knew they existed; never looked at them: 23, 1.9%<br>Read &lt; 25%: 227, 19.2%<br>Read ~ 25%: 145, 12.3%<br>Read ~ 50%: 164, 13.9%<br>Read ~ 75%: 203, 17.2%<br>Read ~ all: 306, 24.9%<br>No answer: 16, 1.4%<br><br><em>Dear 8.4% of people: there is this <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">collection of old blog posts called the Sequences</a>. It is by Eliezer, the same guy who wrote Harry Potter and the Methods of Rationality. It is really good! If you read it, you will understand what we're talking about much better!</em><br><br><strong>REFERRALS: </strong><br>Been here since Overcoming Bias: 265, 22.4%<br>Referred by a link on another blog: 23.5%<br>Referred by a friend: 147, 12.4%<br>Referred by HPMOR: 262, 22.1%<br>No answer: 35, 3%<br><strong><br>BLOG REFERRALS:</strong><br>Common Sense Atheism: 20 people<br>Hacker News: 20 people<br>Reddit: 15 people<br>Unequally Yoked: 7 people<br>TV Tropes: 7 people<br>Marginal Revolution: 6 people<br>gwern.net: 5 people<br>RationalWiki: 4 people<br>Shtetl-Optimized: 4 people<br>XKCD fora: 3 people<br>Accelerating Future: 3 people<br><br><em>These are all the sites that referred at least three people in a way that was obvious to disentangle from the raw data. You can see <a href=\"http://raikoth.net/Stuff/LessWrong/referrals.gif\">a more complete list, including the long tail, here</a>.</em><br><br><strong>MEETUPS:</strong><br>Never been to one: 834, 70.5%<br>Have been to one: 320, 27%<br>No answer: 29, 2.5%<br><br><strong>CATASTROPHE:</strong><br>Pandemic (bioengineered): 272, 23%<br>Environmental collapse: 171, 14.5%<br>Unfriendly AI: 160, 13.5%<br>Nuclear war: 155, 13.1%<br>Economic/Political collapse: 137, 11.6%<br>Pandemic (natural): 99, 8.4%<br>Nanotech: 49, 4.1%<br>Asteroid: 43, 3.6%<br><br><em>The wording of this question was \"which disaster do you think is most likely to wipe out greater than 90% of humanity before the year 2100?\"</em><br><br><strong>CRYONICS STATUS:</strong><br>No, don't want to: 275, 23.2%<br>No, still thinking: 472, 39.9%<br>No, procrastinating: 178, 15%<br>No, unavailable: 120, 10.1%<br>Yes, signed up: 44, 3.7%<br>Never thought about it: 46, 3.9%<br>No answer: 48, 4.1%<br><br><strong>VEGETARIAN:</strong><br>No: 906, 76.6%<br>Yes: 147, 12.4%<br>No answer: 130, 11%<br><em><br>For comparison, 3.2% of US adults are vegetarian.</em><br><br><strong>SPACED REPETITION SYSTEMS</strong><br>Don't use them: 511, 43.2%<br>Do use them: 235, 19.9%<br>Never heard of them: 302, 25.5%<br><br><em>Dear 25.5% of people: spaced repetition systems are nifty, mostly free computer programs that allow you to study and memorize facts more efficiently. See for example <a href=\"http://ankisrs.net/\">http://ankisrs.net/</a></em><br><br><strong>HPMOR:</strong><br>Never read it: 219, 18.5%<br>Started, haven't finished: 190, 16.1%<br>Read all of it so far: 659, 55.7%<br><em><br>Dear 18.5% of people: Harry Potter and the Methods of Rationality is a Harry Potter fanfic about rational thinking written by Eliezer Yudkowsky (the guy who started this site). It's really good. You can find it at <a href=\"http://hpmor.com/\">http://www.hpmor.com/</a>.</em><br><strong><br>ALTERNATIVE POLITICS QUESTION:</strong><br>Progressive: 429, 36.3%<br>Libertarian: 278, 23.5%<br>Reactionary: 30, 2.5%<br>Conservative: 24, 2%<br>Communist: 22, 1.9%<br>Other: 156, 13.2%<br><br><strong>ALTERNATIVE ALTERNATIVE POLITICS QUESTION:</strong><br>Left-Libertarian: 102, 8.6%<br>Progressive: 98, 8.3%<br>Libertarian: 91, 7.7%<br>Pragmatist: 85, 7.2%<br>Social Democrat: 80, 6.8%<br>Socialist: 66, 5.6%<br>Anarchist: 50, 4.1%<br>Futarchist: 29, 2.5%<br>Moderate: 18, 1.5%<br>Moldbuggian: 19, 1.6%<br>Objectivist: 11, 0.9%<br><br><em>These are the only ones that had more than ten people. Other responses notable for their unusualness were Monarchist (5 people), fascist (3 people, plus one who was up for fascism but only if he could be the leader), conservative (9 people), and a bunch of people telling me politics was stupid and I should feel bad for asking the question. You can see <a href=\"http://raikoth.net/Stuff/LessWrong/politics.gif\">the full table here</a>.</em><br><br><strong>CAFFEINE:</strong><br>Never: 162, 13.7%<br>Rarely: 237, 20%<br>At least 1x/week: 207, 17.5<br>Daily: 448, 37.9<br>No answer: 129, 10.9%<br><br><strong>SMOKING:</strong><br>Never: 896, 75.7%<br>Used to: 1-5, 8.9%<br>Still do: 51, 4.3%<br>No answer: 131, 11.1%<br><br><em>For comparison, about 28.4% of the US adult population smokes</em><br><br><strong>NICOTINE (OTHER THAN SMOKING):</strong><br>Never used: 916, 77.4%<br>Rarely use: 82, 6.9%<br>&gt;1x/month: 32, 2.7%<br>Every day: 14, 1.2%<br>No answer: 139, 11.7%<br><br><strong>MODAFINIL:</strong><br>Never: 76.5%<br>Rarely: 78, 6.6%<br>&gt;1x/month: 48, 4.1%<br>Every day: 9, 0.8%<br>No answer: 143, 12.1%<br><br><strong>TRUE PRISONERS' DILEMMA:</strong><br>Defect: 341, 28.8%<br>Cooperate: 316, 26.7%<br>Not sure: 297, 25.1%<br>No answer: 229, 19.4%<br><br><strong>FREE WILL:</strong><br>Not confused: 655, 55.4%<br>Somewhat confused: 296, 25%<br>Confused: 81, 6.8%<br>No answer: 151, 12.8%<br><br><strong>TORTURE VS. DUST SPECKS</strong><br>Choose dust specks: 435, 36.8%<br>Choose torture: 261, 22.1%<br>Not sure: 225, 19%<br>Don't understand: 22, 1.9%<br>No answer: 240, 20.3%<br><br><strong>SCHRODINGER EQUATION:</strong><br>Can't calculate it: 855, 72.3%<br>Can calculate it: 175, 14.8%<br>No answer: 153, 12.9%<br><br><strong>PRIMARY LANGUAGE:</strong><br>English: 797, 67.3%<br>German: 54, 4.5%<br>French: 13, 1.1%<br>Finnish: 11, 0.9%<br>Dutch: 10, 0.9%<br>Russian: 15, 1.3%<br>Portuguese: 10, 0.9%<br><br><em>These are all the languages with ten or more speakers, but we also have everything from Marathi to Tibetan. You can <a href=\"http://raikoth.net/Stuff/LessWrong/languages.gif\">see the full table here.</a>.</em><br><br><strong>NEWCOMB'S PROBLEM</strong><br>One-box: 726, 61.4%<br>Two-box: 78, 6.6%<br>Not sure: 53, 4.5%<br>Don't understand: 86, 7.3%<br>No answer: 240, 20.3%<br><br><strong>ENTREPRENEUR:</strong><br>Don't want to start business: 447, 37.8%<br>Considering starting business: 334, 28.2%<br>Planning to start business: 96, 8.1%<br>Already started business: 112, 9.5%<br>No answer: 194, 16.4%<br><br><strong>ANONYMITY:</strong><br>Post using real name: 213, 18%<br>Easy to find real name: 256, 21.6%<br>Hard to find name, but wouldn't bother me if someone did: 310, 26.2%<br>Anonymity is very important: 170, 14.4%<br>No answer: 234, 19.8%<br><br><strong>HAVE YOU TAKEN A PREVIOUS LW SURVEY?</strong><br>No: 559, 47.3%<br>Yes: 458, 38.7%<br>No answer: 116, 14%<br><br><strong>TROLL TOLL POLICY:</strong><br>Disapprove: 194, 16.4%<br>Approve: 178, 15%<br>Haven't heard of this: 375, 31.7%<br>No opinion: 249, 21%<br>No answer: 187, 15.8%<br><br><strong>MYERS-BRIGGS</strong><br>INTJ: 163, 13.8%<br>INTP: 143, 12.1%<br>ENTJ: 35, 3%<br>ENTP: 30, 2.5%<br>INFP: 26, 2.2%<br>INFJ: 25. 2.1%<br>ISTJ: 14, 1.2%<br>No answer: 715, 60%<br><em><br>This includes all types with greater than 10 people. You can <a href=\"http://raikoth.net/Stuff/LessWrong/myersbriggs.gif\">see the full table here.</a><br></em></p>\n<h2 id=\"Part_3__Numerical_Data\"><strong>Part 3: Numerical Data</strong></h2>\n<p>Except where indicated otherwise, all the numbers below are given in the format:</p>\n<p>mean+standard_deviation (25% level, 50% level/median, 75% level) [n = number of data points]</p>\n<p><strong>INTELLIGENCE:</strong><br><br>IQ (self-reported): 138.7 + 12.7 (130, 138, 145) [n = 382]<br>SAT (out of 1600): 1485.8 + 105.9 (1439, 1510, 1570) [n = 321]<br>SAT (out of 2400): 2319.5 + 1433.7 (2155, 2240, 2320)<br>ACT: 32.7 + 2.3 (31, 33, 34) [n = 207]<br>IQ (on iqtest.dk): 125.63 + 13.4 (118, 130, 133)&nbsp;&nbsp; [n = 378]<br><br>I am going to harp on these numbers because in the past some people have been pretty quick to ridicule this survey's intelligence numbers as completely useless and impossible and so on.<br><br>According to <a href=\"http://www.iqcomparisonsite.com/SATIQ.aspx\">IQ Comparison Site</a>, an SAT score of 1485/1600 corresponds to an IQ of about 144. According to <a href=\"http://www.ivywest.com/acttosat.htm\">Ivy West</a>, an ACT of 33 corresponds to an SAT of 1470 (and thence to IQ of 143).<br><br>So if we consider self-report, SAT, ACT, and iqtest.dk as four measures of IQ, these come out to 139, 144, 143, and 126, respectively.<br><br>All of these are pretty close except iqtest.dk. I ran a correlation between all of them and found that self-reported IQ is correlated with SAT scores at the 1% level and iqtest.dk at the 5% level, but SAT scores and IQTest.dk are not correlated with each other.<br><br>Of all these, I am least likely to trust iqtest.dk. First, it's a random Internet IQ test. Second, it correlates poorly with the other measures. Third, a lot of people have complained in the comments to the survey post that it exhibits some weird behavior. <br><br>But iqtest.dk gave us the lowest number! And even it said the average was 125 to 130! So I suggest that we now have pretty good, pretty believable evidence that the average IQ for this site really is somewhere in the 130s, and that self-reported IQ isn't as terrible a measure as one might think.</p>\n<p><strong>AGE:<br></strong>27.8 + 9.2 (22, 26, 31) [n = 1185]</p>\n<p><strong>LESS WRONG USE:<br></strong>Karma: 1078 + 2939.5 (0, 4.5, 136) [n = 1078]<br>Months on LW: 26.7 + 20.1 (12, 24, 40) [n = 1070] <br>Minutes/day on LW: 19.05 + 24.1 (5, 10, 20) [n = 1105]<br>Wiki views/month: 3.6 + 6.3 (0, 1, 5) [n = 984]<br>Wiki edits/month: 0.1 + 0.8 (0, 0, 0) [n = 984]<br><br><strong>PROBABILITIES:</strong><br>Many Worlds: 51.6 + 31.2 (25, 55, 80) [n = 1005]<br>Aliens (universe): 74.2 + 32.6 (50, 90, 99) [n = 1090]<br>Aliens (galaxy): 42.1 + 38 (5, 33, 80) [n = 1081]<br>Supernatural: 5.9 + 18.6 (0, 0, 1) [n = 1095]<br>God: 6 + 18.7 (0, 0, 1) [n = 1098]<br>Religion: 3.8 + 15.5 (0, 0, 0.8) [n = 1113]<br>Cryonics: 18.5 + 24.8 (2, 8, 25) [n = 1100]<br>Antiagathics: 25.1 + 28.6 (1, 10, 35) [n = 1094]<br>Simulation: 25.1 + 29.7 (1, 10, 50) [n = 1039]<br>Global warming: 79.1 + 25 (75, 90, 97) [n = 1112]<br>No catastrophic risk: 71.1 + 25.5 (55, 80, 90) [n = 1095]<br>Space: 20.1 + 27.5 (1, 5, 30) [n = 953]<br><br><strong>CALIBRATION:</strong><br>Year of Bayes' birth: 1767.5 + 109.1 (1710, 1780, 1830) [n = 1105]<br>Confidence: 33.6 + 23.6 (20, 30, 50) [n= 1082]<br><br><strong>MONEY:</strong><br>Income/year: 50,913 + 60644.6 (12000, 35000, 74750) [n = 644]<br>Charity/year: 444.1 + 1152.4 (0, 30, 250) [n = 950]<br>SIAI/CFAR charity/year: 309.3 + 3921 (0, 0, 0) [n = 961]<br>Aging charity/year: 13 + 184.9 (0, 0, 0) [n = 953]<br><br><strong>TIME USE:</strong><br>Hours online/week: 42.4 + 30 (21, 40, 59) [n = 944]<br>Hours reading/week: 30.8 + 19.6 (18, 28, 40) [n = 957]<br>Hours writing/week: 7.9 + 9.8 (2, 5, 10) [n = 951]<br><br><strong>POLITICAL COMPASS:</strong><br>Left/Right: -2.4 + 4 (-5.5, -3.4, -0.3) [n = 476]<br>Libertarian/Authoritarian: -5 + 2 (-6.2, -5.2, -4)</p>\n<p><strong>BIG 5 PERSONALITY TEST:</strong><br>Big 5 (O): 60.6 + 25.7 (41, 65, 84) [n = 453]<br>Big 5 (C): 35.2 + 27.5 (10, 30, 58) [n = 453]<br>Big 5 (E): 30.3 + 26.7 (7, 22, 48) [n = 454]<br>Big 5 (A): 41 + 28.3 (17, 38, 63) [n = 453]<br>Big 5 (N): 36.6 + 29 (11, 27, 60) [n = 449]<br><br><em>These scores are in percentiles, so LWers are more Open, but less Conscientious, Agreeable, Extraverted, and Neurotic than average test-takers. Note that people who take online psychometric tests are probably a pretty skewed category already so this tells us nothing. Also, several people got confusing results on this test or found it different than other tests that they took, and I am pretty unsatisfied with it and don't trust the results.</em></p>\n<p><strong>AUTISM QUOTIENT</strong><br>AQ: 24.1 + 12.2 (17, 24, 30) [n = 367]<br><br><em>This test says the average control subject got 16.4 and 80% of those diagnosed with autism spectrum disorders get 32+ (which of course doesn't tell us what percent of people above 32 have autism...). If we trust them, most LWers are more autistic than average.</em><br><br><strong>CALIBRATION:</strong><br><br>Reverend Thomas Bayes was born in 1701. Survey takers were asked to guess this date within 20 years, so anyone who guessed between 1681 and 1721 was recorded as getting a correct answer. The percent of people who answered correctly is recorded below, stratified by the confidence they gave of having guessed correctly and with the number of people at that confidence level.<br><br>0-5: 10% [n = 30]<br>5-15: 14.8% [n = 183]<br>15-25: 10.3% [n = 242]<br>25-35: 10.7% [n = 225]<br>35-45: 11.2% [n = 98]<br>45-55: 17% [n = 118]<br>55-65: 20.1% [n = 62]<br>65-75: 26.4% [n = 34]<br>75-85: 36.4% [n = 33]<br>85-95: 60.2% [n = 20]<br>95-100: 85.7% [n = 23]</p>\n<p><img src=\"http://www.raikoth.net/Stuff/LessWrong/calibration_table.png\" alt=\"\"></p>\n<p>Here's a classic calibration chart. The blue line is perfect calibration. The orange line is you guys. And the yellow line is average calibration from an experiment I did with untrained subjects a few years ago (which of course was based on different questions and so not directly comparable). <br><br>The results are atrocious; when Less Wrongers are 50% certain, they only have about a 17% chance of being correct. On this problem, at least, they are as bad or worse at avoiding overconfidence bias as the general population.</p>\n<p>My hope was that this was the result of a lot of lurkers who don't know what they're doing stumbling upon the survey and making everyone else look bad, so I ran a second analysis. This one used only the numbers of people who had been in the community at least 2 years and accumulated at least 100 karma; this limited my sample size to about 210 people.</p>\n<p>I'm not going to post exact results, because I made some minor mistakes which means they're off by a percentage point or two, but the general trend was that they looked exactly like the results above: atrocious. If there is some core of elites who are less biased than the general population, they are well past the 100 karma point and probably too rare to feel confident even detecting at this kind of a sample size.</p>\n<p>I really have no idea what went so wrong.&nbsp; <a href=\"http://www.raikoth.net/Stuff/graph_lw.png \">Last year's results</a> were pretty good - encouraging, even. I wonder if it's just an especially bad question. Bayesian statistics is pretty new; one would expect Bayes to have been born in rather more modern times. It's also possible that I've handled the statistics wrong on this one; I wouldn't mind someone double-checking my work.<br><br>Or we could just be <em>really horrible</em>. If we haven't even learned to avoid the one bias that we can measure super well and which is most susceptible to training, what are we even <em>doing</em> here? Some remedial time at <a href=\"http://predictionbook.com/\">PredictionBook</a> might be in order.<br><br><strong>HYPOTHESIS TESTING:</strong></p>\n<p>I tested a very few of the possible hypothesis that were proposed in the survey design threads.<br><br>Are people who understand quantum mechanics are more likely to believe in Many Worlds? We perform a t-test, checking whether one's probability of the MWI being true depends on whether or not one can solve the Schrodinger Equation. People who could solve the equation had on average a 54.3% probability of MWI, compared to 51.3% in those who could not. The p-value is 0.26; there is a 26% probability this occurs by chance. Therefore, we fail to establish that people's probability of MWI varies with understanding of quantum mechanics.<br><br>Are there any interesting biological correlates of IQ? We run a correlation between self-reported IQ, height, maternal age, and paternal age. The correlations are in the expected direction but not significant.<br><br>Are there differences in the ways men and women interact with the community? I had sort of vaguely gotten the impression that women were proportionally younger, newer to the community, and more likely to be referred via HPMOR. The average age of women on LW is 27.6 compared to 27.7 for men; obviously this difference is not significant. 14% of the people referred via HPMOR were women compared to about 10% of the community at large, but this difference is pretty minor. Women were on average newer to the community - 21 months vs. 39 for men - but to my surprise a t-test was unable to declare this significant. Maybe I'm doing it wrong?<br><br>Does the amount of time spent in the community affect one's beliefs in the same way as in previous surveys? I ran some correlations and found that it does. People who have been around longer continue to be more likely to believe in MWI, less likely to believe in aliens in the universe (though not in our galaxy), and less likely to believe in God (though not religion). There was no effect on cryonics this time.<br><br>In addition, the classic correlations between different beliefs continue to hold true. There is an obvious cluster of God, religion, and the supernatural. There's also a scifi cluster of cryonics, antiagathics, MWI, aliens, and the Simulation Hypothesis, and catastrophic risk (this also seems to include global warming, for some reason).<br><br>Are there any differences between men and women in regards to their belief in these clusters? We run a t-test between men and women. Men and women have about the same probability of God (men: 5.9, women: 6.2, p = .86) and similar results for the rest of the religion cluster, but men have much higher beliefs in for example antiagathics (men 24.3, women: 10.5, p &lt; .001) and the rest of the scifi cluster.<br><br><strong>DESCRIPTIONS OF LESS WRONG</strong><br><br>Survey users were asked to submit a description of Less Wrong in 140 characters or less. I'm not going to post all of them, but here is a representative sample:<br><br>- \"Probably the most sensible philosophical resource avaialble.\"<br>- \"Contains the great Sequences, some of Luke's posts, and very little else.\"<br>- \"The currently most interesting site I found ont the net.\"<br>- \"EY cult\"<br>- \"How to think correctly, precisely, and efficiently.\"<br>- \"HN for even bigger nerds.\"<br>- \"Social skills philosophy and AI theorists on the same site, not noticing each other.\"<br>- \"Cool place. Any others like it?\"<br>- \"How to avoid predictable pitfalls in human psychology, and understand hard things well: The Website.\"<br>- \"A bunch of people trying to make sense of the wold through their own lens, which happens to be one of calculation and rigor\"<br>- \"Nice.\"<br>- \"A font of brilliant and unconventional wisdom.\"<br>- \"One of the few sane places on Earth.\"<br>- \"Robot god apocalypse cult spinoff from Harry Potter.\"<br>- \"A place to converse with intelligent, reasonably open-minded people.\"<br>- \"Callahan's Crosstime Saloon\"<br>- \"Amazing rational transhumanist calming addicting Super Reddit\"<br>- \"Still wrong\"<br>- \"A forum for helping to train people to be more rational\"<br>- \"A very bright community interested in amateur ethical philosophy, mathematics, and decision theory.\"<br>- \"Dying. Social games and bullshit now &gt;50% of LW content.\"<br>- \"The good kind of strange, addictive, so much to read!\"<br>- \"Part genuinely useful, part mental masturbation.\"<br>- \"Mostly very bright and starry-eyed adults who never quite grew out of their science-fiction addiction as adolescents.\"<br>- \"Less Wrong: Saving the world with MIND POWERS!\"<br>- \"Perfectly patternmatches the 'young-people-with-all-the-answers' cliche\"<br>- \"Rationalist community dedicated to self-improvement.\"<br>- \"Sperglord hipsters pretending that being a sperglord hipster is cool.\" <em>(this person's Autism Quotient was two points higher than LW average, by the way)</em><br>- \"An interesting perspective and valuable database of mental techniques.\"<br>- \"A website with kernels of information hidden among aspy nonsense.\"<br>- \"Exclusive, elitist, interesting, potentially useful, personal depression trigger.\"<br>- \"A group blog about rationality and related topics. Tends to be overzealous about cryogenics and other pet ideas of Eliezer Yudkowsky.\"<br>- \"Things to read to make you think better.\"<br>- \"Excellent rationality. New-age self-help. Worrying groupthink.\"<br>- \"Not a cult at all.\"<br>- \"A cult.\"<br>- \"The new thing for people who would have been Randian Objectivists 30 years ago.\"<br>- \"Fascinating, well-started, risking bloat and failure modes, best as archive.\"<br>- \"A fun, insightful discussion of probability theory and cognition.\"<br>- \"More interesting than useful.\"<br>- \"The most productive and accessible mind-fuckery on the Internet.\"<br>- \"A blog for rationality, cognitive bias, futurism, and the Singularity.\"<br>- \"Robo-Protestants attempting natural theology.\"<br>- \"Orderly quagmire of tantalizing ideas drawn from disagreeable priors.\"<br>- \"Analyze everything. And I do mean everything. Including analysis. Especially analysis. And analysis of analysis.\"<br>- \"Very interesting and sometimes useful.\"<br>- \"Where people discuss and try to implement ways that humans can make their values, actions, and beliefs more internally consistent.\"<br>- \"Eliezer Yudkowsky personality cult.\"<br>- \"It's like the Mormons would be if everyone were an atheist and good at math and didn't abstain from substances.\"<br>- \"Seems wacky at first, but gradually begins to seem normal.\"<br>- \"A varied group of people interested in philosophy with high Openness and a methodical yet amateur approach.\"<br>- \"Less Wrong is where human algorithms go to debug themselves.\"<br>- \"They're kind of like a cult, but that doesn't make them wrong.\"<br>- \"A community blog devoted to nerds who think they're smarter than everyone else.\"<br>- \"90% sane! A new record!\"<br>- \"The Sequences are great. LW now slowly degenerating to just another science forum.\"<br>- \"The meetup groups are where it's at, it seems to me. I reserve judgment till I attend one.\"<br>- \"All I really know about it is this long survey I took.\"<br>- \"The royal road of rationality.\"<br>- \"Technically correct: The best kind of correct!\"<br>- \"Full of angry privilege.\"<br>- \"A sinister instrument of billionaire Peter Thiel.\"<br>- \"Dangerous apocalypse cult bent on the systematic erasure of traditional values and culture by any means necessary.\"<br>- \"Often interesting, but I never feel at home.\"<br>- \"One of the few places I truly feel at home, knowing that there are more people like me.\"<br>- \"Currently the best internet source of information-dense material regarding cog sci, debiasing, and existential risk.\"<br>- \"Prolific and erudite writing on practical techniques to enhance the effectiveness of our reason.\"<br>- \"An embarrassing Internet community formed around some genuinely great blog writings.\"<br>- \"I bookmarked it a while ago and completely forgot what it is about. I am taking the survey to while away my insomnia.\"<br>- \"A somewhat intimidating but really interesting website that helps refine rational thinking.\"<br>- \"A great collection of ways to avoid systematic bias and come to true and useful conclusions.\"<br>- \"Obnoxious self-serving, foolish trolling dehumanizing pseudointellectualism, aesthetically bankrupt.\"<br>- \"The cutting edge of human rationality.\"<br>- \"A purveyor of exceedingly long surveys.\"<br><br><strong>PUBLIC RELEASE</strong><br><br>That last commenter was right. This survey had vastly more data than any previous incarnation; although there are many more analyses I would like to run I am pretty exhausted and I know people are anxious for the results. I'm going to let CFAR analyze and report on their questions, but the rest should be a community effort. So I'm releasing the survey to everyone in the hopes of getting more information out of it. If you find something interesting you can either post it in the comments or start a new thread somewhere.<br><br>The data I'm providing is the raw data EXCEPT:<br><br>- I deleted a few categories that I removed halfway through the survey for various reasons<br>- I deleted 9 entries that were duplicates of other entries, ie someone pressed 'submit' twice.<br>- I deleted the timestamp, which would have made people extra-identifiable, and sorted people by their CFAR random number to remove time order information.<br>- I removed one person whose information all came out as weird symbols.<br>- I numeralized some of the non-numeric data, especially on the number of months in community question. This is not the version I cleaned up fully, so you will get to experience some of the same pleasure I did working with the rest.<br>- I deleted 117 people who either didn't answer the privacy question or who asked me to keep them anonymous, leaving 1067 people.<br><br>Here it is: <a href=\"http://raikoth.net/Stuff/LessWrong/for_public.csv\"><strong>Data in .csv format</strong></a> , <a href=\"http://raikoth.net/Stuff/LessWrong/for_public.xls\"><strong>Data in Excel format</strong></a></p>", "sections": [{"title": "Part 1: Population", "anchor": "Part_1__Population", "level": 1}, {"title": "Part 2: Categorical Data", "anchor": "Part_2__Categorical_Data", "level": 1}, {"title": "Part 3: Numerical Data", "anchor": "Part_3__Numerical_Data", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "653 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 653, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HAEPbGaMygJq8L59k"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-08T01:17:40.821Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Permitted Possibilities, & Locality", "slug": "seq-rerun-permitted-possibilities-and-locality", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qc3BckfTbxovNDp2a/seq-rerun-permitted-possibilities-and-locality", "pageUrlRelative": "/posts/qc3BckfTbxovNDp2a/seq-rerun-permitted-possibilities-and-locality", "linkUrl": "https://www.lesswrong.com/posts/qc3BckfTbxovNDp2a/seq-rerun-permitted-possibilities-and-locality", "postedAtFormatted": "Saturday, December 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Permitted%20Possibilities%2C%20%26%20Locality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Permitted%20Possibilities%2C%20%26%20Locality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqc3BckfTbxovNDp2a%2Fseq-rerun-permitted-possibilities-and-locality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Permitted%20Possibilities%2C%20%26%20Locality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqc3BckfTbxovNDp2a%2Fseq-rerun-permitted-possibilities-and-locality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqc3BckfTbxovNDp2a%2Fseq-rerun-permitted-possibilities-and-locality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 166, "htmlBody": "<p>Today's post, <a href=\"/lw/wg/permitted_possibilities_locality/\">Permitted Possibilities, &amp; Locality</a> was originally published on 03 December 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Permitted_Possibilities.2C_.26_Locality\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Yudkowsky's attempt to summarize Hanson's positions, list the possible futures discussed so far, and identify which ones seem most and least likely to Yudkowsky.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fsb/seq_rerun_whither_manufacturing/\">Whither Manufacturing?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qc3BckfTbxovNDp2a", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0526281721686383e-06, "legacy": true, "legacyId": "20525", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["oFKLSvbDkX8h7amRs", "JypJQdTkNDPBKMR8j", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-08T03:53:35.658Z", "modifiedAt": null, "url": null, "title": "Akrasia survey data analysis", "slug": "akrasia-survey-data-analysis", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:32.869Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xiZ8TqhhMMHJJrerJ/akrasia-survey-data-analysis", "pageUrlRelative": "/posts/xiZ8TqhhMMHJJrerJ/akrasia-survey-data-analysis", "linkUrl": "https://www.lesswrong.com/posts/xiZ8TqhhMMHJJrerJ/akrasia-survey-data-analysis", "postedAtFormatted": "Saturday, December 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Akrasia%20survey%20data%20analysis&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAkrasia%20survey%20data%20analysis%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxiZ8TqhhMMHJJrerJ%2Fakrasia-survey-data-analysis%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Akrasia%20survey%20data%20analysis%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxiZ8TqhhMMHJJrerJ%2Fakrasia-survey-data-analysis", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxiZ8TqhhMMHJJrerJ%2Fakrasia-survey-data-analysis", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 671, "htmlBody": "<p><strong>Followup to</strong>:&nbsp;<a href=\"/lw/fpf/please_decide_whether_youll_take_this_minutelong/\">Akrasia hack survey</a></p>\n<hr />\n<h3>p(hack akrasia|heard of hack and thought it was worth trying)</h3>\n<p>What are the odds of you succumbing to \"hack akrasia\", never trying or not consistently applying a hack, given that you'd heard of it and thought it was worth trying?</p>\n<p>lukeprog's algorithm for beating procrastination: 83%</p>\n<p>The Pomodoro Technique: 68%</p>\n<p>Exercise for increased energy: 60%</p>\n<p>LeechBlock or similar: 38%</p>\n<p>Comments: Hack akrasia seems pretty darn high overall. &nbsp;LeechBlock is least susceptible.</p>\n<p>&nbsp;</p>\n<h3>p(using hack profitably|heard of hack and thought it was worth trying)</h3>\n<p>The \"real success rate\". &nbsp;What percentage of the time does thinking a hack is worth trying translate in to adopting it and using it consistently?</p>\n<p>lukeprog's algorithm for beating procrastination: 02%</p>\n<p>The Pomodoro Technique: 04%</p>\n<p>Exercise for increased energy: 25%</p>\n<p>LeechBlock or similar: 15%</p>\n<p>Comments: Exercise is the clear winner. &nbsp;If you didn't think exercise was worth trying (5% of survey respondents), you might want to reconsider.</p>\n<p>&nbsp;</p>\n<h3>p(hack seems to work|tried hack)</h3>\n<p>In a world without hack akrasia, what success rates would be be seeing?</p>\n<p>lukeprog's algorithm for beating procrastination: 42%</p>\n<p>The Pomodoro Technique: 58%</p>\n<p>Exercise for increased energy: 84%</p>\n<p>LeechBlock or similar: 37%</p>\n<p>Comments: Again, exercise is the clear winner. &nbsp;If you don't exercise, next time you're in an akrasia-killing mood, it seems you'd be well advised to try and set up some sort of regular exercise regimen for yourself. &nbsp;Setting up a Pomodoro regime for yourself seems like a solid 2nd choice.</p>\n<p>&nbsp;</p>\n<h3>p(hack seemed worth trying|heard of hack)</h3>\n<p>lukeprog's algorithm for beating procrastination: 75%</p>\n<p>The Pomodoro Technique: 79%</p>\n<p>Exercise for increased energy: 94%</p>\n<p>LeechBlock or similar: 60%</p>\n<p>Comments: This was for comparison with actual success rates. &nbsp;Multiple people wrote in that they didn't have the problem LeechBlock tries to solve, so this may account for its low rate. &nbsp;If you <em>do</em> have the problem LeechBlock tries to solve but you did't think it's worth trying, you may wish to revise your opinion, as its \"real success rate\" is in 2nd place at 15%.</p>\n<p>Yes, LeechBlock may be relatively easy to subvert. &nbsp;I was turned off by this initially as well. &nbsp;But now I think that it's not all that important--the main thing is to disrupt your distraction-seeking behavior, not present an impenetrable barrier. &nbsp;I'd guess that if you could set up LeechBlock as a reminder to engage in some non-variable-reinforcement break activity, that'd be ideal.</p>\n<p>By the way, does anyone have an opinion on the best LeechBlock equivalent for Google Chrome?</p>\n<p>&nbsp;</p>\n<h3>Graphs from Villiam Bur</h3>\n<p><img src=\"http://bur.sk/share/2012/antiakrasia1.png\" alt=\"\" width=\"538\" height=\"339\" /></p>\n<p>&nbsp;</p>\n<p><img src=\"http://bur.sk/share/2012/antiakrasia2.png\" alt=\"\" /></p>\n<p>&nbsp;</p>\n<h3>More commentary</h3>\n<p>Initially I'd been thinking of \"hack akrasia\" as a different type of akrasia than the regular akrasia it tried to defeat. &nbsp;But recently it occurred to me to question this.</p>\n<p>There are probably a variety of akrasia subtypes, some of which have disproportionate impact on executing hacks vs doing other stuff. &nbsp;Akrasia subtypes I can think of offhand:</p>\n<ul>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Ugh_field\">Ugh field</a> akrasia.</li>\n<li>Near/far akrasia: Something seems like a good idea, but you never think \"oh, I should actually <em>do</em> this now\" or make a plan to do it at a <em>specific</em> time or in a <em>specific</em> situation.</li>\n<li>Forgetting akrasia: You do make a plan/resolution but you forget about it.</li>\n<li>Slippery slope akrasia: \"Just this once\" and other ways to gradually rationalize your way around good intentions. &nbsp;(<a href=\"/lw/flr/thoughts_on_designing_policies_for_oneself/\">My recent policy design post</a> offers some suggestions for this.)</li>\n<li>Low morale, low energy, low motivation, depression.</li>\n</ul>\n<p>Anyway, given the high rate of hack akrasia, it may make sense to concentrate on developing hacks that are themselves substantially less susceptible to akrasia. &nbsp;For example, if I told you to watch <a href=\"http://thedailyshow.cc.com/\">funny videos</a> on the internet when your morale is low (psychologists have speculated that laughter helps with ego depletion--works OK for me), it seems unlikely you'd fall prey to any akrasia subtype except forgetting akrasia. &nbsp;(Optimal Breaks in general seems like it might fall in this category, especially if the breaks involve 0 setup cost.)</p>\n<p>To fight inconsistent application of hacks that you know work when you use them, <a href=\"https://www.beeminder.com/\">BeeMinder</a> might be useful.</p>\n<p>Regarding write-ins: They were under 5% for every category and I threw them out when computing conditional probabilities.</p>\n<p>Conditional probability calculator here:&nbsp;<a href=\"https://gist.github.com/4238473\">https://gist.github.com/4238473</a>&nbsp; Hopefully there aren't any bugs.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"r7qAjcbfhj2256EHH": 2, "kJrjorSx3hXa7q7CJ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xiZ8TqhhMMHJJrerJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 20, "extendedScore": null, "score": 1.05271786014653e-06, "legacy": true, "legacyId": "20532", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Followup to</strong>:&nbsp;<a href=\"/lw/fpf/please_decide_whether_youll_take_this_minutelong/\">Akrasia hack survey</a></p>\n<hr>\n<h3 id=\"p_hack_akrasia_heard_of_hack_and_thought_it_was_worth_trying_\">p(hack akrasia|heard of hack and thought it was worth trying)</h3>\n<p>What are the odds of you succumbing to \"hack akrasia\", never trying or not consistently applying a hack, given that you'd heard of it and thought it was worth trying?</p>\n<p>lukeprog's algorithm for beating procrastination: 83%</p>\n<p>The Pomodoro Technique: 68%</p>\n<p>Exercise for increased energy: 60%</p>\n<p>LeechBlock or similar: 38%</p>\n<p>Comments: Hack akrasia seems pretty darn high overall. &nbsp;LeechBlock is least susceptible.</p>\n<p>&nbsp;</p>\n<h3 id=\"p_using_hack_profitably_heard_of_hack_and_thought_it_was_worth_trying_\">p(using hack profitably|heard of hack and thought it was worth trying)</h3>\n<p>The \"real success rate\". &nbsp;What percentage of the time does thinking a hack is worth trying translate in to adopting it and using it consistently?</p>\n<p>lukeprog's algorithm for beating procrastination: 02%</p>\n<p>The Pomodoro Technique: 04%</p>\n<p>Exercise for increased energy: 25%</p>\n<p>LeechBlock or similar: 15%</p>\n<p>Comments: Exercise is the clear winner. &nbsp;If you didn't think exercise was worth trying (5% of survey respondents), you might want to reconsider.</p>\n<p>&nbsp;</p>\n<h3 id=\"p_hack_seems_to_work_tried_hack_\">p(hack seems to work|tried hack)</h3>\n<p>In a world without hack akrasia, what success rates would be be seeing?</p>\n<p>lukeprog's algorithm for beating procrastination: 42%</p>\n<p>The Pomodoro Technique: 58%</p>\n<p>Exercise for increased energy: 84%</p>\n<p>LeechBlock or similar: 37%</p>\n<p>Comments: Again, exercise is the clear winner. &nbsp;If you don't exercise, next time you're in an akrasia-killing mood, it seems you'd be well advised to try and set up some sort of regular exercise regimen for yourself. &nbsp;Setting up a Pomodoro regime for yourself seems like a solid 2nd choice.</p>\n<p>&nbsp;</p>\n<h3 id=\"p_hack_seemed_worth_trying_heard_of_hack_\">p(hack seemed worth trying|heard of hack)</h3>\n<p>lukeprog's algorithm for beating procrastination: 75%</p>\n<p>The Pomodoro Technique: 79%</p>\n<p>Exercise for increased energy: 94%</p>\n<p>LeechBlock or similar: 60%</p>\n<p>Comments: This was for comparison with actual success rates. &nbsp;Multiple people wrote in that they didn't have the problem LeechBlock tries to solve, so this may account for its low rate. &nbsp;If you <em>do</em> have the problem LeechBlock tries to solve but you did't think it's worth trying, you may wish to revise your opinion, as its \"real success rate\" is in 2nd place at 15%.</p>\n<p>Yes, LeechBlock may be relatively easy to subvert. &nbsp;I was turned off by this initially as well. &nbsp;But now I think that it's not all that important--the main thing is to disrupt your distraction-seeking behavior, not present an impenetrable barrier. &nbsp;I'd guess that if you could set up LeechBlock as a reminder to engage in some non-variable-reinforcement break activity, that'd be ideal.</p>\n<p>By the way, does anyone have an opinion on the best LeechBlock equivalent for Google Chrome?</p>\n<p>&nbsp;</p>\n<h3 id=\"Graphs_from_Villiam_Bur\">Graphs from Villiam Bur</h3>\n<p><img src=\"http://bur.sk/share/2012/antiakrasia1.png\" alt=\"\" width=\"538\" height=\"339\"></p>\n<p>&nbsp;</p>\n<p><img src=\"http://bur.sk/share/2012/antiakrasia2.png\" alt=\"\"></p>\n<p>&nbsp;</p>\n<h3 id=\"More_commentary\">More commentary</h3>\n<p>Initially I'd been thinking of \"hack akrasia\" as a different type of akrasia than the regular akrasia it tried to defeat. &nbsp;But recently it occurred to me to question this.</p>\n<p>There are probably a variety of akrasia subtypes, some of which have disproportionate impact on executing hacks vs doing other stuff. &nbsp;Akrasia subtypes I can think of offhand:</p>\n<ul>\n<li><a href=\"http://wiki.lesswrong.com/wiki/Ugh_field\">Ugh field</a> akrasia.</li>\n<li>Near/far akrasia: Something seems like a good idea, but you never think \"oh, I should actually <em>do</em> this now\" or make a plan to do it at a <em>specific</em> time or in a <em>specific</em> situation.</li>\n<li>Forgetting akrasia: You do make a plan/resolution but you forget about it.</li>\n<li>Slippery slope akrasia: \"Just this once\" and other ways to gradually rationalize your way around good intentions. &nbsp;(<a href=\"/lw/flr/thoughts_on_designing_policies_for_oneself/\">My recent policy design post</a> offers some suggestions for this.)</li>\n<li>Low morale, low energy, low motivation, depression.</li>\n</ul>\n<p>Anyway, given the high rate of hack akrasia, it may make sense to concentrate on developing hacks that are themselves substantially less susceptible to akrasia. &nbsp;For example, if I told you to watch <a href=\"http://thedailyshow.cc.com/\">funny videos</a> on the internet when your morale is low (psychologists have speculated that laughter helps with ego depletion--works OK for me), it seems unlikely you'd fall prey to any akrasia subtype except forgetting akrasia. &nbsp;(Optimal Breaks in general seems like it might fall in this category, especially if the breaks involve 0 setup cost.)</p>\n<p>To fight inconsistent application of hacks that you know work when you use them, <a href=\"https://www.beeminder.com/\">BeeMinder</a> might be useful.</p>\n<p>Regarding write-ins: They were under 5% for every category and I threw them out when computing conditional probabilities.</p>\n<p>Conditional probability calculator here:&nbsp;<a href=\"https://gist.github.com/4238473\">https://gist.github.com/4238473</a>&nbsp; Hopefully there aren't any bugs.</p>", "sections": [{"title": "p(hack akrasia|heard of hack and thought it was worth trying)", "anchor": "p_hack_akrasia_heard_of_hack_and_thought_it_was_worth_trying_", "level": 1}, {"title": "p(using hack profitably|heard of hack and thought it was worth trying)", "anchor": "p_using_hack_profitably_heard_of_hack_and_thought_it_was_worth_trying_", "level": 1}, {"title": "p(hack seems to work|tried hack)", "anchor": "p_hack_seems_to_work_tried_hack_", "level": 1}, {"title": "p(hack seemed worth trying|heard of hack)", "anchor": "p_hack_seemed_worth_trying_heard_of_hack_", "level": 1}, {"title": "Graphs from Villiam Bur", "anchor": "Graphs_from_Villiam_Bur", "level": 1}, {"title": "More commentary", "anchor": "More_commentary", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "8 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pjQYFqdfHS9Z2qRgt", "aNRYQFnMQbA7uu99u"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-08T17:49:13.947Z", "modifiedAt": null, "url": null, "title": "Procedural Knowledge Gaps, part 2", "slug": "procedural-knowledge-gaps-part-2", "viewCount": null, "lastCommentedAt": "2017-06-17T04:28:06.581Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "W55i9XXdye9ETAyu5", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JybPNvGTWNNJ4bhYo/procedural-knowledge-gaps-part-2", "pageUrlRelative": "/posts/JybPNvGTWNNJ4bhYo/procedural-knowledge-gaps-part-2", "linkUrl": "https://www.lesswrong.com/posts/JybPNvGTWNNJ4bhYo/procedural-knowledge-gaps-part-2", "postedAtFormatted": "Saturday, December 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Procedural%20Knowledge%20Gaps%2C%20part%202&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProcedural%20Knowledge%20Gaps%2C%20part%202%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJybPNvGTWNNJ4bhYo%2Fprocedural-knowledge-gaps-part-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Procedural%20Knowledge%20Gaps%2C%20part%202%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJybPNvGTWNNJ4bhYo%2Fprocedural-knowledge-gaps-part-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJybPNvGTWNNJ4bhYo%2Fprocedural-knowledge-gaps-part-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 337, "htmlBody": "<p>With Alicorn's permission, I'm resurrecting <a href=\"/r/lesswrong/lw/453/procedural_knowledge_gaps/\">this thread</a>.</p>\n<blockquote>\n<p>&nbsp;I am beginning to suspect that it is surprisingly common for intelligent, competent adults to somehow make it through the world for a few decades while missing some ordinary skill, like mailing a physical letter, folding a fitted sheet, depositing a check, or reading a bus schedule.  Since these tasks are often presented atomically - or, worse, embedded implicitly into other instructions - and it is often possible to get around the need for them, this ignorance is not self-correcting.  One can Google \"how to deposit a check\" and similar phrases, but the sorts of instructions that crop up are often misleading, rely on entangled and potentially similarly-deficient knowledge to be understandable, or are not so much instructions as they are tips and tricks and warnings for people who already know the basic procedure.  Asking other people is more effective because they can respond to requests for clarification (and physically pointing at stuff is useful too), but embarrassing, since lacking these skills as an adult is stigmatized.  (They are rarely even considered skills by people who have had them for a while.)<br /><br />This seems like a bad situation.  And - if I am correct and gaps like these are common - then it is something of a collective action problem to handle gap-filling without undue social drama.  Supposedly, we're good at collective action problems, us rationalists, right?  So I propose a thread for the purpose here, with the stipulation that all replies to gap announcements are to be constructive attempts at conveying the relevant procedural knowledge.  No asking \"how did you manage to be X years old without knowing that?\" - if the gap-haver wishes to volunteer the information, that is fine, but asking is to be considered poor form.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>I'll start off with one of my own: What kinds of exercise can I do at home (I do have 5- and 20-pound weights), and what are good ways to get motivation to do so regularly?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JybPNvGTWNNJ4bhYo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 17, "extendedScore": null, "score": 1.0531987837257953e-06, "legacy": true, "legacyId": "20545", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 88, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ka8eveZpT7hXLhRTM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-08T19:52:17.556Z", "modifiedAt": null, "url": null, "title": "Science, Engineering, and Uncoolness; Here and Now, Then and There", "slug": "science-engineering-and-uncoolness-here-and-now-then-and", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:38.080Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Ritalin", "createdAt": "2012-05-01T18:00:25.863Z", "isAdmin": false, "displayName": "Ritalin"}, "userId": "ACGKS9W7iQQywxrWW", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JZJGs6teDEug6fm3Z/science-engineering-and-uncoolness-here-and-now-then-and", "pageUrlRelative": "/posts/JZJGs6teDEug6fm3Z/science-engineering-and-uncoolness-here-and-now-then-and", "linkUrl": "https://www.lesswrong.com/posts/JZJGs6teDEug6fm3Z/science-engineering-and-uncoolness-here-and-now-then-and", "postedAtFormatted": "Saturday, December 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Science%2C%20Engineering%2C%20and%20Uncoolness%3B%20Here%20and%20Now%2C%20Then%20and%20There&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AScience%2C%20Engineering%2C%20and%20Uncoolness%3B%20Here%20and%20Now%2C%20Then%20and%20There%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJZJGs6teDEug6fm3Z%2Fscience-engineering-and-uncoolness-here-and-now-then-and%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Science%2C%20Engineering%2C%20and%20Uncoolness%3B%20Here%20and%20Now%2C%20Then%20and%20There%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJZJGs6teDEug6fm3Z%2Fscience-engineering-and-uncoolness-here-and-now-then-and", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJZJGs6teDEug6fm3Z%2Fscience-engineering-and-uncoolness-here-and-now-then-and", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 622, "htmlBody": "<p><em>[Feel free to read this poor little unrigorous and unsourced post in <a href=\"http://www.youtube.com/watch?v=-9fVx9k96Ns\">JK Simmons' voice</a>. That is entirely optional and you are of course free to read it in any voice you like; I only thought it might be interesting in the light of what is mentioned in the edit at the bottom of the text]</em></p>\n<p>Nowadays, it seems that the correlation between sciency stuff, social ineptitude, and uncoolness, is cemented in the mind of the public. But this seems to be very era-specific, even time-specific.<br /><br />As a lesswronger, I find what follows ironic: In Islamic countries, \"scientists\" are called with the same word use for religious leaders and other teachers, \"olama\", literally \"knowers\"; historically, there's been a huge overlap between the two, and, when one of these folks speaks, you're supposed to shut up and <em>listen</em>. This is still true to this day. There might not be much wealth to be gained from marrying a scientist, but there was <em>status</em>; amusingly enough, it's in modern-day materialism that is pushing them into irrelevance as money becomes, more and more, the sole measure of status.<br /><br />In the West, in the XIXth century, Science and Progress were hip and awesome. Being a scientist of some sort was practically a requirement for any pulp hero. In the USA, an era of great works of engineering that had a dramatic impact on life quality made engineers heroes of popular fiction, men of knowledge and rigour who would not bow down to money and lawyer-cushioned bourgeois, or to corrupt and fickle politicians, men who would stand up against injustice and get the job done no matter what. Everyone wanted to call themselves an engineer, and the word was rampantly abused into meaninglessness; florists called themselves \"flower engineers\"! That's how cool being an engineer was.<br /><br />In the Soviet Union, as long as they didn't step on the toes of the Party, scientists were highly acclaimed and respected, they got tons of honour and status. There was a huge emphasis on technological progress, on mankind reaching its full potential (at least on paper).<br /><br />Nowadays, nearly the entire leadership of China is made of technicians and engineers. Not lawyers, or economists, or literati. These people only care about one thing, getting the job done - and that's what Science does.<br /><br />So, I've really got to ask, when and *how* did Science and Engineering become \"uncool\", and why are they termed \"<a href=\"http://en.wikipedia.org/wiki/Geek\">geek</a>\", the term used for sideshow circus performers whose speciality was eating chickens alive (or something like that), and which, before that, used to be synonymous with freak and fook? When and how did we become <em>worse than clowns </em>in the eyes of society?&nbsp; Most importantly: how can the process be reversed?</p>\n<p>After all, from a utilitarian standpoint, Science being cool and appreciated and respectable is kind of important.</p>\n<p>&nbsp;</p>\n<p>EDIT: There's also the strange relationship, in the public mind, between science and dangerous, callous, abusive insanity, with a long tradition in popular fiction from Victor Von Frankenstein and Captain Nemo to Tony Stark and GLaDOS, and some Real Life counterparts, especially in brutal totalitarian regimes. Wikipedia has an interesting <a href=\"http://en.wikipedia.org/wiki/Mad_scientist\">article</a> on the topic, and how the characterization and prevalence of the Mad Scientist related to time-pertinent perceptions of Science.</p>\n<p>For some reason, that aspect is <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/DoNotDoThisCoolThing\">often treated as cool and dramatic and impressive (<em>besides</em> being characterized as repulsive)</a>, perhaps because it involves displays of power over others, which is a high-status thing to do. Is that one of the existing paths to social prestige? Achieving power, and being inconsiderate about flaunting it? I'd like to hear more constructive alternatives, because that one doesn't seem viable, from where I stand.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JZJGs6teDEug6fm3Z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 18, "extendedScore": null, "score": 3e-05, "legacy": true, "legacyId": "20546", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-08T22:45:24.550Z", "modifiedAt": null, "url": null, "title": "Help Reform A Philosophy Curriculum", "slug": "help-reform-a-philosophy-curriculum", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:58.383Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JonathanLivengood", "createdAt": "2011-08-30T18:58:39.952Z", "isAdmin": false, "displayName": "JonathanLivengood"}, "userId": "baG36x3deQTMSPCSi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gvZMevZ265JrMaLGo/help-reform-a-philosophy-curriculum", "pageUrlRelative": "/posts/gvZMevZ265JrMaLGo/help-reform-a-philosophy-curriculum", "linkUrl": "https://www.lesswrong.com/posts/gvZMevZ265JrMaLGo/help-reform-a-philosophy-curriculum", "postedAtFormatted": "Saturday, December 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Help%20Reform%20A%20Philosophy%20Curriculum&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHelp%20Reform%20A%20Philosophy%20Curriculum%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgvZMevZ265JrMaLGo%2Fhelp-reform-a-philosophy-curriculum%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Help%20Reform%20A%20Philosophy%20Curriculum%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgvZMevZ265JrMaLGo%2Fhelp-reform-a-philosophy-curriculum", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgvZMevZ265JrMaLGo%2Fhelp-reform-a-philosophy-curriculum", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1514, "htmlBody": "<p>A couple of days ago, Luke posted <a href=\"/lw/frp/train_philosophers_with_pearl_and_kahneman_not/\">a recommendation for reforming how philosophy is taught</a>. <a href=\"http://www.philosophy.illinois.edu/\">My department</a> at <a href=\"http://illinois.edu/\">the University of Illinois</a> is in the midst of some potentially large-scale changes.* Hence, now seems to be a great time to think about concrete steps towards reforming or partially reforming the curriculum in an actual philosophy department. I would appreciate some help thinking through how to make changes that will (a) improve the philosophy education of our undergraduates, (b) recruit and retain better students, (c) improve faculty experiences with teaching philosophy, and (d) be salable to the rest of the philosophy faculty. To some extent, this post is me thinking out loud through what I want to say to my department's curriculum committee (probably in January).</p>\n<p>&nbsp;</p>\n<h2>How Things Stand Right Now</h2>\n<p>In this section, I will try to lay out the situation as I see it right now.</p>\n<p>First, we have the following problem: philosophy courses that we offer are not sufficiently gated. In the mathematics department at my university, you can't take mathematical logic until you've taken a course called <a href=\"https://my.illinois.edu/uPortal/render.userLayoutRootNode.target.u41998l1n6.uP?pltc_target=210253.u41998l1n6&amp;pltc_type=RENDER&amp;pltp_action=catalogCourseView&amp;pltp_classNumber=347&amp;pltp_course=MATH&amp;pltp_term=spring&amp;pltp_year=2013#u41998l1n6\">Fundamental Mathematics</a>, which looks to be a class about proof techniques, mathematical induction, etc. And you can't take that until you've taken the second semester of calculus. Computer science, economics, physics, and most every other science curriculum works like this. If you want to take advanced courses, you have to pass through the <em>gates</em> of less advanced courses, which (theoretically, at least) prepare you for the material covered in the more advanced course.</p>\n<p>By contrast, in the philosophy department, you may take a senior-level (400 at my school) course after taking a freshman-level (100 at my school) introduction to philosophy. The result is that students who take our 400-level courses are typically unprepared. At least, that has been my experience. (Shockingly, many students taking 400-level classes then complain that they were expected to know things about philosophy!)&nbsp;A big part of the problem here is that we do not presently have enough faculty to cover intermediate-level courses on a regular enough basis, and let's be honest, faculty members don't usually want to teach lower-level courses anyway.</p>\n<p>Second, we have the following resource: our department currently has strong and growing connections with several world-class science or science-related departments. We have cross-appointed faculty and/or cross-listed courses with mathematics, linguistics, psychology, and physics, all of which are very strong departments.&nbsp;We have philosophy graduate students who do research and teach courses in these disciplines as well.&nbsp;And I am hoping to expand our connections to include computer science and statistics. I think there ought to be a good way to make use of these resources.</p>\n<p>Rather than trying to reform the <em>entire</em>&nbsp;philosophy curriculum all at once, I want to focus first on our logic offerings. What we have now is the following mess.</p>\n<p>\n<ul>\n<li>102 -- Introduction to Logic: A critical thinking course almost never taught by a faculty member.</li>\n<li>103 -- Quantitative Introduction to Logic: An introductory formal logic course taught by me about half the time</li>\n<li>202 -- Symbolic Logic: A basic symbolic logic course (unclear in how it is different from 103 except in that it is completely restricted to deductive logic)</li>\n<li>307 -- Elements of Semantics and Pragmatics: Cross-listed with linguistics</li>\n<li>407 -- Logic and Linguistics: Cross-listed with linguistics</li>\n<li>453 -- Formal Logic and Philosophy: An extension of 202 but with emphasis on philosophical issues</li>\n<li>454 -- Advanced Symbolic Logic: Basically, a math logic course covering completeness, compactness, Lowenheim-Skolem, incompleteness, and undecidability</li>\n</ul>\n</p>\n<p>The only pre-requisite for 453 and 454 is 202, and 202 has no pre-requisites at all; the pre-req for 407 is 307, and 307 depends on a 100-level linguistics course or (more commonly) consent of the instructor. We also have a 400-level philosophy of mathematics course.&nbsp;Along with these, the mathematics department has a 400-level mathematical logic course and a 400-level course on set theory and topology, neither of which is currently cross-listed, but both of which, I think, <em>should be</em> cross-listed as philosophy courses, which would also raise the bar for the philosophy students interested in logic by requiring that they take the calculus sequence and the fundamentals course.</p>\n<p>On the defects side, I think we have poor use of gating and spotty coverage of even deductive logic. For example, we have no courses on modal logics, we have no courses on intuitionist/constructivist logic, we have no courses on relevance logic, we have no courses on more exotic logics, we have no courses on set theory or category theory, and we have no courses on computation. We get sort of close to the last two in 453/454, and we might address them more directly by cross-listing with mathematics. But as it is, we do not do those things. And we have a <strong>huge gaping hole</strong>&nbsp;where inductive logic, probability theory, statistics, causal inference, and so on should be. On an individual level, that hole could be filled somewhat by taking courses in statistics; however, that is not quite the same as having courses available on confirmation theory or inductive logics.</p>\n<h2><br /></h2>\n<h2>Recommendations</h2>\n<p>So, now you know the basic situation ... what to do?</p>\n<p>Below are some specific recommendations that I want to make to my department's curriculum committee. I would really appreciate input on how to refine my recommendations, how to make them more palatable, and so on.</p>\n<p>First, we need to make the 100- and 200-level courses connect in a relevant way. I recommend entirely relabeling (and maybe even renumbering) 102 so that it is clear that it is a terminal, service course intended for non-majors. The course should stand to philosophy education as courses like <a href=\"https://my.illinois.edu/uPortal/render.userLayoutRootNode.target.u41998l1n6.uP?pltc_target=210253.u41998l1n6&amp;pltc_type=RENDER&amp;pltp_course=PHYS&amp;pltp_term=spring&amp;pltp_classNumber=123&amp;pltp_action=catalogCourseView&amp;pltp_year=2013#u41998l1n6\">Physics Made Easy</a>&nbsp;stands to physics education. I further recommend making a relabeled (and maybe renumbered) version of 103 a pre-requisite for all 200-level logic courses. In terms of material covered, ideally 103 would introduce symbolic conventions (to be made as standard as possible across the curriculum), proof skills, and basic ideas in model theory, set theory, and probability theory. (I go back and forth between liking this idea, which fits closely with how I teach 103 now, and wanting to do something more like, deductive logic in the first half and confirmation theory in the second half with no formal exposure to set theory. The biggest barrier to the second approach is in formally developing confirmation theory without set theory.) Then 202 could do more meta-logic, go into more detail on model theory, go into more detail on set theory, or whatever.</p>\n<p>Second, we need more courses covering inductive logic, probability theory, statistics, and so on. I recommend adding a 200-level course parallel to 202, which would cover some probability theory and some causal and statistical reasoning.&nbsp;Let's call this proposed course PHIL 204, since we don't offer anything under that number right now.&nbsp;I have in mind something slightly more advanced than CMU's Open Learning Initiative course <a href=\"https://oli.cmu.edu/jcourse/lms/students/syllabus.do?section=0baa507780020ca601df85439613bd8f\">here</a>.</p>\n<p>Third, I recommend expanding our 300-level course-offerings as follows. We need a second semester of inductive logic (etc.) that builds off of 204. And we need a course that does a simple survey of exotic logics, like modal logics, intuitionistic logic, relevance logic, free logic, etc. The 300-level survey of exotic logics need not be a pre-req for 400-level courses, provided the 400-level courses cover the same material that they have been covering. And that seems fine to me, although it might be in our long-term interests to drop our 454 in the event that we cross-list with mathematics on their math logic course. Depending on how their math logic course is taught, we might try to convince them that our 202 should satisfy the pre-req as effectively as their fundamentals course. (I don't know if that would be a hard sell or not.)</p>\n<p>Fourth, I recommend cross-listing some courses with mathematics and statistics to get more regular coverage of the tools we want our students to have without necessarily having our faculty teach those courses all the time.</p>\n<p>&nbsp;</p>\n<h2>What Is The Goal Here?</h2>\n<p>I haven't spent any time in this write-up thinking about the goal(s) of logic education in philosophy. I am not sure whether it would be worth backing up to address this question or whether there is sufficient implicit agreement about the value and goals of logic to leave it alone. If you think I should be saying something about the place of logic in the overall curriculum or making an argument for teaching logic at all or making an argument for understanding logic broadly enough to get probability and statistics through the door, please tell me and make a suggestion about how to develop the heading.</p>\n<p>&nbsp;</p>\n<p>--------------------------------------------------------------------------------------------------------------------</p>\n<p>*My department is currently much too small relative to the size of <a href=\"http://illinois.edu/\">my university</a>, but the powers that be have recently become receptive to our requests to expand (really, to replace a large number of retirements from the last five years). Hence, we are about to undergo an external review, which we hope will result in a plan for phased growth of the department to a little more than <em>twice</em> its current size by the end of the decade. (Yes, our numbers are seriously depleted!)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GLykb6NukBeBQtDvQ": 1, "fH8jPjHF2R27sRTTG": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gvZMevZ265JrMaLGo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 38, "extendedScore": null, "score": 1.053369331679239e-06, "legacy": true, "legacyId": "20548", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>A couple of days ago, Luke posted <a href=\"/lw/frp/train_philosophers_with_pearl_and_kahneman_not/\">a recommendation for reforming how philosophy is taught</a>. <a href=\"http://www.philosophy.illinois.edu/\">My department</a> at <a href=\"http://illinois.edu/\">the University of Illinois</a> is in the midst of some potentially large-scale changes.* Hence, now seems to be a great time to think about concrete steps towards reforming or partially reforming the curriculum in an actual philosophy department. I would appreciate some help thinking through how to make changes that will (a) improve the philosophy education of our undergraduates, (b) recruit and retain better students, (c) improve faculty experiences with teaching philosophy, and (d) be salable to the rest of the philosophy faculty. To some extent, this post is me thinking out loud through what I want to say to my department's curriculum committee (probably in January).</p>\n<p>&nbsp;</p>\n<h2 id=\"How_Things_Stand_Right_Now\">How Things Stand Right Now</h2>\n<p>In this section, I will try to lay out the situation as I see it right now.</p>\n<p>First, we have the following problem: philosophy courses that we offer are not sufficiently gated. In the mathematics department at my university, you can't take mathematical logic until you've taken a course called <a href=\"https://my.illinois.edu/uPortal/render.userLayoutRootNode.target.u41998l1n6.uP?pltc_target=210253.u41998l1n6&amp;pltc_type=RENDER&amp;pltp_action=catalogCourseView&amp;pltp_classNumber=347&amp;pltp_course=MATH&amp;pltp_term=spring&amp;pltp_year=2013#u41998l1n6\">Fundamental Mathematics</a>, which looks to be a class about proof techniques, mathematical induction, etc. And you can't take that until you've taken the second semester of calculus. Computer science, economics, physics, and most every other science curriculum works like this. If you want to take advanced courses, you have to pass through the <em>gates</em> of less advanced courses, which (theoretically, at least) prepare you for the material covered in the more advanced course.</p>\n<p>By contrast, in the philosophy department, you may take a senior-level (400 at my school) course after taking a freshman-level (100 at my school) introduction to philosophy. The result is that students who take our 400-level courses are typically unprepared. At least, that has been my experience. (Shockingly, many students taking 400-level classes then complain that they were expected to know things about philosophy!)&nbsp;A big part of the problem here is that we do not presently have enough faculty to cover intermediate-level courses on a regular enough basis, and let's be honest, faculty members don't usually want to teach lower-level courses anyway.</p>\n<p>Second, we have the following resource: our department currently has strong and growing connections with several world-class science or science-related departments. We have cross-appointed faculty and/or cross-listed courses with mathematics, linguistics, psychology, and physics, all of which are very strong departments.&nbsp;We have philosophy graduate students who do research and teach courses in these disciplines as well.&nbsp;And I am hoping to expand our connections to include computer science and statistics. I think there ought to be a good way to make use of these resources.</p>\n<p>Rather than trying to reform the <em>entire</em>&nbsp;philosophy curriculum all at once, I want to focus first on our logic offerings. What we have now is the following mess.</p>\n<p>\n</p><ul>\n<li>102 -- Introduction to Logic: A critical thinking course almost never taught by a faculty member.</li>\n<li>103 -- Quantitative Introduction to Logic: An introductory formal logic course taught by me about half the time</li>\n<li>202 -- Symbolic Logic: A basic symbolic logic course (unclear in how it is different from 103 except in that it is completely restricted to deductive logic)</li>\n<li>307 -- Elements of Semantics and Pragmatics: Cross-listed with linguistics</li>\n<li>407 -- Logic and Linguistics: Cross-listed with linguistics</li>\n<li>453 -- Formal Logic and Philosophy: An extension of 202 but with emphasis on philosophical issues</li>\n<li>454 -- Advanced Symbolic Logic: Basically, a math logic course covering completeness, compactness, Lowenheim-Skolem, incompleteness, and undecidability</li>\n</ul>\n<p></p>\n<p>The only pre-requisite for 453 and 454 is 202, and 202 has no pre-requisites at all; the pre-req for 407 is 307, and 307 depends on a 100-level linguistics course or (more commonly) consent of the instructor. We also have a 400-level philosophy of mathematics course.&nbsp;Along with these, the mathematics department has a 400-level mathematical logic course and a 400-level course on set theory and topology, neither of which is currently cross-listed, but both of which, I think, <em>should be</em> cross-listed as philosophy courses, which would also raise the bar for the philosophy students interested in logic by requiring that they take the calculus sequence and the fundamentals course.</p>\n<p>On the defects side, I think we have poor use of gating and spotty coverage of even deductive logic. For example, we have no courses on modal logics, we have no courses on intuitionist/constructivist logic, we have no courses on relevance logic, we have no courses on more exotic logics, we have no courses on set theory or category theory, and we have no courses on computation. We get sort of close to the last two in 453/454, and we might address them more directly by cross-listing with mathematics. But as it is, we do not do those things. And we have a <strong>huge gaping hole</strong>&nbsp;where inductive logic, probability theory, statistics, causal inference, and so on should be. On an individual level, that hole could be filled somewhat by taking courses in statistics; however, that is not quite the same as having courses available on confirmation theory or inductive logics.</p>\n<h2><br></h2>\n<h2 id=\"Recommendations\">Recommendations</h2>\n<p>So, now you know the basic situation ... what to do?</p>\n<p>Below are some specific recommendations that I want to make to my department's curriculum committee. I would really appreciate input on how to refine my recommendations, how to make them more palatable, and so on.</p>\n<p>First, we need to make the 100- and 200-level courses connect in a relevant way. I recommend entirely relabeling (and maybe even renumbering) 102 so that it is clear that it is a terminal, service course intended for non-majors. The course should stand to philosophy education as courses like <a href=\"https://my.illinois.edu/uPortal/render.userLayoutRootNode.target.u41998l1n6.uP?pltc_target=210253.u41998l1n6&amp;pltc_type=RENDER&amp;pltp_course=PHYS&amp;pltp_term=spring&amp;pltp_classNumber=123&amp;pltp_action=catalogCourseView&amp;pltp_year=2013#u41998l1n6\">Physics Made Easy</a>&nbsp;stands to physics education. I further recommend making a relabeled (and maybe renumbered) version of 103 a pre-requisite for all 200-level logic courses. In terms of material covered, ideally 103 would introduce symbolic conventions (to be made as standard as possible across the curriculum), proof skills, and basic ideas in model theory, set theory, and probability theory. (I go back and forth between liking this idea, which fits closely with how I teach 103 now, and wanting to do something more like, deductive logic in the first half and confirmation theory in the second half with no formal exposure to set theory. The biggest barrier to the second approach is in formally developing confirmation theory without set theory.) Then 202 could do more meta-logic, go into more detail on model theory, go into more detail on set theory, or whatever.</p>\n<p>Second, we need more courses covering inductive logic, probability theory, statistics, and so on. I recommend adding a 200-level course parallel to 202, which would cover some probability theory and some causal and statistical reasoning.&nbsp;Let's call this proposed course PHIL 204, since we don't offer anything under that number right now.&nbsp;I have in mind something slightly more advanced than CMU's Open Learning Initiative course <a href=\"https://oli.cmu.edu/jcourse/lms/students/syllabus.do?section=0baa507780020ca601df85439613bd8f\">here</a>.</p>\n<p>Third, I recommend expanding our 300-level course-offerings as follows. We need a second semester of inductive logic (etc.) that builds off of 204. And we need a course that does a simple survey of exotic logics, like modal logics, intuitionistic logic, relevance logic, free logic, etc. The 300-level survey of exotic logics need not be a pre-req for 400-level courses, provided the 400-level courses cover the same material that they have been covering. And that seems fine to me, although it might be in our long-term interests to drop our 454 in the event that we cross-list with mathematics on their math logic course. Depending on how their math logic course is taught, we might try to convince them that our 202 should satisfy the pre-req as effectively as their fundamentals course. (I don't know if that would be a hard sell or not.)</p>\n<p>Fourth, I recommend cross-listing some courses with mathematics and statistics to get more regular coverage of the tools we want our students to have without necessarily having our faculty teach those courses all the time.</p>\n<p>&nbsp;</p>\n<h2 id=\"What_Is_The_Goal_Here_\">What Is The Goal Here?</h2>\n<p>I haven't spent any time in this write-up thinking about the goal(s) of logic education in philosophy. I am not sure whether it would be worth backing up to address this question or whether there is sufficient implicit agreement about the value and goals of logic to leave it alone. If you think I should be saying something about the place of logic in the overall curriculum or making an argument for teaching logic at all or making an argument for understanding logic broadly enough to get probability and statistics through the door, please tell me and make a suggestion about how to develop the heading.</p>\n<p>&nbsp;</p>\n<p>--------------------------------------------------------------------------------------------------------------------</p>\n<p>*My department is currently much too small relative to the size of <a href=\"http://illinois.edu/\">my university</a>, but the powers that be have recently become receptive to our requests to expand (really, to replace a large number of retirements from the last five years). Hence, we are about to undergo an external review, which we hope will result in a plan for phased growth of the department to a little more than <em>twice</em> its current size by the end of the decade. (Yes, our numbers are seriously depleted!)</p>", "sections": [{"title": "How Things Stand Right Now", "anchor": "How_Things_Stand_Right_Now", "level": 1}, {"title": "Recommendations", "anchor": "Recommendations", "level": 1}, {"title": "What Is The Goal Here?", "anchor": "What_Is_The_Goal_Here_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "24 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["LcEzxX2FNTKbB6KXS"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-08T23:42:14.949Z", "modifiedAt": null, "url": null, "title": "Poll - Is endless September a threat to LW and what should be done?", "slug": "poll-is-endless-september-a-threat-to-lw-and-what-should-be", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:56.623Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Epiphany", "createdAt": "2012-08-12T03:33:21.256Z", "isAdmin": false, "displayName": "Epiphany"}, "userId": "BbbFp6hQzKF4YX8em", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yrKxWvjm6S3uWhSH6/poll-is-endless-september-a-threat-to-lw-and-what-should-be", "pageUrlRelative": "/posts/yrKxWvjm6S3uWhSH6/poll-is-endless-september-a-threat-to-lw-and-what-should-be", "linkUrl": "https://www.lesswrong.com/posts/yrKxWvjm6S3uWhSH6/poll-is-endless-september-a-threat-to-lw-and-what-should-be", "postedAtFormatted": "Saturday, December 8th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Poll%20-%20Is%20endless%20September%20a%20threat%20to%20LW%20and%20what%20should%20be%20done%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APoll%20-%20Is%20endless%20September%20a%20threat%20to%20LW%20and%20what%20should%20be%20done%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyrKxWvjm6S3uWhSH6%2Fpoll-is-endless-september-a-threat-to-lw-and-what-should-be%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Poll%20-%20Is%20endless%20September%20a%20threat%20to%20LW%20and%20what%20should%20be%20done%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyrKxWvjm6S3uWhSH6%2Fpoll-is-endless-september-a-threat-to-lw-and-what-should-be", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyrKxWvjm6S3uWhSH6%2Fpoll-is-endless-september-a-threat-to-lw-and-what-should-be", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 632, "htmlBody": "<p>Various people <a href=\"/lw/ecr/call_for_agreement_should_lesswrong_have_better/7bxs\">raised concerns</a> that growth might ruin the culture after reading my \"<a href=\"/lw/e5r/lesswrong_could_grow_a_lot_but_were_doing_it_wrong/\">LessWrong could grow a lot</a>\" thread.&nbsp; There has been some discussion about whether endless September, <a href=\"http://en.wikipedia.org/wiki/Endless_September\">a phenomenon that kills online discussion groups</a>, is a significant threat to LessWrong and <a href=\"/lw/ec2/preventing_discussion_from_being_watered_down_by/\">what can be done</a>.&nbsp; I really care about it, so I volunteered to code a solution myself for free if needed.&nbsp; Luke <a href=\"/lw/ec2/preventing_discussion_from_being_watered_down_by/7bry\">invited debate on the subject</a> (the debate is <a href=\"/lw/ecr/call_for_agreement_should_lesswrong_have_better/\">here</a>) and will be sent the results of this poll and asked to make a decision.&nbsp; It was suggested by him in an email that I wait a little while and then post my poll (meta threads are apparently annoying to some, so we let people cool off).&nbsp; Here it is, preceded by a Cliff's notes summary of the concerns.</p>\n<h2><br /></h2>\n<h2>Why this is worth your consideration:</h2>\n<p>&nbsp;- Yvain and I checked the IQ figures in the survey against other data this time, and the good news is that it's <a href=\"/lw/fp5/2012_survey_results/7xkg\">more believable that the average LessWronger is gifted</a>.&nbsp; The bad news is that LessWrong's IQ average has decreased on each survey.&nbsp; It can be argued that it's not decreasing by a lot or we don't have enough data, but if the data is good, <a href=\"/lw/fp5/2012_survey_results/7y17\">LessWrong's average has lost 52% of it's giftedness since March of 2009.</a></p>\n<p>&nbsp;- Eliezer documented the arrival of <a href=\"http://en.wikipedia.org/wiki/Poseur\">poseurs</a> (people who superficially copycat cultural behaviors - they are reported to over-run subcultures) which he termed \"<a href=\"/lw/1ww/undiscriminating_skepticism/\">Undiscriminating Skeptics</a>\".</p>\n<p>&nbsp;- Efforts to grow LessWrong could trigger an overwhelming deluge of newbies.</p>\n<p>&nbsp;- LessWrong registrations have been increasing fast and it's possible that growth could outstrip acculturation capacity. <a href=\"/lw/ec2/preventing_discussion_from_being_watered_down_by/\">(Chart here)</a></p>\n<p>&nbsp;- The Singularity Summit appears to cause a deluge of new users that may have similar effect to the September deluges of college freshman that endless September is named after.&nbsp; (<a href=\"/lw/ec2/preventing_discussion_from_being_watered_down_by/\">This chart</a> shows a spike correlated with the 2011 summit where 921 users joined that month, which is roughly equal to the total number of active users LW tends to have in a month if you go by the surveys or <a href=\"/lw/e4j/number_of_members_on_lesswrong/77xz\">Vladmir's wget</a>.)</p>\n<p>&nbsp;- A <a href=\"http://en.wikipedia.org/wiki/Slashdot_effect\">Slashdot effect</a> could result in a tsunami of new users if a publication with lots of readers like the Wall Street Journal (they used LessWrong data in <a href=\"/lw/ess/link_inside_the_cold_calculating_mind_of_lesswrong/\">this article</a>) decides to write an article on LessWrong.</p>\n<p>&nbsp;- The sequences contain a lot of the culture and are long meaning that \"TLDR\" may make LessWrong vulnerable to cultural disintegration.&nbsp; (New users may not know how detailed LW culture is or that the sequences contain so much culture.&nbsp; I didn't.)</p>\n<p>&nbsp;- Eliezer said in August that the site was \"<a href=\"/lw/eb9/meta_karma_for_last_30_days/7aub\">seriously going to hell</a>\" due to trolls.</p>\n<p>&nbsp;- <a href=\"/lw/ecr/call_for_agreement_should_lesswrong_have_better/7bxs\">A lot of people raised concerns.</a></p>\n<p>&nbsp;</p>\n<h2>Two Theories on How Online Cultures Die:<br /></h2>\n<p><br />&nbsp; <strong>Overwhelming user influx.</strong><br />&nbsp; There are too many new users to be acculturated by older members, so they form their own, larger new culture and dominate the group.</p>\n<p>&nbsp; <strong>Trending toward the mean.&nbsp; </strong><br />&nbsp; A group forms because people who are very different want a place to be different together.&nbsp; The group attracts more people that are closer to mainstream than people who are equally different because there are more mainstream people than different people.&nbsp; The larger group attracts people who are even less different in the original group's way for similar reasons.&nbsp; The original group is slowly overwhelmed by people who will never understand because they are too different.</p>\n<p>&nbsp;</p>\n<h2>Poll Link:</h2>\n<p><a href=\"/r/discussion/lw/fqj/poll_is_endless_september_a_threat_to_lw_and_what/808i\">Endless September Poll.</a></p>\n<h2><br /></h2>\n<h2>Request for Feedback:</h2>\n<p>In addition to constructive criticism, I'd also like the following:</p>\n<ul>\n<li>\n<p>Your observations of a decline or increase in quality, culture or enjoyment at LessWrong, if any.</p>\n</li>\n<li>\n<p>Ideas to protect the culture.</p>\n</li>\n<li>\n<p>Ideas for tracking cultural erosion.</p>\n</li>\n<li>Ways to test the ideas to protect the culture.</li>\n</ul>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yrKxWvjm6S3uWhSH6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 5, "extendedScore": null, "score": 1.6e-05, "legacy": true, "legacyId": "20395", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Various people <a href=\"/lw/ecr/call_for_agreement_should_lesswrong_have_better/7bxs\">raised concerns</a> that growth might ruin the culture after reading my \"<a href=\"/lw/e5r/lesswrong_could_grow_a_lot_but_were_doing_it_wrong/\">LessWrong could grow a lot</a>\" thread.&nbsp; There has been some discussion about whether endless September, <a href=\"http://en.wikipedia.org/wiki/Endless_September\">a phenomenon that kills online discussion groups</a>, is a significant threat to LessWrong and <a href=\"/lw/ec2/preventing_discussion_from_being_watered_down_by/\">what can be done</a>.&nbsp; I really care about it, so I volunteered to code a solution myself for free if needed.&nbsp; Luke <a href=\"/lw/ec2/preventing_discussion_from_being_watered_down_by/7bry\">invited debate on the subject</a> (the debate is <a href=\"/lw/ecr/call_for_agreement_should_lesswrong_have_better/\">here</a>) and will be sent the results of this poll and asked to make a decision.&nbsp; It was suggested by him in an email that I wait a little while and then post my poll (meta threads are apparently annoying to some, so we let people cool off).&nbsp; Here it is, preceded by a Cliff's notes summary of the concerns.</p>\n<h2><br></h2>\n<h2 id=\"Why_this_is_worth_your_consideration_\">Why this is worth your consideration:</h2>\n<p>&nbsp;- Yvain and I checked the IQ figures in the survey against other data this time, and the good news is that it's <a href=\"/lw/fp5/2012_survey_results/7xkg\">more believable that the average LessWronger is gifted</a>.&nbsp; The bad news is that LessWrong's IQ average has decreased on each survey.&nbsp; It can be argued that it's not decreasing by a lot or we don't have enough data, but if the data is good, <a href=\"/lw/fp5/2012_survey_results/7y17\">LessWrong's average has lost 52% of it's giftedness since March of 2009.</a></p>\n<p>&nbsp;- Eliezer documented the arrival of <a href=\"http://en.wikipedia.org/wiki/Poseur\">poseurs</a> (people who superficially copycat cultural behaviors - they are reported to over-run subcultures) which he termed \"<a href=\"/lw/1ww/undiscriminating_skepticism/\">Undiscriminating Skeptics</a>\".</p>\n<p>&nbsp;- Efforts to grow LessWrong could trigger an overwhelming deluge of newbies.</p>\n<p>&nbsp;- LessWrong registrations have been increasing fast and it's possible that growth could outstrip acculturation capacity. <a href=\"/lw/ec2/preventing_discussion_from_being_watered_down_by/\">(Chart here)</a></p>\n<p>&nbsp;- The Singularity Summit appears to cause a deluge of new users that may have similar effect to the September deluges of college freshman that endless September is named after.&nbsp; (<a href=\"/lw/ec2/preventing_discussion_from_being_watered_down_by/\">This chart</a> shows a spike correlated with the 2011 summit where 921 users joined that month, which is roughly equal to the total number of active users LW tends to have in a month if you go by the surveys or <a href=\"/lw/e4j/number_of_members_on_lesswrong/77xz\">Vladmir's wget</a>.)</p>\n<p>&nbsp;- A <a href=\"http://en.wikipedia.org/wiki/Slashdot_effect\">Slashdot effect</a> could result in a tsunami of new users if a publication with lots of readers like the Wall Street Journal (they used LessWrong data in <a href=\"/lw/ess/link_inside_the_cold_calculating_mind_of_lesswrong/\">this article</a>) decides to write an article on LessWrong.</p>\n<p>&nbsp;- The sequences contain a lot of the culture and are long meaning that \"TLDR\" may make LessWrong vulnerable to cultural disintegration.&nbsp; (New users may not know how detailed LW culture is or that the sequences contain so much culture.&nbsp; I didn't.)</p>\n<p>&nbsp;- Eliezer said in August that the site was \"<a href=\"/lw/eb9/meta_karma_for_last_30_days/7aub\">seriously going to hell</a>\" due to trolls.</p>\n<p>&nbsp;- <a href=\"/lw/ecr/call_for_agreement_should_lesswrong_have_better/7bxs\">A lot of people raised concerns.</a></p>\n<p>&nbsp;</p>\n<h2 id=\"Two_Theories_on_How_Online_Cultures_Die_\">Two Theories on How Online Cultures Die:<br></h2>\n<p><br>&nbsp; <strong>Overwhelming user influx.</strong><br>&nbsp; There are too many new users to be acculturated by older members, so they form their own, larger new culture and dominate the group.</p>\n<p>&nbsp; <strong>Trending toward the mean.&nbsp; </strong><br>&nbsp; A group forms because people who are very different want a place to be different together.&nbsp; The group attracts more people that are closer to mainstream than people who are equally different because there are more mainstream people than different people.&nbsp; The larger group attracts people who are even less different in the original group's way for similar reasons.&nbsp; The original group is slowly overwhelmed by people who will never understand because they are too different.</p>\n<p>&nbsp;</p>\n<h2 id=\"Poll_Link_\">Poll Link:</h2>\n<p><a href=\"/r/discussion/lw/fqj/poll_is_endless_september_a_threat_to_lw_and_what/808i\">Endless September Poll.</a></p>\n<h2><br></h2>\n<h2 id=\"Request_for_Feedback_\">Request for Feedback:</h2>\n<p>In addition to constructive criticism, I'd also like the following:</p>\n<ul>\n<li>\n<p>Your observations of a decline or increase in quality, culture or enjoyment at LessWrong, if any.</p>\n</li>\n<li>\n<p>Ideas to protect the culture.</p>\n</li>\n<li>\n<p>Ideas for tracking cultural erosion.</p>\n</li>\n<li>Ways to test the ideas to protect the culture.</li>\n</ul>\n<p>&nbsp;</p>", "sections": [{"title": "Why this is worth your consideration:", "anchor": "Why_this_is_worth_your_consideration_", "level": 1}, {"title": "Two Theories on How Online Cultures Die:", "anchor": "Two_Theories_on_How_Online_Cultures_Die_", "level": 1}, {"title": "Poll Link:", "anchor": "Poll_Link_", "level": 1}, {"title": "Request for Feedback:", "anchor": "Request_for_Feedback_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "262 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 262, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GEtLkHyPhG8m4aepb", "MLaSGq6A6bLTdt6r8", "48pAgPMKd63LA4QCZ", "Jko7pt7MwwTBrfG3A", "vAxxBCFDM2oCZ8DxT"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-09T04:43:58.029Z", "modifiedAt": null, "url": null, "title": "What information has surprised you most recently? ", "slug": "what-information-has-surprised-you-most-recently", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:33.749Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FiftyTwo", "createdAt": "2011-03-21T03:38:33.142Z", "isAdmin": false, "displayName": "FiftyTwo"}, "userId": "BZL3TWZXx2wyptxeJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/C9rcRfSiatbnycNXh/what-information-has-surprised-you-most-recently", "pageUrlRelative": "/posts/C9rcRfSiatbnycNXh/what-information-has-surprised-you-most-recently", "linkUrl": "https://www.lesswrong.com/posts/C9rcRfSiatbnycNXh/what-information-has-surprised-you-most-recently", "postedAtFormatted": "Sunday, December 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20information%20has%20surprised%20you%20most%20recently%3F%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20information%20has%20surprised%20you%20most%20recently%3F%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC9rcRfSiatbnycNXh%2Fwhat-information-has-surprised-you-most-recently%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20information%20has%20surprised%20you%20most%20recently%3F%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC9rcRfSiatbnycNXh%2Fwhat-information-has-surprised-you-most-recently", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FC9rcRfSiatbnycNXh%2Fwhat-information-has-surprised-you-most-recently", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 113, "htmlBody": "<p>Information that surprises you is interesting as it exposes where you have been miscalibrated, and allows you to correct for that.&nbsp;</p>\n<p>I suspect the users of LessWrong have fairly similar beliefs, so it is probable that information that has surprised you would surprise others here, so it would be useful for them if you shared them.&nbsp;</p>\n<p style=\"padding-left: 30px;\"><strong>Example</strong>: In a discussion with a friend recently I realised I had massively miscalibrated on the percentage of the UK population who shared my beliefs on certain subjects, in general the population was far more&nbsp;conservative&nbsp;than I had expected.</p>\n<p style=\"padding-left: 30px;\">In retrospect I was <a href=\"/lw/dr/generalizing_from_one_example/\">assuming my own personal experience was more representative than it was</a>, even when attempting to correct for that.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "C9rcRfSiatbnycNXh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 20, "extendedScore": null, "score": 1.0535758661558798e-06, "legacy": true, "legacyId": "20549", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 123, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["baTWMegR42PAsH9qJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-09T06:57:20.586Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Underconstrained Abstractions", "slug": "seq-rerun-underconstrained-abstractions", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bqxa5PJHQXTpbHjqj/seq-rerun-underconstrained-abstractions", "pageUrlRelative": "/posts/bqxa5PJHQXTpbHjqj/seq-rerun-underconstrained-abstractions", "linkUrl": "https://www.lesswrong.com/posts/bqxa5PJHQXTpbHjqj/seq-rerun-underconstrained-abstractions", "postedAtFormatted": "Sunday, December 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Underconstrained%20Abstractions&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Underconstrained%20Abstractions%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbqxa5PJHQXTpbHjqj%2Fseq-rerun-underconstrained-abstractions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Underconstrained%20Abstractions%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbqxa5PJHQXTpbHjqj%2Fseq-rerun-underconstrained-abstractions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbqxa5PJHQXTpbHjqj%2Fseq-rerun-underconstrained-abstractions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 178, "htmlBody": "<p>Today's post, <a href=\"/lw/wh/underconstrained_abstractions/\">Underconstrained Abstractions</a> was originally published on 04 December 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries#Underconstrained_Abstractions\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The problem with selecting abstractions is that for your data, there are probably lots of abstractions that fit the data equally well. In that case, we need some other way to decide which abstractions are useful.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fu5/seq_rerun_permitted_possibilities_locality/\">Permitted Possibilities, &amp; Locality</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bqxa5PJHQXTpbHjqj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 1.0536527108568684e-06, "legacy": true, "legacyId": "20550", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BpSEpGxtF664uRNkf", "qc3BckfTbxovNDp2a", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-09T12:15:42.385Z", "modifiedAt": null, "url": null, "title": "Participation in the LW Community Associated with Less Bias", "slug": "participation-in-the-lw-community-associated-with-less-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:29.621Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Unnamed", "createdAt": "2009-02-27T06:08:10.900Z", "isAdmin": false, "displayName": "Unnamed"}, "userId": "PdzQ73mN7S4SvRMhu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2odrQi8cxKvTu6v7s/participation-in-the-lw-community-associated-with-less-bias", "pageUrlRelative": "/posts/2odrQi8cxKvTu6v7s/participation-in-the-lw-community-associated-with-less-bias", "linkUrl": "https://www.lesswrong.com/posts/2odrQi8cxKvTu6v7s/participation-in-the-lw-community-associated-with-less-bias", "postedAtFormatted": "Sunday, December 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Participation%20in%20the%20LW%20Community%20Associated%20with%20Less%20Bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AParticipation%20in%20the%20LW%20Community%20Associated%20with%20Less%20Bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2odrQi8cxKvTu6v7s%2Fparticipation-in-the-lw-community-associated-with-less-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Participation%20in%20the%20LW%20Community%20Associated%20with%20Less%20Bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2odrQi8cxKvTu6v7s%2Fparticipation-in-the-lw-community-associated-with-less-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2odrQi8cxKvTu6v7s%2Fparticipation-in-the-lw-community-associated-with-less-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3521, "htmlBody": "<p><strong>Summary</strong></p>\n<p>CFAR included 5 questions on the <a href=\"/lw/fp5/2012_survey_results/\">2012 LW Survey</a> which were adapted from the heuristics and biases literature, based on five different cognitive biases or reasoning errors.&nbsp; LWers, on the whole, showed less bias than is typical in the published research (on all 4 questions where this was testable), but did show clear evidence of bias on 2-3 of those 4 questions.&nbsp; Further, those with closer ties to the LW community (e.g., those who had read more of the sequences) showed significantly less bias than those with weaker ties (on 3 out of 4-5 questions where that was testable).&nbsp; These results all held when controlling for measures of intelligence.<br /><br /><br /><strong>METHOD &amp; RESULTS</strong><br /><br />Being less susceptible to cognitive biases or reasoning errors is one sign of rationality (see the work of Keith Stanovich &amp; his colleagues, for example).&nbsp; You'd hope that a community dedicated to rationality would be less prone to these biases, so I selected 5 cognitive biases and reasoning errors from the heuristics &amp; biases literature to include on the LW survey.&nbsp; There are two possible patterns of results which would point in this direction:</p>\n<ul>\n<li>high scores: LWers show less bias than other populations that have answered these questions (like students at top universities)&nbsp; </li>\n<li>correlation with strength of LW exposure: those who have read the sequences (or have been around LW a long time, have high karma, attend meetups, make posts) score better than those who have not.&nbsp; </li>\n</ul>\n<p>The 5 biases were selected in part because they can be tested with everyone answering the same questions; I also preferred biases that haven't been discussed in detail on LW.&nbsp; On some questions there is a definitive wrong answer and on others there is reason to believe that a bias will tend to lead people towards one answer (so that, even though there might be good reasons for a person to choose that answer, in the aggregate it is evidence of bias if more people choose that answer).<br /><br />This is only one quick, rough survey.&nbsp; If the results are as predicted, that could be because LW makes people more rational, or because LW makes people more familiar with the heuristics &amp; biases literature (including how to avoid falling for the standard tricks used to test for biases), or because the people who are attracted to LW are already unusually rational (or just unusually good at avoiding standard biases).&nbsp; Susceptibility to standard biases is just one angle on rationality.&nbsp; Etc.<br /><br />Here are the question-by-question results, in brief.&nbsp; The next section contains the exact text of the questions, and more detailed explanations.<br /><br />Question 1 was a disjunctive reasoning task, which had a definitive correct answer.&nbsp; Only 13% of undergraduates got the answer right in the published paper that I took it from.&nbsp; 46% of LWers got it right, which is much better but still a very high error rate.&nbsp; Accuracy was 58% for those high in LW exposure vs. 31% for those low in LW exposure.&nbsp; So for this question, that's:&nbsp; <br />1. LWers biased: yes&nbsp; <br />2. LWers less biased than others: yes&nbsp; <br />3. Less bias with more LW exposure: yes&nbsp; <br /><br />Question 2 was a temporal discounting question; in the original paper about half the subjects chose money-now (which reflects a very high discount rate).&nbsp; Only 8% of LWers did; that did not leave much room for differences among LWers (and there was only a weak &amp; nonsignificant trend in the predicted direction). So for this question:&nbsp; <br />1. LWers biased: not really&nbsp; <br />2. LWers less biased than others: yes&nbsp; <br />3. Less bias with more LW exposure: n/a (or no)&nbsp; <br /><br />Question 3 was about the law of large numbers.&nbsp; Only 22% got it right in Tversky &amp; Kahneman's original paper. 84% of LWers did: 93% of those high in LW exposure, 75% of those low in LW exposure.&nbsp; So:&nbsp; <br />1. LWers biased: a bit&nbsp; <br />2. LWers less biased than others: yes&nbsp; <br />3. Less bias with more LW exposure: yes&nbsp; <br /><br />Question 4 was based on the decoy effect aka asymmetric dominance aka attraction effect (but missing a control condition).&nbsp; I don't have numbers from the original study (and there is no correct answer) so I can't really answer 1 or 2 for this question, but there was a difference based on LW exposure: 57% vs. 44% selecting the less bias related answer.&nbsp; <br />1. LWers biased: n/a&nbsp; <br />2. LWers less biased than others: n/a&nbsp; <br />3. Less bias with more LW exposure: yes&nbsp; <br /><br />Question 5 was an anchoring question.&nbsp; The original study found an effect (measured by slope) of 0.55 (though it was less transparent about the randomness of the anchor; transparent studies w. other questions have found effects around 0.3 on average).&nbsp; For LWers there was a significant anchoring effect but it was only 0.14 in magnitude, and it did not vary based on LW exposure (there was a weak &amp; nonsignificant trend in the wrong direction).&nbsp; <br />1. LWers biased: yes&nbsp; <br />2. LWers less biased than others: yes&nbsp; <br />3. Less bias with more LW exposure: no&nbsp; <br /><br />One thing you might wonder: how much of this is just intelligence?&nbsp; There were several questions on the survey about performance on IQ tests or SATs.&nbsp; Controlling for scores on those tests, all of the results about the effects of LW exposure held up nearly as strongly.&nbsp; Intelligence test scores were also predictive of lower bias, independent of LW exposure, and those two relationships were almost the same in magnitude.&nbsp; If we extrapolate the relationship between IQ scores and the 5 biases to someone with an IQ of 100 (on either of the 2 IQ measures), they are still less biased than the participants in the original study, which suggests that the \"LWers less biased than others\" effect is not based solely on IQ.</p>\n<p>&nbsp;</p>\n<p><strong>MORE DETAILED RESULTS</strong><br /><br />There were 5 questions related to strength of membership in the LW community which I standardized and combined into a single composite measure of LW exposure (LW use, sequence reading, time in community, karma, meetup attendance); this was the main predictor variable I used (time per day on LW also seems related, but I found out while <a href=\"/lw/8p4/2011_survey_results/5dzg\">analyzing last year's survey</a> that it doesn't hang together with the others or associate the same way with other variables).&nbsp; I analyzed the results using a continuous measure of LW exposure, but to simplify reporting, I'll give the results below by comparing those in the top third on this measure of LW exposure with those in the bottom third.<br /><br />There were 5 intelligence-related measures which I combined into a single composite measure of Intelligence (SAT out of 2400, SAT out of 1600, ACT, previously-tested IQ, extra credit IQ test); I used this to control for intelligence and to compare the effects of LW exposure with the effects of Intelligence (for the latter, I did a similar split into thirds).&nbsp; Sample sizes: 1101 people answered at least one of the CFAR questions; 1099 of those answered at least one LW exposure question and 835 of those answered at least one of the Intelligence questions.&nbsp; Further details about method available on request.<br /><br />Here are the results, question by question.<br /><br /><strong>Question 1</strong>: <em>Jack is looking at Anne, but Anne is looking at George. Jack is married but George is not. Is a married person looking at an unmarried person? <br /></em></p>\n<ul>\n<li><em>Yes&nbsp; </em></li>\n<li><em>No&nbsp; </em></li>\n<li><em>Cannot be determined&nbsp; </em></li>\n</ul>\n<p>This is a \"disjunctive reasoning\" question, which means that getting the correct answer requires using \"or\".&nbsp; That is, it requires considering multiple scenarios.&nbsp; In this case, either Anne is married or Anne is unmarried.&nbsp; If Anne is married then married Anne is looking at unmarried George; if Anne is unmarried then married Jack is looking at unmarried Anne.&nbsp; So the correct answer is \"yes\".&nbsp; A study by Toplak &amp; Stanovich (2002) of students at a large Canadian university found that only 13% correctly answered \"yes\" while 86% answered \"cannot be determined\" (2% answered \"no\").<br /><br />On this LW survey, 46% of participants correctly answered \"yes\"; 54% chose \"cannot be determined\" (and 0.4% said\"no\").&nbsp; Further, correct answers were much more common among those high in LW exposure: 58% of those in the top third of LW exposure answered \"yes\", vs. only 31% of those in the bottom third.&nbsp; The effect remains nearly as big after controlling for Intelligence (the gap between the top third and the bottom third shrinks from 27% to 24% when Intelligence is included as a covariate).&nbsp; The effect of LW exposure is very close in magnitude to the effect of Intelligence; 60% of those in the top third in Intelligence answered correctly vs. 37% of those in the bottom third.<br /><br />original study: 13%&nbsp; <br />weakly-tied LWers: 31%&nbsp; <br />strongly-tied LWers: 58%&nbsp; <br /><br /><br /><strong>Question 2</strong>: <em>Would you prefer to receive $55 today or $75 in 60 days?</em><br /><br />This is a temporal discounting question.&nbsp; Preferring $55 today implies an extremely (and, for most people, implausibly) high discount rate, is often indicative of <a href=\"http://en.wikipedia.org/wiki/Hyperbolic_discounting\">a pattern of discounting</a> that involves preference reversals, and is correlated with other biases.&nbsp; The question was used in a study by Kirby (2009) of undergraduates at Williams College (with a delay of 61 days instead of 60; I took it from a secondary source that said \"60\" without checking the original), and based on the graph of parameter values in that paper it looks like just under half of participants chose the larger later option of $75 in 61 days.<br /><br />LW survey participants almost uniformly showed a low discount rate: 92% chose $75 in 61 days.&nbsp; This is near ceiling, which didn't leave much room for differences among LWers.&nbsp; For LW exposure, top third vs. bottom third was 93% vs. 90%, and this relationship was not statistically significant (p=.15); for Intelligence it was 96% vs. 91% and the relationship was statistically significant (p=.007).&nbsp; (EDITED: I originally described the Intelligence result as nonsignificant.) <br /><br />original study: ~47%&nbsp; <br />weakly-tied LWers: 90%&nbsp; <br />strongly-tied LWers: 93%&nbsp; <br /><br /><br /><strong>Question 3</strong>: <em>A certain town is served by two hospitals. In the larger hospital, about 45 babies are born each day. In the smaller one, about 15 babies are born each day. Although the overall proportion of girls is about 50%, the actual proportion at either hospital may be greater or less on any day. At the end of a year, which hospital will have the greater number of days on which more than 60% of the babies born were girls?&nbsp; <br /></em></p>\n<ul>\n<li><em>The larger hospital&nbsp; </em></li>\n<li><em>The smaller hospital&nbsp; </em></li>\n<li><em>Neither - the number of these days will be about the same&nbsp; </em></li>\n</ul>\n<p>This is a statistical reasoning question, which requires applying the law of large numbers.&nbsp; In Tversky &amp; Kahneman's (1974) original paper, only 22% of participants correctly chose the smaller hospital; 57% said \"about the same\" and 22% chose the larger hospital.<br /><br />On the LW survey, 84% of people correctly chose the smaller hospital; 15% said \"about the same\" and only 1% chose the larger hospital.&nbsp; Further, this was strongly correlated with strength of LW exposure: 93% of those in the top third answered correctly vs. 75% of those in the bottom third.&nbsp; As with #1, controlling for Intelligence barely changed this gap (shrinking it from 18% to 16%), and the measure of Intelligence produced a similarly sized gap: 90% for the top third vs. 79% for the bottom third.<br /><br />original study: 22%&nbsp; <br />weakly-tied LWers: 75%&nbsp; <br />strongly-tied LWers: 93%</p>\n<p><br /><strong>Question 4</strong>: <em>Imagine that you are a doctor, and one of your patients suffers from migraine headaches that last about 3 hours and involve intense pain, nausea, dizziness, and hyper-sensitivity to bright lights and loud noises. The patient usually needs to lie quietly in a dark room until the headache passes. This patient has a migraine headache about 100 times each year. You are considering three medications that you could prescribe for this patient. The medications have similar side effects, but differ in effectiveness and cost. The patient has a low income and must pay the cost because her insurance plan does not cover any of these medications. Which medication would you be most likely to recommend?&nbsp; <br /></em></p>\n<ul>\n<li><em>Drug A: reduces the number of headaches per year from 100 to 30. It costs $350 per year.&nbsp; </em></li>\n<li><em>Drug B: reduces the number of headaches per year from 100 to 50. It costs $100 per year.&nbsp; </em></li>\n<li><em>Drug C: reduces the number of headaches per year from 100 to 60. It costs $100 per year.&nbsp; </em></li>\n</ul>\n<p>This question is based on research on the <a href=\"http://en.wikipedia.org/wiki/Decoy_effect\">decoy effect</a> (aka \"asymmetric dominance\" or the \"attraction effect\").&nbsp; Drug C is obviously worse than Drug B (it is strictly dominated by it) but it is not obviously worse than Drug A, which tends to make B look more attractive by comparison.&nbsp; This is normally tested by comparing responses to the three-option question with a control group that gets a two-option question (removing option C), but I cut a corner and only included the three-option question.&nbsp; The assumption is that more-biased people would make similar choices to unbiased people in the two-option question, and would be more likely to choose Drug B on the three-option question.&nbsp; The model behind that assumption is that there are various reasons for choosing Drug A and Drug B; the three-option question gives biased people one more reason to choose Drug B but other than that the reasons are the same (on average) for more-biased people and unbiased people (and for the three-option question and the two-option question).<br /><br />Based on the discussion on the original survey thread, this assumption might not be correct.&nbsp; Cost-benefit reasoning seems to favor Drug A (and those with more LW exposure or higher intelligence might be more likely to run the numbers).&nbsp; Part of the problem is that I didn't update the costs for inflation - the original problem appears to be from 1995 which means that the real price difference was over 1.5 times as big then.<br /><br />I don't know the results from the original study; I found this particular example online (and edited it heavily for length) with a reference to Chapman &amp; Malik (1995), but after looking for that paper I see that it's listed on Chapman's CV as only a \"published abstract\".<br /><br />49% of LWers chose Drug A (the one that is more likely for unbiased reasoners), vs. 50% for Drug B (which benefits from the decoy effect) and 1% for Drug C (the decoy).&nbsp; There was a strong effect of LW exposure: 57% of those in the top third chose Drug A vs. only 44% of those in the bottom third.&nbsp; Again, this gap remained nearly the same when controlling for Intelligence (shrinking from 14% to 13%), and differences in Intelligence were associated with a similarly sized effect: 59% for the top third vs. 44% for the bottom third.<br /><br />original study: ??&nbsp; <br />weakly-tied LWers: 44%&nbsp; <br />strongly-tied LWers: 57%&nbsp; <br /><br /><br /><strong>Question 5</strong>: <em>Get a random three digit number (000-999) from http://goo.gl/x45un and enter the number here.&nbsp; <br /><br />Treat the three digit number that you just wrote down as a length, in feet. Is the height of the tallest redwood tree in the world more or less than the number that you wrote down?&nbsp; <br /><br />What is your best guess about the height of the tallest redwood tree in the world (in feet)? </em><br /><br />This is an anchoring question; if there are anchoring effects then people's responses will be positively correlated with the random number they were given (and a regression analysis can estimate the size of the effect to compare with published results, which used two groups instead of a random number).<br /><br />Asking a question with the answer in feet was a mistake which generated a great deal of controversy and discussion.&nbsp; Dealing with unfamiliar units could interfere with answers in various ways so the safest approach is to look at only the US respondents; I'll also see if there are interaction effects based on country.<br /><br />The <a href=\"http://www.psychologyofgames.com/2012/06/the-psychology-of-diablo-iii-loot-part-1-anchoring-the-auction-house/\">question</a> is from a paper by Jacowitz &amp; Kahneman (1995), who provided anchors of 180 ft. and 1200 ft. to two groups and found mean estimates of 282 ft. and 844 ft., respectively.&nbsp; One natural way of expressing the strength of an anchoring effect is as a slope (change in estimates divided by change in anchor values), which in this case is 562/1020 = 0.55.&nbsp; However, that study did not explicitly lead participants through the randomization process like the LW survey did.&nbsp; The classic Tversky &amp; Kahneman (1974) <a href=\"/lw/j7/anchoring_and_adjustment/\">anchoring question</a> did use an explicit randomization procedure (spinning a wheel of fortune; though it was actually rigged to create two groups) and found a slope of 0.36.&nbsp; Similarly, several studies by Ariely &amp; colleagues (2003) which used the participant's Social Security number to explicitly randomize the anchor value found slopes averaging about 0.28.<br /><br />There was a significant anchoring effect among US LWers (n=578), but it was much weaker, with a slope of only 0.14 (p=.0025).&nbsp; That means that getting a random number that is 100 higher led to estimates that were 14 ft. higher, on average.&nbsp; LW exposure did not moderate this effect (p=.88); looking at the pattern of results, if anything the anchoring effect was slightly higher among the top third (slope of 0.17) than among the bottom third (slope of 0.09). Intelligence did not moderate the results either (slope of 0.12 for both the top third and bottom third).&nbsp; It's not relevant to this analysis, but in case you're curious, the median estimate was 350 ft. and the <a href=\"http://en.wikipedia.org/wiki/Sequoia_sempervirens#Statistics\">actual answer</a> is 379.3 ft. (115.6 meters).<br /><br />Among non-US LWers (n=397), the anchoring effect was slightly smaller in magnitude compared with US LWers (slope of 0.08), and not significantly different from the US LWers or from zero.<br /><br />original study: slope of 0.55 (0.36 and 0.28 in similar studies)&nbsp; <br />weakly-tied LWers: slope of 0.09&nbsp; <br />strongly-tied LWers: slope of 0.17&nbsp; <br /><br /><br />If we break the LW exposure variable down into its 5 components, every one of the five is strongly predictive of lower susceptibility to bias.&nbsp; We can combine the first four CFAR questions into a composite measure of unbiasedness, by taking the percentage of questions on which a person gave the \"correct\" answer (the answer suggestive of lower bias).&nbsp; Each component of LW exposure is correlated with lower bias on that measure, with r ranging from 0.18 (meetup attendance) to 0.23 (LW use), all p &lt; .0001 (time per day on LW is uncorrelated with unbiasedness, r=0.03, p=.39).&nbsp; For the composite LW exposure variable the correlation is 0.28; another way to express this relationship is that people one standard deviation above average on LW exposure 75% of CFAR questions \"correct\" while those one standard deviation below average got 61% \"correct\".&nbsp; Alternatively, focusing on sequence-reading, the accuracy rates were:<br /><br />75%&nbsp;&nbsp;&nbsp; Nearly all of the Sequences (n = 302)&nbsp; <br />70%&nbsp;&nbsp;&nbsp; About 75% of the Sequences (n = 186)&nbsp; <br />67%&nbsp;&nbsp;&nbsp; About 50% of the Sequences (n = 156)&nbsp; <br />64%&nbsp;&nbsp;&nbsp; About 25% of the Sequences (n = 137)&nbsp; <br />64%&nbsp;&nbsp;&nbsp; Some, but less than 25% (n = 210)&nbsp; <br />62%&nbsp;&nbsp;&nbsp; Know they existed, but never looked at them (n = 19)&nbsp; <br />57%&nbsp;&nbsp;&nbsp; Never even knew they existed until this moment (n = 89)&nbsp; <br /><br />Another way to summarize is that, on 4 of the 5 questions (all but question 4 on the decoy effect) we can make comparisons to the results of previous research, and in all 4 cases LWers were much less susceptible to the bias or reasoning error.&nbsp; On 1 of the 5 questions (question 2 on temporal discounting) there was a ceiling effect which made it extremely difficult to find differences within LWers; on 3 of the other 4 LWers with a strong connection to the LW community were much less susceptible to the bias or reasoning error than those with weaker ties.<br /><br /><br /><strong>REFERENCES</strong><br />Ariely, Loewenstein, &amp; Prelec (2003), \"Coherent Arbitrariness: Stable demand curves without stable preferences\"&nbsp; <br />Chapman &amp; Malik (1995), \"The attraction effect in prescribing decisions and consumer choice\"&nbsp; <br />Jacowitz &amp; Kahneman (1995), \"Measures of Anchoring in Estimation Tasks\"&nbsp; <br />Kirby (2009), \"One-year temporal stability of delay-discount rates\"&nbsp; <br />Toplak &amp; Stanovich (2002), \"The Domain Specificity and Generality of Disjunctive Reasoning: Searching for a Generalizable Critical Thinking Skill\"&nbsp; <br />Tversky &amp; Kahneman's (1974), \"Judgment under Uncertainty: Heuristics and Biases\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1, "kJrjorSx3hXa7q7CJ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2odrQi8cxKvTu6v7s", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 56, "extendedScore": null, "score": 0.000142, "legacy": true, "legacyId": "20551", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 56, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 50, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["x9FNKTEt68Rz6wQ6P", "bMkCEZoBNhgRBtzoj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-09T13:28:16.278Z", "modifiedAt": null, "url": null, "title": "Lifeism in the midst of death", "slug": "lifeism-in-the-midst-of-death", "viewCount": null, "lastCommentedAt": "2021-06-25T22:49:22.643Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TobyBartels", "createdAt": "2010-07-22T02:29:20.470Z", "isAdmin": false, "displayName": "TobyBartels"}, "userId": "ZS9o62ou6aQMDj5yn", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PDWK7SAD2GYMnNsDY/lifeism-in-the-midst-of-death", "pageUrlRelative": "/posts/PDWK7SAD2GYMnNsDY/lifeism-in-the-midst-of-death", "linkUrl": "https://www.lesswrong.com/posts/PDWK7SAD2GYMnNsDY/lifeism-in-the-midst-of-death", "postedAtFormatted": "Sunday, December 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Lifeism%20in%20the%20midst%20of%20death&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALifeism%20in%20the%20midst%20of%20death%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPDWK7SAD2GYMnNsDY%2Flifeism-in-the-midst-of-death%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Lifeism%20in%20the%20midst%20of%20death%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPDWK7SAD2GYMnNsDY%2Flifeism-in-the-midst-of-death", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPDWK7SAD2GYMnNsDY%2Flifeism-in-the-midst-of-death", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1110, "htmlBody": "<p style=\"padding-left: 30px;\">tl;dr:&nbsp; My grandpa died, and I gave a eulogy with a mildly anti-deathist message, in a Catholic funeral service that was mostly pretty disagreeable.</p>\n<p>I'm a little uncomfortable writing this post, because it's very personal, and I'm not exactly a regular with friends here.&nbsp; But I need to get it out, and I don't know any other place to put it.</p>\n<p>My grandfather (one of two) died last week, and there was a funeral mass (Catholic) today.&nbsp; Although a &lsquo;pro-life&rsquo; organisation, the Roman Catholic Church has a very deathist funeral liturgy.&nbsp; It wasn't just &lsquo;Stanley has gone on to a better place&rsquo;, and all that; the priest had the gall to say that Grandpa had probably done everything that he wanted to do in life, so it was OK for him to die now.&nbsp; I know from discussions with my mother and my aunt that Grandpa did <em>not</em> want to die now; although his life and health were not what they used to be, he was happy to live.&nbsp; Yes, he had gone to his great-granddaughter's second birthday party, but he wanted to go to her third, and that will never happen.</p>\n<p>There are four of us grandchildren, two (not including me) with spouses.&nbsp; At first, it was suggested that each of us six say one of the <a href=\"http://www.mayjesuschristbepraised.com/index.cfm?load=page&amp;page=277\">Prayers of the Faithful</a> (which are flexible).&nbsp; Mom thought that I might find one that I was willing to recite, so I looked them up online.&nbsp; It wasn't so bad that they end with &lsquo;We pray to the Lord.&rsquo; recited by the congregation; I would normally remain silent during that, but I decided that I could say it, and even lead others in saying it, pro forma.&nbsp; And I could endorse the content of some (at least #6 from that list) with some moderate edits.&nbsp; But overall, the whole thing was very disturbing to me.&nbsp; (I had to read <a href=\"http://hpmor.com/chapter/45\">HPMoR 45</a> afterwards to get rid of the bad taste.)&nbsp; I told Mom &lsquo;This is a part of the Mass where I would normally remain in respectful silence.&rsquo;, and she apologised for &lsquo;put[ting] [me] in an uncomfortable position&rsquo; (to quote from our text messages).&nbsp; In the end, the two grandchildren-in-law were assigned to say these prayers.</p>\n<p>But we grandchildren still had a place in the programme; we would give eulogies.&nbsp; So I had to think about what to say.&nbsp; I was never close to Grandpa; I loved him well enough, but we didn't have much in common.&nbsp; I tried to think about what I remembered about him and what I would want to tell people about him.&nbsp; It was a little overwhelming; in the end, I read my sibling's notes and decided to discuss only what she did not plan to discuss, and that narrowed it down enough.&nbsp; So then I knew what I wanted to say about Grandpa.</p>\n<p>But I wanted to say something more.&nbsp; I wanted to say something to counter the idea that Grandpa's death was OK.&nbsp; I didn't yet know how appalling the priest's sermon would be, but I knew that there would be a lot of excuses made for death.&nbsp; I wanted to preach &lsquo;Grandpa should not have died.&rsquo; and go on from there, but I knew that this would be disturbing to people who wanted comfort from their grief, and a lecture on death would not really be a eulogy.&nbsp; Still, I wanted to say something.</p>\n<p>(I also didn't want to say anything that could be interpreted as critical of the decision to remove life support.&nbsp; I wasn't consulted on that decision, but under the circumstances, I agree with it.&nbsp; As far as I'm concerned, he was killed on Monday, even though he didn't finally die until Wednesday.&nbsp; In the same conversation in which Mom and I talked about how Grandpa wanted to live, we talked about how he didn't want to live under the circumstances under which he was living on Tuesday, conditions which his doctors expected would never improve.&nbsp; Pulling the plug was the best option available in a bad situation.)</p>\n<p>Enough background; here is my eulogy.&nbsp; Some of this is paraphrase, since my written notes were only an outline.</p>\n<p style=\"padding-left: 30px;\">When I was young, we would visit my grandparents every year, for Thanksgiving or Christmas.&nbsp; Grandma and Grandpa would greet us at the door with hugs and kisses.&nbsp; The first thing that I remember about their place was the candy.&nbsp; Although I didn't realise it at the time, they didn't eat it; it was there as a gift for us kids.</p>\n<p style=\"padding-left: 30px;\">Later I noticed the books that they had, on all topics: religion, history, humour, science fiction, technical material.&nbsp; Most of it was older than I was used to reading, and I found it fascinating.&nbsp; All of this was open to me, and sometimes I would ask Grandpa about some of it; but mostly I just read his books, and to a large extent, this was his influence on me.</p>\n<p style=\"padding-left: 30px;\">Grandpa was a chemical engineer, although he was retired by the time I was able to appreciate that, and this explains the technical material, and to some extent the science fiction.&nbsp; Even that science fiction mostly took death for granted; but Grandpa was with us as long as he was because of the chemists and other people who studied medicine and the arts of healing.&nbsp; They helped him to stay healthy and happy until the heart attack that ended his life.</p>\n<p style=\"padding-left: 30px;\">So, I thank them for what they did for Grandpa, and I wish them success in their future work, to help other people live longer and better, until we never have to go through this again.</p>\n<p>I was working on this until the ceremony began, and I even edited it a little in the pew.&nbsp; I wasn't sure until I got up to the podium how strong to make the ending.&nbsp; Ultimately, I said something that could be interpreted as a reference to the Second Coming, but Catholics are not big on that, and my family knows that I don't believe in it.&nbsp; So I don't know how the church officials and Grandpa's personal friends interpreted it, but it could only mean transhumanism to my family.</p>\n<p>Nobody said anything, positive or negative, afterwards.&nbsp; Well, a couple of people said that my eulogy was well done; but without specifics, it sounded like they were just trying to make me feel good, to comfort my grief.&nbsp; After my speech, the other three grandchildren went, and then the priest said more pleasant falsehoods, and then it was over.</p>\n<p>Goodbye, Grandpa.&nbsp; I wish that you were alive and happy in Heaven, but at least you were alive and happy here on Earth for a while.&nbsp; I'll miss you.</p>\n<p>[Edit:&nbsp; Fix my cousin's age.]</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NSMKfa8emSbGNXRKD": 1, "t7t9nW6BtJhfGNSR6": 1, "LDTSbmXtokYAsEq8e": 1, "E9ihK6bA9YKkmJs2f": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PDWK7SAD2GYMnNsDY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 74, "baseScore": 97, "extendedScore": null, "score": 0.000228, "legacy": true, "legacyId": "20552", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 97, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-09T21:16:42.564Z", "modifiedAt": null, "url": null, "title": "My workflow", "slug": "my-workflow", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:03.398Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "paulfchristiano", "createdAt": "2010-07-28T17:04:08.586Z", "isAdmin": false, "displayName": "paulfchristiano"}, "userId": "gb44edJjXhte8DA3A", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/E4gud47NNqgtsEeEr/my-workflow", "pageUrlRelative": "/posts/E4gud47NNqgtsEeEr/my-workflow", "linkUrl": "https://www.lesswrong.com/posts/E4gud47NNqgtsEeEr/my-workflow", "postedAtFormatted": "Sunday, December 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20My%20workflow&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMy%20workflow%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE4gud47NNqgtsEeEr%2Fmy-workflow%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=My%20workflow%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE4gud47NNqgtsEeEr%2Fmy-workflow", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE4gud47NNqgtsEeEr%2Fmy-workflow", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 726, "htmlBody": "<p>&nbsp;</p>\n<p class=\"p1\">Over the last 6 months I've started doing a lot of things differently. Some of these changes seem to have increased my work output a good bit and made me happier. I normally hesitate to share habits, but I'm pretty happy with these in particular, and even if they will work for only a few people I think they are worth sharing. Most of the habits I've adopted are fairly common, but I hope I can help people anyway by identifying the habits that have most helped me.</p>\n<p class=\"p1\">I'm curious to hear about alternatives that have worked for you.&nbsp;</p>\n<p>&nbsp;</p>\n<h3><a href=\"http://workflowy.com\">Workflowy</a>:&nbsp;</h3>\n<p><span style=\"font-weight: normal;\">Workflowy lets you edit a single collapsible outline. I use it very extensively. It is much more convenient than the network of google docs it replaced, and I use it much more often. It is much like other outliners, but (1) has a slicker interface, (2) works offline, (3) lets you recurse on and share sublists.</span></p>\n<p>Workflowy is free to try but costs $5 a month. This may seem expensive for what it does, but if you use (or could use!) outliners a lot this is not enough to matter. After some searching Workflowy seems like the best option. I'm sure I like Workflowy more than most people, but I <em>really</em> like it, so I think it's worth trying.</p>\n<p><a href=\"https://workflowy.com/shared/4e036a22-b65a-b1dd-50bc-774dc516e13a/\">Here</a> is a skeleton of my workflowy list, which hosts many of the other systems in this post.</p>\n<h3>Checklists:</h3>\n<p>I have a checklist of tasks to do each night before sleeping. In the past I would often forget one of these things; putting them in a checklist helps me do them more reliably and makes me more relaxed.&nbsp;</p>\n<p>Checklists for other occasions, particularly waking up and traveling, are also helpful, but are much less important to me.&nbsp;</p>\n<h3>Todo lists:</h3>\n<p>I now maintain two todo lists: one with a list of tasks for each upcoming day, and one with a list of tasks for future events (\"I'm in the UK,\" \"it is Thursday,\" \"I'm going grocery shopping\"). Whenever I think of something I should do, I either put it under a future day and do it when that day arrives, or I put it with an associated event. Each night I check both lists and decide what to do tomorrow.&nbsp;</p>\n<h3><a href=\"http://www.beeminder.com\">Beeminder</a>:</h3>\n<p>Beeminder is a service that holds you to commitments and tracks your progress. It has helped me a lot over the last months. I've experimented with a few different commitments, but two have been most useful: following a daily routine, and doing a minimum amount of work each day (on average). Beeminder has pretty low overhead.</p>\n<h3>Reflection:</h3>\n<p>I spend about 10% of my productive time reflecting on how things have been going and what I should do differently. I benefit from producing concrete possible changes each time I sit down to think. I realized how important this is for me recently; since I've started doing it more reliably, I have gotten a lot more out of reflection.</p>\n<h3>Pomodoro:</h3>\n<p>I do my work in uninterrupted blocks of 20 minutes, punctuated by 2-3 minute breaks. This is my bastardized, minimalist version of the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Pomodoro_Technique\">pomodoro technique</a>, which I arrived at by trial and error. I use <a href=\"https://itunes.apple.com/us/app/alinof-timer/id512464723?mt=12\">Alinof timer</a>, which was recommended to me by a friend.&nbsp;</p>\n<h3>Calendar:</h3>\n<p>I now record commitments on my calendar reliably and check it each night. I failed to do this for 6 months after finishing my undergraduate degree, which I think was a serious mistake. I became much more reliable at checking my calendar after adopting a daily checklist.</p>\n<h3>Time Logging:</h3>\n<p>Whenever I start a new activity, I write down the current time and a description of what I just stopped doing. At the end of the day I spend a few minutes reading this log and estimating how much time I spent on each activity. This makes me more attentive to time during the day, helps me remember what I did throughout the day, and frees up attention. Sometimes I use the logs to try and notice trends. For example, I've been exercising on random days and measuring how this affects my time. I don't yet know if this helps at all.</p>\n<h3><a href=\"https://catch.com/m/\">Catch</a>:</h3>\n<p>Catch is a note-taking app. It is very minimal, and lets you record a voice note by pressing a single button. It has substantially increased my affordance for taking notes during the day, which I use to remember todo items and help with time logging.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"TkZ7MFwCi4D63LJ5n": 1, "udPbn9RthmgTtHMiG": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "E4gud47NNqgtsEeEr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 63, "extendedScore": null, "score": 0.000209, "legacy": true, "legacyId": "20553", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 43, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>&nbsp;</p>\n<p class=\"p1\">Over the last 6 months I've started doing a lot of things differently. Some of these changes seem to have increased my work output a good bit and made me happier. I normally hesitate to share habits, but I'm pretty happy with these in particular, and even if they will work for only a few people I think they are worth sharing. Most of the habits I've adopted are fairly common, but I hope I can help people anyway by identifying the habits that have most helped me.</p>\n<p class=\"p1\">I'm curious to hear about alternatives that have worked for you.&nbsp;</p>\n<p>&nbsp;</p>\n<h3 id=\"Workflowy__\"><a href=\"http://workflowy.com\">Workflowy</a>:&nbsp;</h3>\n<p><span style=\"font-weight: normal;\">Workflowy lets you edit a single collapsible outline. I use it very extensively. It is much more convenient than the network of google docs it replaced, and I use it much more often. It is much like other outliners, but (1) has a slicker interface, (2) works offline, (3) lets you recurse on and share sublists.</span></p>\n<p>Workflowy is free to try but costs $5 a month. This may seem expensive for what it does, but if you use (or could use!) outliners a lot this is not enough to matter. After some searching Workflowy seems like the best option. I'm sure I like Workflowy more than most people, but I <em>really</em> like it, so I think it's worth trying.</p>\n<p><a href=\"https://workflowy.com/shared/4e036a22-b65a-b1dd-50bc-774dc516e13a/\">Here</a> is a skeleton of my workflowy list, which hosts many of the other systems in this post.</p>\n<h3 id=\"Checklists_\">Checklists:</h3>\n<p>I have a checklist of tasks to do each night before sleeping. In the past I would often forget one of these things; putting them in a checklist helps me do them more reliably and makes me more relaxed.&nbsp;</p>\n<p>Checklists for other occasions, particularly waking up and traveling, are also helpful, but are much less important to me.&nbsp;</p>\n<h3 id=\"Todo_lists_\">Todo lists:</h3>\n<p>I now maintain two todo lists: one with a list of tasks for each upcoming day, and one with a list of tasks for future events (\"I'm in the UK,\" \"it is Thursday,\" \"I'm going grocery shopping\"). Whenever I think of something I should do, I either put it under a future day and do it when that day arrives, or I put it with an associated event. Each night I check both lists and decide what to do tomorrow.&nbsp;</p>\n<h3 id=\"Beeminder_\"><a href=\"http://www.beeminder.com\">Beeminder</a>:</h3>\n<p>Beeminder is a service that holds you to commitments and tracks your progress. It has helped me a lot over the last months. I've experimented with a few different commitments, but two have been most useful: following a daily routine, and doing a minimum amount of work each day (on average). Beeminder has pretty low overhead.</p>\n<h3 id=\"Reflection_\">Reflection:</h3>\n<p>I spend about 10% of my productive time reflecting on how things have been going and what I should do differently. I benefit from producing concrete possible changes each time I sit down to think. I realized how important this is for me recently; since I've started doing it more reliably, I have gotten a lot more out of reflection.</p>\n<h3 id=\"Pomodoro_\">Pomodoro:</h3>\n<p>I do my work in uninterrupted blocks of 20 minutes, punctuated by 2-3 minute breaks. This is my bastardized, minimalist version of the&nbsp;<a href=\"http://en.wikipedia.org/wiki/Pomodoro_Technique\">pomodoro technique</a>, which I arrived at by trial and error. I use <a href=\"https://itunes.apple.com/us/app/alinof-timer/id512464723?mt=12\">Alinof timer</a>, which was recommended to me by a friend.&nbsp;</p>\n<h3 id=\"Calendar_\">Calendar:</h3>\n<p>I now record commitments on my calendar reliably and check it each night. I failed to do this for 6 months after finishing my undergraduate degree, which I think was a serious mistake. I became much more reliable at checking my calendar after adopting a daily checklist.</p>\n<h3 id=\"Time_Logging_\">Time Logging:</h3>\n<p>Whenever I start a new activity, I write down the current time and a description of what I just stopped doing. At the end of the day I spend a few minutes reading this log and estimating how much time I spent on each activity. This makes me more attentive to time during the day, helps me remember what I did throughout the day, and frees up attention. Sometimes I use the logs to try and notice trends. For example, I've been exercising on random days and measuring how this affects my time. I don't yet know if this helps at all.</p>\n<h3 id=\"Catch_\"><a href=\"https://catch.com/m/\">Catch</a>:</h3>\n<p>Catch is a note-taking app. It is very minimal, and lets you record a voice note by pressing a single button. It has substantially increased my affordance for taking notes during the day, which I use to remember todo items and help with time logging.</p>", "sections": [{"title": "Workflowy:\u00a0", "anchor": "Workflowy__", "level": 1}, {"title": "Checklists:", "anchor": "Checklists_", "level": 1}, {"title": "Todo lists:", "anchor": "Todo_lists_", "level": 1}, {"title": "Beeminder:", "anchor": "Beeminder_", "level": 1}, {"title": "Reflection:", "anchor": "Reflection_", "level": 1}, {"title": "Pomodoro:", "anchor": "Pomodoro_", "level": 1}, {"title": "Calendar:", "anchor": "Calendar_", "level": 1}, {"title": "Time Logging:", "anchor": "Time_Logging_", "level": 1}, {"title": "Catch:", "anchor": "Catch_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "11 comments"}], "headingsCount": 11}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-09T22:09:21.984Z", "modifiedAt": null, "url": null, "title": "My experience as an Australian work-holiday maker", "slug": "my-experience-as-an-australian-work-holiday-maker", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:26.982Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MileyCyrus", "createdAt": "2011-06-13T22:40:17.423Z", "isAdmin": false, "displayName": "MileyCyrus"}, "userId": "8CmnnrsSHHMsNC9ku", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ysasr7kPiFuSr5MLa/my-experience-as-an-australian-work-holiday-maker", "pageUrlRelative": "/posts/ysasr7kPiFuSr5MLa/my-experience-as-an-australian-work-holiday-maker", "linkUrl": "https://www.lesswrong.com/posts/ysasr7kPiFuSr5MLa/my-experience-as-an-australian-work-holiday-maker", "postedAtFormatted": "Sunday, December 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20My%20experience%20as%20an%20Australian%20work-holiday%20maker&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMy%20experience%20as%20an%20Australian%20work-holiday%20maker%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fysasr7kPiFuSr5MLa%2Fmy-experience-as-an-australian-work-holiday-maker%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=My%20experience%20as%20an%20Australian%20work-holiday%20maker%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fysasr7kPiFuSr5MLa%2Fmy-experience-as-an-australian-work-holiday-maker", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fysasr7kPiFuSr5MLa%2Fmy-experience-as-an-australian-work-holiday-maker", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 837, "htmlBody": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> </w:Compatibility> <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel> </w:WordDocument> </xml><![endif]--></p>\n<p><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" LatentStyleCount=\"156\"> </w:LatentStyles> </xml><![endif]--><!--[if !mso]> <span class=\"mceItemObject\" classid=\"clsid:38481807-CA0E-42D2-BF39-B33AF135CC4D\" id=ieooui> </span> <mce:style><! st1\\:*{behavior:url(#ieooui) } --> <!--[endif] --><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:10.0pt; font-family:\"Times New Roman\"; mso-ansi-language:#0400; mso-fareast-language:#0400; mso-bidi-language:#0400;} --> <!--[endif] --></p>\n<p class=\"MsoNormal\">I read <a href=\"/lw/43m/optimal_employment/\">Optimal Employment</a> and decided to try a work-holiday in Australia.&nbsp; Several people have asked me how it's going, so here's my first take. I haven&rsquo;t finished my work-holiday yet, but I want&nbsp; to provide this information now so that potential travelers can make a decision before the hiring season peaks next summer.</p>\n<p class=\"MsoNormal\"><strong>Alice Springs</strong></p>\n<p class=\"MsoNormal\">I came to Alice Springs in late April, and secured a job at a fast-food immediately. They offered me $26/hr after super and penalties. I didn&rsquo;t get many hours, but it was enough to cover expenses. I took the job so that I wouldn&rsquo;t lose money while I was looking for a better job.</p>\n<p class=\"MsoNormal\">Finding better job proved tough. I quickly applied to all of the sit-down restaurants and bars in town, and they all said they preferred locals. I signed up for a couple of employment agencies, but they were concerned that I had didn&rsquo;t have any experience serving alcohol. I borrowed a phonebook and called random business on the Plenty Highway and that didn&rsquo;t work.</p>\n<p class=\"MsoNormal\">This continued for five weeks. I found this period stressful because I get anxious asking businesses if they&rsquo;re hiring, and then I feel humiliated when they say they&rsquo;re not. There were fun times: I toured Uluru and met cool people in the hostel. But in the back of my mind I was always stressed about finding a better job. After five weeks I was about to head south in search of fruit picking work. But then I saw a flyer.</p>\n<p class=\"MsoNormal\"><strong>Remote Australia</strong></p>\n<p class=\"MsoNormal\">A gas-station/convenience store was looking for staff. It was a full-time job, with meals and accommodation costing $40/week. This was exactly what I was looking for! I called immediately and got the job because I was American and nobody else had called before me.</p>\n<p class=\"MsoNormal\">This job was pretty much everything Louie described in Optimal Employment. I had practically no expenses, no responsibilities, and free entertainment after hours. At any given time there were three backpackers (including myself) working there, and one would be switched out about every month. We played a lot of video games and ate a lot of barbeques. We also attended a concert and a rodeo. I saved a lot of money and had a lot of fun, but I was anxious to leave after six months.</p>\n<p class=\"MsoNormal\"><strong>Adelaide</strong></p>\n<p class=\"MsoNormal\">Australia prohibits backpackers from working the same job for more than six months, so I had to leave. Around this time I discovered that I had distant relatives living in Victoria, and they wanted me over for Christmas. This left me only three weeks to work. I decided to look for fruit picking work, since I heard they&rsquo;re more willing to employ backpackers for short periods.</p>\n<p class=\"MsoNormal\">I flew to Adelaide, but I cannot find any fruit picking work. And I hear there's very little in Victoria. I could go to West Australia, but even if I found a job over there, it hardly seems worth it to fly there, work for two weeks, and fly back to Victoria on Christmas Eve. So now I&rsquo;ve got to fill up two weeks in South Australia or Victoria.</p>\n<p class=\"MsoNormal\"><strong>What&rsquo;s next?</strong></p>\n<p class=\"MsoNormal\">I had originally planned to work 88 days of fruit picking work, so that I could get a second working holiday visa next year. (Most Americans are NOT eligible to do this. I can because I&rsquo;m half British.) But I&rsquo;ve heard a lot of bad things about fruit picking, that it&rsquo;s miserable work and that you make little money (or even negative money). And I&rsquo;m 70% sure I don&rsquo;t want to come back to Australia next year anyway. (I want to <a href=\"/r/discussion/lw/f9o/teaching_english_in_shanghai/\">teach English in Shangha</a>i). So the new plan is to spend Christmas in Victoria while applying to TEFL jobs, and then tour the east coast for a month. Then I&rsquo;ll fly back to the states and hopefully start teaching English in March. In the meantime I will have to entertain myself Adelaide/Melbourne.</p>\n<p class=\"MsoNormal\"><strong>Is Optimal Employment accurate?</strong></p>\n<p class=\"MsoNormal\">Pretty much. You&rsquo;ll probably earn more money on the job than Louie estimates, because of penalty rates and an increasing minimum wage. And there&rsquo;s a chance to hit jackpot: I met a LW lurker in Alice Springs who said she was banking $1000/wk <em style=\"mso-bidi-font-style: normal;\">after expenses</em> working at Lasseters. Mining and fishing jobs are also lucrative if you can get them.</p>\n<p class=\"MsoNormal\">But there&rsquo;s also a risk of not finding a job. It took me five weeks to find my remote area job, and I&rsquo;ve met some backpackers who ran out of money and while looking for work. Immigration requires you to save up $5000 before you start your work-holiday; skirt that law at your peril.</p>\n<p class=\"MsoNormal\">Overall I&rsquo;m pleased with my visit to Australia. Even if I though I&rsquo;ll be bleeding money the next couple of months, I&rsquo;ll still come back with a nice profit and wicked memories. If you&rsquo;re young with a sense of adventure, you should definitely consider it. A lot of people want to visit Australia &ldquo;someday&rdquo;, but it gets more difficult when you&rsquo;re older and have more commitments.</p>\n<p class=\"MsoNormal\">I&rsquo;m planning to write another post with tips for working in Australia. Requests welcome!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"4kQXps8dYsKJgaayN": 1, "o4rMP6GJto7ccBL3a": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ysasr7kPiFuSr5MLa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 48, "extendedScore": null, "score": 1.0541784400651541e-06, "legacy": true, "legacyId": "20555", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><!--[if gte mso 9]><xml> <w:WordDocument> <w:View>Normal</w:View> <w:Zoom>0</w:Zoom> <w:PunctuationKerning /> <w:ValidateAgainstSchemas /> <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid> <w:IgnoreMixedContent>false</w:IgnoreMixedContent> <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText> <w:Compatibility> <w:BreakWrappedTables /> <w:SnapToGridInCell /> <w:WrapTextWithPunct /> <w:UseAsianBreakRules /> <w:DontGrowAutofit /> </w:Compatibility> <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel> </w:WordDocument> </xml><![endif]--></p>\n<p><!--[if gte mso 9]><xml> <w:LatentStyles DefLockedState=\"false\" LatentStyleCount=\"156\"> </w:LatentStyles> </xml><![endif]--><!--[if !mso]> <span class=\"mceItemObject\" classid=\"clsid:38481807-CA0E-42D2-BF39-B33AF135CC4D\" id=ieooui> </span> <mce:style><! st1\\:*{behavior:url(#ieooui) } --> <!--[endif] --><!--[if gte mso 10]> <mce:style><! /* Style Definitions */ table.MsoNormalTable {mso-style-name:\"Table Normal\"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-parent:\"\"; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin:0in; mso-para-margin-bottom:.0001pt; mso-pagination:widow-orphan; font-size:10.0pt; font-family:\"Times New Roman\"; mso-ansi-language:#0400; mso-fareast-language:#0400; mso-bidi-language:#0400;} --> <!--[endif] --></p>\n<p class=\"MsoNormal\">I read <a href=\"/lw/43m/optimal_employment/\">Optimal Employment</a> and decided to try a work-holiday in Australia.&nbsp; Several people have asked me how it's going, so here's my first take. I haven\u2019t finished my work-holiday yet, but I want&nbsp; to provide this information now so that potential travelers can make a decision before the hiring season peaks next summer.</p>\n<p class=\"MsoNormal\"><strong id=\"Alice_Springs\">Alice Springs</strong></p>\n<p class=\"MsoNormal\">I came to Alice Springs in late April, and secured a job at a fast-food immediately. They offered me $26/hr after super and penalties. I didn\u2019t get many hours, but it was enough to cover expenses. I took the job so that I wouldn\u2019t lose money while I was looking for a better job.</p>\n<p class=\"MsoNormal\">Finding better job proved tough. I quickly applied to all of the sit-down restaurants and bars in town, and they all said they preferred locals. I signed up for a couple of employment agencies, but they were concerned that I had didn\u2019t have any experience serving alcohol. I borrowed a phonebook and called random business on the Plenty Highway and that didn\u2019t work.</p>\n<p class=\"MsoNormal\">This continued for five weeks. I found this period stressful because I get anxious asking businesses if they\u2019re hiring, and then I feel humiliated when they say they\u2019re not. There were fun times: I toured Uluru and met cool people in the hostel. But in the back of my mind I was always stressed about finding a better job. After five weeks I was about to head south in search of fruit picking work. But then I saw a flyer.</p>\n<p class=\"MsoNormal\"><strong id=\"Remote_Australia\">Remote Australia</strong></p>\n<p class=\"MsoNormal\">A gas-station/convenience store was looking for staff. It was a full-time job, with meals and accommodation costing $40/week. This was exactly what I was looking for! I called immediately and got the job because I was American and nobody else had called before me.</p>\n<p class=\"MsoNormal\">This job was pretty much everything Louie described in Optimal Employment. I had practically no expenses, no responsibilities, and free entertainment after hours. At any given time there were three backpackers (including myself) working there, and one would be switched out about every month. We played a lot of video games and ate a lot of barbeques. We also attended a concert and a rodeo. I saved a lot of money and had a lot of fun, but I was anxious to leave after six months.</p>\n<p class=\"MsoNormal\"><strong id=\"Adelaide\">Adelaide</strong></p>\n<p class=\"MsoNormal\">Australia prohibits backpackers from working the same job for more than six months, so I had to leave. Around this time I discovered that I had distant relatives living in Victoria, and they wanted me over for Christmas. This left me only three weeks to work. I decided to look for fruit picking work, since I heard they\u2019re more willing to employ backpackers for short periods.</p>\n<p class=\"MsoNormal\">I flew to Adelaide, but I cannot find any fruit picking work. And I hear there's very little in Victoria. I could go to West Australia, but even if I found a job over there, it hardly seems worth it to fly there, work for two weeks, and fly back to Victoria on Christmas Eve. So now I\u2019ve got to fill up two weeks in South Australia or Victoria.</p>\n<p class=\"MsoNormal\"><strong id=\"What_s_next_\">What\u2019s next?</strong></p>\n<p class=\"MsoNormal\">I had originally planned to work 88 days of fruit picking work, so that I could get a second working holiday visa next year. (Most Americans are NOT eligible to do this. I can because I\u2019m half British.) But I\u2019ve heard a lot of bad things about fruit picking, that it\u2019s miserable work and that you make little money (or even negative money). And I\u2019m 70% sure I don\u2019t want to come back to Australia next year anyway. (I want to <a href=\"/r/discussion/lw/f9o/teaching_english_in_shanghai/\">teach English in Shangha</a>i). So the new plan is to spend Christmas in Victoria while applying to TEFL jobs, and then tour the east coast for a month. Then I\u2019ll fly back to the states and hopefully start teaching English in March. In the meantime I will have to entertain myself Adelaide/Melbourne.</p>\n<p class=\"MsoNormal\"><strong id=\"Is_Optimal_Employment_accurate_\">Is Optimal Employment accurate?</strong></p>\n<p class=\"MsoNormal\">Pretty much. You\u2019ll probably earn more money on the job than Louie estimates, because of penalty rates and an increasing minimum wage. And there\u2019s a chance to hit jackpot: I met a LW lurker in Alice Springs who said she was banking $1000/wk <em style=\"mso-bidi-font-style: normal;\">after expenses</em> working at Lasseters. Mining and fishing jobs are also lucrative if you can get them.</p>\n<p class=\"MsoNormal\">But there\u2019s also a risk of not finding a job. It took me five weeks to find my remote area job, and I\u2019ve met some backpackers who ran out of money and while looking for work. Immigration requires you to save up $5000 before you start your work-holiday; skirt that law at your peril.</p>\n<p class=\"MsoNormal\">Overall I\u2019m pleased with my visit to Australia. Even if I though I\u2019ll be bleeding money the next couple of months, I\u2019ll still come back with a nice profit and wicked memories. If you\u2019re young with a sense of adventure, you should definitely consider it. A lot of people want to visit Australia \u201csomeday\u201d, but it gets more difficult when you\u2019re older and have more commitments.</p>\n<p class=\"MsoNormal\">I\u2019m planning to write another post with tips for working in Australia. Requests welcome!</p>", "sections": [{"title": "Alice Springs", "anchor": "Alice_Springs", "level": 1}, {"title": "Remote Australia", "anchor": "Remote_Australia", "level": 1}, {"title": "Adelaide", "anchor": "Adelaide", "level": 1}, {"title": "What\u2019s next?", "anchor": "What_s_next_", "level": 1}, {"title": "Is Optimal Employment accurate?", "anchor": "Is_Optimal_Employment_accurate_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "17 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jtedBLdducritm8y6", "4aaPheZh5eAowZdjW"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-09T23:40:37.485Z", "modifiedAt": null, "url": null, "title": "Claiming Connotations", "slug": "claiming-connotations", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:33.303Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Error", "createdAt": "2012-09-14T14:09:41.325Z", "isAdmin": false, "displayName": "Error"}, "userId": "uLxJE9XRTCBrzn8uL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/r7GMAKuwGbvFNThzM/claiming-connotations", "pageUrlRelative": "/posts/r7GMAKuwGbvFNThzM/claiming-connotations", "linkUrl": "https://www.lesswrong.com/posts/r7GMAKuwGbvFNThzM/claiming-connotations", "postedAtFormatted": "Sunday, December 9th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Claiming%20Connotations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AClaiming%20Connotations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr7GMAKuwGbvFNThzM%2Fclaiming-connotations%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Claiming%20Connotations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr7GMAKuwGbvFNThzM%2Fclaiming-connotations", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fr7GMAKuwGbvFNThzM%2Fclaiming-connotations", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 462, "htmlBody": "<p>I was chasing links a few days ago and ran into <a href=\"/lw/np/disputing_definitions/\">Disputing Definitions</a> from the sequences (again), and was thinking about the idea of having definition-argument participants expand their definition of a word to dissolve the dispute. And I thought, I've tried it a couple of times, and I don't think it has ever actually worked.&nbsp;</p>\n<p>I think something else is going on here: People arguing about the \"true\" definition of a word are trying to lay social claim to its connotations by dictating its denotation. <br /><br />Consider an atheist gay-rights supporter and a Christian gay-rights opponent. The former says \"homosexuality is not immoral\" the latter says \"homosexuality is immoral.\"<br /><br />Expand on \"immoral.\" Perhaps the atheist considers an act immoral if it harms someone else without their consent, while the Christian considers an act immoral if it goes against the teachings of the Bible.<br /><br />The expanded forms look like: <br /><br />A: \"Homosexuality does not harm anyone without their consent.<br />B: \"Homosexuality goes against the teachings of the Bible.<br /><br />The atheist is unlikely to disagree with B; Leviticus 18:22 is pretty straightforward. He won't *care* that homosexuality is against biblical teachings, but he won't disagree that it does. The Christian may disagree with A, but assume for the sake of argument that he doesn't. <br /><br />It would seem the factual disagreement has been dissolved; they <a href=\"/lw/sz/moral_error_and_moral_disagreement/\">aren't actually contradicting each other</a>, they are making *entirely orthogonal statements.* <br /><br />Present the above analysis to both sides, though, and I suspect that instead of acknowledging the dissolution, they would fall to arguing over the *correct* definition of the *label* \"immoral.\" Even after the expansion is presented. This has happened to me a couple of times.<br /><br />Question: If they're not arguing about the biblical status or harm status of homosexuality, and they acknowledge that they mean entirely different things by the label \"immoral,\" what are they actually contesting when they argue the proper denotation of that label?<br /><br />This seems to me to be the reversed version of <a href=\"/lw/ny/sneaking_in_connotations/\">sneaking in connotations</a>. For that someone applies a word to a case for which it is denotationally true but connotation-ally questionable, as when referring to Martin Luther King, Jr as a <a href=\"/lw/e95/the_noncentral_fallacy_the_worst_argument_in_the/\">criminal</a>.<br /><br />Arging over definitions, though, strikes me as trying to *lay claim* to connotations rather than sneak them in. If you can dictate the denotation of a commonly-used but disputed word, then you succeed in applying its connotations to all cases that match that denotation. \"Homosexuality is a larommi act\" does not have the same impact as \"Homosexuality is an immoral act,\" even if the two words are given the same literal definition.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"FtT2T9bRbECCGYxrL": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "r7GMAKuwGbvFNThzM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 37, "extendedScore": null, "score": 1.0542310708181077e-06, "legacy": true, "legacyId": "20556", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7X2j8HAkWdmMoS8PE", "BkkwXtaTf5LvbA6HB", "yuKaWPRTxZoov4z8K", "yCWPkLi8wJvewPbEp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-10T00:45:36.358Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Beware Hockey Stick Plans", "slug": "seq-rerun-beware-hockey-stick-plans", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZZizufEwJJRZj7o6Z/seq-rerun-beware-hockey-stick-plans", "pageUrlRelative": "/posts/ZZizufEwJJRZj7o6Z/seq-rerun-beware-hockey-stick-plans", "linkUrl": "https://www.lesswrong.com/posts/ZZizufEwJJRZj7o6Z/seq-rerun-beware-hockey-stick-plans", "postedAtFormatted": "Monday, December 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Beware%20Hockey%20Stick%20Plans&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Beware%20Hockey%20Stick%20Plans%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZZizufEwJJRZj7o6Z%2Fseq-rerun-beware-hockey-stick-plans%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Beware%20Hockey%20Stick%20Plans%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZZizufEwJJRZj7o6Z%2Fseq-rerun-beware-hockey-stick-plans", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZZizufEwJJRZj7o6Z%2Fseq-rerun-beware-hockey-stick-plans", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 167, "htmlBody": "<p>Today's post, <a href=\"http://www.overcomingbias.com/2008/12/beware-hockey-s.html\">Beware Hockey Stick Plans</a> was originally published on December 4, 2008.  A summary:</p>\n<p>&nbsp;</p>\n<blockquote>Most hockey stick style growth models have a problem, which is that they have almost never come true. It seems likely, therefore, that some part of the reasoning is long.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/fuu/seq_rerun_underconstrained_abstractions/\">Underconstrained Abstractions</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZZizufEwJJRZj7o6Z", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 4, "extendedScore": null, "score": 1.0542685497757735e-06, "legacy": true, "legacyId": "20557", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bqxa5PJHQXTpbHjqj", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-10T04:26:23.835Z", "modifiedAt": null, "url": null, "title": "By Which It May Be Judged", "slug": "by-which-it-may-be-judged", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:38.082Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Eliezer_Yudkowsky", "createdAt": "2009-02-23T21:58:56.739Z", "isAdmin": false, "displayName": "Eliezer Yudkowsky"}, "userId": "nmk3nLpQE89dMRzzN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zqwWicCLNBSA5Ssmn/by-which-it-may-be-judged", "pageUrlRelative": "/posts/zqwWicCLNBSA5Ssmn/by-which-it-may-be-judged", "linkUrl": "https://www.lesswrong.com/posts/zqwWicCLNBSA5Ssmn/by-which-it-may-be-judged", "postedAtFormatted": "Monday, December 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20By%20Which%20It%20May%20Be%20Judged&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABy%20Which%20It%20May%20Be%20Judged%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzqwWicCLNBSA5Ssmn%2Fby-which-it-may-be-judged%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=By%20Which%20It%20May%20Be%20Judged%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzqwWicCLNBSA5Ssmn%2Fby-which-it-may-be-judged", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzqwWicCLNBSA5Ssmn%2Fby-which-it-may-be-judged", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3189, "htmlBody": "<p><strong>Followup to</strong>:&nbsp;<a href=\"/lw/frz/mixed_reference_the_great_reductionist_project/\">Mixed Reference: The Great Reductionist Project</a></p>\n<blockquote>\n<p style=\"padding-left: 30px;\"><span style=\"font-variant: small-caps;\">Humans need fantasy to be human.</span></p>\n<p style=\"padding-left: 30px;\">\"Tooth fairies? Hogfathers? Little&mdash;\"</p>\n<p style=\"padding-left: 30px;\"><span style=\"font-variant: small-caps;\">Yes. As practice. You have to start out learning to believe the&nbsp;<em>little</em>&nbsp;lies.</span></p>\n<p style=\"padding-left: 30px;\">\"So we can believe the big ones?\"</p>\n<p style=\"padding-left: 30px;\"><span style=\"font-variant: small-caps;\">Yes. Justice. Mercy. Duty. That sort of thing.</span></p>\n<p style=\"padding-left: 30px;\">\"They're not the same at all!\"</p>\n<p style=\"padding-left: 30px;\"><span style=\"font-variant: small-caps;\">You think so? Then take the universe and grind it down to the finest powder and sieve it through the finest sieve and then&nbsp;<em>show</em>&nbsp;me one atom of justice, one molecule of mercy.</span></p>\n<p style=\"padding-left: 60px;\">- Susan and Death, in&nbsp;<em>Hogfather</em>&nbsp;by Terry Pratchett</p>\n</blockquote>\n<p><a href=\"/lw/ru/the_bedrock_of_fairness/\">Suppose three people find a pie</a>&nbsp;- that is, three people exactly simultaneously spot a pie which has been exogenously generated in unclaimed territory. Zaire wants the entire pie; Yancy thinks that 1/3 each is fair; and Xannon thinks that fair would be taking into equal account everyone's ideas about what is \"fair\".<a id=\"more\"></a></p>\n<p>I myself would say unhesitatingly that a third of the pie each, is fair. \"Fairness\", as an ethical concept, can get a lot more complicated in more elaborate contexts. But in this simple context, a lot of other things that \"fairness\" could depend on, like work inputs, have been eliminated or made constant. Assuming no relevant conditions other than those already stated, \"fairness\" simplifies to the mathematical procedure of splitting the pie into equal parts; and when this logical function is run over physical reality, it outputs \"1/3 for Zaire, 1/3 for Yancy, 1/3 for Xannon\".</p>\n<p>Or to put it another way - just like we get \"If Oswald hadn't shot Kennedy, nobody else would've\" by&nbsp;<a href=\"/lw/frz/mixed_reference_the_great_reductionist_project/\">running a logical function over a true causal model</a>&nbsp;- similarly, we can get the hypothetical 'fair' situation, whether or not it actually happens, by running the physical starting scenario through a logical function that describes what a 'fair' outcome would look like:</p>\n<p><img src=\"http://wiki.lesswrong.com/mediawiki/images/e/e1/Fairness.jpg\" alt=\"\" width=\"240\" height=\"460\" /></p>\n<p>So am I (as Zaire would claim) just assuming-by-authority that I get to have everything my way, since I'm not defining 'fairness' the way&nbsp;<em>Zaire</em>&nbsp;wants to define it?</p>\n<p>No more than mathematicians are flatly ordering everyone to assume-without-proof that&nbsp;<a href=\"/lw/f4e/logical_pinpointing/\">two different numbers can't have the same successor</a>.&nbsp;For fairness to be what everyone thinks is \"fair\" would be&nbsp;<em>entirely</em>&nbsp;circular, structurally isomorphic to \"Fzeem is what everyone thinks is fzeem\"... or like trying to define the counting numbers as \"whatever anyone thinks is a number\". It only even&nbsp;<em>looks</em>&nbsp;coherent because everyone secretly already has a mental picture of \"numbers\" - because their brain already navigated to the referent. &nbsp;But&nbsp;<em>something&nbsp;</em>akin to axioms is needed to talk about \"numbers, as opposed to something else\" in the first place. Even an inchoate mental image of \"0, 1, 2, ...\" implies the axioms no less than a formal statement - we can extract the axioms back out by asking&nbsp;<a href=\"/lw/f4e/logical_pinpointing/\">questions about this rough mental image</a>.</p>\n<p>Similarly, the intuition that fairness has&nbsp;<em>something</em>&nbsp;to do with dividing up the pie equally, plays a role akin to secretly already having \"0, 1, 2, ...\" in mind as the subject of mathematical conversation. You need axioms, not as assumptions that aren't justified, but as pointers to what the heck the conversation is supposed to be&nbsp;<em>about.</em></p>\n<p>Multiple philosophers have suggested that this stance seems similar to \"rigid designation\", i.e., when I say 'fair' it intrinsically, rigidly refers to something-to-do-with-equal-division. I confess I don't see it that way myself - if somebody thinks of Euclidean geometry when you utter the sound \"num-berz\" they're not doing anything false, they're associating the sound to a different logical thingy. It's not about words with intrinsically rigid referential power, it's that the words are&nbsp;<em>window dressing</em>&nbsp;on the underlying entities. I want to&nbsp;<em>talk about</em>&nbsp;a particular&nbsp;<em>logical entity,</em>&nbsp;as it might be defined by either axioms or inchoate images, regardless of which word-sounds may be associated to it. &nbsp;If you want to call that \"rigid designation\", that seems to me like adding a level of indirection; I don't care&nbsp;about the&nbsp;<em>word&nbsp;</em>'fair' in the first place, I care about the logical entity of fairness. &nbsp;(Or to put it even more sharply: since my ontology does not have room for physics, logic,&nbsp;<em>plus</em>&nbsp;designation, I'm not very interested in discussing this 'rigid designation' business unless it's being reduced to something else.)</p>\n<p>Once issues of justice become more&nbsp;complicated and all the contextual variables get added back in,&nbsp;we might not be sure if a&nbsp;<em>disagreement</em>&nbsp;about 'fairness' reflects:</p>\n<ol>\n<li>The equivalent of a multiplication error within the same axioms - incorrectly dividing by 3. &nbsp;(Or more complicatedly: &nbsp;You might have a sophisticated axiomatic concept of 'equity', and&nbsp;<em>incorrectly&nbsp;</em>process those axioms to invalidly yield the assertion that, in a context where 2 of the 3 must starve and there's only enough pie for at most 1 person to survive, you should still divide the pie equally instead of flipping a 3-sided coin. &nbsp;Where I'm assuming that this conclusion is 'incorrect', not because I disagree with it, but because it didn't actually follow from the axioms.)</li>\n<li>Mistaken models of the physical world fed into the function - mistakenly thinking there's 2 pies, or mistakenly thinking that Zaire has no subjective experiences and is not an object of ethical value.</li>\n<li>People associating different logical functions to the letters F-A-I-R, which isn't a&nbsp;<em>disagreement&nbsp;about</em>&nbsp;some common pinpointed variable,&nbsp;but just different people wanting different things.</li>\n</ol>\n<p>There's a lot of people who feel that this picture leaves out something fundamental, especially once we make the jump from \"fair\" to the broader concept of \"moral\", \"good\", or \"right\". &nbsp;And it's this worry about leaving-out-something-fundamental that I hope to address next...</p>\n<p>...but please note, if we confess that 'right' lives in a world of&nbsp;physics and logic -&nbsp;because&nbsp;<em>everything</em>&nbsp;lives in a world of physics and logic -&nbsp;then we&nbsp;<em>have</em>&nbsp;to translate 'right' into those terms&nbsp;<em>somehow.</em></p>\n<p>And that is the answer Susan should have given - if she could talk about sufficiently advanced epistemology, sufficiently fast - to Death's&nbsp;entire statement:</p>\n<p style=\"padding-left: 30px;\"><span style=\"font-variant: small-caps;\">You think so? Then take the universe and grind it down to the finest powder and sieve it through the finest sieve and then&nbsp;<em>show</em>&nbsp;me one atom of justice, one molecule of mercy. And yet&nbsp;</span>&mdash; Death waved a hand.&nbsp;<span style=\"font-variant: small-caps;\">And yet you act as if there is some ideal order in the world, as if there is some ...&nbsp;</span><em style=\"font-variant: small-caps;\">rightness</em><span style=\"font-variant: small-caps;\">&nbsp;in the universe by which it may be judged.</span></p>\n<p>\"But!\" Susan should've said. &nbsp;\"When we judge the universe we're comparing it to a&nbsp;<em>logical&nbsp;</em>referent, a sort of thing that isn't&nbsp;<em>in</em>&nbsp;the universe! &nbsp;Why, it's just like looking at a heap of 2 apples and a heap of 3 apples on a table, and comparing their invisible product to the number 6 - there isn't any 6 if you grind up the whole table, even if you grind up the whole universe, but the product is&nbsp;<em>still</em>&nbsp;6, physico-logically speaking.\"</p>\n<hr />\n<p>If you require that Rightness be written on some particular great Stone Tablet somewhere - to be \"a light that shines from the sky\", outside people, as a different Terry Pratchett book put it - then indeed, there's no such Stone Tablet anywhere in our universe.</p>\n<p>But there&nbsp;<em>shouldn't</em>&nbsp;be such a Stone Tablet,&nbsp;<em>given</em>&nbsp;standard intuitions about morality. &nbsp;This follows from the Euthryphro&nbsp;Dilemma out of ancient Greece.</p>\n<p>The original Euthryphro dilemma goes, \"Is it pious because it is loved by the gods, or loved by the gods because it is pious?\" The religious version goes, \"Is it good because it is commanded by God, or does God command it because it is good?\"</p>\n<p>The standard atheist reply is: &nbsp;\"Would you say that it's an intrinsically&nbsp;good thing - even if the event has no further causal consequences which are good - to slaughter babies or torture people, if that's what God says to do?\"</p>\n<p>If we can't make it good to slaughter babies by tweaking the state of God, then morality doesn't come from God; so goes the standard atheist argument.</p>\n<p>But if you can't make it good to slaughter babies by tweaking the physical state of&nbsp;<em>anything</em>&nbsp;-&nbsp;if we can't imagine a world where some great Stone Tablet of Morality has been physically rewritten, and what is right has changed - then this is telling us that...</p>\n<p>(drumroll)</p>\n<p>...what's \"right\" is a logical thingy rather than a physical thingy, that's all. &nbsp;The mark of a logical validity is that we can't concretely visualize a coherent possible world where the proposition is false.</p>\n<p>And I mention this in hopes that I can show that it is not moral anti-realism to say that moral statements take their truth-value from logical entities. &nbsp;Even&nbsp;in Ancient Greece, philosophers implicitly knew that 'morality' ought to be such an entity - that it&nbsp;<em>couldn't</em>&nbsp;be something you found when you ground the Universe to powder, because then you could resprinkle the powder and make it wonderful to kill babies -&nbsp;though they didn't know how to say what they knew.</p>\n<hr />\n<p>There's a lot of people who still feel that Death&nbsp;<em>would</em>&nbsp;be right, if the universe were all physical; that the kind of dry logical entity I'm describing here, isn't sufficient to carry the bright alive feeling of goodness.</p>\n<p>And there are others who accept that physics and logic is everything, but who - I think&nbsp;<em>mistakenly</em>&nbsp;- go ahead and also accept Death's stance that this makes morality a lie, or, in lesser form, that the bright alive feeling can't make it. &nbsp;(Sort of like people who accept an incompatibilist theory of free will, also accept physics, and conclude with sorrow that they are indeed being <a href=\"/lw/r0/thou_art_physics/\">controlled by physics</a>.)</p>\n<p>In case anyone is bored that I'm&nbsp;<em>still</em>&nbsp;trying to fight this battle, well,&nbsp;here's a quote from a recent Facebook conversation with a famous early transhumanist:</p>\n<blockquote>\n<p>No doubt a \"crippled\" AI that didn't understand the existence or nature of first-person facts could be nonfriendly towards sentient beings... Only a zombie wouldn't value Heaven over Hell. For reasons we simply don't understand, the negative value and normative aspect of agony and despair is built into the nature of the experience itself. Non-reductionist? Yes, on a standard materialist ontology. But not IMO within a more defensible&nbsp;Strawsonian physicalism.</p>\n</blockquote>\n<p>It would actually be <em>quite surprisingly helpful</em>&nbsp;for increasing the percentage of people who will participate meaningfully in saving the planet, if there were some reliably-working standard explanation for why physics and logic together have enough room to contain morality. &nbsp;People who think that reductionism means we have to lie to our children, as Pratchett's Death advocates, won't be much enthused about the Center for Applied Rationality. &nbsp;And there are a fair number of people out there who still advocate proceeding in the confidence of ineffable morality to construct sloppily designed AIs.</p>\n<p>So far I don't know of any exposition that works reliably - for the thesis for how morality&nbsp;<em>including</em>&nbsp;our intuitions about whether things&nbsp;<em>really are justified</em>&nbsp;and so on, is preserved in the analysis to physics plus logic; that morality has been&nbsp;<a href=\"/lw/oo/explaining_vs_explaining_away/\">explained rather than explained away</a>. &nbsp;Nonetheless I shall now take another stab at it, starting with a simpler bright feeling:</p>\n<hr />\n<p>When I see an unusually neat mathematical proof, unexpectedly short or surprisingly general, my brain gets a joyous sense of&nbsp;<em>elegance.</em></p>\n<p><em></em>There's presumably some functional slice through my brain that implements this emotion - some configuration subspace of spiking neural circuitry which corresponds to my <em>feeling</em>&nbsp;of elegance. &nbsp;Perhaps I should&nbsp;say that elegance is&nbsp;<em>merely </em>about&nbsp;my brain switching on its elegance-signal? &nbsp;But there are concepts like&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Kolmogorov_complexity\">Kolmogorov complexity</a>&nbsp;that give more formal meanings of \"simple\" than \"Simple is whatever makes my brain feel the emotion of simplicity.\" &nbsp;Anything you do to fool my brain wouldn't make the proof&nbsp;<em>really</em>&nbsp;elegant, not in that sense. &nbsp;The emotion is not free of semantic content; we could build a correspondence theory for it and navigate to its logical+physical referent, and say: &nbsp;\"Sarah feels like this proof is elegant, and her feeling is&nbsp;<em>true.</em>\"&nbsp; You could even say that certain proofs are elegant even if no conscious agent sees them.</p>\n<p>My description of 'elegance' admittedly did invoke agent-dependent concepts like 'unexpectedly' short or 'surprisingly' general. &nbsp;It's almost certainly true that with a different mathematical background, I would have different standards of elegance and experience that feeling on&nbsp;<em>somewhat</em>&nbsp;different occasions. &nbsp;Even so, that still seems like moving around in a field of&nbsp;<em>similar&nbsp;</em>referents for the emotion - much more similar to each other than to, say, the distant cluster of 'anger'.</p>\n<p>Rewiring my brain so that the 'elegance' sensation gets activated when I see mathematical proofs where the words have lots of vowels - that wouldn't <em>change</em>&nbsp;what is elegant. &nbsp;Rather, it would make the feeling be&nbsp;<em>about&nbsp;</em>something else entirely; different semantics with a different truth-condition.</p>\n<p>Indeed, it's not clear that this thought experiment is, or should be,&nbsp;<em>really</em>&nbsp;conceivable. &nbsp;If all the associated computation is about vowels instead of elegance, then&nbsp;<a href=\"/lw/no/how_an_algorithm_feels_from_inside/\">from the inside</a>&nbsp;you would expect that to&nbsp;<em>feel vowelly</em>, not&nbsp;<em>feel elegant</em>...</p>\n<p>...which is to say that even feelings can be associated with logical entities. &nbsp;Though unfortunately not in any way that will&nbsp;<em>feel like</em>&nbsp;qualia if you can't read your own source code. &nbsp;I could write out an exact description of your visual cortex's spiking code for 'blue' on paper, and it wouldn't actually <em>look&nbsp;</em><span style=\"color:#0000FF\">blue&nbsp;</span>to you. &nbsp;Still, on the higher level of description, it should seem intuitively plausible that if you tried rewriting the relevant part of your brain to count vowels, the resulting sensation would no longer have the content or even the <em>feeling&nbsp;</em>of elegance. &nbsp;It would compute vowelliness, and&nbsp;feel&nbsp;vowelly.</p>\n<hr />\n<p>My feeling of mathematical elegance is motivating; it makes me more likely to search for similar such proofs later and go on doing math. &nbsp;You could construct an agent that tried to add more vowels instead, and if the agent asked itself why it was doing that, the resulting justification-thought wouldn't <em>feel like</em> because-it's-elegant, it would <em>feel like</em> because-it's-vowelly.</p>\n<p>In the same sense, when you try to do what's right, you're motivated by things like (to yet again quote Frankena's list of terminal values):</p>\n<blockquote>\n<p>\"Life, consciousness, and activity; health and strength; pleasures and satisfactions of all or certain kinds; happiness, beatitude, contentment, etc.; truth; knowledge and true opinions of various kinds, understanding, wisdom; beauty, harmony, proportion in objects contemplated; aesthetic experience; morally good dispositions or virtues; mutual affection, love, friendship, cooperation; just distribution of goods and evils; harmony and proportion in one's own life; power and experiences of achievement; self-expression; freedom; peace, security; adventure and novelty; and good reputation, honor, esteem, etc.\"</p>\n</blockquote>\n<p>If we reprogrammed you to count paperclips instead, it wouldn't feel like&nbsp;<em>different</em>&nbsp;things having the&nbsp;<em>same</em>&nbsp;kind of motivation behind it. &nbsp;It wouldn't feel like doing-what's-right for a different guess about what's right. &nbsp;It would feel like doing-what-leads-to-paperclips.</p>\n<p>And I quoted the above list because the feeling of rightness isn't&nbsp;<em>about</em>&nbsp;implementing a particular logical function; it contains no mention of logical functions at all; in the environment of evolutionary ancestry nobody has&nbsp;<em>heard</em>&nbsp;of axiomatization; these feelings are&nbsp;<em>about</em>&nbsp;life, consciousness, etcetera. &nbsp;If I could write out the whole truth-condition of the feeling in a way you could compute, you would still feel Moore's Open Question: &nbsp;\"I can see that this event is high-rated by logical function X, but is X really&nbsp;<em>right?</em>\"&nbsp;- since you can't read your own source code and the description wouldn't be commensurate with your brain's native format.</p>\n<p>\"But!\" you cry. &nbsp;\"But, is it&nbsp;really&nbsp;<em>better</em>&nbsp;to do what's right, than to maximize paperclips?\" &nbsp;Yes! &nbsp;As soon as you start trying to cash out the logical function that gives betterness its truth-value, it will output \"life, consciousness, etc. &gt;<sub>B</sub> paperclips\". &nbsp;And if your brain were computing a different logical function instead, like makes-more-paperclips, it wouldn't&nbsp;feel <em>better,</em>&nbsp;it would feel&nbsp;<em>moreclippy.</em></p>\n<p>But is it really&nbsp;<em>justified</em>&nbsp;to keep our own sense of betterness? &nbsp;Sure, and that's a logical fact - it's the objective output of the logical function corresponding to your experiential sense of what it means for something to be 'justified' in the first place. &nbsp;This doesn't mean that Clippy the Paperclip Maximizer will self-modify to do only things that are justified; Clippy doesn't judge between self-modifications by computing justifications, but rather, computing <em>clippyflurphs.</em></p>\n<p>But isn't it&nbsp;<em>arbitrary</em>&nbsp;for Clippy to maximize paperclips? &nbsp;Indeed; once you implicitly or explicitly pinpoint the logical function that gives judgments of arbitrariness their truth-value - presumably, revolving around the presence or absence of justifications - then this logical function will objectively yield that there's no justification whatsoever for maximizing paperclips (which is why<em> I'm </em>not going to do it) and hence that Clippy's decision is arbitrary. Conversely, Clippy finds that there's no clippyflurph for preserving life, and hence that it is unclipperiffic. &nbsp;But unclipperifficness isn't arbitrariness any more than the number 17 is a right triangle; they're different logical entities pinned down by different axioms, and the corresponding judgments will have different semantic content and&nbsp;<em>feel different.</em>&nbsp; If Clippy is architected to experience that-which-you-call-qualia, Clippy's feeling of clippyflurph will be&nbsp;<em>structurally</em>&nbsp;different from the way justification feels, not just red versus blue, but vision versus sound.</p>\n<p>But surely one&nbsp;<em>shouldn't</em>&nbsp;praise the clippyflurphers rather than the just? &nbsp;I quite agree; and as soon as you navigate referentially to the coherent logical entity that is the truth-condition of&nbsp;<em>should</em>&nbsp;- a function on potential actions and future states - it will agree with you that it's better to avoid the arbitrary than the unclipperiffic. &nbsp;Unfortunately, this logical fact does not correspond to the truth-condition of any meaningful proposition computed by Clippy in the course of how it efficiently transforms the universe into paperclips, in much the same way that rightness plays no role in that-which-is-maximized by the blind processes of natural selection.</p>\n<p>Where moral judgment is concerned, it's logic all the way down. &nbsp;<em>ALL&nbsp;</em>the way down. &nbsp;Any frame of reference where you're worried that it's <em>really </em>no better to do what's right then to maximize paperclips... well, that <em>really</em>&nbsp;part has a truth-condition (or what does the \"really\" mean?) and as soon as you write out the truth-condition you're going to end up with yet another&nbsp;ordering over actions or algorithms or meta-algorithms or&nbsp;<em>something.</em>&nbsp; And since grinding up the universe won't and <em>shouldn't</em>&nbsp;yield any miniature '&gt;' tokens, it must be a&nbsp;<em>logical</em>&nbsp;ordering. &nbsp;And so whatever logical ordering it is you're worried about, it probably&nbsp;<em>does</em>&nbsp;produce 'life &gt; paperclips' - but Clippy isn't computing that logical fact any more than your pocket calculator is computing it.</p>\n<p>Logical facts have no power to directly affect the universe except when some part of the universe is computing them, and morality is (and <em>should</em>&nbsp;be) logic, not physics.</p>\n<p>Which is to say:</p>\n<blockquote>\n<p>The old wizard was staring at him, a sad look in his eyes. \"I suppose I&nbsp;<em>do</em>&nbsp;understand now,\" he said quietly.</p>\n<p>\"Oh?\" said Harry. \"Understand what?\"</p>\n<p>\"Voldemort,\" said the old wizard. \"I understand him now at last. Because to believe that the world is truly like that, you must believe there is no justice in it, that it is woven of darkness at its core. I asked you why he became a monster, and you could give no reason. And if I could ask&nbsp;<em>him</em>, I suppose, his answer would be: Why not?\"</p>\n<p>They stood there gazing into each other's eyes, the old wizard in his robes, and the young boy with the lightning-bolt scar on his forehead.</p>\n<p>\"Tell me, Harry,\" said the old wizard, \"will&nbsp;<em>you</em>&nbsp;become a monster?\"</p>\n<p>\"No,\" said the boy, an iron certainty in his voice.</p>\n<p>\"Why not?\" said the old wizard.</p>\n<p>The young boy stood very straight, his chin raised high and proud, and said: \"There is no justice in the laws of Nature, Headmaster, no term for fairness in the equations of motion. The universe is neither evil, nor good, it simply does not care. The stars don't care, or the Sun, or the sky. But they don't have to!&nbsp;<em>We</em>&nbsp;care! There&nbsp;<em>is</em>&nbsp;light in the world, and it is&nbsp;<em>us!</em>\"</p>\n</blockquote>\n<p>&nbsp;</p>\n<p style=\"text-align:right\">Part of the sequence <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\"><em>Highly Advanced Epistemology 101 for Beginners</em></a></p>\n<p style=\"text-align:right\">Next post: \"<a href=\"/lw/g0i/standard_and_nonstandard_numbers/\">Standard and Nonstandard Numbers</a>\"</p>\n<p style=\"text-align:right\">Previous post: \"<a href=\"/lw/frz/mixed_reference_the_great_reductionist_project/\">Mixed Reference: The Great Reductionist Project</a>\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"wMPYFGmhcFg4bSb4Z": 2, "LnEEs8xGooYmQ8iLA": 4, "nSHiKwWyMZFdZg5qt": 4, "xgpBASEThXPuKRhbS": 4, "5A5ZGTQovxbay6fpr": 2, "Z8wZZLeLMJ3NSK7kR": 6}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zqwWicCLNBSA5Ssmn", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 55, "baseScore": 70, "extendedScore": null, "score": 0.000155, "legacy": true, "legacyId": "20559", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "SqFbMbtxGybdS2gRs", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "", "canonicalPrevPostSlug": "mixed-reference-the-great-reductionist-project", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 70, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 940, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bTsiPnFndZeqTnWpu", "iAxkfiyG8WizPSPbq", "3FoMuCLqZggTxoC3S", "NEeW7eSXThPz7o4Ne", "cphoF8naigLhRf3tu", "yA4gF5KrboK2m2Xu7", "i7oNcHR3ZSnEAM29X"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 7, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-10T04:45:12.063Z", "modifiedAt": null, "url": null, "title": "New Sequences and General Update From Castify", "slug": "new-sequences-and-general-update-from-castify", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:39.583Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "scott_from_castify", "createdAt": "2012-12-03T09:22:52.098Z", "isAdmin": false, "displayName": "scott_from_castify"}, "userId": "Q8YfKQGevZXcRDNDt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qc5uvn96XkgEArvy3/new-sequences-and-general-update-from-castify", "pageUrlRelative": "/posts/qc5uvn96XkgEArvy3/new-sequences-and-general-update-from-castify", "linkUrl": "https://www.lesswrong.com/posts/qc5uvn96XkgEArvy3/new-sequences-and-general-update-from-castify", "postedAtFormatted": "Monday, December 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20Sequences%20and%20General%20Update%20From%20Castify&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20Sequences%20and%20General%20Update%20From%20Castify%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqc5uvn96XkgEArvy3%2Fnew-sequences-and-general-update-from-castify%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20Sequences%20and%20General%20Update%20From%20Castify%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqc5uvn96XkgEArvy3%2Fnew-sequences-and-general-update-from-castify", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fqc5uvn96XkgEArvy3%2Fnew-sequences-and-general-update-from-castify", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 180, "htmlBody": "<p>You may have noticed <a href=\"/lw/fr2/lesswrong_podcasts/\">the&nbsp;announcement&nbsp;last week</a> of Less Wrong's partnership with <a href=\"http://castify.co/\">Castify</a>&nbsp;to create Less Wrong podcasts. We're happy to announce that we now offer three new sequences. For those who want to try us out, we are also offering Eliezer's seminal essay, The Simple Truth, as a standalone 99&cent; purchase (it's also part of the Map and Territory sequence).</p>\n<p>Our channels:</p>\n<ul>\n<li><a href=\"http://castify.co/channels/3-less-wrong-the-simple-truth\">The Simple Truth</a></li>\n<li><a href=\"http://castify.co/channels/4-less-wrong-map-and-territory\">Map and Territory</a></li>\n<li><a href=\"http://castify.co/channels/2-less-wrong-ethical-injunctions\">Ethical Injunctions</a></li>\n<li>and our pilot sequence, <a href=\"http://castify.co/channels/1-less-wrong-mysterious-answers-to-mysterious-questions\">Mysterious Answers to Mysterious Questions</a></li>\n</ul>\n<p>We have audio samples on our site if you're curious.</p>\n<p>In other news, we're still working to make the promoted posts available as a monthly subscription. We'll keep you up to date with the progress on that. It's still a few weeks away.</p>\n<p>Side note: Because of user feedback, we have removed the one year download restriction for sequences purchased. However, we have added a clause in our terms of service that gives you a minimum of two years to download the audio, if only because <a href=\"/lw/5qm/living_forever_is_hard_or_the_gompertz_curve/\">living forever is hard</a>, even for digital goods.</p>\n<p>See you in the comments.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qc5uvn96XkgEArvy3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 16, "extendedScore": null, "score": 1.0544067604733095e-06, "legacy": true, "legacyId": "20562", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BTM5wskCnzbNLDZdS", "GytPrQ9cT46k9etoz"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-10T05:06:00.819Z", "modifiedAt": null, "url": null, "title": "Taking into account another's preferences", "slug": "taking-into-account-another-s-preferences", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:29.401Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "GuySrinivasan", "createdAt": "2009-02-27T21:00:32.986Z", "isAdmin": false, "displayName": "GuySrinivasan"}, "userId": "HMnfd9HdRCfuRcdBG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uRxHtpgzmjEupXG3a/taking-into-account-another-s-preferences", "pageUrlRelative": "/posts/uRxHtpgzmjEupXG3a/taking-into-account-another-s-preferences", "linkUrl": "https://www.lesswrong.com/posts/uRxHtpgzmjEupXG3a/taking-into-account-another-s-preferences", "postedAtFormatted": "Monday, December 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Taking%20into%20account%20another's%20preferences&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATaking%20into%20account%20another's%20preferences%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuRxHtpgzmjEupXG3a%2Ftaking-into-account-another-s-preferences%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Taking%20into%20account%20another's%20preferences%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuRxHtpgzmjEupXG3a%2Ftaking-into-account-another-s-preferences", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuRxHtpgzmjEupXG3a%2Ftaking-into-account-another-s-preferences", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 562, "htmlBody": "<p>Question regarding:&nbsp;<a href=\"/lw/2xb/if_you_dont_know_the_name_of_the_game_just_tell/\">If you don't know the name of the game, just tell me what I mean to you</a></p>\n<p>I've been thinking about what it means to prefer that someone else achieve their preferences. In particular, what happens if you and I both prefer to adopt and chase after each other's preferences to some extent. This has clear good points, like cooperating and making more resources more fungible and thus probably being more efficient and achieving more preferences overall, and clear failure modes, like \"What do you want to do? -- I don't know, I want to do whatever <em>you</em> want to do. Repeat.\"</p>\n<p>My first thought: okay, simple, I'll just define my utility function U' to be U + <em>a</em>V where U was my previous utility function and V is your utility function and a is an appropriate scaling factor as per Stuart's post and then I can follow U'!<sup>1</sup></p>\n<p>This has a couple problems. First, if you're also trying to change your actions based on what I want, there's a circular reference issue. Second, U already contains part of V by definition, or something<sup>2</sup>.</p>\n<p>My second thought: Fine, first we'll both factor our preferences into U = U<sub>1</sub> +&nbsp;<em>a</em>V&nbsp;where U<sub>1</sub> is my preference without regards to what you want. (Yours is V = V<sub>1</sub>&nbsp;+ <em>b</em>U) Basically what I want to say is \"What do you want to do, 'cause ignoring you I want burgers a bit more than Italian, which I want significantly more than sandwiches from home\" and then you could say \"well ignoring you I want sandwiches more than Italian more than burgers but it's not a big thing, so since you mean <em>b</em> to me, let's do Italian\". It's that \"ignoring you\" bit that I don't know how to correctly intuit. And by intuit I mean put into math.</p>\n<p>Assuming it means something coherent to factor U into U<sub>1</sub> + <em>a</em>V, there's still a problem. Watch what happens when we remove the self-reference. First scale U and V to something you and I can agree is approximately fungible. Maybe marginal hours, maybe marginal dollars, whatever. Now U = U<sub>1</sub> + <em>a</em>(V<sub>1</sub> + <em>b</em>U), so U - <em>ab</em>U = U<sub>1</sub> + <em>a</em>V<sub>1</sub> and as long as <em>ab</em>&lt;1, you can maximize U by maximizing U<sub>1</sub> + <em>a</em>V<sub>1</sub>. Which sounds great, except that my intuition screams that maximizing U should depend on <em>b</em>. So what's up there? My guess is that somewhere I snuck a dependence on <em>b</em> into <em>a</em>...</p>\n<p>(I like that the&nbsp;<em>ab</em>&lt;1 constraint appears... intuitively I think it should mean that if we both try to care too much about what the other person wants, neither of us will get anywhere making a decision. \"I don't know, what do <em>you</em> want to do?\" In general if no one ever lets <em>a</em>&gt;=1 then things should converge.)</p>\n<p>I feel like the obvious next step is to list some simple outcomes and play pretend with two people trying to care about each other and fake-elicit their preferences and translate that into utility functions and just <em>check</em> to see how those functions factor. But I've felt like that for a week and haven't done it yet, so here's what I've got.</p>\n<p><sup>1</sup>Of course I know humans don't work like this, I just want the math.</p>\n<p><sup>2</sup>\"Or something\" means I have an idea that sounds maybe right but it's pretty&nbsp;hand-wavy&nbsp;and maybe completely wrong and I certainly can't or don't want to formalize it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uRxHtpgzmjEupXG3a", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 18, "extendedScore": null, "score": 1.0544187676378207e-06, "legacy": true, "legacyId": "20561", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["W2ufY8ihDDWWqJA7h"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-12-10T07:02:14.394Z", "modifiedAt": null, "url": null, "title": "Do I really not believe in God? Do you?", "slug": "do-i-really-not-believe-in-god-do-you", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:30.040Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/trCeFmHqGfk6SPncG/do-i-really-not-believe-in-god-do-you", "pageUrlRelative": "/posts/trCeFmHqGfk6SPncG/do-i-really-not-believe-in-god-do-you", "linkUrl": "https://www.lesswrong.com/posts/trCeFmHqGfk6SPncG/do-i-really-not-believe-in-god-do-you", "postedAtFormatted": "Monday, December 10th 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Do%20I%20really%20not%20believe%20in%20God%3F%20Do%20you%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADo%20I%20really%20not%20believe%20in%20God%3F%20Do%20you%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtrCeFmHqGfk6SPncG%2Fdo-i-really-not-believe-in-god-do-you%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Do%20I%20really%20not%20believe%20in%20God%3F%20Do%20you%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtrCeFmHqGfk6SPncG%2Fdo-i-really-not-believe-in-god-do-you", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtrCeFmHqGfk6SPncG%2Fdo-i-really-not-believe-in-god-do-you", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 231, "htmlBody": "<p>Well, I used to think that I do not believe in anything supernatural that affects what happens to me, but I'm wondering if maybe I actually do&nbsp;<a href=\"http://en.wikipedia.org/wiki/Alief_(belief)\">alieve</a>&nbsp;in it. For example, a few days ago I had a close call in traffic, and when a collision I fully expected to happen just a second prior did not transpire, I mentally thanked... whom? I definitely had a clear feeling of gratitude for escaping, and I don't normally mean it literally when I say \"Thank God!\". So, who or what did I feel thankful to? I've never been religious, and I got rid of most of my superstitions over the years, but apparently there is still something there, and I do not know how to react to this knowledge.</p>\n<p>What would be the proper reaction after a close call? Shrug and say \"got lucky this time, should be more cautious next time\"? What about when waiting for a diagnosis, what does sort-of-praying \"please, please, let everything be OK\" say about one's true beliefs? I know that I am much better at not blaming the world when something bad happens to me by chance than at not thanking the world when something good happens. Should it not be symmetric? Which part of a normally non-religious person wakes up and asserts itself in a crisis situation out of their control? Should it be embraced, suppressed, worked on?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "trCeFmHqGfk6SPncG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 18, "extendedScore": null, "score": 1.0544858257951174e-06, "legacy": true, "legacyId": "20569", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 58, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}