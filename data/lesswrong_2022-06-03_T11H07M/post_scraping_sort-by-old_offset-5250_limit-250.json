{"results": [{"createdAt": null, "postedAt": "2011-12-02T14:02:30.262Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Oakville ON, Atlanta, Berkeley, Melbourne, London", "slug": "weekly-lw-meetups-oakville-on-atlanta-berkeley-melbourne", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:37.190Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XD5qiqHTQ8dbwErHP/weekly-lw-meetups-oakville-on-atlanta-berkeley-melbourne", "pageUrlRelative": "/posts/XD5qiqHTQ8dbwErHP/weekly-lw-meetups-oakville-on-atlanta-berkeley-melbourne", "linkUrl": "https://www.lesswrong.com/posts/XD5qiqHTQ8dbwErHP/weekly-lw-meetups-oakville-on-atlanta-berkeley-melbourne", "postedAtFormatted": "Friday, December 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Oakville%20ON%2C%20Atlanta%2C%20Berkeley%2C%20Melbourne%2C%20London&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Oakville%20ON%2C%20Atlanta%2C%20Berkeley%2C%20Melbourne%2C%20London%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXD5qiqHTQ8dbwErHP%2Fweekly-lw-meetups-oakville-on-atlanta-berkeley-melbourne%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Oakville%20ON%2C%20Atlanta%2C%20Berkeley%2C%20Melbourne%2C%20London%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXD5qiqHTQ8dbwErHP%2Fweekly-lw-meetups-oakville-on-atlanta-berkeley-melbourne", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXD5qiqHTQ8dbwErHP%2Fweekly-lw-meetups-oakville-on-atlanta-berkeley-melbourne", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 365, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/55\">Sheridan College - Oakville, ON:&nbsp;<span class=\"date\">25 November 2011 08:30PM</span></a></li>\n<li><a href=\"/meetups/54\">Next Atlanta Less Wrong Meetup:&nbsp;<span class=\"date\">10 December 2011 06:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/56\">Monthly Bay Area meetup:&nbsp;<span class=\"date\">26 November 2011 07:00PM</span></a></li>\n<li><a href=\"/meetups/4y\">Melbourne practical rationality meetup:&nbsp;<span class=\"date\">02 December 2011 07:00PM</span></a></li>\n<li><span class=\"date\"><a href=\"/lw/3op/london_meetup_shakespeares_head_sunday_20110306/\">London, 04 December 2011 02:00PM.</a> (Details the same as in the link, just with a a different date.)<br /></span></li>\n</ul>\n<p>The Mountain View meetup is now every other week, rather than weekly.</p>\n<p>Cities with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>,</strong><strong> <a href=\"/r/discussion/lw/5pd/southern_california_meetup_may_21_weekly_irvine\">Irvine</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison, WI</a></strong>,<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin, CA</a> </strong>(uses the Bay Area List)<strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#San_Francisco\">San Francisco</a>,</strong><strong>&nbsp;</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>, and <strong><a href=\"/r/discussion/lw/6at/west_la_biweekly_meetups\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong><strong>.</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XD5qiqHTQ8dbwErHP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 8.086665560083764e-07, "legacy": true, "legacyId": "11108", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["36D3SH3WnYhLW8cEL", "pAHo9zSFXygp5A5dL", "tHFu6kvy2HMvQBEhW", "d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-02T15:04:24.480Z", "modifiedAt": null, "url": null, "title": "Good interview with Kahneman [link]", "slug": "good-interview-with-kahneman-link", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mrwxRvQ8QXWBeLH94/good-interview-with-kahneman-link", "pageUrlRelative": "/posts/mrwxRvQ8QXWBeLH94/good-interview-with-kahneman-link", "linkUrl": "https://www.lesswrong.com/posts/mrwxRvQ8QXWBeLH94/good-interview-with-kahneman-link", "postedAtFormatted": "Friday, December 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Good%20interview%20with%20Kahneman%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AGood%20interview%20with%20Kahneman%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmrwxRvQ8QXWBeLH94%2Fgood-interview-with-kahneman-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Good%20interview%20with%20Kahneman%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmrwxRvQ8QXWBeLH94%2Fgood-interview-with-kahneman-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FmrwxRvQ8QXWBeLH94%2Fgood-interview-with-kahneman-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 61, "htmlBody": "<p><a href=\"http://uctv.ucsd.edu/search-details.aspx?showID=12295\">http://uctv.ucsd.edu/search-details.aspx?showID=12295</a></p>\n<p>Direct audio link:</p>\n<p><a class=\"src-url\" style=\"display: inline-block; color: #5566dd; max-width: 500px; white-space: nowrap; overflow-x: hidden; overflow-y: hidden; text-overflow: ellipsis; text-decoration: none; padding-top: 4px; padding-bottom: 1px; font-family: Arial, sans-serif; font-size: 13px;\" href=\"http://podcast.uctv.tv/mp3/12295.mp3\" target=\"_blank\">http://podcast.uctv.tv/mp3/12295.mp3</a></p>\n<p>One thing that I found fascinating is the way he describes his collaboration with Tversky; he basically&nbsp;said&nbsp;that he had a \"common mind\" with his collaborator that was significantly better than their separate minds. This seems interesting and under explored/utilized area of Intelligence Augmentation (with some credit due to Vinge for his exploration of mind combinations in Tines world).&nbsp;</p>\n<p>Possibly relevant:</p>\n<p><a href=\"/lw/6j1/find_yourself_a_worthy_opponent_a_chavruta/\">http://lesswrong.com/lw/6j1/find_yourself_a_worthy_opponent_a_chavruta/</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mrwxRvQ8QXWBeLH94", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 8.086888895218041e-07, "legacy": true, "legacyId": "11245", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["AZJhBqBvdMCH4oLSP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-02T21:42:48.608Z", "modifiedAt": null, "url": null, "title": "\"Ray Kurzweil and Uploading: Just Say No!\", Nick Agar", "slug": "ray-kurzweil-and-uploading-just-say-no-nick-agar", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:59.379Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/agbSrvyL3tDh3ZP7h/ray-kurzweil-and-uploading-just-say-no-nick-agar", "pageUrlRelative": "/posts/agbSrvyL3tDh3ZP7h/ray-kurzweil-and-uploading-just-say-no-nick-agar", "linkUrl": "https://www.lesswrong.com/posts/agbSrvyL3tDh3ZP7h/ray-kurzweil-and-uploading-just-say-no-nick-agar", "postedAtFormatted": "Friday, December 2nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Ray%20Kurzweil%20and%20Uploading%3A%20Just%20Say%20No!%22%2C%20Nick%20Agar&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Ray%20Kurzweil%20and%20Uploading%3A%20Just%20Say%20No!%22%2C%20Nick%20Agar%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FagbSrvyL3tDh3ZP7h%2Fray-kurzweil-and-uploading-just-say-no-nick-agar%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Ray%20Kurzweil%20and%20Uploading%3A%20Just%20Say%20No!%22%2C%20Nick%20Agar%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FagbSrvyL3tDh3ZP7h%2Fray-kurzweil-and-uploading-just-say-no-nick-agar", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FagbSrvyL3tDh3ZP7h%2Fray-kurzweil-and-uploading-just-say-no-nick-agar", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1734, "htmlBody": "<p>A new paper has gone up in the November 2011 JET: <a href=\"http://jetpress.org/v22/agar.htm\">\"</a><a href=\"http://jetpress.org/v22/agar.htm\">Ray Kurzweil and Uploading: Just Say No!\"</a> (<a href=\"https://whatsortsofpeople.wordpress.com/tag/nick-agar/\">videos</a>) by <a href=\"http://www.victoria.ac.nz/hppi/staff/nicholas-agar.aspx\">Nick Agar</a> (<a href=\"https://en.wikipedia.org/wiki/Nicholas_Agar\">Wikipedia</a>); abstract:</p>\n<blockquote>\n<p class=\"MsoNormal\" style=\"margin-left:36.0pt;text-align:justify\"><span style=\"font-size:11.0pt;mso-ansi-language:EN-GB\" lang=\"EN-GB\">There is a debate about the possibility of mind-uploading &ndash; a process that purportedly transfers human minds and therefore human identities into computers. This paper bypasses the debate about the metaphysics of mind-uploading to address the rationality of submitting yourself to it. I argue that an <span class=\"SpellE\">ineliminable</span> risk that mind-uploading will fail <span class=\"GramE\">makes</span> it prudentially irrational for humans to undergo it.</span></p>\n</blockquote>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>The argument is a variant of Pascal's wager he calls Searle's wager. As far as I can tell, the paper contains mostly ideas he has already written on in his book; from Michael Hauskeller's <a href=\"https://lirias.kuleuven.be/bitstream/123456789/296849/2/Book+Reviews.pdf\">review</a> of Agar's <em>Humanity's End: Why We Should Reject Radical Enhancement</em></p>\n<blockquote>\n<p>Starting with Kurzweil, he gives a detailed account of the latter&rsquo;s &ldquo;Law of Accelerating Returns&rdquo; and the ensuing techno-optimism,&nbsp; which leads Kurzweil to believe that we will eventually be able to get rid of our messy bodies and gain virtual immortality by uploading ourselves into a computer. The whole idea is ludicrous, of course, but Agar takes it quite seriously and tries hard to convince us that &ldquo;it may take longer than Kurzweil thinks for us to know enough about the human brain to successfully upload it&rdquo; (45) &ndash; as if this lack of knowledge was the main obstacle to mind-uploading. Agar&rsquo;s principal objection, however, is that it will always be irrational for us to upload our minds onto computers, because we will never be able to completely rule out the possibility that, instead of continuing to live, we will simply die and be replaced by something that may be conscious or unconscious, but in any case is not identical with us. While this is certainly a reasonable objection, the way Agar presents it is rather odd. He takes Pascal&rsquo;s &lsquo;Wager&rsquo; (which was designed to convince us that believing in God is always the rational thing to do, because by doing so we have little to lose and a lot to win) and refashions it so that it appears irrational to upload one&rsquo;s mind, because the procedure might end in death, whereas refusing to upload will keep us alive and is hence always a safe bet. The latter conclusion does not work, of course, since the whole point of mind-uploading is to escape death (which is unavoidable as long as we are stuck with our mortal, organic bodies). Agar argues, however, that by the time we are able to upload minds to computers, other life extension technologies will be available, so that uploading will no longer be an attractive option. This seems to be a curiously techno-optimistic view to take.</p>\n</blockquote>\n<p><a href=\"https://sites.google.com/site/johndanaher84/\">John Danaher</a> (<a href=\"/user/JohnD/\">User:JohnD</a>) examines the wager, as expressed in the book, further in 2 blog posts:</p>\n<ol>\n<li><a href=\"http://philosophicaldisquisitions.blogspot.com/2011/05/should-we-upload-our-minds-agar-on.html\">\"Should we Upload Our Minds? Agar on Searle's Wager (Part One)\"</a></li>\n<li><a href=\"http://philosophicaldisquisitions.blogspot.com/2011/05/should-we-upload-our-minds-agar-on_22.html\">\"Should we Upload Our Minds? Agar on Searle's Wager (Part Two)\" </a></li>\n</ol>\n<p>After laying out what seems to be Agar's argument, Danaher constructs the game-theoretic tree and continues the criticism above:</p>\n<blockquote>\n<p>The initial force of the Searlian Wager derives from recognising the possibility that Weak AI is true. For if Weak AI is true, the act of uploading would effectively amount to an act of self-destruction. But recognising the possibility that Weak AI is true is not enough to support the argument. Expected utility calculations can often have strange and counterintuitive results. To know what we should really do, we have to know whether the following inequality really holds (numbering follows part one):</p>\n<ul>\n<li>(6) Eu(~U) &gt; Eu(U)</li>\n</ul>\nBut there&rsquo;s a problem: we have no figures to plug into the relevant equations, and even if we did come up with figures, people would probably dispute them (&ldquo;You&rsquo;re underestimating the benefits of uploading&rdquo;, &ldquo;You&rsquo;re underestimating the costs of uploading&rdquo; etc. etc.). So what can we do? Agar employs an interesting strategy. He reckons that if he can show that the following two propositions hold true, he can defend (6).<br /> \n<ul>\n<li>(8) Death (outcome c) is much worse for those considering to upload than living (outcome b or d).</li>\n</ul>\n<ul>\n<li>(9) Uploading and surviving (a) is not much better, and possibly worse, than not uploading and living (b or d).</li>\n</ul>\n...<strong><span style=\"text-decoration: underline;\">2. A Fate Worse than Death?</span></strong><br /> On the face of it, (8) seems to be obviously false. There would appear to be contexts in which the risk of self-destruction does not outweigh the potential benefit (however improbable) of continued existence. Such a context is often exploited by the purveyors of cryonics. It looks something like this:<br /> <br />\n<blockquote>You have recently been diagnosed with a terminal illness. The doctors say you&rsquo;ve got six months to live, tops. They tell you to go home, get your house in order, and prepare to die. But you&rsquo;re having none of it. You recently read some adverts for a cryonics company in California. For a fee, they will freeze your disease-ridden body (or just the brain!) to a cool -196 C and keep it in storage with instructions that it only be thawed out at such a time when a cure for your illness has been found. What a great idea, you think to yourself. Since you&rsquo;re going to die anyway, why not take the chance (make the bet) that they&rsquo;ll be able to resuscitate and cure you in the future? After all, you&rsquo;ve got nothing to lose.</blockquote>\n</blockquote>\n<blockquote><br /> This is a persuasive argument. Agar concedes as much. But he thinks the wager facing our potential uploader is going to be crucially different from that facing the cryonics patient. The uploader will not face the choice between certain death, on the one hand, and possible death/possible survival, on the other. No; the uploader will face the choice between continued biological existence with biological enhancements, on the one hand, and possible death/possible survival (with electronic enhancements), on the other.<br /> <br /> The reason has to do with the kinds of technological wonders we can expect to have developed by the time we figure out how to upload our minds. Agar reckons we can expect such wonders to allow for the indefinite continuance of biological existence. To support his point, he appeals to the ideas of Aubrey de Grey. de Grey thinks that -- given appropriate funding -- medical technologies could soon help us to achieve longevity escape velocity (LEV). This is when new anti-aging therapies consistently add years to our life expectancies faster than age consumes them.<br /> <br /> If we do achieve LEV, and we do so before we achieve uploadability, then premise (8) would seem defensible. Note that this argument does not actually require LEV to be highly probable. It only requires it to be relatively more probable than the combination of uploadability and Strong AI.</blockquote>\n<blockquote>...<strong><span style=\"text-decoration: underline;\">3. Don&rsquo;t you want Wikipedia on the Brain?</span></strong><br /> Premise (9) is a little trickier. It proposes that the benefits of continued biological existence are not much worse (and possibly better) than the benefits of Kurweil-ian uploading. How can this be defended? Agar provides us with two reasons.<br /> <br /> The first relates to the disconnect between our subjective perception of value and the objective reality. Agar points to findings in experimental economics that suggest we have a non-linear appreciation of value. I&rsquo;ll just quote him directly since he explains the point pretty well:<br /> <br />\n<blockquote><em>For most of us, a prize of $100,000,000 is not 100 times better than one of $1,000,000. We would not trade a ticket in a lottery offering a one-in-ten chance of winning $1,000,000 for one that offers a one-in-a-thousand chance of winning $100,000,000, even when informed that both tickets yield an expected return of $100,000....We have no difficulty in recognizing the bigger prize as better than the smaller one. But we don&rsquo;t prefer it to the extent that it&rsquo;s objectively...The conversion of objective monetary values into subjective benefits reveals the one-in-ten chance at $1,000,000 to be significantly better than the one-in-a-thousand chance at $100,000,000&nbsp;</em>(pp. 68-69).</blockquote>\n</blockquote>\n<blockquote><br /> How do these quirks of subjective value affect the wager argument? Well, the idea is that continued biological existence with LEV is akin to the one-in-ten chance of $1,000,000, while uploading is akin to the one-in-a-thousand chance of $100,000,000: people are going to prefer the former to the latter, even if the latter might yield the same (or even a higher) payoff.<br /> <br /> I have two concerns about this. First, my original formulation of the wager argument relied on the straightforward expected-utility-maximisation-principle of rational choice. But by appealing to the risks associated with the respective wagers, Agar would seem to be incorporating some element of risk aversion into his preferred rationality principle. This would force a revision of the original argument (premise 5 in particular), albeit one that works in Agar&rsquo;s favour. Second, the use of subjective valuations might affect our interpretation of the argument. In particular it raises the question: Is Agar saying that this is how people will <em><span style=\"text-decoration: underline;\">in fact</span></em> react to the uploading decision, or is he saying that this is how they <em><span style=\"text-decoration: underline;\">should</span></em> react to the decision?</blockquote>\n<p>One point is worth noting: the asymmetry of uploading with cryonics is <em>deliberate</em>. There is nothing in cryonics which renders it different from Searle's wager with 'destructive uploading', because one can always commit suicide and then be cryopreserved (symmetrical with committing suicide and then being destructively scanned / committing suicide by being destructively scanned). The asymmetry exists as a matter of policy: the cryonics organizations refuse to take suicides.</p>\n<p>Overall, I agree with the 2 quoted people; there is a small intrinsic philosophical risk to uploading as well as the obvious practical risk that it won't work, and this means uploading does not strictly dominate life-extension or other actions. But this is not a controversial point and has already in practice been embraced by cryonicists in their analogous way (and we can expect any uploading to be either non-destructive or post-mortem), and to the extent that Agar thinks that this is a <em>large</em> or <em>overwhelming</em> disadvantage for uploading (\"<span style=\"font-size: 11pt;\" lang=\"EN-US\">It is unlikely to be rational to make an electronic copy of yourself and destroy your original biological brain and body.\")</span>, he is incorrect.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"jQytxyauJ7kPhhGj3": 2, "ksdiAMKfgSyEeKMo6": 2, "5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "agbSrvyL3tDh3ZP7h", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 6, "extendedScore": null, "score": 8.088326506806748e-07, "legacy": true, "legacyId": "11247", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 79, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-03T03:07:14.269Z", "modifiedAt": null, "url": null, "title": "Does Hyperbolic Discounting Really Exist?", "slug": "does-hyperbolic-discounting-really-exist", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:38.031Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MDSt7vSqFPZRzk26B/does-hyperbolic-discounting-really-exist", "pageUrlRelative": "/posts/MDSt7vSqFPZRzk26B/does-hyperbolic-discounting-really-exist", "linkUrl": "https://www.lesswrong.com/posts/MDSt7vSqFPZRzk26B/does-hyperbolic-discounting-really-exist", "postedAtFormatted": "Saturday, December 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Does%20Hyperbolic%20Discounting%20Really%20Exist%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADoes%20Hyperbolic%20Discounting%20Really%20Exist%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMDSt7vSqFPZRzk26B%2Fdoes-hyperbolic-discounting-really-exist%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Does%20Hyperbolic%20Discounting%20Really%20Exist%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMDSt7vSqFPZRzk26B%2Fdoes-hyperbolic-discounting-really-exist", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMDSt7vSqFPZRzk26B%2Fdoes-hyperbolic-discounting-really-exist", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1577, "htmlBody": "<p><a href=\"/lw/17x/beware_of_weird_psychological_samples/\">&ldquo;Beware of WEIRD psychological samples&rdquo;</a> because results derived from them may reflect the specific sample more than any kind of generalized truth. And LessWrong has generalized <a href=\"http://en.wikipedia.org/wiki/Hyperbolic_discounting\">hyperbolic discounting</a> <em>out the wazoo</em>. (See the tags <a href=\"/tag/akrasia/\">akrasia</a> and <a href=\"/tag/discounting/\">discounting</a>.) Hyperbolic discounting is bad, of course, because among other things it leaves on vulnerable to preference reversals and inconsistencies and hence money-pumping.</p>\n<p>But isn&rsquo;t it odd that for a fundamental fact of human psychology, a huge bias we have spent a ton of collective time discussing and fighting, that it doesn&rsquo;t seem to lead to much actual money-pumping? The obvious examples like the dieting or gambling industries are pretty small, all things considered. And online services like <a href=\"/lw/7z1/antiakrasia_tool_like_stickkcom_for_data_nerds/\">BeeMinder</a> specifically devised on a hyperbolic discounting/picoeconomics basis are, as far as I know, useful but no dramatic breakthrough or silver bullet; again, not quite what one would expect. Like many other heuristics and biases, perhaps hyperbolic discounting isn&rsquo;t so bad after all, <em>in practice</em>.</p>\n<p>Ainslie mentions in <em>Breakdown of Will</em> somewhere that financial incentives can cause people to begin discounting exponentially. What if&hellip; hyperbolic discounting doesn&rsquo;t really exist, in practice? If it may reflect a failure of self-control, a kind of teenager trait, one we find in younger (but not older) populations - like university students?</p>\n<p><a id=\"more\"></a></p>\n<p>The following quotes are extracted from the paper <a href=\"http://www.dur.ac.uk/resources/dbs/faculty/working-papers/WP2011-01--DiscountingBehavior.pdf\">&ldquo;Discounting Behavior: A Reconsideration&rdquo;</a> (102 pages) by Steffen Andersen, Glenn W. Harrison, Morten Lau &amp; E. Elisabet Rutstr&ouml;m, January 2011:</p>\n<blockquote>\n<p>The implied econometrics calls for structural estimation of the theoretical models, allowing for joint estimation of utility functions and discounting functions. Using data collected from a representative sample of 413 adult Danes in 2009, we draw striking conclusions. Assuming an exponential discounting model we estimate discount rates to be 5.6% on average: this is significantly lower than all previous estimates using controlled experiments. We also find no evidence to support quasi-hyperbolic discounting or &ldquo;fixed cost&rdquo; discounting, and only modest evidence to support other specifications of non-constant discounting. Furthermore, the evidence for non-constant discounting, while statistically significant, is not economically significant in terms of the size of the estimated discount rates. We undertake extensive robustness checks on these findings, including a detailed review of the previous, comparable literature.</p>\n<p>&hellip;We do find evidence in favor of flexible Hyperbolic specifications and other nonstandard specifications, but with very modest variations in discount rates compared to those often assumed. We find that a significant portion of the Danish population uses Exponential discounting, even if it is not the single model that best explains observed behavior.</p>\n<p>Given the contrary nature of our findings, in terms of the received empirical wisdom, section 6 contains a systematic cataloguing of the samples, experimental procedures, and econometric procedures of the alleged evidence for Quasi-Hyperbolic and non-constant discounting. We conclude that the evidence needed reconsideration. The one clear pattern to emerge from the received literature is that non-constant discounting occurs for some university student samples.</p>\n<p>One major robustness check is therefore to see if the disappointing showing for the Quasi- Hyperbolic model is attributable to our population being the entire adult Danish population, rather than university students. Although it is apparent that the wider population is typically of greater interest, virtually all prior experimental evidence that we give credence to comes from convenience samples of university students. We find that there is indeed a difference in the elicited discount rates with (Danish) university students, and that they exhibit statistically significant evidence of declining discount rates. On the other hand, the size of the discount rates for shorter time horizons is much smaller than the received wisdom suggests.</p>\n<p>&hellip;Coller and Williams [1999] were the first to demonstrate the effect of a front end delay; their estimates show a drop in elicited discount rates over money of just over 30 percentage points from an average 71% with no front end delay.11 Using the same experimental and econometric methods, and with all choices having a front end delay, Harrison, Lau and Williams [2002] estimated average discount rates over money of 28.1% for the adult Danish population. Andersen, Harrison, Lau and Rutstr&ouml;m [2008a] were the first to demonstrate the effect of correcting for non-linear utility; their estimates show a drop in elicited discount rates of 15.1 percentage points from a discount rate over money of 25.2%. These results would lead us to expect discount rates around 10% with a front end delay, with a significantly higher rate when there is no front end delay.</p>\n<p>&hellip;The Exponential discounting model indicates a discount rate of only 5.6%, where all discount rates will be presented on an annualized basis. The 95% confidence interval for this estimate is between 4.1% and 7.0%, so this indicates even lower discount rates than the 10.1% reported by Andersen, Harrison, Lau and Rutstr&ouml;m [2008a] for the same population in 2003.25 For comparison, the Exponential discounting model assuming a linear utility function implies an 18.3% discount rate, with a 95% confidence interval between 15.5% and 21.2%, so this is also lower than the estimate for 2003 (25.2%, with a 95% confidence interval between 22.8% and 27.6%). We again conclude that correcting for the non-linearity of the utility function makes a significant quantitative difference to estimated discount rates.</p>\n<p>The most striking finding from Table 1, for us, is that there is no Quasi-Hyperbolic discounting. The key parameter, $, is not statistically or economically significantly different from 1, and the parameter * is virtually identical to the estimate from the Exponential discounting model. The p-value on a test of the hypothesis that $=1 has value 0.55, although the 95% confidence interval for $ is enough to see that it is not significantly different from 1.</p>\n<p>&hellip;The Weibull discounting model in panel F allows a very different pattern of non-constant discounting. Indeed, these parameter estimates do imply discount rates that vary slightly, from 6.7% for a 1 day horizon, to 6.0% for a 2 week horizon, and then down to 5.1% for a one year horizon. But the 95% confidence intervals on all of these is at least between 3% and 7%, and one cannot reject the Exponential discounting model hypothesis that s=1 (p-value of 0.73).</p>\n<p>&hellip;The only demographic covariate to have any statistically significant impact on elicited discount rates is whether the individual is a female. Women have discount rates that are 6.6 percentage points lower than men, and the p-value on this estimated effect of 0.092. In turn, this derives from women being more risk averse: their RRA is 0.294 higher than men, with a p-value on this estimated effect of 0.026. Hence they have a more concave utility function and, by Jensen&rsquo;s inequality applied to (0), have a lower implied discount rate. Looking at total effects instead of marginal effects, men on average have discount rates of 7.4% and women have discount rates of 3.6%, and the difference is statistically significant (p-value = 0.004).</p>\n<p>&hellip;Our results were a surprise to us, and the robustness checks reported above did not lead us to qualify that reaction. We fully expected to see much more &ldquo;hyperbolicky&rdquo; behavior when we removed the front end delay, and particularly when that was interacted with not providing the implied interest rates of each choice. We were not wedded to one hyperbolicky specification or the other, and did not expect the exponential model to be completely overwhelmed by the alternatives, but we did expect to see much more non-constant discounting. We therefore examined the literature, and tried to draw some inferences about what might explain the apparent differences in results.</p>\n<p>&hellip;[Literature survey] We ignored all hypothetical survey studies, on the grounds that the evidence is overwhelming that there can be huge and systematic hypothetical biases, and it is simply inefficient to repeat those arguments and waste time taking such evidence seriously. [Like prisoners doing a long sentence, and knowing the jokes and arguments of cellmates by heart, we would rather just point to surveys and evaluations of the evidence in Harrison [2006] and Harrison and Rutstr&ouml;m [2008b].] We also focused on experiments, rather than econometric inferences from naturally occurring data, because those data are easier to interpret and have generated the conventional wisdom.36 We excluded studies that did not lend themselves to inferring a discount function.37 Finally, we excluded any study that used procedures that were patently not incentive- compatible or that involved deception.38</p>\n<p>&hellip;One conclusion that we draw is that virtually all previous evidence of non-constant discounting comes from studies undertaken with students. We therefore conducted a conventional laboratory experiment, described below, using the same procedures as in our (artefactual) field experiment but with students recruited in Copenhagen&hellip;In order to determine if the evidence for non-constant discounting, such as it is, derives from the general focus on students samples, we replicated our field experiments with a student sample in Copenhagen recruited using standard methods.39 The experimental tasks were identical, to ensure comparability. Table 7 lists estimates from the student responses of the basic models in Table 1. The background risk attitudes of this sample were virtually identical to those of the adult Danish population.40 The results are clear: we obtain no evidence of quasi-hyperbolic discounting, no evidence of fixed-cost discounting, and no evidence of simple hyperbolic discounting. We do observe some non-constancy of some discount rates with the Weibull discounting specification, although the overall effect of the student sample is not statistically significant, as shown by the p- value of 0.18 on the null hypothesis that the specification is actually Exponential.41</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MDSt7vSqFPZRzk26B", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 29, "extendedScore": null, "score": 8.089495783554591e-07, "legacy": true, "legacyId": "11254", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["33YYcoWwtmqzAq9QR", "6oYETaG248zGF45aD"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-03T04:21:14.927Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Zen and the Art of Rationality", "slug": "seq-rerun-zen-and-the-art-of-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:39.345Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Ehbzeh5tf5b9cwzpW/seq-rerun-zen-and-the-art-of-rationality", "pageUrlRelative": "/posts/Ehbzeh5tf5b9cwzpW/seq-rerun-zen-and-the-art-of-rationality", "linkUrl": "https://www.lesswrong.com/posts/Ehbzeh5tf5b9cwzpW/seq-rerun-zen-and-the-art-of-rationality", "postedAtFormatted": "Saturday, December 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Zen%20and%20the%20Art%20of%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Zen%20and%20the%20Art%20of%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEhbzeh5tf5b9cwzpW%2Fseq-rerun-zen-and-the-art-of-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Zen%20and%20the%20Art%20of%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEhbzeh5tf5b9cwzpW%2Fseq-rerun-zen-and-the-art-of-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEhbzeh5tf5b9cwzpW%2Fseq-rerun-zen-and-the-art-of-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 177, "htmlBody": "<p>Today's post, <a href=\"/lw/m7/zen_and_the_art_of_rationality/\">Zen and the Art of Rationality</a> was originally published on 24 December 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Rationality is very different in its propositional statements from Eastern religions, like Taoism or Buddhism. But, it is sometimes easier to express ideas in rationality using the language of Zen or the Tao.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8oa/seq_rerun_effortless_technique/\">Effortless Technique</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Ehbzeh5tf5b9cwzpW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.089764701779702e-07, "legacy": true, "legacyId": "11255", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HLERouG7QBt7jzLt4", "tJsZDZkT9EvoM6NyL", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-03T04:58:30.013Z", "modifiedAt": null, "url": null, "title": "Scooby Doo and Secular Humanism [link]", "slug": "scooby-doo-and-secular-humanism-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:02.585Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dreaded_Anomaly", "createdAt": "2010-12-30T06:38:34.106Z", "isAdmin": false, "displayName": "Dreaded_Anomaly"}, "userId": "sBHF4CXWBLakPFzfu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nA9smziFLJzJu88Qo/scooby-doo-and-secular-humanism-link", "pageUrlRelative": "/posts/nA9smziFLJzJu88Qo/scooby-doo-and-secular-humanism-link", "linkUrl": "https://www.lesswrong.com/posts/nA9smziFLJzJu88Qo/scooby-doo-and-secular-humanism-link", "postedAtFormatted": "Saturday, December 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Scooby%20Doo%20and%20Secular%20Humanism%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AScooby%20Doo%20and%20Secular%20Humanism%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnA9smziFLJzJu88Qo%2Fscooby-doo-and-secular-humanism-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Scooby%20Doo%20and%20Secular%20Humanism%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnA9smziFLJzJu88Qo%2Fscooby-doo-and-secular-humanism-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnA9smziFLJzJu88Qo%2Fscooby-doo-and-secular-humanism-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 214, "htmlBody": "<p><a href=\"http://www.comicsalliance.com/2011/11/25/ask-chris-81-scooby-doo-and-secular-humanism\" target=\"_blank\">A great column by Chris Sims at the Comics Alliance</a>.</p>\n<p>Excerpt:</p>\n<blockquote>\n<p>Because that's the thing about Scooby-Doo: <strong>The bad guys in every episode aren't monsters, they're <em>liars</em></strong>.<br /> <br /> I can't imagine how scandalized those critics who were relieved to have  something that was mild enough to not excite their kids would've been if  they'd stopped for a second and realized what was actually going on.  The very first rule of <em>Scooby-Doo</em>, the single premise that sits  at the heart of their adventures, is that the world is full of  grown-ups who lie to kids, and that it's up to those kids to figure out  what those lies are and call them on it, even if there are other adults  who believe those lies with every fiber of their being. And the way that  you win isn't through supernatural powers, or even through fighting.  The way that you win is by doing the most dangerous thing that any  person being lied to by someone in power can do: You <em>think</em>.</p>\n</blockquote>\n<p>Tim Minchin fans may recall him mentioning Scooby Doo in a similar light in his beat poem Storm, and it's been <a href=\"/lw/3m/rationalist_fiction/2mb\">brought up</a> on Less Wrong before.</p>\n<p>When viewed in this light, Scooby Doo really is like an elementary version of Methods of Rationality.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GBpwq8cWvaeRoE9X5": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nA9smziFLJzJu88Qo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 40, "extendedScore": null, "score": 8.089899186709894e-07, "legacy": true, "legacyId": "11256", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-03T12:01:53.922Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne social meetup", "slug": "meetup-melbourne-social-meetup-21", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:55.805Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "toner", "createdAt": "2009-02-27T05:14:52.530Z", "isAdmin": false, "displayName": "toner"}, "userId": "XaYyFkpvhiiMscJJZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PYJyrw8utCWTwWyEA/meetup-melbourne-social-meetup-21", "pageUrlRelative": "/posts/PYJyrw8utCWTwWyEA/meetup-melbourne-social-meetup-21", "linkUrl": "https://www.lesswrong.com/posts/PYJyrw8utCWTwWyEA/meetup-melbourne-social-meetup-21", "postedAtFormatted": "Saturday, December 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20social%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20social%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPYJyrw8utCWTwWyEA%2Fmeetup-melbourne-social-meetup-21%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20social%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPYJyrw8utCWTwWyEA%2Fmeetup-melbourne-social-meetup-21", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPYJyrw8utCWTwWyEA%2Fmeetup-melbourne-social-meetup-21", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 70, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/5e'>Melbourne social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 December 2011 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">See mailing list; Carlton VIC 3053</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>All welcome! 6:30pm for 7pm.</p>\n\n<p>We'll order takeaway for dinner and I'll get some snacks. BYO drinks and games.</p>\n\n<p>If you have problems getting in, you can call me on 0412 996 288. This meetup repeats on the third Friday of each month.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/5e'>Melbourne social meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PYJyrw8utCWTwWyEA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.091428017290957e-07, "legacy": true, "legacyId": "11260", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_social_meetup\">Discussion article for the meetup : <a href=\"/meetups/5e\">Melbourne social meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 December 2011 07:00:00PM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">See mailing list; Carlton VIC 3053</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>All welcome! 6:30pm for 7pm.</p>\n\n<p>We'll order takeaway for dinner and I'll get some snacks. BYO drinks and games.</p>\n\n<p>If you have problems getting in, you can call me on 0412 996 288. This meetup repeats on the third Friday of each month.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_social_meetup1\">Discussion article for the meetup : <a href=\"/meetups/5e\">Melbourne social meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne social meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_social_meetup", "level": 1}, {"title": "Discussion article for the meetup : Melbourne social meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_social_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "6 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-03T12:33:31.484Z", "modifiedAt": null, "url": null, "title": "Why safe Oracle AI is easier than safe general AI, in a nutshell", "slug": "why-safe-oracle-ai-is-easier-than-safe-general-ai-in-a", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:01.020Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7nLMhdhXMKBLWiynJ/why-safe-oracle-ai-is-easier-than-safe-general-ai-in-a", "pageUrlRelative": "/posts/7nLMhdhXMKBLWiynJ/why-safe-oracle-ai-is-easier-than-safe-general-ai-in-a", "linkUrl": "https://www.lesswrong.com/posts/7nLMhdhXMKBLWiynJ/why-safe-oracle-ai-is-easier-than-safe-general-ai-in-a", "postedAtFormatted": "Saturday, December 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20safe%20Oracle%20AI%20is%20easier%20than%20safe%20general%20AI%2C%20in%20a%20nutshell&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20safe%20Oracle%20AI%20is%20easier%20than%20safe%20general%20AI%2C%20in%20a%20nutshell%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7nLMhdhXMKBLWiynJ%2Fwhy-safe-oracle-ai-is-easier-than-safe-general-ai-in-a%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20safe%20Oracle%20AI%20is%20easier%20than%20safe%20general%20AI%2C%20in%20a%20nutshell%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7nLMhdhXMKBLWiynJ%2Fwhy-safe-oracle-ai-is-easier-than-safe-general-ai-in-a", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7nLMhdhXMKBLWiynJ%2Fwhy-safe-oracle-ai-is-easier-than-safe-general-ai-in-a", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 213, "htmlBody": "<p><strong>Moderator</strong>: \"In our televised forum, 'Moral problems of our time, as seen by dead people', we are proud and&nbsp;privileged&nbsp;to welcome two of the most important men of the twentieth century: Adolf Hitler and Mahatma&nbsp;Gandhi. So, gentleman, if you had a general autonomous&nbsp;superintelligence&nbsp;at your disposal, what would you want it to do?\"</p>\n<p><strong>Hitler</strong>: \"I'd want it to kill all the Jews... and humble France... and crush communism... and give a rebirth to the glory of all the beloved Germanic people... and cure our blond blue eyed (plus me) glorious Aryan nation of the corruption of lesser brown-eyed races (except for me)... and... and... and... and...&nbsp;and... and... and...&nbsp;and... and... and...&nbsp;and... and... and...&nbsp;and... and... and...&nbsp;and... and... and...&nbsp;and... and... and...&nbsp;and... and... and...&nbsp;and... and... and...\"</p>\n<p><strong>Gandhi</strong>: \"I'd want it to convince the British to grant Indian independence... and overturn the cast system... and cause people of different colours to value and respect one another... and grant self-sustaining&nbsp;livelihoods&nbsp;to all the poor and oppressed of this world... and purge violence from the heart of men... and&nbsp;reconcile religions... and... and... and... and...&nbsp;and... and... and...&nbsp;and... and... and...&nbsp;and... and... and...&nbsp;and... and... and...&nbsp;and... and... and...&nbsp;and... and... and...&nbsp;and... and... and...&nbsp;and... and... and...\"</p>\n<p><strong>Moderator</strong>: \"And if instead you had a superintelligent Oracle, what would you want it to do?\"</p>\n<p><strong>Hitler and Gandhi together</strong>: \"Stay inside the box and answer questions&nbsp;accurately\".</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb26d": 4}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7nLMhdhXMKBLWiynJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 5, "extendedScore": null, "score": 5e-06, "legacy": true, "legacyId": "11261", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 66, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-03T18:57:11.917Z", "modifiedAt": null, "url": null, "title": "[Link] How Should Rationalists Approach Death? (Skepticon 4)", "slug": "link-how-should-rationalists-approach-death-skepticon-4", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:48.756Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CharlesR", "createdAt": "2011-01-20T15:48:46.140Z", "isAdmin": false, "displayName": "CharlesR"}, "userId": "JcnBrK6C7uP8sPP2G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RTqRKrEHg7cqwoLCS/link-how-should-rationalists-approach-death-skepticon-4", "pageUrlRelative": "/posts/RTqRKrEHg7cqwoLCS/link-how-should-rationalists-approach-death-skepticon-4", "linkUrl": "https://www.lesswrong.com/posts/RTqRKrEHg7cqwoLCS/link-how-should-rationalists-approach-death-skepticon-4", "postedAtFormatted": "Saturday, December 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20How%20Should%20Rationalists%20Approach%20Death%3F%20(Skepticon%204)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20How%20Should%20Rationalists%20Approach%20Death%3F%20(Skepticon%204)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRTqRKrEHg7cqwoLCS%2Flink-how-should-rationalists-approach-death-skepticon-4%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20How%20Should%20Rationalists%20Approach%20Death%3F%20(Skepticon%204)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRTqRKrEHg7cqwoLCS%2Flink-how-should-rationalists-approach-death-skepticon-4", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRTqRKrEHg7cqwoLCS%2Flink-how-should-rationalists-approach-death-skepticon-4", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>http://www.youtube.com/watch?v=Cd0yF2ucFbE</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RTqRKrEHg7cqwoLCS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": -5, "extendedScore": null, "score": -3e-06, "legacy": true, "legacyId": "11262", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-03T19:22:25.731Z", "modifiedAt": null, "url": null, "title": "[LINK] Fermi Paradox paper touching on FAI", "slug": "link-fermi-paradox-paper-touching-on-fai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:48.655Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "malthrin", "createdAt": "2011-03-22T15:23:59.536Z", "isAdmin": false, "displayName": "malthrin"}, "userId": "5b5DcLkcYGD9YGRfF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kYTcfWLzu3aX4jLaB/link-fermi-paradox-paper-touching-on-fai", "pageUrlRelative": "/posts/kYTcfWLzu3aX4jLaB/link-fermi-paradox-paper-touching-on-fai", "linkUrl": "https://www.lesswrong.com/posts/kYTcfWLzu3aX4jLaB/link-fermi-paradox-paper-touching-on-fai", "postedAtFormatted": "Saturday, December 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Fermi%20Paradox%20paper%20touching%20on%20FAI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Fermi%20Paradox%20paper%20touching%20on%20FAI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkYTcfWLzu3aX4jLaB%2Flink-fermi-paradox-paper-touching-on-fai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Fermi%20Paradox%20paper%20touching%20on%20FAI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkYTcfWLzu3aX4jLaB%2Flink-fermi-paradox-paper-touching-on-fai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkYTcfWLzu3aX4jLaB%2Flink-fermi-paradox-paper-touching-on-fai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 121, "htmlBody": "<p><a href=\"http://arxiv.org/PS_cache/arxiv/pdf/1111/1111.6131v1.pdf\">This paper</a> discusses the Fermi Paradox in the context of civilizations that can build self-replicating probes (SRPs) to explore/exploit the galaxy. In passing, it discusses some FAI-related objections to self-replicating machine intelligence.</p>\n<blockquote>\n<p>One popular argument against SRPs is presented by Sagan and Newman (Sagan and Newman, 1983). They argue that any presumably wise and cautious civilization would never develop SRPs because such machines would pose an existential risk to the original civilization. The concern is that the probes may undergo a mutation which permits and motivates them to either wipe out the homeworld or overcome any reasonable limit on their reproduction rate, in effect becoming a technological cancer that converts every last ounce of matter in the galaxy into SRPs.</p>\n</blockquote>\n<p>Bad Clippy.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kYTcfWLzu3aX4jLaB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 2, "extendedScore": null, "score": 8.093019250486506e-07, "legacy": true, "legacyId": "11264", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-03T23:21:45.834Z", "modifiedAt": null, "url": null, "title": "Intelligence Explosion analysis draft #2: snippet 1", "slug": "intelligence-explosion-analysis-draft-2-snippet-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:39.236Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rWwmRvp8FEKDZ49d6/intelligence-explosion-analysis-draft-2-snippet-1", "pageUrlRelative": "/posts/rWwmRvp8FEKDZ49d6/intelligence-explosion-analysis-draft-2-snippet-1", "linkUrl": "https://www.lesswrong.com/posts/rWwmRvp8FEKDZ49d6/intelligence-explosion-analysis-draft-2-snippet-1", "postedAtFormatted": "Saturday, December 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Intelligence%20Explosion%20analysis%20draft%20%232%3A%20snippet%201&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntelligence%20Explosion%20analysis%20draft%20%232%3A%20snippet%201%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrWwmRvp8FEKDZ49d6%2Fintelligence-explosion-analysis-draft-2-snippet-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Intelligence%20Explosion%20analysis%20draft%20%232%3A%20snippet%201%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrWwmRvp8FEKDZ49d6%2Fintelligence-explosion-analysis-draft-2-snippet-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrWwmRvp8FEKDZ49d6%2Fintelligence-explosion-analysis-draft-2-snippet-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2214, "htmlBody": "<p>I've been writing a new draft of the <a href=\"/r/discussion/lw/8et/toward_an_overview_analysis_of_intelligence/\">intelligence explosion analysis I'm writing with Anna Salamon</a>. I've incorporated much of the feedback LWers have given me, and will now present snippets of the <strong>new draft</strong> for feedback. Please ignore the formatting issues caused by moving the text from Google Docs to Less Wrong.</p>\n<p>&nbsp;</p>\n<p>_____</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><br id=\"internal-source-marker_0.8263228819705546\" /></p>\n<h2 id=\"internal-source-marker_0.8263228819705546\" dir=\"ltr\"><span style=\"font-size: 24px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Intelligence Explosion: Evidence and Import</span></h2>\n<p><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Anna Salamon</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Luke Muehlhauser</span><br /><br /><br /></p>\n<p style=\"margin-left: 36pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The best answer to the question, \"Will computers ever be as smart as humans?\" is probably &ldquo;Yes, but only briefly.\"</span></p>\n<p>&nbsp;</p>\n<p style=\"text-align: right; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Vernor Vinge</span></p>\n<p><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> </span><br /><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Humans may create human-level artificial intelligence in this century.</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>1</sup></span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> Shortly thereafter, we may see an &ldquo;intelligence explosion&rdquo; &mdash; a chain of events by which human-level AI leads, fairly rapidly, to intelligent systems whose capabilities far surpass those of biological humanity as a whole.</span><br /><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">How likely is this, and what should we do about it? Others have discussed these questions previously (Turing 1950; Good 1965; Von Neumann 1966; Solomonoff 1985; Vinge 1993; Bostrom 2003; Yudkowsky 2008; Chalmers 2010), but no brief, systematic review of the relevant issues has been published. In this chapter we aim to provide such a review.</span></p>\n<h2 dir=\"ltr\"><span style=\"font-size: 24px; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Why study intelligence explosion?</span></h2>\n<p><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">As Chalmers (2010) notes, the singularity is of great practical interest:</span></p>\n<p style=\"margin-left: 36pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">If there is a singularity, it will be one of the most important events in the history of the planet. An intelligence explosion has enormous potential benefits: a cure for all known diseases, an end to poverty, extraordinary scientific advances, and much more. It also has enormous potential dangers: an end to the human race, an arms race of warring machines, the power to destroy the planet...</span></p>\n<p><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The singularity is also a challenging scientific and philosophical topic. Under the spectre of intelligence explosion, long-standing philosophical puzzles about values, other minds, and personal identity become, as Chalmers puts it, \"life-or-death questions that may confront us in coming decades or centuries.\" In science, the development of AI will require progress in several of mankind's grandest scientific projects, including reverse-engineering the brain (Schierwagen 2011) and developing artificial minds (Nilsson 2010), while the development of AI safety mechanisms may require progress on the confinement problem in computer science (Lampson 1973; Yampolskiy 2011) and the cognitive science of human values (Muehlhauser and Helm, this volume). The creation of AI would also revolutionize scientific method, as most science would be done by intelligent machines (Sparkes et al. 2010).</span><br /><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Such questions are complicated, the future is uncertain, and our chapter is brief. Our aim, then, is not to provide detailed arguments but only to sketch the issues involved, pointing the reader to authors who have analyzed each component in more detail. We believe these matters are important, and our discussion of them must be permitted to begin at a low level because there is no other place to lay the first stones.</span></p>\n<h2 dir=\"ltr\"><span style=\"font-size: 24px; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">What we will (not) argue</span></h2>\n<p><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">\"Technological singularity\" has come to mean many things (Sandberg, this volume), including: accelerating technological change (Kurzweil 2005), a limit in our ability to predict the future (Vinge 1993), and the topic we will discuss: an intelligence explosion leading to the creation of machine superintelligence (Yudkowsky 1996). Because the singularity is associated with such a variety of views and arguments, we must clarify what this chapter will and will not argue.</span><br /><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">First, we will not tell detailed stories about the future. In doing so, we would likely commit the &ldquo;if and then&rdquo; fallacy, by which an improbable conditional becomes a supposed actual (Nordmann 2007). For example, we will not assume the continuation of Moore&rsquo;s law, nor that hardware trajectories determine software progress, nor that technological trends will be exponential rather than logistic (see Modis, this volume), nor indeed that progress will accelerate rather than decelerate (see Plebe and Perconti, this volume). Instead, we will examine convergent outcomes that &mdash; like the evolution of eyes or the emergence of markets &mdash; can come about through many different paths and can gather momentum once they begin. Humans tend to underestimate the likelihood of such convergent outcomes (Tversky and Kahneman 1974), and we believe intelligence explosion is one of them.</span><br /><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Second, we will not assume that human intelligence is realized by a classical computation system, nor that intelligent machines will have internal mental properties like consciousness or \"understanding.\" Such factors are mostly irrelevant to the occurrence of a singularity, so objections to these claims (Lucas 1961; Dreyfus 1972; Searle 1980; Block 1981; Penrose 1994; Van Gelder and Port 1995) are not objections to the singularity.</span><br /><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">What, then, will we argue? First, we suggest there is a significant probability we will create human-level AI (hereafter, \"AI\") within a century. Second, we suggest that AI is likely to lead rather quickly to machine superintelligence. Finally, we discuss the possible consequences of machine superintelligence and consider which actions we can take now to shape our future.</span></p>\n<h2 dir=\"ltr\"><span style=\"font-size: 24px; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">From here to AI</span></h2>\n<p><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Our first step is to survey the evidence concerning whether we should expect the creation of AI within a century.</span><br /><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">By \"AI,\" we refer to \"systems which match or exceed the cognitive performance of humans in virtually all domains of interest\" (Shulman &amp; Bostrom 2011). On this definition, IBM's </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-style: italic; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Jeopardy!</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">-playing Watson computer is not an AI but merely a \"narrow AI\" because it can only solve a narrow set of problems. Drop Watson in a pond or ask it to do original science, and it is helpless. Imagine instead a machine that can invent new technologies, manipulate humans with acquired social skills, and otherwise learn to navigate new environments as needed.</span><br /><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">There are many types of AI. To name just three:</span></p>\n<ul>\n<li style=\"list-style-type: disc; font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline;\"><span style=\"background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The code of a </span><span style=\"background-color: transparent; font-style: italic; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">transparent AI</span><span style=\"background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> is written explicitly by, and largely understood by, its programmers.</span><span style=\"background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>2</sup></span></li>\n<li style=\"list-style-type: disc; font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline;\"><span style=\"background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">An </span><span style=\"background-color: transparent; font-style: italic; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">opaque AI</span><span style=\"background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> is not transparent to its creators. For example it could be, like the human brain, a messy ensemble of cognitive modules. In an AI, these modules might be written by different teams for different purposes, using different languages and approaches.</span></li>\n<li style=\"list-style-type: disc; font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline;\"><span style=\"background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">A </span><span style=\"background-color: transparent; font-style: italic; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">whole brain emulation</span><span style=\"background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> (WBE) is a computer emulation of the brain structures required to functionally reproduce human thought and perhaps consciousness (Sandberg and Bostrom 2008). We need not understand the detailed mechanisms of general intelligence to reproduce a brain functionally on a computing substrate.</span></li>\n</ul>\n<p><br /><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Whole brain emulation uses the human software for intelligence already invented by evolution, while other forms of AI (\"</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-style: italic; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">de novo</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> AI\") require inventing intelligence anew, to varying degrees.</span><br /><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">When should we expect AI? Unfortunately, expert elicitation methods have not proven useful for long-term forecasting,</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>3</sup></span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> and prediction markets have not yet been tested much for technological forecasting (Williams 2011), so our analysis must allow for a wide range of outcomes. We will first consider how difficult the problem seems to be, and then which inputs toward solving the problem &mdash; and which \"speed bumps\" &mdash; we can expect in the next century. </span></p>\n<h3 dir=\"ltr\"><span style=\"font-size: 19px; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">How hard is whole brain emulation?</span></h3>\n<p><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Because whole brain emulation will rely mostly on scaling up existing technologies like microscopy and large-scale cortical simulation, WBE may be largely an \"engineering\" problem, and thus more predictable than other kinds of AI.</span><br /><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Several authors have discussed the difficulty of WBE in detail (Sandberg and Bostrom 2008; de Garis et al. 2010; Modha et al. 2011; Cattell &amp; Parker 2011). In short: The difficulty of WBE depends on many factors, and in particular on the resolution of emulation required for successful WBE. For example, proteome-resolution emulation will require more resources and technological development than emulation at the resolution of the brain's neural network. In perhaps the most likely scenario,</span></p>\n<p style=\"margin-left: 36pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">WBE on the neuronal/synaptic level requires relatively modest increases in microscopy resolution, a less trivial development of automation for scanning and image processing, a research push at the problem of inferring functional properties of neurons and synapses, and relatively business\u2010as\u2010usual development of computational neuroscience models and computer hardware.</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>4</sup></span></p>\n<p>&nbsp;</p>\n<h3 dir=\"ltr\"><span style=\"font-size: 19px; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">How hard is </span><span style=\"font-size: 19px; font-family: Arial; background-color: transparent; font-style: italic; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">de novo</span><span style=\"font-size: 19px; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> AI?</span></h3>\n<p><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">There is a vast space of possible mind designs for </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-style: italic; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">de novo</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> AI; talking about \"non-human intelligence\" is like talking about \"non-platypus animals\" (Dennett 1997; Pennachin and Goertzel 2007; Yudkowsky 2008).</span><br /><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">We do not know what it takes to build </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-style: italic; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">de novo</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> AI. Because of this, we do not know what groundwork will be needed to understand general intelligence, nor how long it may take to get there.</span><br /><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Worse, it&rsquo;s easy to think we do know. Studies show that except for weather forecasters (Murphy and Winkler 1984), nearly all of us give inaccurate probability estimates when we try, and in particular we are overconfident of our predictions (Lichtenstein, Fischoff, and Phillips 1982; Griffin and Tversky 1992; Yates et al. 2002). Experts, too, often do little better than chance (Tetlock 2005), and are outperformed by crude computer algorithms (Grove and Meehl 1996; Grove et al. 2000; Tetlock 2005). So if you have a gut feeling about when digital intelligence will arrive, it is probably wrong.</span><br /><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">But uncertainty is not a &ldquo;get out of prediction free&rdquo; card. You either will or will not save for retirement, encourage WBE development, or support AI risk reduction. The outcomes of these choices will depend, among other things, on whether AI is created in the near future. Should you plan as though there are 50/50 odds of achieving AI in the next 30 years? Are you 99% confident we won't create AI in the next 30 years? Or is your estimate somewhere in between?</span><br /><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">If we can't use our intuitions for prediction or defer to experts how might one estimate the time until AI? We consider several strategies below.</span></p>\n<p>&nbsp;</p>\n<p>[end of snippet]</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br /></span></p>\n<h2><span style=\"font-family: Garamond; font-size: small;\"><span style=\"white-space: pre-wrap;\">Notes</span></span></h2>\n<p>1&nbsp;<span style=\"font-family: Garamond; font-size: 13px; white-space: pre-wrap;\"> Bainbridge 2006; Baum, Goertzel, and Goertzel 2011; Bostrom 2003; Legg 2008; Sandberg and Bostrom 2011.</span></p>\n<p>&nbsp;</p>\n<div style=\"background-color: transparent;\"><span id=\"internal-source-marker_0.8263228819705546\" style=\"font-size: 13px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">2 Examples include many of today&rsquo;s reinforcement learners (Sutton and Barto 1998), and also many abstract models such as AIXI (Hutter 2004), G&ouml;del machines (Schmidhuber 2007), and Dewey&rsquo;s (2011) &ldquo;implemented agents.&rdquo;</span></div>\n<p>&nbsp;</p>\n<p>3 &nbsp;&nbsp;<span style=\"font-family: Garamond; font-size: 13px; white-space: pre-wrap;\">Armstrong 1985; Woudenberg 1991; Rowe and Wright 2001. But, see Anderson and Anderson-Parente (2011).</span></p>\n<p><span style=\"font-family: Garamond; font-size: 13px; white-space: pre-wrap;\">4 </span><span style=\"font-family: Garamond; font-size: 13px; white-space: pre-wrap;\">Sandberg and Bostrom (2008), p. 83.</span></p>\n<p><span style=\"font-family: Garamond; font-size: 13px; white-space: pre-wrap;\"><br /></span></p>\n<h4 dir=\"ltr\"><span style=\"font-size: 16px; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">References</span></h4>\n<p><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Modha et al. 2011, cognitive computing, communications of the ACM</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Cattell &amp; Parker 2011 challenges for brain emulation</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Schierwagen 2011 Reverse engineering for biologically-inspired cognitive architectures</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Floreano and Mattiussi 2008 bio-inspired artificial intelligence</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">de Garis et al. 2010 a world survey of artificial brain projects part 1</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Nilsson 2010 the quest for artificial intelligence</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Sparkes et al. 2010 Towards Robot Scientists for autonomous scientific discovery</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Turing 1950 machine intelligence</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Good 1965 speculations concerning the first ultraintelligent machine</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Von Neumann 1966 theory of self-reproducing autonomata</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Solomonoff 1985 the time scale of artificial intelligence</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Vinge 1993 coming technological singularity</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Bostrom 2003 ethical issues in advanced artificial intelligence</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Yampolsky 2011 leakproofing the singularity</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Lampson 1973 a note on the confinement problem</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Yudkowsky 2008 artificial intelligence as a negative and positive factor in global risk</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Chalmers 2010 the singularity a philosophical analysis</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Schierwagen 2011 Reverse engineering for biologically-inspired cognitive architectures, a critical analysis</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Kurzweil 2005 the singularity is near</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Yudkowsky 1996 staring into the singularity</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Nordmann 2007 If and then: a critique of speculative nanoethics</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Tversky and Kahneman 1974 Judgment under uncertainty: Heuristics and biases</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Lucas 1961 minds, machines, and godel</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Dreyfus 1972 what computers can't do</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Searle 1980 minds brains and programs</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Block 1981 Psychologism and behaviorism</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Penrose 1994 shadows of the mind</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Van Gelder and Port 1995 It's about time, an overview of the dynamical approach to cognition</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Shulman and Bostrom 2011 How hard is artificial intelligence</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Sandberg and Bostrom 2008 whole brain emulation a roadmap</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Williams 2011 prediction markets theory and applications</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Dennett 1997 kinds of minds</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Pennachin and Goertzel 2007 an overview of contemporary approaches to AGI</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Murphy and Winkler 1984 probability forecasting in meteorology</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Lichtenstein, Fischoff, and Phillips 1982 calibration of probabilities the state of the art to 1980</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Griffin and Tversky 1992 The weighing of evidence and the determinants of confidence</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Grove and Meehl 1996 Comparative Efficiency of Informal...</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Grove et al. 2000 Clinical versus mechanical prediction: A meta-analysis</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Yates, Lee, Sieck, Choi, Price 2002 Probability judgment across cultures</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Tetlock 2005 expert political judgment</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Bainbridge 2006 Managing Nano-Bio-Info-Cogno Innovations: Converging Technologies...</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Baum, Goertzel, and Goertzel 2011 How long until human-level AI? </span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Legg 2008 machine superintelligence</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Sandberg &amp; Bostrom 2011 machine intelligence survey</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Sutton and Barto 1998 reinforcement learning an introduction</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Hutter 2004 universal ai</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Schmidhuber 2007 godel machines</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Dewey 2011 learning what to value</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Armstrong 1985 Long-Range Forecasting: From Crystal Ball to Computer, 2nd edition</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Woudenberg 1991 an evaluation of delphi</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Rowe and Wright 2001 expert opinions in forecasting</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Anderson and Anderson-Parente 2011 A case study of long-term Delphi accuracy</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Muehlhauser and Helm, this volume: The Singularity and Machine Ethics</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Sandberg, this volume: models of technological singularity</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Modis, this volume: there will be no singularity</span><br /><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Plebe and Perconti, this volume: the slowdown hypothesis</span></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rWwmRvp8FEKDZ49d6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 3, "extendedScore": null, "score": 8.093883981782988e-07, "legacy": true, "legacyId": "11265", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I've been writing a new draft of the <a href=\"/r/discussion/lw/8et/toward_an_overview_analysis_of_intelligence/\">intelligence explosion analysis I'm writing with Anna Salamon</a>. I've incorporated much of the feedback LWers have given me, and will now present snippets of the <strong>new draft</strong> for feedback. Please ignore the formatting issues caused by moving the text from Google Docs to Less Wrong.</p>\n<p>&nbsp;</p>\n<p>_____</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><br id=\"internal-source-marker_0.8263228819705546\"></p>\n<h2 id=\"Intelligence_Explosion__Evidence_and_Import\" dir=\"ltr\"><span style=\"font-size: 24px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Intelligence Explosion: Evidence and Import</span></h2>\n<p><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Anna Salamon</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Luke Muehlhauser</span><br><br><br></p>\n<p style=\"margin-left: 36pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The best answer to the question, \"Will computers ever be as smart as humans?\" is probably \u201cYes, but only briefly.\"</span></p>\n<p>&nbsp;</p>\n<p style=\"text-align: right; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Vernor Vinge</span></p>\n<p><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> </span><br><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Humans may create human-level artificial intelligence in this century.</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>1</sup></span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> Shortly thereafter, we may see an \u201cintelligence explosion\u201d \u2014 a chain of events by which human-level AI leads, fairly rapidly, to intelligent systems whose capabilities far surpass those of biological humanity as a whole.</span><br><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">How likely is this, and what should we do about it? Others have discussed these questions previously (Turing 1950; Good 1965; Von Neumann 1966; Solomonoff 1985; Vinge 1993; Bostrom 2003; Yudkowsky 2008; Chalmers 2010), but no brief, systematic review of the relevant issues has been published. In this chapter we aim to provide such a review.</span></p>\n<h2 dir=\"ltr\" id=\"Why_study_intelligence_explosion_\"><span style=\"font-size: 24px; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Why study intelligence explosion?</span></h2>\n<p><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">As Chalmers (2010) notes, the singularity is of great practical interest:</span></p>\n<p style=\"margin-left: 36pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">If there is a singularity, it will be one of the most important events in the history of the planet. An intelligence explosion has enormous potential benefits: a cure for all known diseases, an end to poverty, extraordinary scientific advances, and much more. It also has enormous potential dangers: an end to the human race, an arms race of warring machines, the power to destroy the planet...</span></p>\n<p><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The singularity is also a challenging scientific and philosophical topic. Under the spectre of intelligence explosion, long-standing philosophical puzzles about values, other minds, and personal identity become, as Chalmers puts it, \"life-or-death questions that may confront us in coming decades or centuries.\" In science, the development of AI will require progress in several of mankind's grandest scientific projects, including reverse-engineering the brain (Schierwagen 2011) and developing artificial minds (Nilsson 2010), while the development of AI safety mechanisms may require progress on the confinement problem in computer science (Lampson 1973; Yampolskiy 2011) and the cognitive science of human values (Muehlhauser and Helm, this volume). The creation of AI would also revolutionize scientific method, as most science would be done by intelligent machines (Sparkes et al. 2010).</span><br><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Such questions are complicated, the future is uncertain, and our chapter is brief. Our aim, then, is not to provide detailed arguments but only to sketch the issues involved, pointing the reader to authors who have analyzed each component in more detail. We believe these matters are important, and our discussion of them must be permitted to begin at a low level because there is no other place to lay the first stones.</span></p>\n<h2 dir=\"ltr\" id=\"What_we_will__not__argue\"><span style=\"font-size: 24px; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">What we will (not) argue</span></h2>\n<p><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">\"Technological singularity\" has come to mean many things (Sandberg, this volume), including: accelerating technological change (Kurzweil 2005), a limit in our ability to predict the future (Vinge 1993), and the topic we will discuss: an intelligence explosion leading to the creation of machine superintelligence (Yudkowsky 1996). Because the singularity is associated with such a variety of views and arguments, we must clarify what this chapter will and will not argue.</span><br><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">First, we will not tell detailed stories about the future. In doing so, we would likely commit the \u201cif and then\u201d fallacy, by which an improbable conditional becomes a supposed actual (Nordmann 2007). For example, we will not assume the continuation of Moore\u2019s law, nor that hardware trajectories determine software progress, nor that technological trends will be exponential rather than logistic (see Modis, this volume), nor indeed that progress will accelerate rather than decelerate (see Plebe and Perconti, this volume). Instead, we will examine convergent outcomes that \u2014 like the evolution of eyes or the emergence of markets \u2014 can come about through many different paths and can gather momentum once they begin. Humans tend to underestimate the likelihood of such convergent outcomes (Tversky and Kahneman 1974), and we believe intelligence explosion is one of them.</span><br><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Second, we will not assume that human intelligence is realized by a classical computation system, nor that intelligent machines will have internal mental properties like consciousness or \"understanding.\" Such factors are mostly irrelevant to the occurrence of a singularity, so objections to these claims (Lucas 1961; Dreyfus 1972; Searle 1980; Block 1981; Penrose 1994; Van Gelder and Port 1995) are not objections to the singularity.</span><br><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">What, then, will we argue? First, we suggest there is a significant probability we will create human-level AI (hereafter, \"AI\") within a century. Second, we suggest that AI is likely to lead rather quickly to machine superintelligence. Finally, we discuss the possible consequences of machine superintelligence and consider which actions we can take now to shape our future.</span></p>\n<h2 dir=\"ltr\" id=\"From_here_to_AI\"><span style=\"font-size: 24px; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">From here to AI</span></h2>\n<p><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Our first step is to survey the evidence concerning whether we should expect the creation of AI within a century.</span><br><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">By \"AI,\" we refer to \"systems which match or exceed the cognitive performance of humans in virtually all domains of interest\" (Shulman &amp; Bostrom 2011). On this definition, IBM's </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-style: italic; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Jeopardy!</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">-playing Watson computer is not an AI but merely a \"narrow AI\" because it can only solve a narrow set of problems. Drop Watson in a pond or ask it to do original science, and it is helpless. Imagine instead a machine that can invent new technologies, manipulate humans with acquired social skills, and otherwise learn to navigate new environments as needed.</span><br><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">There are many types of AI. To name just three:</span></p>\n<ul>\n<li style=\"list-style-type: disc; font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline;\"><span style=\"background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">The code of a </span><span style=\"background-color: transparent; font-style: italic; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">transparent AI</span><span style=\"background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> is written explicitly by, and largely understood by, its programmers.</span><span style=\"background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>2</sup></span></li>\n<li style=\"list-style-type: disc; font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline;\"><span style=\"background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">An </span><span style=\"background-color: transparent; font-style: italic; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">opaque AI</span><span style=\"background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> is not transparent to its creators. For example it could be, like the human brain, a messy ensemble of cognitive modules. In an AI, these modules might be written by different teams for different purposes, using different languages and approaches.</span></li>\n<li style=\"list-style-type: disc; font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline;\"><span style=\"background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">A </span><span style=\"background-color: transparent; font-style: italic; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">whole brain emulation</span><span style=\"background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> (WBE) is a computer emulation of the brain structures required to functionally reproduce human thought and perhaps consciousness (Sandberg and Bostrom 2008). We need not understand the detailed mechanisms of general intelligence to reproduce a brain functionally on a computing substrate.</span></li>\n</ul>\n<p><br><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Whole brain emulation uses the human software for intelligence already invented by evolution, while other forms of AI (\"</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-style: italic; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">de novo</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> AI\") require inventing intelligence anew, to varying degrees.</span><br><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">When should we expect AI? Unfortunately, expert elicitation methods have not proven useful for long-term forecasting,</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>3</sup></span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> and prediction markets have not yet been tested much for technological forecasting (Williams 2011), so our analysis must allow for a wide range of outcomes. We will first consider how difficult the problem seems to be, and then which inputs toward solving the problem \u2014 and which \"speed bumps\" \u2014 we can expect in the next century. </span></p>\n<h3 dir=\"ltr\" id=\"How_hard_is_whole_brain_emulation_\"><span style=\"font-size: 19px; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">How hard is whole brain emulation?</span></h3>\n<p><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Because whole brain emulation will rely mostly on scaling up existing technologies like microscopy and large-scale cortical simulation, WBE may be largely an \"engineering\" problem, and thus more predictable than other kinds of AI.</span><br><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Several authors have discussed the difficulty of WBE in detail (Sandberg and Bostrom 2008; de Garis et al. 2010; Modha et al. 2011; Cattell &amp; Parker 2011). In short: The difficulty of WBE depends on many factors, and in particular on the resolution of emulation required for successful WBE. For example, proteome-resolution emulation will require more resources and technological development than emulation at the resolution of the brain's neural network. In perhaps the most likely scenario,</span></p>\n<p style=\"margin-left: 36pt; margin-top: 0pt; margin-bottom: 0pt;\" dir=\"ltr\"><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">WBE on the neuronal/synaptic level requires relatively modest increases in microscopy resolution, a less trivial development of automation for scanning and image processing, a research push at the problem of inferring functional properties of neurons and synapses, and relatively business\u2010as\u2010usual development of computational neuroscience models and computer hardware.</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><sup>4</sup></span></p>\n<p>&nbsp;</p>\n<h3 dir=\"ltr\" id=\"How_hard_is_de_novo_AI_\"><span style=\"font-size: 19px; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">How hard is </span><span style=\"font-size: 19px; font-family: Arial; background-color: transparent; font-style: italic; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">de novo</span><span style=\"font-size: 19px; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> AI?</span></h3>\n<p><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">There is a vast space of possible mind designs for </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-style: italic; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">de novo</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> AI; talking about \"non-human intelligence\" is like talking about \"non-platypus animals\" (Dennett 1997; Pennachin and Goertzel 2007; Yudkowsky 2008).</span><br><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">We do not know what it takes to build </span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; font-style: italic; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">de novo</span><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"> AI. Because of this, we do not know what groundwork will be needed to understand general intelligence, nor how long it may take to get there.</span><br><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Worse, it\u2019s easy to think we do know. Studies show that except for weather forecasters (Murphy and Winkler 1984), nearly all of us give inaccurate probability estimates when we try, and in particular we are overconfident of our predictions (Lichtenstein, Fischoff, and Phillips 1982; Griffin and Tversky 1992; Yates et al. 2002). Experts, too, often do little better than chance (Tetlock 2005), and are outperformed by crude computer algorithms (Grove and Meehl 1996; Grove et al. 2000; Tetlock 2005). So if you have a gut feeling about when digital intelligence will arrive, it is probably wrong.</span><br><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">But uncertainty is not a \u201cget out of prediction free\u201d card. You either will or will not save for retirement, encourage WBE development, or support AI risk reduction. The outcomes of these choices will depend, among other things, on whether AI is created in the near future. Should you plan as though there are 50/50 odds of achieving AI in the next 30 years? Are you 99% confident we won't create AI in the next 30 years? Or is your estimate somewhere in between?</span><br><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">If we can't use our intuitions for prediction or defer to experts how might one estimate the time until AI? We consider several strategies below.</span></p>\n<p>&nbsp;</p>\n<p>[end of snippet]</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\"><br></span></p>\n<h2 id=\"Notes\"><span style=\"font-family: Garamond; font-size: small;\"><span style=\"white-space: pre-wrap;\">Notes</span></span></h2>\n<p>1&nbsp;<span style=\"font-family: Garamond; font-size: 13px; white-space: pre-wrap;\"> Bainbridge 2006; Baum, Goertzel, and Goertzel 2011; Bostrom 2003; Legg 2008; Sandberg and Bostrom 2011.</span></p>\n<p>&nbsp;</p>\n<div style=\"background-color: transparent;\"><span id=\"internal-source-marker_0.8263228819705546\" style=\"font-size: 13px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">2 Examples include many of today\u2019s reinforcement learners (Sutton and Barto 1998), and also many abstract models such as AIXI (Hutter 2004), G\u00f6del machines (Schmidhuber 2007), and Dewey\u2019s (2011) \u201cimplemented agents.\u201d</span></div>\n<p>&nbsp;</p>\n<p>3 &nbsp;&nbsp;<span style=\"font-family: Garamond; font-size: 13px; white-space: pre-wrap;\">Armstrong 1985; Woudenberg 1991; Rowe and Wright 2001. But, see Anderson and Anderson-Parente (2011).</span></p>\n<p><span style=\"font-family: Garamond; font-size: 13px; white-space: pre-wrap;\">4 </span><span style=\"font-family: Garamond; font-size: 13px; white-space: pre-wrap;\">Sandberg and Bostrom (2008), p. 83.</span></p>\n<p><span style=\"font-family: Garamond; font-size: 13px; white-space: pre-wrap;\"><br></span></p>\n<h4 dir=\"ltr\" id=\"References\"><span style=\"font-size: 16px; font-family: Arial; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">References</span></h4>\n<p><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Modha et al. 2011, cognitive computing, communications of the ACM</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Cattell &amp; Parker 2011 challenges for brain emulation</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Schierwagen 2011 Reverse engineering for biologically-inspired cognitive architectures</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Floreano and Mattiussi 2008 bio-inspired artificial intelligence</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">de Garis et al. 2010 a world survey of artificial brain projects part 1</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Nilsson 2010 the quest for artificial intelligence</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Sparkes et al. 2010 Towards Robot Scientists for autonomous scientific discovery</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Turing 1950 machine intelligence</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Good 1965 speculations concerning the first ultraintelligent machine</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Von Neumann 1966 theory of self-reproducing autonomata</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Solomonoff 1985 the time scale of artificial intelligence</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Vinge 1993 coming technological singularity</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Bostrom 2003 ethical issues in advanced artificial intelligence</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Yampolsky 2011 leakproofing the singularity</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Lampson 1973 a note on the confinement problem</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Yudkowsky 2008 artificial intelligence as a negative and positive factor in global risk</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Chalmers 2010 the singularity a philosophical analysis</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Schierwagen 2011 Reverse engineering for biologically-inspired cognitive architectures, a critical analysis</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Kurzweil 2005 the singularity is near</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Yudkowsky 1996 staring into the singularity</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Nordmann 2007 If and then: a critique of speculative nanoethics</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Tversky and Kahneman 1974 Judgment under uncertainty: Heuristics and biases</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Lucas 1961 minds, machines, and godel</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Dreyfus 1972 what computers can't do</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Searle 1980 minds brains and programs</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Block 1981 Psychologism and behaviorism</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Penrose 1994 shadows of the mind</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Van Gelder and Port 1995 It's about time, an overview of the dynamical approach to cognition</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Shulman and Bostrom 2011 How hard is artificial intelligence</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Sandberg and Bostrom 2008 whole brain emulation a roadmap</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Williams 2011 prediction markets theory and applications</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Dennett 1997 kinds of minds</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Pennachin and Goertzel 2007 an overview of contemporary approaches to AGI</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Murphy and Winkler 1984 probability forecasting in meteorology</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Lichtenstein, Fischoff, and Phillips 1982 calibration of probabilities the state of the art to 1980</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Griffin and Tversky 1992 The weighing of evidence and the determinants of confidence</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Grove and Meehl 1996 Comparative Efficiency of Informal...</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Grove et al. 2000 Clinical versus mechanical prediction: A meta-analysis</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Yates, Lee, Sieck, Choi, Price 2002 Probability judgment across cultures</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Tetlock 2005 expert political judgment</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Bainbridge 2006 Managing Nano-Bio-Info-Cogno Innovations: Converging Technologies...</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Baum, Goertzel, and Goertzel 2011 How long until human-level AI? </span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Legg 2008 machine superintelligence</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Sandberg &amp; Bostrom 2011 machine intelligence survey</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Sutton and Barto 1998 reinforcement learning an introduction</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Hutter 2004 universal ai</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Schmidhuber 2007 godel machines</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Dewey 2011 learning what to value</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Armstrong 1985 Long-Range Forecasting: From Crystal Ball to Computer, 2nd edition</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Woudenberg 1991 an evaluation of delphi</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Rowe and Wright 2001 expert opinions in forecasting</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Anderson and Anderson-Parente 2011 A case study of long-term Delphi accuracy</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Muehlhauser and Helm, this volume: The Singularity and Machine Ethics</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Sandberg, this volume: models of technological singularity</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Modis, this volume: there will be no singularity</span><br><span style=\"font-size: 16px; font-family: Garamond; background-color: transparent; text-decoration: none; vertical-align: baseline; white-space: pre-wrap;\">Plebe and Perconti, this volume: the slowdown hypothesis</span></p>\n<p>&nbsp;</p>", "sections": [{"title": "Intelligence Explosion: Evidence and Import", "anchor": "Intelligence_Explosion__Evidence_and_Import", "level": 1}, {"title": "Why study intelligence explosion?", "anchor": "Why_study_intelligence_explosion_", "level": 1}, {"title": "What we will (not) argue", "anchor": "What_we_will__not__argue", "level": 1}, {"title": "From here to AI", "anchor": "From_here_to_AI", "level": 1}, {"title": "How hard is whole brain emulation?", "anchor": "How_hard_is_whole_brain_emulation_", "level": 2}, {"title": "How hard is de novo AI?", "anchor": "How_hard_is_de_novo_AI_", "level": 2}, {"title": "Notes", "anchor": "Notes", "level": 1}, {"title": "References", "anchor": "References", "level": 3}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "7 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ebRZPDBg5qff9oTs5"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-03T23:56:15.054Z", "modifiedAt": null, "url": null, "title": "Why study the cognitive science of concepts?", "slug": "why-study-the-cognitive-science-of-concepts", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:51.583Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aMfa3Lf9CN86qF64k/why-study-the-cognitive-science-of-concepts", "pageUrlRelative": "/posts/aMfa3Lf9CN86qF64k/why-study-the-cognitive-science-of-concepts", "linkUrl": "https://www.lesswrong.com/posts/aMfa3Lf9CN86qF64k/why-study-the-cognitive-science-of-concepts", "postedAtFormatted": "Saturday, December 3rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20study%20the%20cognitive%20science%20of%20concepts%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20study%20the%20cognitive%20science%20of%20concepts%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaMfa3Lf9CN86qF64k%2Fwhy-study-the-cognitive-science-of-concepts%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20study%20the%20cognitive%20science%20of%20concepts%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaMfa3Lf9CN86qF64k%2Fwhy-study-the-cognitive-science-of-concepts", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaMfa3Lf9CN86qF64k%2Fwhy-study-the-cognitive-science-of-concepts", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1042, "htmlBody": "<p>I've <a href=\"/lw/7tz/philosophy_by_humans_1_concepts_dont_work_that_way/\">written</a> <a href=\"/lw/8lz/philosophy_by_humans_2_living_metaphorically/\">several</a> <a href=\"/r/discussion/lw/8ms/review_of_machery_doing_without_concepts/\">posts</a> on the cognitive science of concepts in prep for <a href=\"/lw/8ix/metaethics_where_im_headed/\">later posts</a> in my <a href=\"http://wiki.lesswrong.com/wiki/No-Nonsense_Metaethics\">metaethics sequence</a>. Why?</p>\n<p>I'm doing this because one common way of trying to solve the \"friendliness content\" problem in Friendly AI theory is to analyze (via thought experiment and via cognitive science) our <em>concept</em>&nbsp;of \"good\" or \"ought\" or \"right\" so that we can figure out what an FAI \"ought\" to do, or what it would be \"good\" for an FAI to do, or what it would be \"right\" for an FAI to do.</p>\n<p>That's what Eliezer does in <a href=\"/lw/sm/the_meaning_of_right/\">The Meaning of Right</a>, that's what many other LWers do, and that's what most mainstream metaethicists do.</p>\n<p>With my recent posts on the cognitive science of concepts, I'm trying to show that cognitive science presents a number of difficult problems for this approach.</p>\n<p>Let me illustrate with a concrete example. Math <a href=\"http://www.ctpost.com/news/article/Wisdom-beyond-his-years-1390299.php\">prodigy</a>&nbsp;<a href=\"/lw/64j/a_defense_of_naive_metaethics/\">Will Sawin</a> once proposed to me (over the phone) that our concept of \"ought\" might be realized by way of something like a dedicated cognitive module. In an <a href=\"/lw/64j/a_defense_of_naive_metaethics/4ex0\">earlier comment</a>, I tried to paraphrase his idea:</p>\n<blockquote>\n<p>Imagine a species of artificial agents. These agents have a list of belief statements that relate physical phenomena to normative properties (let's call them 'moral primitives'):</p>\n<ul>\n<li>'Liking' reward signals in human brains are good.</li>\n<li>Causing physical pain in human infants is forbidden.</li>\n<li>etc.</li>\n</ul>\n<p>These agents also have a list of belief statements about physical phenomena in general:</p>\n<ul>\n<li>Sweet tastes on the tongue produces reward signals in human brains.</li>\n<li>Cutting the fingers of infants produces physical pain in infants.</li>\n<li>Things are made of atoms.</li>\n<li>etc.</li>\n</ul>\n<p>These agents also have an 'ought' function that includes a series of logical statements that relate normative concepts to each other, such as:</p>\n<ul>\n<li>A thing can't be both permissible and forbidden.</li>\n<li>A thing can't be both obligatory and non-obligatory.</li>\n<li>etc.</li>\n</ul>\n<p>Finally, these robots have actuators that are activated by a series of rules like:</p>\n<ul>\n<li>When the agent observes an opportunity to perform an action that is 'obligatory', then it will take that action.</li>\n<li>An agent will avoid any action that is labeled as 'forbidden.'</li>\n</ul>\n<p>Some of these rules might include utility functions that encode ordinal or cardinal value for varying combinations of normative properties.</p>\n<p>These agents can't see their own source code. The combination of the moral primitives and the ought function and the non-ought belief statements and a set of rules about behavior produces their action and their verbal statements about what ought to be done.</p>\n<p>From their behavior and verbal ought statements these robots can infer to some degree how their ought function works, but they can't fully describe their ought function because they haven't run enough tests or the ought function is just too complicated or the problem is made worse because they also can't see their moral primitives.</p>\n<p>The ought function doesn't reduce to physics because it's a set of purely logical statements. The 'meaning' of ought in this sense is determined by the role that the ought function plays in producing intentional behavior by the robots.</p>\n<p>Of course, the robots could speak in ought language in stipulated ways, such that 'ought' means 'that which produces pleasure in human brains' or something like that, and this could be a useful way to communicate efficiently, but it wouldn't capture what the ought function is doing or how it is contributing to the production of behavior by these agents.</p>\n<p>What Will is saying is that it's convenient to use 'ought' language to refer to this ought function only, and not also to a combination of the ought function and statements about physics, as happens when we stipulatively use 'ought' to talk about 'that which produces well-being in conscious creatures' (for example).</p>\n<p>I'm saying that's fine, but it can also be convenient (and intuitive) for people to use 'ought' language in ways that reduce to logical-physical statements, and not only in ways that express a logical function that contains only transformations between normative properties. So we don't have substantive disagreement on this point; we merely have different intuitions about the pragmatic value of particular uses for 'ought' language.</p>\n<p>We also drew up a simplified model of the production of human action in which there is a cognitive module that processes the 'ought' function (made of purely logical statements like in the robots' ought function), a cognitive module that processes habits, a cognitive module that processes reflexes, and so on. Each of these produces an output, and another module runs arg(max) on these action options to determine which actions 'wins' and actually occurs.</p>\n<p>Of course, the human 'ought' function is probably spread across multiple modules, as is the 'habit' function.</p>\n<p>Will likes to think of the 'meaning' of 'ought' as being captured by the algorithm of this 'ought' function in the human brain. This ought function doesn't contain physical beliefs, but rather processes primitive normative/moral beliefs (from outside the ought function) and outputs particular normative/moral judgments, which contribute to the production of human behavior (including spoken moral judgments). In this sense, 'ought' in Will's sense of the term doesn't reduce to physical facts, but to a logical function...</p>\n<p>Will also thinks that the 'ought' function (in his sense) inside human brains is probably very similar between humans - ones that aren't brain damaged or neurologically deranged... [And] if the 'ought' function is the same in all healthy humans, then there needn't be a separate 'meaning' of ought (in Will's sense) for each speaker, but instead there could be a shared 'meaning' of ought (in Will's sense) that is captured by the algorithms of the 'ought' cognitive module that is shared by healthy human brains.</p>\n</blockquote>\n<p>The reason I'm investigating the cognitive science of concepts is because I think it shows that the claims about the human brain in these last two paragraphs are probably false, and so are many other claims about human brains that are <a href=\"/lw/7tz/philosophy_by_humans_1_concepts_dont_work_that_way/4x17\">implicit</a> in certain <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Henderson-Horgan-Grades-of-a-priori-justification.pdf\">varieties</a> of <a href=\"/lw/5kn/conceptual_analysis_and_moral_theory/\">the 'conceptual analysis' approach to value theory</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aMfa3Lf9CN86qF64k", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 12, "extendedScore": null, "score": 8.094006861419452e-07, "legacy": true, "legacyId": "11266", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["wHjpCxeDeuFadG3jF", "nEA5vYzYS5zqrtQTm", "rKL4aecWLNBpvfW5Q", "tyGbG4XGkJzDmLTdh", "fG3g3764tSubr6xvs", "QuxWskbsDvNTozE9a", "2YPbdHgcjt7g5ZaFN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-04T04:11:32.473Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Amazing Virgin Pregnancy", "slug": "seq-rerun-the-amazing-virgin-pregnancy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:51.292Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4H6drjEPkx4nufYDc/seq-rerun-the-amazing-virgin-pregnancy", "pageUrlRelative": "/posts/4H6drjEPkx4nufYDc/seq-rerun-the-amazing-virgin-pregnancy", "linkUrl": "https://www.lesswrong.com/posts/4H6drjEPkx4nufYDc/seq-rerun-the-amazing-virgin-pregnancy", "postedAtFormatted": "Sunday, December 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Amazing%20Virgin%20Pregnancy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Amazing%20Virgin%20Pregnancy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4H6drjEPkx4nufYDc%2Fseq-rerun-the-amazing-virgin-pregnancy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Amazing%20Virgin%20Pregnancy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4H6drjEPkx4nufYDc%2Fseq-rerun-the-amazing-virgin-pregnancy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4H6drjEPkx4nufYDc%2Fseq-rerun-the-amazing-virgin-pregnancy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 171, "htmlBody": "<p>Today's post, <a href=\"/lw/m8/the_amazing_virgin_pregnancy/\">The Amazing Virgin Pregnancy</a> was originally published on 24 December 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A story in which Mary tells Joseph that God made her pregnant so Joseph won't realize she's been cheating on him with the village rabbi.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8on/seq_rerun_zen_and_the_art_of_rationality/\">Zen and the Art of Rationality</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4H6drjEPkx4nufYDc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.094931182715878e-07, "legacy": true, "legacyId": "11267", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["FkwKGQFS5XL9mQSQb", "Ehbzeh5tf5b9cwzpW", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-04T05:51:19.123Z", "modifiedAt": null, "url": null, "title": "Russ Roberts and Gary Taubes on confirmation bias [podcast]", "slug": "russ-roberts-and-gary-taubes-on-confirmation-bias-podcast", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:43.389Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fortyeridania", "createdAt": "2010-07-21T15:35:12.558Z", "isAdmin": false, "displayName": "fortyeridania"}, "userId": "roBPqtzsvG6dC3YFT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xGhXDSghfMZCLmQPi/russ-roberts-and-gary-taubes-on-confirmation-bias-podcast", "pageUrlRelative": "/posts/xGhXDSghfMZCLmQPi/russ-roberts-and-gary-taubes-on-confirmation-bias-podcast", "linkUrl": "https://www.lesswrong.com/posts/xGhXDSghfMZCLmQPi/russ-roberts-and-gary-taubes-on-confirmation-bias-podcast", "postedAtFormatted": "Sunday, December 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Russ%20Roberts%20and%20Gary%20Taubes%20on%20confirmation%20bias%20%5Bpodcast%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARuss%20Roberts%20and%20Gary%20Taubes%20on%20confirmation%20bias%20%5Bpodcast%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxGhXDSghfMZCLmQPi%2Fruss-roberts-and-gary-taubes-on-confirmation-bias-podcast%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Russ%20Roberts%20and%20Gary%20Taubes%20on%20confirmation%20bias%20%5Bpodcast%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxGhXDSghfMZCLmQPi%2Fruss-roberts-and-gary-taubes-on-confirmation-bias-podcast", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxGhXDSghfMZCLmQPi%2Fruss-roberts-and-gary-taubes-on-confirmation-bias-podcast", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 153, "htmlBody": "<p><a href=\"http://www.econtalk.org/archives/2011/11/taubes_on_fat_s.html\">Here is the link</a>. The context is nutritional science and epidemiology, but confirmation bias is the primary theme pumping throughout the discussion.&nbsp;<a href=\"http://garytaubes.com/\">Gary</a> <a href=\"http://en.wikipedia.org/wiki/Gary_Taubes\">Taubes</a>&nbsp;has gained a reputation for contrarianism.* According to Taubes, the current nutritional paradigm (fat is bad, exercise is good, carbs are OK) does not deserve high credibility.</p>\r\n<p><a href=\"http://en.wikipedia.org/wiki/Russell_Roberts_(economist)\">Roberts</a> brings up the role of identity in perpetuating confirmation bias--a hypothesis has become part of you, so it has become that much harder to countenance contrary evidence. In this context they also talk about theism (Roberts is Jewish, while Taubes is an atheist). And, the program being <a href=\"http://www.econtalk.org/\">EconTalk</a>, Roberts draws analogies with economics.</p>\r\n<p>*Sometime between 45 and 50 minutes in, Roberts points out that&nbsp;given this reputation, Taubes is susceptible to belief distortion as well:</p>\r\n<blockquote>\r\n<p>What's your evidence that you are not just falling prey to the Ancel Keys and other folks who have made the same mistake?</p>\r\n</blockquote>\r\n<p>I do not think Taubes gives a direct answer.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xGhXDSghfMZCLmQPi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 8.095291815369604e-07, "legacy": true, "legacyId": "11268", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-04T10:33:41.875Z", "modifiedAt": null, "url": null, "title": "\"Vulnerable Cyborgs: Learning to Live with our Dragons\", Mark Coeckelbergh", "slug": "vulnerable-cyborgs-learning-to-live-with-our-dragons-mark", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:49.425Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eKFHNmCcmuogQifQN/vulnerable-cyborgs-learning-to-live-with-our-dragons-mark", "pageUrlRelative": "/posts/eKFHNmCcmuogQifQN/vulnerable-cyborgs-learning-to-live-with-our-dragons-mark", "linkUrl": "https://www.lesswrong.com/posts/eKFHNmCcmuogQifQN/vulnerable-cyborgs-learning-to-live-with-our-dragons-mark", "postedAtFormatted": "Sunday, December 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Vulnerable%20Cyborgs%3A%20Learning%20to%20Live%20with%20our%20Dragons%22%2C%20Mark%20Coeckelbergh&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Vulnerable%20Cyborgs%3A%20Learning%20to%20Live%20with%20our%20Dragons%22%2C%20Mark%20Coeckelbergh%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeKFHNmCcmuogQifQN%2Fvulnerable-cyborgs-learning-to-live-with-our-dragons-mark%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Vulnerable%20Cyborgs%3A%20Learning%20to%20Live%20with%20our%20Dragons%22%2C%20Mark%20Coeckelbergh%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeKFHNmCcmuogQifQN%2Fvulnerable-cyborgs-learning-to-live-with-our-dragons-mark", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeKFHNmCcmuogQifQN%2Fvulnerable-cyborgs-learning-to-live-with-our-dragons-mark", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2866, "htmlBody": "<p><a href=\"http://jetpress.org/v22/coeckelbergh.htm\">\"Vulnerable Cyborgs: Learning to Live with our Dragons\"</a>, <a href=\"http://users.belgacom.net/mark.coeckelbergh/index.htm\">Mark Coeckelbergh</a> (<a href=\"http://www.utwente.nl/gw/wijsb/organization/coeckelbergh/\">university</a>); abstract:</p>\n<blockquote>\n<p class=\"MsoNormal\" style=\"margin-left: 35.4pt; text-align: justify;\"><span lang=\"NL\">Transhumanist visions appear to aim at invulnerability. We are invited to fight the dragon of death and disease, to shed our old, human bodies, and to live on as invulnerable minds or cyborgs. This paper argues that even if we managed to enhance humans in one of these ways, we would remain highly vulnerable entities given the fundamentally relational and dependent nature of posthuman existence. After discussing the need for minds to be embodied, the issue of disease and death in the infosphere, and problems of psychological, social and axiological vulnerability, I conclude that transhumanist human enhancement would not erase our current vulnerabilities, but instead transform them. Although the struggle against vulnerability is typically human and would probably continue to mark posthumans, we had better recognize that we can never win that fight and that the many dragons that threaten us are part of us. As vulnerable humans and posthumans, we are at once the hero and the dragon.</span></p>\n</blockquote>\n<hr />\n<blockquote>\n<p><span lang=\"EN-GB\">Bostrom has written <a href=\"http://www.nickbostrom.com/fable/dragon.html\">a tale</a> about a dragon that terrorizes a kingdom and people who submit to the dragon rather than fighting it. According to <span class=\"SpellE\">Bostrom</span>, the &ldquo;moral&rdquo; of the story is that we should fight the dragon, that is, extend the (healthy) human life span and not accept aging as a fact of life (<span class=\"SpellE\">Bostrom</span> 2005, 277). And in <em>The Singularity is Near </em>(2005) Kurzweil has suggested that following the acceleration of information technology, we will become cyborgs, upload ourselves, have nanobots in our bloodstream, and enjoy nonbiological experience. Although not all transhumanist authors explicitly state it, these ideas seem to aim toward invulnerability and immortality: by means of human enhancement technologies, we can transcend our present limited existence and become strong, invulnerable cyborgs or immortal minds living in an eternal, virtual world.</span></p>\n<p><span lang=\"EN-GB\">...</span><span lang=\"EN-GB\">However, in this paper, I will ask neither the ethical-normative question (Should we develop human enhancement techniques and should we aim for invulnerability?) nor the hermeneutical question (How can we best interpret and understand transhumanism in the light of cultural, religious, and scientific history?). Instead, I ask the question: <em>If and to the extent that</em> transhumanism aims at invulnerability, can it &ndash; in principle &ndash; reach that aim? The following discussion offers some obvious and some much less obvious reasons why posthumans would remain vulnerable, and why human vulnerability would be transformed rather than diminished or eliminated...</span><span lang=\"EN-GB\">However, to focus only on a <span class=\"SpellE\">defense</span> or rejection of what is valuable in humans would leave out of sight the relation between (in<span class=\"GramE\">)vulnerability</span> and <em>posthuman</em> possibilities. It would lead us back to the ethical-normative questions (Is human enhancement morally acceptable? Is vulnerability something to be valued? Is the <span class=\"SpellE\">transhumanist</span> project acceptable or desirable?), which is not what I want to do in this paper. Moreover, ethical arguments that present the problem as if we have a choice between &ldquo;natural&rdquo; humanity and &ldquo;artificial&rdquo; <span class=\"SpellE\">posthumanity</span> are based on essentialist assumptions that make a sharp distinction between &ldquo;what we are&rdquo; (the natural) and technology (the artificial), whereas this distinction is at least questionable. Perhaps there is no fixed human nature apart from technology, perhaps we are &ldquo;artificial by nature&rdquo; (<span class=\"SpellE\">Plessner</span> 1975). If this is so, then the problem is not whether or not we want to transcend the human but how we want to shape that posthuman existence. Should we aim at invulnerability and if so, can we? As indicated before, here I limit the discussion to the &ldquo;can&rdquo; question.</span></p>\n</blockquote>\n<p><span lang=\"EN-GB\">Breaking down the potential improvements:</span></p>\n<h2 class=\"MsoNormal\" style=\"text-align: justify; page-break-before: always;\"><em><span lang=\"EN-GB\">Physical vulnerability</span></em></h2>\n<blockquote>\n<p class=\"MsoNormal\" style=\"text-align: justify; page-break-before: always;\"><span lang=\"EN-GB\">Not only could human enhancement make us immune to current viruses; it could also offer other &ldquo;immunities,&rdquo; broadly understood...</span><span lang=\"EN-GB\">However, the project of total vulnerability or even overall reduction of vulnerability is bound to fail. If we consider the history of medical technology, we observe that for every disease new technology helps to prevent or cure, there is at least one new disease that escapes our techno-scientific control. We can win one battle, but we can never win the war. There will be always new diseases, new viruses, and, more generally, new threats to physical vulnerability. Consider also natural disasters caused by floods, earthquakes, volcanic eruptions, and so on.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; page-break-before: always;\"><span lang=\"EN-GB\">Moreover, the very means to fight those threats sometimes create new threats themselves. This can happen within the same domain, as is the case with antibiotics that lead to the development of more resistant bacteria, or in another domain, as is the case with new security measures in airports, which are meant as protections against physical harm by terrorism but might pose new (health?) risks. Paradoxically, technologies that are meant to reduce vulnerability often create new ones. This is also true for posthuman technologies. For example, posthumans would also be vulnerable to at least some of the risks <span class=\"SpellE\">Bostrom</span> calls &ldquo;existential risks&rdquo; (<span class=\"SpellE\">Bostrom</span> 2002), which could wipe out <span class=\"SpellE\">posthumankind</span>. Nanotechnology or nuclear technology could be misused, <span class=\"GramE\">a <span class=\"SpellE\">superintelligence</span></span> could take over and annihilate humankind, or technology could cause (further) resource depletion and ecological destruction. Military technologies are meant to protect us but they can become a threat, making us vulnerable in a new way. We wanted to master nature in order to become less dependent on it, but now we risk destroying the ecology that sustains us. And of course there are many physical threats we cannot foresee &ndash; not even in the near future. </span></p>\n</blockquote>\n<h2 class=\"MsoNormal\" style=\"text-align: justify;\"><em><span lang=\"EN-GB\">Material and immaterial vulnerability</span></em></h2>\n<blockquote>\n<p class=\"MsoNormal\" style=\"text-align: justify; page-break-before: always;\"><span lang=\"EN-GB\">Consider computer viruses. Here the story is similar to the story of biological viruses: there are ongoing cycles of threats, counter-measures, and new threats. We can also consider physical damage to computers, although that is much less common. In any case, if we extend ourselves with software and hardware, this creates additional vulnerabilities. We must cope with &ldquo;software&rdquo; vulnerability and &ldquo;hardware&rdquo; vulnerability. If humans and posthumans live in an &ldquo;infosphere&rdquo; (see for example Floridi 2002), this is not a sphere of immunity. Perhaps our vulnerability becomes less material, but we cannot escape it. For instance, a virtual body in a virtual world may well be shielded from biological viruses, but it is vulnerable to at least three kinds of threats.</span></p>\n<ol>\n<li><span lang=\"EN-GB\">First, there are threats within the virtual world itself (consider for instance virtual rape), which constitutes virtual vulnerability. <br /></span></li>\n<li><span lang=\"EN-GB\">Second, the software programme that provides a platform for the virtual world might be damaged, for example by means of a cyber attack. This can lead to the &ldquo;death&rdquo; of the virtual character or entity. <br /></span></li>\n<li><span lang=\"EN-GB\">Third, all these processes depend on (material) hardware. The <span class=\"GramE\">world wide web</span> and its wired and wireless communications rest on material infrastructures without which the web would be impossible. Therefore, if posthumans uploaded themselves into an infosphere and dispensed with their biological bodies, they would not gain invulnerability and immortality but merely transform their vulnerability.</span></li>\n</ol></blockquote>\n<h2 class=\"MsoNormal\" style=\"text-align: justify;\"><em><span lang=\"EN-GB\">Bodily vulnerability</span></em></h2>\n<blockquote>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span lang=\"EN-GB\">Minds need bodies. This is in line with contemporary research in cognitive science, which argues that &ldquo;embodiment&rdquo; is necessary since minds can develop and function only in interaction with their environment (<span class=\"SpellE\">Lakoff</span> and Johnson 1999 and others). This direction of thought is also taken in contemporary robotics, for example when it recognizes that manipulation plays an important role in the development of cognition (<span class=\"SpellE\">Sandini</span> et al. 2004). In his famous 1988 book on &ldquo;mind children&rdquo; <span class=\"SpellE\">Moravec</span> argued that true AI can be achieved only if machines have a body (Moravec 1988)...</span><span lang=\"EN-GB\">Thus, uploading and nano-based cyborgization would not dispense with the body but transform it into a virtual body or a nano-body. This would create vulnerabilities that sometimes resemble the vulnerabilities we know today (for instance virtual violence) but also new vulnerabilities.</span></p>\n</blockquote>\n<h2 class=\"MsoNormal\" style=\"text-align: justify;\"><em><span lang=\"EN-GB\">Metaphysical vulnerability</span></em></h2>\n<blockquote>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span lang=\"EN-GB\">With this atomism comes that atomist view of death: there is always the possibility of disintegration; neither physical-material objects nor information objects exist forever. Information can disintegrate and the material conditions for information are vulnerable to disintegration as well. Thus, at a fundamental level everything is vulnerable to disintegration, understood by atomism as a re-organization of elementary particles. This &ldquo;metaphysical&rdquo; vulnerability is unavoidable for posthumans, whatever the status of their elementary particles and the organs and systems constituted by these particles (biological or not). According to their own metaphysics, the cyborgs and <span class=\"SpellE\">inforgs</span> that <span class=\"SpellE\">transhumanists</span> and their supporters wish to create would be only temporal orders that have only temporary stability &ndash; if any.</span></p>\n<p><span lang=\"EN-GB\">&nbsp;Note, however, that recently both <span class=\"SpellE\">Floridi</span> and contemporary physics seem to move toward a more ecological, holistic metaphysics, which suggests a different definition of death. In information ecologies, perhaps death means the absence of relations, disconnection. Or it means: deletion, understood ecologically and holistically as the removal out of the whole. But in the light of this metaphysics, too, there seems no reason why posthumans would be able to escape death in this sense.</span></p>\n</blockquote>\n<h2 class=\"MsoNormal\" style=\"text-align: justify;\"><em><span lang=\"EN-GB\">Existential and psychological vulnerabilities</span></em></h2>\n<blockquote>\n<p><span lang=\"EN-GB\">This gives rise to what we may call &ldquo;indirect&rdquo; or &ldquo;second-order&rdquo; vulnerabilities. For instance, we can become aware of the possibility of disintegration, the possibility of death. We can also become aware of less threatening risks, such as disease. There are many first-order vulnerabilities. Awareness of them renders us extra vulnerable as opposed to beings <span class=\"GramE\">who</span> lack such an ability to take distance from ourselves. From an existential-phenomenological point of view (which has its roots in work by Heidegger and others), but also from the point of view of common sense psychology, we must extend the meaning of vulnerability to the sufferings of the mind. Vulnerability awareness itself constitutes a higher-order vulnerability that is typical of humans. In posthumans, we could only erase this vulnerability if we were prepared to abandon the particular higher form of consciousness that we &ldquo;enjoy.&rdquo; No transhumanist would seriously consider that solution to the problem.</span></p>\n</blockquote>\n<h2 class=\"MsoNormal\" style=\"text-align: justify;\"><em><span lang=\"EN-GB\">Social and emotional vulnerability</span></em></h2>\n<blockquote>\n<p><span lang=\"EN-GB\">If I depend on you socially and emotionally, then I am vulnerable to what you say or do. Unless posthumans were to live in complete isolation without any possibility of inter-posthuman communication, they would be as vulnerable as we are to the sufferings created by the social life, although the precise relation between their social life and their emotional make-up might differ...</span><span lang=\"EN-GB\">For example, in <span class=\"SpellE\">Houellebecq&rsquo;s</span> novel the posthumans have a reduced capacity to feel sad, but at the cost of a reduced capacity to desire and to feel joy. More generally, the lesson seems to be: emotional enhancement comes at a high price. Are we prepared to pay it? Even if we succeed in diminishing this kind of vulnerability, we might lose something that is of value to us. This brings me to the next kind of vulnerability.</span></p>\n</blockquote>\n<h2 class=\"MsoNormal\" style=\"text-align: justify;\"><em><span lang=\"EN-GB\">Ethical-axiological vulnerability</span></em></h2>\n<blockquote>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span lang=\"EN-GB\">We value not only people and our relationships with them; we are also attached to many other things in life. Caring makes us vulnerable (Nussbaum 1986). We develop ties out of our engagement with humans, animals, objects, buildings, landscapes, and many other things. This renders us vulnerable since it makes us dependent on (what we experience as) &ldquo;external&rdquo; things. We sometimes get emotional about things since we care and since we value. We <em>suffer</em> since we depend on external things...</span><span lang=\"EN-GB\">Posthumans could be cognitively equipped to follow this strategy, for instance by means of emotional enhancement that allows more self-control and prevents them forming too strong ties to things. If we really wanted to become invulnerable in this respect, we should create posthumans who no longer care at all about external things &ndash; including other posthumans. That would be &ldquo;<span class=\"SpellE\">posthumans</span>&rdquo; who no longer have the ability to care and to value. They would &ldquo;connect&rdquo; to others and to things, but they would not really engage with them, since that would render them vulnerable. They would be perfectly rational Stoics, perhaps, but it would be odd to call them &ldquo;posthumans&rdquo; at all since the term &ldquo;human&rdquo; would lose its meaning. It is even doubtful if this extreme form of Stoicism would be <em>possible</em> for any entity that possesses the capacity of valuing and that <em>engages</em> with the world.</span></p>\n</blockquote>\n<h2 class=\"MsoNormal\" style=\"text-align: justify;\">'<span lang=\"EN-GB\">Relational vulnerability'</span><span lang=\"EN-GB\">/'</span><span lang=\"EN-GB\">Conclusion: Heels and dragons</span>'</h2>\n<blockquote>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span lang=\"EN-GB\">The only way to make an entity invulnerable, it turns out, would be to create one that exists in absolute isolation and is absolutely independent of anything else. Such a being seems inconceivable &ndash; or would be a particularly strange kind of god. (It would have to be a &ldquo;philosopher&rsquo;s&rdquo; god that could hardly stir any religious feelings. Moreover, the god would not even be a &ldquo;first mover,&rdquo; let alone a creator, since that would imply a relation to our world. It is also hard to see how we would be aware of its existence or be able to form an idea about it, given the absence of <em>any </em>relation between us and the god.) Of course we could &ndash; if ethically acceptable at all &ndash; create posthumans that are less vulnerable in some particular areas, as long as we keep in mind that there are other sources of vulnerability, that new sources of vulnerability will emerge, and that our measure to decrease vulnerability in one area may increase it in another area.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span lang=\"EN-GB\">If transhumanists accept the results of this discussion, they should carefully reflect on, and <span class=\"GramE\">redefine,</span> the aims of human enhancement and avoid confusion about how these aims relate to vulnerability. If the aim is invulnerability, then I have offered some reasons why this aim is problematic. If their project has nothing to do with trying to reach invulnerability, then why should we transcend the human? Of course one could formulate no &ldquo;ultimate&rdquo; goals and choose less ambitious goals, such as more health and less suffering. For instance, one could use a utilitarian argument and say that we should avoid overall suffering and pain. Harris seems to have taken these routes (Harris 2007). And <span class=\"SpellE\">Bostrom</span> frequently mentions &ldquo;life extension&rdquo; as a goal rather than &ldquo;invulnerability&rdquo; or &ldquo;immortality.&rdquo; But even in these &ldquo;weakened&rdquo; or at least more modest forms, the transhumanist project can be interpreted as a particularly hostile response to (human) vulnerability that probably has no parallel in human history.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><em><span lang=\"EN-GB\">...</span></em><span lang=\"EN-GB\">Furthermore, this paper suggests that if we can and must make an ethical choice at all, then it is not a choice between vulnerable humans and invulnerable <span class=\"SpellE\">posthumans</span>, or even between vulnerability and invulnerability, but a choice between different <em>forms </em>of humanity and vulnerability. If implemented, human enhancement technologies such as mind uploading will not cancel vulnerability but transform it. As far as ethics is concerned, then, what we need to ask is which new forms of the human we want and how (in<span class=\"GramE\">)vulnerable</span> we wish to be. But this inquiry is possible only if we first fine-tune our ideas of what is possible in terms of enhancement and (in<span class=\"GramE\">)vulnerability</span>. To do this requires stretching our moral and technological imaginations.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span lang=\"EN-GB\">Moreover, if I&rsquo;m right about the different forms of posthuman vulnerability as discussed above, then we must dispense with the dragon metaphor used by <span class=\"SpellE\">Bostrom</span>: vulnerability is not a matter of &ldquo;external&rdquo; dangers that threaten or tyrannize us, but that have nothing to do with what we are; instead, it is bound up with our relational, technological and transient kind of being &ndash; human or posthuman. If there are dragons, they are part of us. It is our tragic condition that as relational entities we are at once the heel and the arrow, the hero and the dragon.</span></p>\n</blockquote>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span lang=\"EN-GB\">Before criticizing it, I'd like to point to the introduction where the author lays out his mission: to discuss what problems cannot \"in principle\" be avoided, what vulnerabilities are \"necessary\". In other words, he thinks he is laying out fundamental limits, on some level as inexorable and universal as, say, Turing's Halting Theorem.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span lang=\"EN-GB\">But he is manifestly doing no such thing! He lists countless 'vulnerabilities' which could easily be circumvented to arbitrary degrees. For example, the computer viruses he puts such stock on: there is no fundamental reason computer viruses must exist. There are many ways they could be eliminated starting from formal static proofs of security and functionality; the only fundamental limit relevant here would be Turing/Rice's theorem, which is applicable only if we wanted to run all possible programs, which we manifestly cannot and do not. Similar points apply to the rest of his software vulnerabilities.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span lang=\"EN-GB\">I would also like to single out his 'Metaphysical vulnerability'; physicists, SF authors, and transhumanists have been, for decades, outlining a multitude of models and possibilities for true immortality, ranging from Dyson's eternal intelligences to Tipler's collapse to Omega point to baby blackhole-universes. To appeal to atomism is to already beg the question (why <em>not</em> run intelligence on waves or more exotic forms of existence, why this particle-chauvinism?).</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span lang=\"EN-GB\">This applies again and again - the author supplies no solid proofs from any field, and apparently lacks the imagination or background to imagine ways to circumvent or dissolve his suggested limits. They may be exotic methods, but they still exist; were the author to reply that to employ such methods would result in intelligences so alien as to no longer be human, then I should accuse him of begging the question on a even larger scale - of defining the human as desirable and, essentially, as that which is compatible with his chosen limits.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span lang=\"EN-GB\">Since that question is at the heart of transhumanism, his paper offers nothing of interest to us.<br /></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eKFHNmCcmuogQifQN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 20, "extendedScore": null, "score": 8.096312591979706e-07, "legacy": true, "legacyId": "11269", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><a href=\"http://jetpress.org/v22/coeckelbergh.htm\">\"Vulnerable Cyborgs: Learning to Live with our Dragons\"</a>, <a href=\"http://users.belgacom.net/mark.coeckelbergh/index.htm\">Mark Coeckelbergh</a> (<a href=\"http://www.utwente.nl/gw/wijsb/organization/coeckelbergh/\">university</a>); abstract:</p>\n<blockquote>\n<p class=\"MsoNormal\" style=\"margin-left: 35.4pt; text-align: justify;\"><span lang=\"NL\">Transhumanist visions appear to aim at invulnerability. We are invited to fight the dragon of death and disease, to shed our old, human bodies, and to live on as invulnerable minds or cyborgs. This paper argues that even if we managed to enhance humans in one of these ways, we would remain highly vulnerable entities given the fundamentally relational and dependent nature of posthuman existence. After discussing the need for minds to be embodied, the issue of disease and death in the infosphere, and problems of psychological, social and axiological vulnerability, I conclude that transhumanist human enhancement would not erase our current vulnerabilities, but instead transform them. Although the struggle against vulnerability is typically human and would probably continue to mark posthumans, we had better recognize that we can never win that fight and that the many dragons that threaten us are part of us. As vulnerable humans and posthumans, we are at once the hero and the dragon.</span></p>\n</blockquote>\n<hr>\n<blockquote>\n<p><span lang=\"EN-GB\">Bostrom has written <a href=\"http://www.nickbostrom.com/fable/dragon.html\">a tale</a> about a dragon that terrorizes a kingdom and people who submit to the dragon rather than fighting it. According to <span class=\"SpellE\">Bostrom</span>, the \u201cmoral\u201d of the story is that we should fight the dragon, that is, extend the (healthy) human life span and not accept aging as a fact of life (<span class=\"SpellE\">Bostrom</span> 2005, 277). And in <em>The Singularity is Near </em>(2005) Kurzweil has suggested that following the acceleration of information technology, we will become cyborgs, upload ourselves, have nanobots in our bloodstream, and enjoy nonbiological experience. Although not all transhumanist authors explicitly state it, these ideas seem to aim toward invulnerability and immortality: by means of human enhancement technologies, we can transcend our present limited existence and become strong, invulnerable cyborgs or immortal minds living in an eternal, virtual world.</span></p>\n<p><span lang=\"EN-GB\">...</span><span lang=\"EN-GB\">However, in this paper, I will ask neither the ethical-normative question (Should we develop human enhancement techniques and should we aim for invulnerability?) nor the hermeneutical question (How can we best interpret and understand transhumanism in the light of cultural, religious, and scientific history?). Instead, I ask the question: <em>If and to the extent that</em> transhumanism aims at invulnerability, can it \u2013 in principle \u2013 reach that aim? The following discussion offers some obvious and some much less obvious reasons why posthumans would remain vulnerable, and why human vulnerability would be transformed rather than diminished or eliminated...</span><span lang=\"EN-GB\">However, to focus only on a <span class=\"SpellE\">defense</span> or rejection of what is valuable in humans would leave out of sight the relation between (in<span class=\"GramE\">)vulnerability</span> and <em>posthuman</em> possibilities. It would lead us back to the ethical-normative questions (Is human enhancement morally acceptable? Is vulnerability something to be valued? Is the <span class=\"SpellE\">transhumanist</span> project acceptable or desirable?), which is not what I want to do in this paper. Moreover, ethical arguments that present the problem as if we have a choice between \u201cnatural\u201d humanity and \u201cartificial\u201d <span class=\"SpellE\">posthumanity</span> are based on essentialist assumptions that make a sharp distinction between \u201cwhat we are\u201d (the natural) and technology (the artificial), whereas this distinction is at least questionable. Perhaps there is no fixed human nature apart from technology, perhaps we are \u201cartificial by nature\u201d (<span class=\"SpellE\">Plessner</span> 1975). If this is so, then the problem is not whether or not we want to transcend the human but how we want to shape that posthuman existence. Should we aim at invulnerability and if so, can we? As indicated before, here I limit the discussion to the \u201ccan\u201d question.</span></p>\n</blockquote>\n<p><span lang=\"EN-GB\">Breaking down the potential improvements:</span></p>\n<h2 class=\"MsoNormal\" style=\"text-align: justify; page-break-before: always;\" id=\"Physical_vulnerability\"><em><span lang=\"EN-GB\">Physical vulnerability</span></em></h2>\n<blockquote>\n<p class=\"MsoNormal\" style=\"text-align: justify; page-break-before: always;\"><span lang=\"EN-GB\">Not only could human enhancement make us immune to current viruses; it could also offer other \u201cimmunities,\u201d broadly understood...</span><span lang=\"EN-GB\">However, the project of total vulnerability or even overall reduction of vulnerability is bound to fail. If we consider the history of medical technology, we observe that for every disease new technology helps to prevent or cure, there is at least one new disease that escapes our techno-scientific control. We can win one battle, but we can never win the war. There will be always new diseases, new viruses, and, more generally, new threats to physical vulnerability. Consider also natural disasters caused by floods, earthquakes, volcanic eruptions, and so on.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify; page-break-before: always;\"><span lang=\"EN-GB\">Moreover, the very means to fight those threats sometimes create new threats themselves. This can happen within the same domain, as is the case with antibiotics that lead to the development of more resistant bacteria, or in another domain, as is the case with new security measures in airports, which are meant as protections against physical harm by terrorism but might pose new (health?) risks. Paradoxically, technologies that are meant to reduce vulnerability often create new ones. This is also true for posthuman technologies. For example, posthumans would also be vulnerable to at least some of the risks <span class=\"SpellE\">Bostrom</span> calls \u201cexistential risks\u201d (<span class=\"SpellE\">Bostrom</span> 2002), which could wipe out <span class=\"SpellE\">posthumankind</span>. Nanotechnology or nuclear technology could be misused, <span class=\"GramE\">a <span class=\"SpellE\">superintelligence</span></span> could take over and annihilate humankind, or technology could cause (further) resource depletion and ecological destruction. Military technologies are meant to protect us but they can become a threat, making us vulnerable in a new way. We wanted to master nature in order to become less dependent on it, but now we risk destroying the ecology that sustains us. And of course there are many physical threats we cannot foresee \u2013 not even in the near future. </span></p>\n</blockquote>\n<h2 class=\"MsoNormal\" style=\"text-align: justify;\" id=\"Material_and_immaterial_vulnerability\"><em><span lang=\"EN-GB\">Material and immaterial vulnerability</span></em></h2>\n<blockquote>\n<p class=\"MsoNormal\" style=\"text-align: justify; page-break-before: always;\"><span lang=\"EN-GB\">Consider computer viruses. Here the story is similar to the story of biological viruses: there are ongoing cycles of threats, counter-measures, and new threats. We can also consider physical damage to computers, although that is much less common. In any case, if we extend ourselves with software and hardware, this creates additional vulnerabilities. We must cope with \u201csoftware\u201d vulnerability and \u201chardware\u201d vulnerability. If humans and posthumans live in an \u201cinfosphere\u201d (see for example Floridi 2002), this is not a sphere of immunity. Perhaps our vulnerability becomes less material, but we cannot escape it. For instance, a virtual body in a virtual world may well be shielded from biological viruses, but it is vulnerable to at least three kinds of threats.</span></p>\n<ol>\n<li><span lang=\"EN-GB\">First, there are threats within the virtual world itself (consider for instance virtual rape), which constitutes virtual vulnerability. <br></span></li>\n<li><span lang=\"EN-GB\">Second, the software programme that provides a platform for the virtual world might be damaged, for example by means of a cyber attack. This can lead to the \u201cdeath\u201d of the virtual character or entity. <br></span></li>\n<li><span lang=\"EN-GB\">Third, all these processes depend on (material) hardware. The <span class=\"GramE\">world wide web</span> and its wired and wireless communications rest on material infrastructures without which the web would be impossible. Therefore, if posthumans uploaded themselves into an infosphere and dispensed with their biological bodies, they would not gain invulnerability and immortality but merely transform their vulnerability.</span></li>\n</ol></blockquote>\n<h2 class=\"MsoNormal\" style=\"text-align: justify;\" id=\"Bodily_vulnerability\"><em><span lang=\"EN-GB\">Bodily vulnerability</span></em></h2>\n<blockquote>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span lang=\"EN-GB\">Minds need bodies. This is in line with contemporary research in cognitive science, which argues that \u201cembodiment\u201d is necessary since minds can develop and function only in interaction with their environment (<span class=\"SpellE\">Lakoff</span> and Johnson 1999 and others). This direction of thought is also taken in contemporary robotics, for example when it recognizes that manipulation plays an important role in the development of cognition (<span class=\"SpellE\">Sandini</span> et al. 2004). In his famous 1988 book on \u201cmind children\u201d <span class=\"SpellE\">Moravec</span> argued that true AI can be achieved only if machines have a body (Moravec 1988)...</span><span lang=\"EN-GB\">Thus, uploading and nano-based cyborgization would not dispense with the body but transform it into a virtual body or a nano-body. This would create vulnerabilities that sometimes resemble the vulnerabilities we know today (for instance virtual violence) but also new vulnerabilities.</span></p>\n</blockquote>\n<h2 class=\"MsoNormal\" style=\"text-align: justify;\" id=\"Metaphysical_vulnerability\"><em><span lang=\"EN-GB\">Metaphysical vulnerability</span></em></h2>\n<blockquote>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span lang=\"EN-GB\">With this atomism comes that atomist view of death: there is always the possibility of disintegration; neither physical-material objects nor information objects exist forever. Information can disintegrate and the material conditions for information are vulnerable to disintegration as well. Thus, at a fundamental level everything is vulnerable to disintegration, understood by atomism as a re-organization of elementary particles. This \u201cmetaphysical\u201d vulnerability is unavoidable for posthumans, whatever the status of their elementary particles and the organs and systems constituted by these particles (biological or not). According to their own metaphysics, the cyborgs and <span class=\"SpellE\">inforgs</span> that <span class=\"SpellE\">transhumanists</span> and their supporters wish to create would be only temporal orders that have only temporary stability \u2013 if any.</span></p>\n<p><span lang=\"EN-GB\">&nbsp;Note, however, that recently both <span class=\"SpellE\">Floridi</span> and contemporary physics seem to move toward a more ecological, holistic metaphysics, which suggests a different definition of death. In information ecologies, perhaps death means the absence of relations, disconnection. Or it means: deletion, understood ecologically and holistically as the removal out of the whole. But in the light of this metaphysics, too, there seems no reason why posthumans would be able to escape death in this sense.</span></p>\n</blockquote>\n<h2 class=\"MsoNormal\" style=\"text-align: justify;\" id=\"Existential_and_psychological_vulnerabilities\"><em><span lang=\"EN-GB\">Existential and psychological vulnerabilities</span></em></h2>\n<blockquote>\n<p><span lang=\"EN-GB\">This gives rise to what we may call \u201cindirect\u201d or \u201csecond-order\u201d vulnerabilities. For instance, we can become aware of the possibility of disintegration, the possibility of death. We can also become aware of less threatening risks, such as disease. There are many first-order vulnerabilities. Awareness of them renders us extra vulnerable as opposed to beings <span class=\"GramE\">who</span> lack such an ability to take distance from ourselves. From an existential-phenomenological point of view (which has its roots in work by Heidegger and others), but also from the point of view of common sense psychology, we must extend the meaning of vulnerability to the sufferings of the mind. Vulnerability awareness itself constitutes a higher-order vulnerability that is typical of humans. In posthumans, we could only erase this vulnerability if we were prepared to abandon the particular higher form of consciousness that we \u201cenjoy.\u201d No transhumanist would seriously consider that solution to the problem.</span></p>\n</blockquote>\n<h2 class=\"MsoNormal\" style=\"text-align: justify;\" id=\"Social_and_emotional_vulnerability\"><em><span lang=\"EN-GB\">Social and emotional vulnerability</span></em></h2>\n<blockquote>\n<p><span lang=\"EN-GB\">If I depend on you socially and emotionally, then I am vulnerable to what you say or do. Unless posthumans were to live in complete isolation without any possibility of inter-posthuman communication, they would be as vulnerable as we are to the sufferings created by the social life, although the precise relation between their social life and their emotional make-up might differ...</span><span lang=\"EN-GB\">For example, in <span class=\"SpellE\">Houellebecq\u2019s</span> novel the posthumans have a reduced capacity to feel sad, but at the cost of a reduced capacity to desire and to feel joy. More generally, the lesson seems to be: emotional enhancement comes at a high price. Are we prepared to pay it? Even if we succeed in diminishing this kind of vulnerability, we might lose something that is of value to us. This brings me to the next kind of vulnerability.</span></p>\n</blockquote>\n<h2 class=\"MsoNormal\" style=\"text-align: justify;\" id=\"Ethical_axiological_vulnerability\"><em><span lang=\"EN-GB\">Ethical-axiological vulnerability</span></em></h2>\n<blockquote>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span lang=\"EN-GB\">We value not only people and our relationships with them; we are also attached to many other things in life. Caring makes us vulnerable (Nussbaum 1986). We develop ties out of our engagement with humans, animals, objects, buildings, landscapes, and many other things. This renders us vulnerable since it makes us dependent on (what we experience as) \u201cexternal\u201d things. We sometimes get emotional about things since we care and since we value. We <em>suffer</em> since we depend on external things...</span><span lang=\"EN-GB\">Posthumans could be cognitively equipped to follow this strategy, for instance by means of emotional enhancement that allows more self-control and prevents them forming too strong ties to things. If we really wanted to become invulnerable in this respect, we should create posthumans who no longer care at all about external things \u2013 including other posthumans. That would be \u201c<span class=\"SpellE\">posthumans</span>\u201d who no longer have the ability to care and to value. They would \u201cconnect\u201d to others and to things, but they would not really engage with them, since that would render them vulnerable. They would be perfectly rational Stoics, perhaps, but it would be odd to call them \u201cposthumans\u201d at all since the term \u201chuman\u201d would lose its meaning. It is even doubtful if this extreme form of Stoicism would be <em>possible</em> for any entity that possesses the capacity of valuing and that <em>engages</em> with the world.</span></p>\n</blockquote>\n<h2 class=\"MsoNormal\" style=\"text-align: justify;\" id=\"_Relational_vulnerability___Conclusion__Heels_and_dragons_\">'<span lang=\"EN-GB\">Relational vulnerability'</span><span lang=\"EN-GB\">/'</span><span lang=\"EN-GB\">Conclusion: Heels and dragons</span>'</h2>\n<blockquote>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span lang=\"EN-GB\">The only way to make an entity invulnerable, it turns out, would be to create one that exists in absolute isolation and is absolutely independent of anything else. Such a being seems inconceivable \u2013 or would be a particularly strange kind of god. (It would have to be a \u201cphilosopher\u2019s\u201d god that could hardly stir any religious feelings. Moreover, the god would not even be a \u201cfirst mover,\u201d let alone a creator, since that would imply a relation to our world. It is also hard to see how we would be aware of its existence or be able to form an idea about it, given the absence of <em>any </em>relation between us and the god.) Of course we could \u2013 if ethically acceptable at all \u2013 create posthumans that are less vulnerable in some particular areas, as long as we keep in mind that there are other sources of vulnerability, that new sources of vulnerability will emerge, and that our measure to decrease vulnerability in one area may increase it in another area.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span lang=\"EN-GB\">If transhumanists accept the results of this discussion, they should carefully reflect on, and <span class=\"GramE\">redefine,</span> the aims of human enhancement and avoid confusion about how these aims relate to vulnerability. If the aim is invulnerability, then I have offered some reasons why this aim is problematic. If their project has nothing to do with trying to reach invulnerability, then why should we transcend the human? Of course one could formulate no \u201cultimate\u201d goals and choose less ambitious goals, such as more health and less suffering. For instance, one could use a utilitarian argument and say that we should avoid overall suffering and pain. Harris seems to have taken these routes (Harris 2007). And <span class=\"SpellE\">Bostrom</span> frequently mentions \u201clife extension\u201d as a goal rather than \u201cinvulnerability\u201d or \u201cimmortality.\u201d But even in these \u201cweakened\u201d or at least more modest forms, the transhumanist project can be interpreted as a particularly hostile response to (human) vulnerability that probably has no parallel in human history.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><em><span lang=\"EN-GB\">...</span></em><span lang=\"EN-GB\">Furthermore, this paper suggests that if we can and must make an ethical choice at all, then it is not a choice between vulnerable humans and invulnerable <span class=\"SpellE\">posthumans</span>, or even between vulnerability and invulnerability, but a choice between different <em>forms </em>of humanity and vulnerability. If implemented, human enhancement technologies such as mind uploading will not cancel vulnerability but transform it. As far as ethics is concerned, then, what we need to ask is which new forms of the human we want and how (in<span class=\"GramE\">)vulnerable</span> we wish to be. But this inquiry is possible only if we first fine-tune our ideas of what is possible in terms of enhancement and (in<span class=\"GramE\">)vulnerability</span>. To do this requires stretching our moral and technological imaginations.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span lang=\"EN-GB\">Moreover, if I\u2019m right about the different forms of posthuman vulnerability as discussed above, then we must dispense with the dragon metaphor used by <span class=\"SpellE\">Bostrom</span>: vulnerability is not a matter of \u201cexternal\u201d dangers that threaten or tyrannize us, but that have nothing to do with what we are; instead, it is bound up with our relational, technological and transient kind of being \u2013 human or posthuman. If there are dragons, they are part of us. It is our tragic condition that as relational entities we are at once the heel and the arrow, the hero and the dragon.</span></p>\n</blockquote>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span lang=\"EN-GB\">Before criticizing it, I'd like to point to the introduction where the author lays out his mission: to discuss what problems cannot \"in principle\" be avoided, what vulnerabilities are \"necessary\". In other words, he thinks he is laying out fundamental limits, on some level as inexorable and universal as, say, Turing's Halting Theorem.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span lang=\"EN-GB\">But he is manifestly doing no such thing! He lists countless 'vulnerabilities' which could easily be circumvented to arbitrary degrees. For example, the computer viruses he puts such stock on: there is no fundamental reason computer viruses must exist. There are many ways they could be eliminated starting from formal static proofs of security and functionality; the only fundamental limit relevant here would be Turing/Rice's theorem, which is applicable only if we wanted to run all possible programs, which we manifestly cannot and do not. Similar points apply to the rest of his software vulnerabilities.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span lang=\"EN-GB\">I would also like to single out his 'Metaphysical vulnerability'; physicists, SF authors, and transhumanists have been, for decades, outlining a multitude of models and possibilities for true immortality, ranging from Dyson's eternal intelligences to Tipler's collapse to Omega point to baby blackhole-universes. To appeal to atomism is to already beg the question (why <em>not</em> run intelligence on waves or more exotic forms of existence, why this particle-chauvinism?).</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span lang=\"EN-GB\">This applies again and again - the author supplies no solid proofs from any field, and apparently lacks the imagination or background to imagine ways to circumvent or dissolve his suggested limits. They may be exotic methods, but they still exist; were the author to reply that to employ such methods would result in intelligences so alien as to no longer be human, then I should accuse him of begging the question on a even larger scale - of defining the human as desirable and, essentially, as that which is compatible with his chosen limits.</span></p>\n<p class=\"MsoNormal\" style=\"text-align: justify;\"><span lang=\"EN-GB\">Since that question is at the heart of transhumanism, his paper offers nothing of interest to us.<br></span></p>", "sections": [{"title": "Physical vulnerability", "anchor": "Physical_vulnerability", "level": 1}, {"title": "Material and immaterial vulnerability", "anchor": "Material_and_immaterial_vulnerability", "level": 1}, {"title": "Bodily vulnerability", "anchor": "Bodily_vulnerability", "level": 1}, {"title": "Metaphysical vulnerability", "anchor": "Metaphysical_vulnerability", "level": 1}, {"title": "Existential and psychological vulnerabilities", "anchor": "Existential_and_psychological_vulnerabilities", "level": 1}, {"title": "Social and emotional vulnerability", "anchor": "Social_and_emotional_vulnerability", "level": 1}, {"title": "Ethical-axiological vulnerability", "anchor": "Ethical_axiological_vulnerability", "level": 1}, {"title": "'Relational vulnerability'/'Conclusion: Heels and dragons'", "anchor": "_Relational_vulnerability___Conclusion__Heels_and_dragons_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "11 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-04T11:05:08.326Z", "modifiedAt": null, "url": null, "title": "Funnel plots: the study that didn't bark, or, visualizing regression to the null", "slug": "funnel-plots-the-study-that-didn-t-bark-or-visualizing", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:56.304Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RYA9uviAJBtn8hqkF/funnel-plots-the-study-that-didn-t-bark-or-visualizing", "pageUrlRelative": "/posts/RYA9uviAJBtn8hqkF/funnel-plots-the-study-that-didn-t-bark-or-visualizing", "linkUrl": "https://www.lesswrong.com/posts/RYA9uviAJBtn8hqkF/funnel-plots-the-study-that-didn-t-bark-or-visualizing", "postedAtFormatted": "Sunday, December 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Funnel%20plots%3A%20the%20study%20that%20didn't%20bark%2C%20or%2C%20visualizing%20regression%20to%20the%20null&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFunnel%20plots%3A%20the%20study%20that%20didn't%20bark%2C%20or%2C%20visualizing%20regression%20to%20the%20null%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRYA9uviAJBtn8hqkF%2Ffunnel-plots-the-study-that-didn-t-bark-or-visualizing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Funnel%20plots%3A%20the%20study%20that%20didn't%20bark%2C%20or%2C%20visualizing%20regression%20to%20the%20null%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRYA9uviAJBtn8hqkF%2Ffunnel-plots-the-study-that-didn-t-bark-or-visualizing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRYA9uviAJBtn8hqkF%2Ffunnel-plots-the-study-that-didn-t-bark-or-visualizing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 722, "htmlBody": "<p><a href=\"http://marginalrevolution.com/marginalrevolution/2011/11/small-samples-mean-statistically-significant-results-should-usually-be-ignored.html\">Marginal Revolution</a> linked a post at Genomes Unzipped, <a href=\"http://www.genomesunzipped.org/2011/11/size-matters-and-other-lessons-from-medical-genetics.php?utm_source=feedburner&amp;utm_medium=feed&amp;utm_campaighttp://www.genomesunzipped.org/2011/11/size-matters-and-other-lessons-from-medical-genetics.php\">\"Size matters, and other lessons from medical genetics\"</a>, with the interesting centerpiece graph:</p>\n<p><img src=\"http://www.genomesunzipped.org/wp-content/uploads/2011/11/NatGen_funnel-plot_cropped.gif\" alt=\"a funnel plot of genetic studies showing null result approached as sample size increasese\" width=\"542\" height=\"318\" /></p>\n<p>This is from pg 3 of an <a href=\"http://www.genetsim.org/workshop/201106/readings/ioannidis2001.pdf\">Ioannidis 2001 et al</a> article (who else?) on what is called a <a href=\"https://en.wikipedia.org/wiki/Funnel_plot\">funnel plot</a>: each line represents a series of studies about some particularly hot gene-disease correlations, plotted where Y =&nbsp; the <a href=\"https://en.wikipedia.org/wiki/Odds_ratio\">odds ratio</a> (measure of <a href=\"https://en.wikipedia.org/wiki/Effect_size\">effect size</a>; all results are '<a href=\"https://en.wikipedia.org/wiki/Statistical_significance\">statistically significant</a>', of course) and X = the sample size. The 1 line is the null hypothesis, here. You will notice something dramatic: as we move along the X-axis and sample sizes increase, everything begins to converge on 1:</p>\n<blockquote>\n<p>Readers familiar with the history of medical association studies will be unsurprised by what happened over the next few years: initial excitement (this same polymorphism was associated with <a href=\"http://www.nature.com/ki/journal/v50/n2/abs/ki1996362a.html\">diabetes</a>! And <a href=\"http://www.nature.com/ng/journal/v6/n1/abs/ng0194-29.html\">longevity</a>!) was followed by inconclusive replication studies and, ultimately, disappointment. In 2000, 8 years after the initial report, <a href=\"http://www.sciencedirect.com/science/article/pii/S0140673600820097\">a large study</a> involving over 5,000 cases and controls found absolutely no detectable effect of the <em>ACE</em> polymorphism on heart attack risk. In the meantime, the same polymorphism had turned up in dozens of other association studies for a wide range of traits ranging from&nbsp;<a href=\"http://www.sciencedirect.com/science/article/pii/S000293780166335X\">obstet&shy;ric cholestasis</a>&nbsp;to&nbsp;<a href=\"http://ajrccm.atsjournals.org/cgi/content/full/165/8/1103\">menin&shy;go&shy;&shy;coccal disease in children</a>, virtually none of which have ever been convincingly replicated.</p>\n</blockquote>\n<p>(See also <a href=\"/lw/72f/why_epidemiology_will_not_correct_itself/\">\"Why epidemiology will not correct itself\"</a> or the <a href=\"http://www.gwern.net/DNB%20FAQ#fn50\">DNB FAQ</a>.)</p>\n<p><a id=\"more\"></a></p>\n<p>This graph is interesting as it shows 8 different regressions to the mean. What is also interesting is what a funnel plot is usually used for, why I ran into it in the first place reading <a href=\"http://www.cochrane-net.org/openlearning/html/mod15-3.htm\">Cochrane Group materials</a> - it's used to show <a href=\"https://en.wikipedia.org/wiki/Publication_bias\"><em>publication bias</em></a>.</p>\n<p>That is, suppose you were looking at a gene you know for certain not to be correlated (you knew the null result to be true), and you ran many trials, each with a different number of samples; you would expect that the trials with small samples would have a wide scattering of results (sometimes the effect size would look <a href=\"/lw/1ib/parapsychology_the_control_group_for_science/\">wildly large</a> and sometimes they would look wildly small or negative), and that this scattering would be equally for and against any connection (on either side of the 1 line). By the same reasoning you would expect that your largest samples would only be scattered a little bit on either side of the 1 line, and the larger the sample the closer they will be to the 1/null line.</p>\n<p>If you plotted your hypothetical trials on the above graph, you'd see what looks pretty much like the above graph - a kind of triangular cloud, wide on the left and ever narrowing towards the right as sample sizes increase and variance decreases.</p>\n<p>Now here's the question: given that all 8 correlations trend steadily towards the null hypothesis, one would seem to expect them to actually <em>be</em> the null result. But if that is so, where are the random trials scattered on the <em>other</em> side of the 1 line? Not one sequence of studies ever crosses the 1 line!</p>\n<blockquote>\n<p>The <em>ACE</em> story is not unique; time and time again, initial reports of associations between candidate genes and complex diseases failed to replicate in subsequent studies. With the benefit of hindsight, the problem is clear: in general, common genetic polymorphisms have very small effects on disease risk. Detecting these subtle effects requires studying not dozens or hundreds, but thousands or tens-of-thousands of individuals. <strong>Smaller studies, which had no power to detect these small effects, were essentially random p-value generators. Sometimes the p-values were &ldquo;significant&rdquo; and sometimes not, without any correlation to whether a variant was truly associated</strong>. Additionally, since investigators were often looking at only a few variants (often just one!) in a single gene that they strongly believed to be involved in the disease, they were often able to subset the data (splitting males and females, for example) to find &ldquo;significant&rdquo; results in some subgroup. <strong>This, combined with a tendency to publish positive results and leave negative results in a desk drawer, resulted in a conflicted and confusing body of literature which actively retarded medical genetics progress.</strong></p>\n</blockquote>\n<p>Wikipedia's funnel chart graph shows us how a plot should look (with this time sample size being the Y axis and odds being the X axis, so the triangle is rotated):</p>\n<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/2/2c/Funnelplot.png\" alt=\"\" width=\"300\" height=\"300\" /></p>\n<p>Does that describe any of the sequences graphed above?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"bh7uxTTqmsQ8jZJdB": 1, "vg4LDxjdwHLotCm8w": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RYA9uviAJBtn8hqkF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 52, "baseScore": 69, "extendedScore": null, "score": 0.00014452762645949364, "legacy": true, "legacyId": "11208", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 47, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["n7E2FC63MZGnvZAdr", "enuGsZoFLR4KyEx3n"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-04T14:54:51.437Z", "modifiedAt": null, "url": null, "title": "K-complexity of everyday things", "slug": "k-complexity-of-everyday-things", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:49.163Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GuQngmjtLEtWbKE67/k-complexity-of-everyday-things", "pageUrlRelative": "/posts/GuQngmjtLEtWbKE67/k-complexity-of-everyday-things", "linkUrl": "https://www.lesswrong.com/posts/GuQngmjtLEtWbKE67/k-complexity-of-everyday-things", "postedAtFormatted": "Sunday, December 4th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20K-complexity%20of%20everyday%20things&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AK-complexity%20of%20everyday%20things%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGuQngmjtLEtWbKE67%2Fk-complexity-of-everyday-things%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=K-complexity%20of%20everyday%20things%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGuQngmjtLEtWbKE67%2Fk-complexity-of-everyday-things", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGuQngmjtLEtWbKE67%2Fk-complexity-of-everyday-things", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 127, "htmlBody": "<p>What can we say about the K-complexity of a non-random string from our universe, e.g. the text of Finnegans Wake? It contains lots of patterns making it easy to compress using a regular archiver, but can we do much better than that?</p>\n<p>On one hand, the laws of physics in our universe seem to be simple, and generating the text is just a matter of generating our universe then pointing to the text. On the other hand, our evolution involved a lot of quantum randomness, so pointing to humans within the universe could require a whole lot of additional bits above and beyond the laws of physics. So does anyone have good arguments whether the K-complexity of Finnegans Wake is closer to 10% or 0.1% of its length?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GuQngmjtLEtWbKE67", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 19, "extendedScore": null, "score": 3.5e-05, "legacy": true, "legacyId": "11270", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-05T02:11:11.400Z", "modifiedAt": null, "url": null, "title": "Larry King: I want to be frozen", "slug": "larry-king-i-want-to-be-frozen", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:58.872Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/B7xgYpnGHNB32R9ji/larry-king-i-want-to-be-frozen", "pageUrlRelative": "/posts/B7xgYpnGHNB32R9ji/larry-king-i-want-to-be-frozen", "linkUrl": "https://www.lesswrong.com/posts/B7xgYpnGHNB32R9ji/larry-king-i-want-to-be-frozen", "postedAtFormatted": "Monday, December 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Larry%20King%3A%20I%20want%20to%20be%20frozen&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALarry%20King%3A%20I%20want%20to%20be%20frozen%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB7xgYpnGHNB32R9ji%2Flarry-king-i-want-to-be-frozen%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Larry%20King%3A%20I%20want%20to%20be%20frozen%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB7xgYpnGHNB32R9ji%2Flarry-king-i-want-to-be-frozen", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FB7xgYpnGHNB32R9ji%2Flarry-king-i-want-to-be-frozen", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 23, "htmlBody": "<p>I know celebrities cryocrastinate just as much as anyone else, but King seems like the kind of guy to go through with it.</p>\n<p><a href=\"http://www.cnn.com/2011/12/02/showbiz/larry-king-i-want-to-be-frozen/index.html?hpt=hp_t3\">http://www.cnn.com/2011/12/02/showbiz/larry-king-i-want-to-be-frozen/index.html?hpt=hp_t3</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "B7xgYpnGHNB32R9ji", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 15, "extendedScore": null, "score": 8.099703178211618e-07, "legacy": true, "legacyId": "11279", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-05T03:45:06.671Z", "modifiedAt": null, "url": null, "title": "Reading Math: Pearl, Causal Bayes Nets, and Functional Causal Models", "slug": "reading-math-pearl-causal-bayes-nets-and-functional-causal", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:49.947Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jwdink", "createdAt": "2009-07-03T19:47:51.303Z", "isAdmin": false, "displayName": "jwdink"}, "userId": "DC6Da4wuEEsLQw7PC", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zP4TkdxFpS5Ryxpib/reading-math-pearl-causal-bayes-nets-and-functional-causal", "pageUrlRelative": "/posts/zP4TkdxFpS5Ryxpib/reading-math-pearl-causal-bayes-nets-and-functional-causal", "linkUrl": "https://www.lesswrong.com/posts/zP4TkdxFpS5Ryxpib/reading-math-pearl-causal-bayes-nets-and-functional-causal", "postedAtFormatted": "Monday, December 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Reading%20Math%3A%20Pearl%2C%20Causal%20Bayes%20Nets%2C%20and%20Functional%20Causal%20Models&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReading%20Math%3A%20Pearl%2C%20Causal%20Bayes%20Nets%2C%20and%20Functional%20Causal%20Models%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzP4TkdxFpS5Ryxpib%2Freading-math-pearl-causal-bayes-nets-and-functional-causal%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Reading%20Math%3A%20Pearl%2C%20Causal%20Bayes%20Nets%2C%20and%20Functional%20Causal%20Models%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzP4TkdxFpS5Ryxpib%2Freading-math-pearl-causal-bayes-nets-and-functional-causal", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzP4TkdxFpS5Ryxpib%2Freading-math-pearl-causal-bayes-nets-and-functional-causal", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 526, "htmlBody": "<p>Hi all,</p>\n<p>I just started a doctoral program in psychology, and my research interest concerns causal reasoning. Since Pearl's <em>Causality, </em>the popularity of causal Bayes nets as psychological models for causal reasoning has really grown.&nbsp;Initially, I had some serious reservations, but now I'm beginning to think a great many of these are due in part to the oversimplified treatment that CBNs get in the psychology literature. For instance, the distinction between a) directed acyclic graphs + underlying conditional probabilities, and b) functional causal models, is rarely mentioned. Ignoring this distinction leads to some weird results, especially when the causal system in question has prominent physical mechanisms.</p>\n<p>Say we represent Gear A as causing Gear B to turn because Gear A is hooked up to an engine, and because the two gears are connected to each other by a chain. Something like this:</p>\n<p>Engine(ON) -&gt; GearA(turn) -&gt; GearB(turn)</p>\n<p>As a causal Net, this is problematic. If I \"intervene\"&nbsp;on GearA&nbsp;(perform <em>do</em>(GearA=stop)), then I get the expected result: GearA stops, GearB stops, and the engine keeps running (the 'undoing' effect [Sloman, 2005]). But what happens if I \"intervene\" on GearB? Since they are connected by a chain, GearA would stop as well. But GearA is the cause, and GearB is the effect: intervening on effects is NOT supposed to change the status of the cause. This violates a host of underlying assumptions for causal Bayes nets.&nbsp;(And you can't represent the gears as causing each other's movement, since that'd be a cyclical graph.)</p>\n<p>However, this can be solved if we're not representing the system as the above net, but we're instead representing the physics of the system, representing the forces involved via something that looks vaguely like newtonian equations. Indeed, this would accord better with people's hypothesis-testing behavior: if they aren't sure which gear has the engine behind it, they wouldn't try \"intervening\" on GearA's motion and GearB's motion, they'd try removing the chain, and seeing which gear is still moving.</p>\n<p>At first it seemed to me like causal Bayes nets only do the first kind of representation, not the latter. However, I was wrong: Pearl's \"functional causal models\" appear to do the latter. These have been vastly less prevalent in the psych literature, yet they seem extremely important.</p>\n<p>Anyways, the moral of the story is that I should really read a lot of Pearl's <em>Causality, </em>and actually have a grasp of some of the math; I can't just read the first chapter like most psychology researchers interested in this stuff.</p>\n<p>I'm not much of an autodidact when it comes to math, though I'm good at it when put in a class. Can anyone who's familiar with Pearl's book give me an idea of what sort of prerequisites it would be good to have in order to understand important chunks of it? Or am I overthinking this, and I should just try and plow through.</p>\n<p>Any suggestions on classes (or textbooks, I guess), or any thoughts on the above gears example, will be helpful and welcome.</p>\n<p>Thanks!&nbsp;</p>\n<p>EDIT: Maybe a more specific request could be phrased as following: will I be better served by taking some extra computer science classes, or some extra math classes (i.e., on calculus and probabilistic systems)?&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zP4TkdxFpS5Ryxpib", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 7, "extendedScore": null, "score": 8.100042997267146e-07, "legacy": true, "legacyId": "11280", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-05T04:39:44.447Z", "modifiedAt": null, "url": null, "title": "On \"Friendly\" Immortality", "slug": "on-friendly-immortality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:54.840Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RrKfYnKiPPYnpkEeW/on-friendly-immortality", "pageUrlRelative": "/posts/RrKfYnKiPPYnpkEeW/on-friendly-immortality", "linkUrl": "https://www.lesswrong.com/posts/RrKfYnKiPPYnpkEeW/on-friendly-immortality", "postedAtFormatted": "Monday, December 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20On%20%22Friendly%22%20Immortality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOn%20%22Friendly%22%20Immortality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRrKfYnKiPPYnpkEeW%2Fon-friendly-immortality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=On%20%22Friendly%22%20Immortality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRrKfYnKiPPYnpkEeW%2Fon-friendly-immortality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRrKfYnKiPPYnpkEeW%2Fon-friendly-immortality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1586, "htmlBody": "<p><strong>Personal Note</strong>: I would like to thank Normal Anomaly for beta-ing this for me and providing counter-arguments. It am asking him/her to comment below, so that everyone can give him/her karma for volunteering and helping me out. Even if you dislike the article, I think it's awesome that they were willing to take time out of their day to help someone they've never met.</p>\n<hr />\n<p>&nbsp;</p>\n<p>&nbsp;Imagine that you live in a world where everyone says \"AI is a good idea. We need to pursue it.\" <br /><br />Sounds great!<br /><br />But what if no one really thought that there was any reason to make sure the AI was friendly. That would be bad, right? You would probably think: \"Hey, AI is a great goal and all, but before we start pursuing it and actually developing the technology, we need to make sure that it's not going to blow up in our faces!\"</p>\n<p>That seems to me to be a rational response.<br /><br />Yet it seems like most people are not applying the same thought processes to life-extending technology.&nbsp;This website in particular has a habit of using some variant of this argument: \"Death is bad. Not dying is good. Therefore life-extending technologies are also good\" However this is missing the same level of contemplation that has been given to AI. Like AI, there are considerations that must be made to ensure this technology is \"friendly\". <br /><br />Most transhumanists have heard many of these issues before, normally sandwiched inside of a \"Death is Bad\" conversation. However&nbsp;these important considerations are often hand-waved away, as the conversation tends to stick to the low-hanging fruit. Here, I present them all in one place, so we can tackle them together, and perhaps come up with some solutions:</p>\n<ol>\n<li><strong>Over-population</strong>: For example, doubling the life-span of humans would <em>at the very least</em>&nbsp;double the number of people on this planet. If we could double life-spans today, we would go from 7 billion to <em>14 billion</em>&nbsp;people on Earth in 80 years, not counting regular population growth. <br /><br />Although currently birthrates are falling, all birthrate information we have is for women being fertile for approximately 25 years. This has not changed much throughout history, so we cannot necessarily extrapolate the current birthrate to what it would be if women were fertile for 50 years instead.<br />In other words, not only will there be a population explosion due to people living longer, but I'd be willing to bet that if life-extension was available today, birth rates would also go up.&nbsp;Right now, people who like to have kids only have enough money and fertile years to raise on average 2-3 kids. If you doubled the time they would have to reproduce, you will likely double the amount of children that child-rearing families have.<br /><br />For example,&nbsp;in modern society, by the time a woman's children are out of the house and done with college, the woman is no longer young and/or fertile. Say for example you had a child when you were 25. By the time your children were 20 you would be 45, and therefore not at a comfortable age to have children. However, if 45 becomes a young/fertile age for women, families might likely decide to re-reproduce.<br /><br />It's one thing to say: \"Well, we will develop technology to increase food yields and decrease fossil food consumption\", but are you positive we will have those technologies ready to go in time to save us? &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</li>\n<li><strong>Social Stagnation</strong>: &nbsp;Have you ever tried having a long conversation with an elderly person, only to realize that they are bigots/homophobes/racists, etc? We all love Grandpa John and Grammy Sue, but they have to die for society to move forward. If there were 180 year-olds alive today, chances are pretty strong that a good amount of them would think that being anti-slavery is pretty progressive. They would have been about 90 years old when women got the right to vote.<br /><br />We don't so much change our minds, and we grow new people and the old ones die. &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;</li>\n<li><strong>Life sucks, but at least you die</strong>: The world is populated with people suffering with mental disorders like depression, social issues like unemployment, and physical deprivations like poverty and hunger. <br /><br />It doesn't make sense to extend life until we have made our lives worth extending. &nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</li>\n<li><strong>Unknown Implications</strong>: How will this change the way society works? How will it change how people live their lives? We can have some educated guesses, but we won't know for sure what far-spread effects this would have.</li>\n</ol>\n<p><br />I have a friend who is a professional magician and \"psychic\", and about a month ago I convinced him to read HPMoR. After cursing me for ruining his sleep schedule for two days, we ended up having a discussion about some of the philosophies in there that we agreed and disagreed with. I was brand-new to LW. He had no prior knowledge of \"rationality\", but like most of his profession was very analytically minded. I would like to share something he wrote:</p>\n<blockquote>\n<p><br />We have a lot of ancient wisdom telling us that wishes are bad because we aren't wise, and you're saying... that if we could make ourselves wise, then we can have wishes and not have it blow up in our faces.</p>\n<p>See the shortest version of Alladin's Tale:<br />Wish One: \"I wish to be wise.\"<br />The End.<br /><br />Since... I am NOT mature, fully rational, and wise, &nbsp;<br />I really think I shouldn't have wishes, &nbsp;<br />Of which, <em>immortality</em> is an obvious specific example.<br /><br />Because I'm just not convinced &nbsp;<br />That I can predict the fallout.</p>\n</blockquote>\n<p>I call this \"The CEV of Immortality\", although at the time, neither of us had heard of the concept of CEV in the first place. The basic idea being that we are not currently prepared enough to even be <em>experimenting</em>&nbsp;with life-extending technologies. We don't know where it will lead and how we will cope. <br /><br />However scientists&nbsp;are working on these technologies right now, discovering genes that cause proteins that can be blocked to greatly increase life-spans of worms, mice and flies. Should a breakthrough discovery be made, who knows what will happen? &nbsp;Once it's developed there's no going back. If the technology exists, people will stop at nothing to use it. You won't be able to control it.</p>\n<p>Just like AI, life-extending technologies are not inherently \"bad\". But supporting the development of life-extending technnologies without already answering the above questions is like supporting the development of AI without knowing how to make it friendly. Once it's out of the box, it's too late.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<h2>Counter-arguments&nbsp;<br />\n<h5>(Provided by Normal Anomaly)</h5>\n</h2>\n<p><strong>Overpopulation Counter-argument</strong>: Birth rates are currently going down, and have fallen below replacement in much of the developed world (including the US). According to an article in <em>The Economist</em> last year, population will peak at about 10-11 billion in about 2050. <a href=\"http://www.economist.com/blogs/dailychart/2011/05/world_population\"> This UN infographic </a> appears to predict that fewer people will be born in 2020-2050 then were born in 1980-2010. I am skeptical that birth rate will increase with life extension. Space colonization is another way of coping with more people (again on a longer timescale than 40 years.) Finally, life extension will probably become available slowly, at first only a few extra years and only for the wealthy. This last also applies to &ldquo;unknown implications.&rdquo;</p>\n<p><br /> <strong>Social Stagnation Counter-argument</strong>: This leads to a slippery slope argument for killing elderly people; it&rsquo;s very unlikely that our current lifespans are at exactly the right tradeoff between social progress and life. Banning elderly people from voting or holding office would be more humane for the same results. <strong>\"Life sucks\" Counter argument</strong>: This is only an argument for working on making life worth extending, or possibly an argument for life extension not having the best marginal return in world-improvement. Also, nobody who doesn&rsquo;t want to live longer would have to, so life extension technology wouldn&rsquo;t result in immortal depressed people.</p>\n<p>&nbsp;</p>\n<hr />\n<p>These counter-arguments are very good points, but I do not think it is enough to guarantee a 100% \"Friendly\" transhumanism. I would love to see some discussions on them.</p>\n<p>&nbsp;<strong>Like last time I posted, I am making some \"root\" comments. They are: General comments, Over-population, Social stagnation, Life sucks, Unknown consequences. Please put your comment under the root it belongs to, in order to help keep the threads organized. Thank you!</strong></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RrKfYnKiPPYnpkEeW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 4, "extendedScore": null, "score": 8.100238926260809e-07, "legacy": true, "legacyId": "11246", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 103, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-05T05:06:22.186Z", "modifiedAt": null, "url": null, "title": "[LINK] How a Computer Game is Reinventing the Science of Expertise ", "slug": "link-how-a-computer-game-is-reinventing-the-science-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:52.117Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "InquilineKea", "createdAt": "2009-04-05T01:28:23.707Z", "isAdmin": false, "displayName": "InquilineKea"}, "userId": "5EqbEvWexa5jGAs3G", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8L55udCRTN4NAJwC2/link-how-a-computer-game-is-reinventing-the-science-of", "pageUrlRelative": "/posts/8L55udCRTN4NAJwC2/link-how-a-computer-game-is-reinventing-the-science-of", "linkUrl": "https://www.lesswrong.com/posts/8L55udCRTN4NAJwC2/link-how-a-computer-game-is-reinventing-the-science-of", "postedAtFormatted": "Monday, December 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20How%20a%20Computer%20Game%20is%20Reinventing%20the%20Science%20of%20Expertise%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20How%20a%20Computer%20Game%20is%20Reinventing%20the%20Science%20of%20Expertise%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8L55udCRTN4NAJwC2%2Flink-how-a-computer-game-is-reinventing-the-science-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20How%20a%20Computer%20Game%20is%20Reinventing%20the%20Science%20of%20Expertise%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8L55udCRTN4NAJwC2%2Flink-how-a-computer-game-is-reinventing-the-science-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8L55udCRTN4NAJwC2%2Flink-how-a-computer-game-is-reinventing-the-science-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 348, "htmlBody": "<p><a href=\"http://blogs.scientificamerican.com/observations/2011/12/01/how-a-computer-game-is-reinventing-the-science-of-expertise-video/\">http://blogs.scientificamerican.com/observations/2011/12/01/how-a-computer-game-is-reinventing-the-science-of-expertise-video/</a></p>\n<p>\n<div id=\"ld_2v0WZl_677\" style=\"color: #333333; font-family: 'Helvetica Neue', Helvetica, Arial, default; font-size: 13px; line-height: 17px; padding: 0px; margin: 0px;\">\n<blockquote style=\"padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 10px; border-left-width: 3px; border-left-color: #d0e5f2; margin: 0px;\">For decades, a different game, chess, has held the exalted position of &ldquo;the drosophila of cognitive science&rdquo;&mdash;the model organism that scientists could poke and prod to learn what makes experts better than the rest of us.&nbsp;<strong style=\"font-weight: bold; padding: 0px; margin: 0px;\">StarCraft 2, however, might be emerging as the rhesus macaque: its added&nbsp;complexity&nbsp;may confound researchers initially, but the answers could ultimately be more telling.</strong></blockquote>\n<strong style=\"font-weight: bold; padding: 0px; margin: 0px;\">&nbsp;</strong><br style=\"padding: 0px; margin: 0px;\" />\n<blockquote style=\"padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 10px; border-left-width: 3px; border-left-color: #d0e5f2; margin: 0px;\"><strong style=\"font-weight: bold; padding: 0px; margin: 0px;\">&ldquo;I can&rsquo;t think of a cognitive process that&rsquo;s not involved in StarCraft,&rdquo; says&nbsp;Mark Blair, a cognitive scientist at Simon Fraser University. &ldquo;It&rsquo;s working memory. It&rsquo;s decision making. It involves very precise motor skills. Everything is important and everything needs to work together.&rdquo;</strong></blockquote>\n<br style=\"padding: 0px; margin: 0px;\" />\n<blockquote style=\"padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 10px; border-left-width: 3px; border-left-color: #d0e5f2; margin: 0px;\">Blair, the Simon Fraser University scientist running the SkillCraft project, asked gamers at all ability levels to submit their replay files. He and his colleagues collected more than 4500 files, of which at least 3500 turned out to usable. &ldquo;<strong style=\"font-weight: bold; padding: 0px; margin: 0px;\">What we&rsquo;ve got is a satellite view of expertise that no one was able to get before,&rdquo; he says. &ldquo;We have hundreds of players at the basic levels, then hundreds more at level slightly better, and so on, in 8 different categories of players.&rdquo; By comparing the techniques and attributes of low-level players with other gamers up the chain of ability, they can start to discern how skills develop&mdash;and perhaps, over the long run, identify the most efficient training regimen.</strong></blockquote>\n<br style=\"padding: 0px; margin: 0px;\" />\n<blockquote style=\"padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 10px; border-left-width: 3px; border-left-color: #d0e5f2; margin: 0px;\">Both Blair and Lewis see parallels between the game and emergency management systems. In a high-stress crisis situation, the people in charge of coordinating a response may find themselves facing competing demands. Alarms might be alerting them to a fire burning in one part of town, a riot breaking out a few streets over, and the contamination of drinking water elsewhere. The mental task of keeping cool and distributing attention among equally urgent activities might closely resemble the core challenge of Starcraft 2. &ldquo;For emergencies, you don&rsquo;t get to train eight hours a day. You get two emergencies in your life but you better be good because lives are at stake,&rdquo; Blair says. &ldquo;Training in something like Starcraft could be really useful.&rdquo;</blockquote>\n</div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8L55udCRTN4NAJwC2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 24, "extendedScore": null, "score": 4.8e-05, "legacy": true, "legacyId": "11281", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 3, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-05T05:12:57.133Z", "modifiedAt": null, "url": null, "title": "Drawing Less Wrong: Technical Skill", "slug": "drawing-less-wrong-technical-skill", "viewCount": null, "lastCommentedAt": "2017-06-17T04:31:35.454Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bTDrKA8ySiCYWrWGE/drawing-less-wrong-technical-skill", "pageUrlRelative": "/posts/bTDrKA8ySiCYWrWGE/drawing-less-wrong-technical-skill", "linkUrl": "https://www.lesswrong.com/posts/bTDrKA8ySiCYWrWGE/drawing-less-wrong-technical-skill", "postedAtFormatted": "Monday, December 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Drawing%20Less%20Wrong%3A%20Technical%20Skill&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADrawing%20Less%20Wrong%3A%20Technical%20Skill%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbTDrKA8ySiCYWrWGE%2Fdrawing-less-wrong-technical-skill%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Drawing%20Less%20Wrong%3A%20Technical%20Skill%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbTDrKA8ySiCYWrWGE%2Fdrawing-less-wrong-technical-skill", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbTDrKA8ySiCYWrWGE%2Fdrawing-less-wrong-technical-skill", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2640, "htmlBody": "<p>The ability to <a href=\"https://www.lesserwrong.com/lw/8i1/drawing_less_wrong_observing_reality/\">observe</a> is probably at least 2/3rds of what separates non-artists from amateur artists. But those 2/3rds are near-useless without the ability to move your pencil the way your eyes want to it to go. And once you&#x27;ve transitioned into an amateur artist, around 9,000 hours of honing your technical skill is what separates you from a professional.</p><p>&quot;Technical Skill&quot; is a broad term - kind of a catch all for all term for various motor skills you&#x27;ll need to develop, background knowledge about how particular types of lines and shapes are perceived by most humans, and how to combine those skills and knowledge to produce particular effects with your drawing.</p><p>I can&#x27;t even begin to cover all of it, and most of it isn&#x27;t really appropriate for Less Wrong. But I will talk about some key motor skills that tie in with the next article, and a significant bias that plays a role in them.</p><p>This article was challenging to write - distilling a kinesthetic process into written words is difficult. This article will not be a substitute for having a teacher and a model, nor will it tell you exactly what exercises to do. But it will try to lay down some concepts that I&#x27;ll further expound on later.</p><h1>Holding the Pencil</h1><p>For many of you this may seem basic, but at least one reader commented that they went for years without understanding this, and because it seemed basic, nobody ever noticed it and corrected them.</p><p>Holding a pencil should look approximately like this:</p><p></p><span><figure><img src=\"http://www.publicdomainpictures.net/pictures/140000/velka/writing-hand-1443450678bLv.jpg\" class=\"draft-image \" style=\"\" /></figure></span><p></p><p>But its a bit more complicated that that. Many skilled artists hold the pencil in different ways. The biggest things to keep in mind are:</p><ol><li>Don&#x27;t grip the pencil too tightly. You&#x27;ll hurt yourself, and it won&#x27;t help.</li><li>If you hold the pencil closer to the tip, you will have more control over it, which is useful for fine details.</li><li>If you hold the pencil towards the back of the pencil, you&#x27;ll have greater range of motion, and allows you to quickly draw larger lines in a single stroke. It also will be looser, which can feel hard to control but can also produce certain line qualities you may want.</li></ol><p>(I personally tend to hold my pencil similar to the image above, but closer to the middle of the pencil)</p><p>A few examples of a pencil grip in motion:</p><p><a href=\"http://vimeo.com/4987983\">This man&#x27;s grip is similar to mine</a>, although the technique he describes isn&#x27;t something I think you should be worrying about just yet. (I&#x27;ll be talking about Darrel Tank&#x27;s website later on - I think he has good tutorials on technical skills, but does not prioritize them based on their low-hanging-fruit-ness.)</p><p><a href=\"http://www.youtube.com/watch?v=sHLxbLUDhlU\">This cartoonist switches grips a few times</a>, demonstrating how they can be useful at different stages of drawing. This video is particularly interesting because his &quot;loose&quot; grip is actually closer to the front, which I haven&#x27;t seen often.</p><h2>Slow Drawing and the Sunk Cost Fallacy</h2><p>I&#x27;ve spoken a few times about &quot;slow, small, precise lines,&quot; and implied that they are a terrible idea. They often are. You&#x27;ll be drawing slowly during some initial exercises that develop observational skills. But as soon as possible, you&#x27;ll want to start developing a form of hand-eye coordination that involves moving quickly using long lines. Until you achieve that, the small, meticulous lines will probably have a choppy quality, and certain compositions will be harder to capture.</p><p>Much of the &quot;energy&quot; of your drawing1, and the quality of the composition, will be established within the first one to two minutes. This isn&#x27;t a hard and fast rule, but holds true most of the time. Yes, you can erase, and rework things. With pure observation, infinite time and brute-force-reworking, you can craft a drawing that perfectly captures reality. But every time you erase and fix a drawing, two things happen:</p><p>One is that the paper smudges, tears slightly and otherwise degrades. This might be okay if you&#x27;re working on a computer tablet, but so long as you&#x27;re practicing with a real pencil, it&#x27;s an issue. After erasing 5-10 times, your drawing will have noticeably degraded. It&#x27;s not game over, but you&#x27;ll have to work harder to overcome it.2</p><p>The other, more important concern, is that the more you&#x27;ve drawn, the more you&#x27;re attached to the existing sections of the drawing. Say you&#x27;ve drawn an arm bent awkwardly. You can erase it and fix it. But the arm doesn&#x27;t exist in isolation. It connects to the shoulder, which connects to the torso and neck. Fix the arm, and you have fix the all those other things.</p><p>You probably won&#x27;t <em>want</em> to fix them all, because it will feel too sad for you to have to erase large sections of your drawing. And even if you DO fix them all, the result won&#x27;t be a fluid, graceful image that captures the motion and interconnected muscles of your subject - it&#x27;ll be a hodge podge of Frankensteinian bodyparts, awkwardly sewn together.</p><p>(There&#x27;s also anchoring involved: once a line exists, you&#x27;ll have trouble evaluating new lines on their own merits, instead of how they compare to the existing ones.)</p><p>Several times over the past year, I&#x27;ve worked on a drawing of a person for 5-10 minutes. By the 1 minute mark, I know something&#x27;s off about the drawing. By the 2 minute mark, I&#x27;ve started erasing and reworking things. I have a nagging sense that I&#x27;ve done this before, and that the next 8 minutes will involve lots of erasing, and a drawing that still isn&#x27;t very good.</p><p>10 minutes, and lots of erasings later, I have a disappointing drawing.</p><p>The nagging sensation that this isn&#x27;t going to work well... that&#x27;s what the sunk cost fallacy feels like. You&#x27;ve put in a few minutes of work (sometimes much longer - you can go down this path for hours). Starting over would feel like defeat, like your previous work was a waste.</p><p>The correct action is to start over anyway. It&#x27;s true when you&#x27;ve only been working for a minute and have just noticed the feeling. It&#x27;s still true 10 minutes later. It&#x27;s still true if you&#x27;ve spent 4 hours painstakingly erasing and doublechecking against reality, adding lots of nuanced shading. It&#x27;s an oddly near-universal truism, not just in drawing but in many projects, that the thing you just spent 4 hours working on, which would take another 4 hours to finish, could be done in 10-30 minutes if you started over.3 </p><p>There are reasons to spend 4 hours on a drawing. Those reasons will not be relevant to you in the near future. All the most important elements of a drawing should take you no more than a few minutes. After that, you&#x27;re getting distracted by details, which might make the drawing more interesting, but won&#x27;t actually fix the existing problems with it.</p><p>Most importantly, it won&#x27;t help you learn to avoid those problems in the first place.</p><h2>Fast, Confident Lines</h2><p>So, you need to capture the most important elements of a drawing, quickly:</p><ul><li>You need to capture how different body parts connect together, as a seamless whole. </li><li>You need to establish a good composition, so that the details you work in later aren&#x27;t just reinforcing a bad design.</li><li>I haven&#x27;t elaborated on it yet, but you&#x27;re going to want an energetic, interesting drawing, which is simply hard to compose without working quickly.</li></ul><p>To do all this, you need to develop a particular kind of hand-eye coordination, which is probably different than what you&#x27;re used to. <em>You need to be able to draw large sections of the body, using a single line. </em>That line can change direction. But it needs to be done in one fluid motion. </p><p><a href=\"http://www.youtube.com/watch?v=zjI0V4guSFo&feature=related\">This artist demonstrates what I mean by &quot;confident lines.&quot;</a> She blocks out large chunks of body with long curves, without second guessing. She doesn&#x27;t bother drawing the arms or feet, but she does end those lines AFTER they&#x27;ve curved in a new direction, so if/when she continues them she&#x27;s set herself to continue them gracefully.</p><h1>No Erasing</h1><p>You need to work quickly, and fluidly. Stopping to erase will interrupt your flow. So you need to learn to draw without erasing. There are two ways of doing that:</p><p><em>1) Don&#x27;t make mistakes ever. </em></p><p>This actually is not as unreasonable as it sounds. In the first 30 seconds, identify the most important lines of the drawing, and draw them. It&#x27;s what the artist in the previous video did. Obviously, this is... essentially impossible for a beginner. You&#x27;re going to make mistakes. But what you CAN do is draw boldly, confidently, let the mistakes happen, and then rather than trying to fix them, move on to the next drawing after 30 seconds. Over time you&#x27;ll get better.</p><p>I haven&#x27;t watched new students try to learn with JUST this philosophy, and I have no idea how long it&#x27;d take to develop from scratch. But if you DO have some previous drawing experience, I think this may be a good approach, at least to try out. If your goal is to produce something like the woman in the video, drawing 5 drawings in 30 seconds with simple, bold lines will probably produce at least one drawing that&#x27;s better than the one you&#x27;d do in two and a half minutes.</p><p><em>2) Be okay with your drawing being messy.</em></p><p>This is what I actually recommend for beginners.</p><p>A big hurdle young artists have, when they&#x27;re transitioning onto the path of a &quot;professional&quot;, is they feel that &quot;messy is bad.&quot; They&#x27;re drawing like this:</p><p><em>[2017 Note: this used to be a link to a perfectly bad high school Dragonball Z fan-art, which was really useful for highlighting the sort of error modes I was pointing at, but a) it was sort of mean to use it, b) in 2017 the link is apparently dead, c) I was unable to find another example that illustrated the exact qualities I want to point to]</em></p><p><em>[Followup: Someone volunteered this old high school art, which doesn&#x27;t <strong>quite </strong>hit on the same set of issues but works well enough to illustrate the basic concept]</em></p><span><figure><img src=\"http://secularsolstice.com/wp-content/uploads/2017/11/ballerina.jpg\" class=\"draft-image \" style=\"\" /></figure></span><p>When they should be drawing like this:</p><p></p><span><figure><img src=\"http://i.vimeocdn.com/video/364878640_1280x720.jpg\" class=\"draft-image center\" style=\"\" /></figure></span><p></p><p>They&#x27;re looking at the former, and seeing it as a fairly clean drawing that just needs to be fixed a little. </p><p>A college professor looks at Example A, cringes, and thinks &quot;man this person is going to need to systematically broken down over the course of two semesters until they&#x27;re ready to begin learning, and it&#x27;s going to be painful for the both of us.&quot; They look at Example B and think &quot;This person knows exactly what to do already, they just need to do it for another 10,000 hours.&quot;</p><p>The problem with Example A is that the artist is copying superficial elements of a particular style, without understanding the underlying principles that make good a good figure drawing. Example B has lots of overlapping lines, and vague messy shapes. But the figures there communicate a good understanding of anatomy, a grasp of weight, decent composition. </p><p>As an aspiring artist, don&#x27;t ask if your drawing is better than A. Ask if it&#x27;s at least as good as Example B. If you want to draw truly good Manga art, you must first learn things OTHER than the superficial characteristics of Manga. And while it may look like a mess at first, as you learn to draw that way, you&#x27;ll understand that there&#x27;s actually a lot of information there that Example A has missed.</p><p>(No offense to those of you out there currently drawing Example A. I&#x27;ve been there. It&#x27;s a rite of passage. In particular, no offense to the blog I took Example A from. The blogger identifies it as one of their old, middle school works and gives other examples that show a lot of improvement. I tried to draw my own version of Example A, but it&#x27;s actually really hard for me to draw that particular way now, and I can&#x27;t find any older examples).</p><h1>Begin Light, Emphasize with Darks</h1><p>One important part of technical skill is being able to draw lines in the location you want them. Another important part is being able to adjust the lightness or darkness of those lines (as well as thick and thin-ness)</p><p>Your drawings are going to be messy. But you want a particular kind of mess. If you look at the right-most figure in Example B, you&#x27;ll see that all the lines are the same thickness. This is okay - the artist has enough skill that they&#x27;re all approximately correct, and the ones that are off have been repurposed - instead of being pure mess, they end up representing the volume of the figure.</p><p>The cluster of scribbles in the face suggest its roundness, and having a bunch of them devalues the importance of each individual line, so that even if none of them end *perfect*, your brain doesn&#x27;t really care - it sees that they&#x27;re all sort of fuzzy and accepts the average position in a sort of &quot;Wisdom of Crowds&quot; way.</p><p>It&#x27;s okay that all the lines are the same thickness, because none of them are *completely* off. There&#x27;s no giant leg that accidentally stuck out way too far and ruined the image. If it had, it&#x27;d be really hard to repair the drawing. Especially since you&#x27;re trying to work quickly, without erasing.</p><p>You&#x27;re going to be making significant mistakes, and you won&#x27;t want to start over <em>every</em> single time.</p><p>The solution is to do your early work <em>lightly, </em>and then, once you&#x27;ve identified the parts you like, use dark lines to emphasize those areas. <a href=\"http://www.youtube.com/watch?v=eRTqpJMs98E\">This tutorial demonstrates how to draw like this.</a> Notice that within 30 seconds, he&#x27;s established a framework, without worrying about making any &quot;clean&quot; shapes. Over the course of 2 minutes, he builds on that framework, filling in the mass of his subject matter, and eventually adding much darker lines to emphasize the final shape.</p><p>To do this, you need to be able to adjust the &quot;value&quot; of your lines (how light or dark they are). This takes some practice. A good exercise is to create a sequence of value-swatches like this: </p><p></p><p>Begin with the swatches on the far sides - make the darkest dark and lightest light you can possibly do. Then try and fill in the rest, gradually darkening.</p><p>Begin your drawing with something close to the second-lightest swatch. For now, try not to get much darker - it&#x27;s easy to accidentally get too dark too quickly, and then are your lines are uniformly black and you can&#x27;t emphasize the parts you want.</p><p></p><h1>So... Recap:</h1><p>These are only some of the skills you&#x27;ll need to acquire, but they&#x27;re the most important in the immediate future. So in summary:</p><ul><li>Hold the pencil with three fingers, not too tightly.</li><li>Don&#x27;t be afraid to start over.</li><li>Work quickly, without stopping to erase.</li><li>Draw strong, confident lines.</li><li>Be okay with your drawing being messy - let extra lines help define the form.</li><li>Start with light lines, make your mistakes, then emphasize the good parts with dark lines.</li></ul><p>[0] There&#x27;s something akin to anchoring bias here as well - once part of the drawing exists, even if you&#x27;re trying to completely ignore it, it&#x27;ll be warping your perception of what&#x27;s actually going on.</p><p>[1] I promise I&#x27;ll explain what I mean by &quot;energy&quot; soon.</p><p>[2] Each drawing tool has separate rules that need mastering. This includes pencils, charcoal, fountain pens... and computer tablets. I&#x27;ll be specifically talking about the pencil here. Information here WILL still generalize to tablets, but I&#x27;ll warn you that you&#x27;ll experience some awkwardness transitioning to or from them.</p><p>[3] <em>Why</em> starting over saves time is a complex question. Part of it has to do with you already having studied the problem. Part of it is that a fresh canvas frees you from bias towards your old solutions. Part of it is that your existing work is suboptimal, and you&#x27;d need to spend extra time fixing it.</p><p><em>[Final 2017 note: We are now at the abrupt ending I warned you about. Sorry!]</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 2, "KDpqtN3MxHSmD4vcB": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bTDrKA8ySiCYWrWGE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 37, "extendedScore": null, "score": 9.3e-05, "legacy": true, "legacyId": "11177", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "WPgA9x5ZvKu9oYvgB", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "", "canonicalPrevPostSlug": "drawing-less-wrong-observing-reality", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>The ability to <a href=\"https://www.lesserwrong.com/lw/8i1/drawing_less_wrong_observing_reality/\">observe</a> is probably at least 2/3rds of what separates non-artists from amateur artists. But those 2/3rds are near-useless without the ability to move your pencil the way your eyes want to it to go. And once you've transitioned into an amateur artist, around 9,000 hours of honing your technical skill is what separates you from a professional.</p><p>\"Technical Skill\" is a broad term - kind of a catch all for all term for various motor skills you'll need to develop, background knowledge about how particular types of lines and shapes are perceived by most humans, and how to combine those skills and knowledge to produce particular effects with your drawing.</p><p>I can't even begin to cover all of it, and most of it isn't really appropriate for Less Wrong. But I will talk about some key motor skills that tie in with the next article, and a significant bias that plays a role in them.</p><p>This article was challenging to write - distilling a kinesthetic process into written words is difficult. This article will not be a substitute for having a teacher and a model, nor will it tell you exactly what exercises to do. But it will try to lay down some concepts that I'll further expound on later.</p><h1 id=\"Holding_the_Pencil\">Holding the Pencil</h1><p>For many of you this may seem basic, but at least one reader commented that they went for years without understanding this, and because it seemed basic, nobody ever noticed it and corrected them.</p><p>Holding a pencil should look approximately like this:</p><p></p><span><figure><img src=\"http://www.publicdomainpictures.net/pictures/140000/velka/writing-hand-1443450678bLv.jpg\" class=\"draft-image \" style=\"\"></figure></span><p></p><p>But its a bit more complicated that that. Many skilled artists hold the pencil in different ways. The biggest things to keep in mind are:</p><ol><li>Don't grip the pencil too tightly. You'll hurt yourself, and it won't help.</li><li>If you hold the pencil closer to the tip, you will have more control over it, which is useful for fine details.</li><li>If you hold the pencil towards the back of the pencil, you'll have greater range of motion, and allows you to quickly draw larger lines in a single stroke. It also will be looser, which can feel hard to control but can also produce certain line qualities you may want.</li></ol><p>(I personally tend to hold my pencil similar to the image above, but closer to the middle of the pencil)</p><p>A few examples of a pencil grip in motion:</p><p><a href=\"http://vimeo.com/4987983\">This man's grip is similar to mine</a>, although the technique he describes isn't something I think you should be worrying about just yet. (I'll be talking about Darrel Tank's website later on - I think he has good tutorials on technical skills, but does not prioritize them based on their low-hanging-fruit-ness.)</p><p><a href=\"http://www.youtube.com/watch?v=sHLxbLUDhlU\">This cartoonist switches grips a few times</a>, demonstrating how they can be useful at different stages of drawing. This video is particularly interesting because his \"loose\" grip is actually closer to the front, which I haven't seen often.</p><h2 id=\"Slow_Drawing_and_the_Sunk_Cost_Fallacy\">Slow Drawing and the Sunk Cost Fallacy</h2><p>I've spoken a few times about \"slow, small, precise lines,\" and implied that they are a terrible idea. They often are. You'll be drawing slowly during some initial exercises that develop observational skills. But as soon as possible, you'll want to start developing a form of hand-eye coordination that involves moving quickly using long lines. Until you achieve that, the small, meticulous lines will probably have a choppy quality, and certain compositions will be harder to capture.</p><p>Much of the \"energy\" of your drawing1, and the quality of the composition, will be established within the first one to two minutes. This isn't a hard and fast rule, but holds true most of the time. Yes, you can erase, and rework things. With pure observation, infinite time and brute-force-reworking, you can craft a drawing that perfectly captures reality. But every time you erase and fix a drawing, two things happen:</p><p>One is that the paper smudges, tears slightly and otherwise degrades. This might be okay if you're working on a computer tablet, but so long as you're practicing with a real pencil, it's an issue. After erasing 5-10 times, your drawing will have noticeably degraded. It's not game over, but you'll have to work harder to overcome it.2</p><p>The other, more important concern, is that the more you've drawn, the more you're attached to the existing sections of the drawing. Say you've drawn an arm bent awkwardly. You can erase it and fix it. But the arm doesn't exist in isolation. It connects to the shoulder, which connects to the torso and neck. Fix the arm, and you have fix the all those other things.</p><p>You probably won't <em>want</em> to fix them all, because it will feel too sad for you to have to erase large sections of your drawing. And even if you DO fix them all, the result won't be a fluid, graceful image that captures the motion and interconnected muscles of your subject - it'll be a hodge podge of Frankensteinian bodyparts, awkwardly sewn together.</p><p>(There's also anchoring involved: once a line exists, you'll have trouble evaluating new lines on their own merits, instead of how they compare to the existing ones.)</p><p>Several times over the past year, I've worked on a drawing of a person for 5-10 minutes. By the 1 minute mark, I know something's off about the drawing. By the 2 minute mark, I've started erasing and reworking things. I have a nagging sense that I've done this before, and that the next 8 minutes will involve lots of erasing, and a drawing that still isn't very good.</p><p>10 minutes, and lots of erasings later, I have a disappointing drawing.</p><p>The nagging sensation that this isn't going to work well... that's what the sunk cost fallacy feels like. You've put in a few minutes of work (sometimes much longer - you can go down this path for hours). Starting over would feel like defeat, like your previous work was a waste.</p><p>The correct action is to start over anyway. It's true when you've only been working for a minute and have just noticed the feeling. It's still true 10 minutes later. It's still true if you've spent 4 hours painstakingly erasing and doublechecking against reality, adding lots of nuanced shading. It's an oddly near-universal truism, not just in drawing but in many projects, that the thing you just spent 4 hours working on, which would take another 4 hours to finish, could be done in 10-30 minutes if you started over.3 </p><p>There are reasons to spend 4 hours on a drawing. Those reasons will not be relevant to you in the near future. All the most important elements of a drawing should take you no more than a few minutes. After that, you're getting distracted by details, which might make the drawing more interesting, but won't actually fix the existing problems with it.</p><p>Most importantly, it won't help you learn to avoid those problems in the first place.</p><h2 id=\"Fast__Confident_Lines\">Fast, Confident Lines</h2><p>So, you need to capture the most important elements of a drawing, quickly:</p><ul><li>You need to capture how different body parts connect together, as a seamless whole. </li><li>You need to establish a good composition, so that the details you work in later aren't just reinforcing a bad design.</li><li>I haven't elaborated on it yet, but you're going to want an energetic, interesting drawing, which is simply hard to compose without working quickly.</li></ul><p>To do all this, you need to develop a particular kind of hand-eye coordination, which is probably different than what you're used to. <em>You need to be able to draw large sections of the body, using a single line. </em>That line can change direction. But it needs to be done in one fluid motion. </p><p><a href=\"http://www.youtube.com/watch?v=zjI0V4guSFo&amp;feature=related\">This artist demonstrates what I mean by \"confident lines.\"</a> She blocks out large chunks of body with long curves, without second guessing. She doesn't bother drawing the arms or feet, but she does end those lines AFTER they've curved in a new direction, so if/when she continues them she's set herself to continue them gracefully.</p><h1 id=\"No_Erasing\">No Erasing</h1><p>You need to work quickly, and fluidly. Stopping to erase will interrupt your flow. So you need to learn to draw without erasing. There are two ways of doing that:</p><p><em>1) Don't make mistakes ever. </em></p><p>This actually is not as unreasonable as it sounds. In the first 30 seconds, identify the most important lines of the drawing, and draw them. It's what the artist in the previous video did. Obviously, this is... essentially impossible for a beginner. You're going to make mistakes. But what you CAN do is draw boldly, confidently, let the mistakes happen, and then rather than trying to fix them, move on to the next drawing after 30 seconds. Over time you'll get better.</p><p>I haven't watched new students try to learn with JUST this philosophy, and I have no idea how long it'd take to develop from scratch. But if you DO have some previous drawing experience, I think this may be a good approach, at least to try out. If your goal is to produce something like the woman in the video, drawing 5 drawings in 30 seconds with simple, bold lines will probably produce at least one drawing that's better than the one you'd do in two and a half minutes.</p><p><em>2) Be okay with your drawing being messy.</em></p><p>This is what I actually recommend for beginners.</p><p>A big hurdle young artists have, when they're transitioning onto the path of a \"professional\", is they feel that \"messy is bad.\" They're drawing like this:</p><p><em>[2017 Note: this used to be a link to a perfectly bad high school Dragonball Z fan-art, which was really useful for highlighting the sort of error modes I was pointing at, but a) it was sort of mean to use it, b) in 2017 the link is apparently dead, c) I was unable to find another example that illustrated the exact qualities I want to point to]</em></p><p><em>[Followup: Someone volunteered this old high school art, which doesn't <strong>quite </strong>hit on the same set of issues but works well enough to illustrate the basic concept]</em></p><span><figure><img src=\"http://secularsolstice.com/wp-content/uploads/2017/11/ballerina.jpg\" class=\"draft-image \" style=\"\"></figure></span><p>When they should be drawing like this:</p><p></p><span><figure><img src=\"http://i.vimeocdn.com/video/364878640_1280x720.jpg\" class=\"draft-image center\" style=\"\"></figure></span><p></p><p>They're looking at the former, and seeing it as a fairly clean drawing that just needs to be fixed a little. </p><p>A college professor looks at Example A, cringes, and thinks \"man this person is going to need to systematically broken down over the course of two semesters until they're ready to begin learning, and it's going to be painful for the both of us.\" They look at Example B and think \"This person knows exactly what to do already, they just need to do it for another 10,000 hours.\"</p><p>The problem with Example A is that the artist is copying superficial elements of a particular style, without understanding the underlying principles that make good a good figure drawing. Example B has lots of overlapping lines, and vague messy shapes. But the figures there communicate a good understanding of anatomy, a grasp of weight, decent composition. </p><p>As an aspiring artist, don't ask if your drawing is better than A. Ask if it's at least as good as Example B. If you want to draw truly good Manga art, you must first learn things OTHER than the superficial characteristics of Manga. And while it may look like a mess at first, as you learn to draw that way, you'll understand that there's actually a lot of information there that Example A has missed.</p><p>(No offense to those of you out there currently drawing Example A. I've been there. It's a rite of passage. In particular, no offense to the blog I took Example A from. The blogger identifies it as one of their old, middle school works and gives other examples that show a lot of improvement. I tried to draw my own version of Example A, but it's actually really hard for me to draw that particular way now, and I can't find any older examples).</p><h1 id=\"Begin_Light__Emphasize_with_Darks\">Begin Light, Emphasize with Darks</h1><p>One important part of technical skill is being able to draw lines in the location you want them. Another important part is being able to adjust the lightness or darkness of those lines (as well as thick and thin-ness)</p><p>Your drawings are going to be messy. But you want a particular kind of mess. If you look at the right-most figure in Example B, you'll see that all the lines are the same thickness. This is okay - the artist has enough skill that they're all approximately correct, and the ones that are off have been repurposed - instead of being pure mess, they end up representing the volume of the figure.</p><p>The cluster of scribbles in the face suggest its roundness, and having a bunch of them devalues the importance of each individual line, so that even if none of them end *perfect*, your brain doesn't really care - it sees that they're all sort of fuzzy and accepts the average position in a sort of \"Wisdom of Crowds\" way.</p><p>It's okay that all the lines are the same thickness, because none of them are *completely* off. There's no giant leg that accidentally stuck out way too far and ruined the image. If it had, it'd be really hard to repair the drawing. Especially since you're trying to work quickly, without erasing.</p><p>You're going to be making significant mistakes, and you won't want to start over <em>every</em> single time.</p><p>The solution is to do your early work <em>lightly, </em>and then, once you've identified the parts you like, use dark lines to emphasize those areas. <a href=\"http://www.youtube.com/watch?v=eRTqpJMs98E\">This tutorial demonstrates how to draw like this.</a> Notice that within 30 seconds, he's established a framework, without worrying about making any \"clean\" shapes. Over the course of 2 minutes, he builds on that framework, filling in the mass of his subject matter, and eventually adding much darker lines to emphasize the final shape.</p><p>To do this, you need to be able to adjust the \"value\" of your lines (how light or dark they are). This takes some practice. A good exercise is to create a sequence of value-swatches like this: </p><p></p><p>Begin with the swatches on the far sides - make the darkest dark and lightest light you can possibly do. Then try and fill in the rest, gradually darkening.</p><p>Begin your drawing with something close to the second-lightest swatch. For now, try not to get much darker - it's easy to accidentally get too dark too quickly, and then are your lines are uniformly black and you can't emphasize the parts you want.</p><p></p><h1 id=\"So____Recap_\">So... Recap:</h1><p>These are only some of the skills you'll need to acquire, but they're the most important in the immediate future. So in summary:</p><ul><li>Hold the pencil with three fingers, not too tightly.</li><li>Don't be afraid to start over.</li><li>Work quickly, without stopping to erase.</li><li>Draw strong, confident lines.</li><li>Be okay with your drawing being messy - let extra lines help define the form.</li><li>Start with light lines, make your mistakes, then emphasize the good parts with dark lines.</li></ul><p>[0] There's something akin to anchoring bias here as well - once part of the drawing exists, even if you're trying to completely ignore it, it'll be warping your perception of what's actually going on.</p><p>[1] I promise I'll explain what I mean by \"energy\" soon.</p><p>[2] Each drawing tool has separate rules that need mastering. This includes pencils, charcoal, fountain pens... and computer tablets. I'll be specifically talking about the pencil here. Information here WILL still generalize to tablets, but I'll warn you that you'll experience some awkwardness transitioning to or from them.</p><p>[3] <em>Why</em> starting over saves time is a complex question. Part of it has to do with you already having studied the problem. Part of it is that a fresh canvas frees you from bias towards your old solutions. Part of it is that your existing work is suboptimal, and you'd need to spend extra time fixing it.</p><p><em>[Final 2017 note: We are now at the abrupt ending I warned you about. Sorry!]</em></p>", "sections": [{"title": "Holding the Pencil", "anchor": "Holding_the_Pencil", "level": 1}, {"title": "Slow Drawing and the Sunk Cost Fallacy", "anchor": "Slow_Drawing_and_the_Sunk_Cost_Fallacy", "level": 2}, {"title": "Fast, Confident Lines", "anchor": "Fast__Confident_Lines", "level": 2}, {"title": "No Erasing", "anchor": "No_Erasing", "level": 1}, {"title": "Begin Light, Emphasize with Darks", "anchor": "Begin_Light__Emphasize_with_Darks", "level": 1}, {"title": "So... Recap:", "anchor": "So____Recap_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "36 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 36, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3ve2pEZS2dBmxbZS2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-05T06:14:25.171Z", "modifiedAt": null, "url": null, "title": "Richard Carrier on the Singularity", "slug": "richard-carrier-on-the-singularity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:51.697Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "peter_hurford", "createdAt": "2011-07-19T19:05:31.793Z", "isAdmin": false, "displayName": "Peter Wildeford"}, "userId": "FMsXugZ8aB5d8nHsm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7gNLnojtuLqZx6uDF/richard-carrier-on-the-singularity", "pageUrlRelative": "/posts/7gNLnojtuLqZx6uDF/richard-carrier-on-the-singularity", "linkUrl": "https://www.lesswrong.com/posts/7gNLnojtuLqZx6uDF/richard-carrier-on-the-singularity", "postedAtFormatted": "Monday, December 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Richard%20Carrier%20on%20the%20Singularity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARichard%20Carrier%20on%20the%20Singularity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7gNLnojtuLqZx6uDF%2Frichard-carrier-on-the-singularity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Richard%20Carrier%20on%20the%20Singularity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7gNLnojtuLqZx6uDF%2Frichard-carrier-on-the-singularity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7gNLnojtuLqZx6uDF%2Frichard-carrier-on-the-singularity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 474, "htmlBody": "<p>Recently I stumbled upon Richard Carrier's essay <a href=\"http://richardcarrier.blogspot.com/2009/06/are-we-doomed.html\">\"Are We Doomed\"</a>&nbsp;(June 5, 2009), when asked to comment about the Singularity, said the following:</p>\n<p><span style=\"font-family: 'times new roman'; color: #333333;\"></span></p>\n<blockquote>\n<p><span style=\"font-family: 'times new roman'; color: #333333;\">I agree the Singularity stuff is often muddled nonsense. I just don't know many advocates of it. Those who do advocate it are often unrealistic about the physical limits of technology, and particularly the nature of IQ. They base their \"predictions\" on two implausible assumptions: that advancement of IQ is potentially unlimited (I am fairly certain it will be bounded by complexity theory: at a certain point it just won't be possible to think any faster or sounder or more creatively) and that high IQ is predictive of accelerating technological advancement. History proves otherwise: even people ten times smarter than people like me produce no more extensive or revolutionary technological or scientific output, much less invent more technologies or make more discoveries--in fact, by some accounts they often produce less in those regards than people of more modest (though still high) intelligence.</span></p>\n<p><span style=\"font-family: 'times new roman'; color: #333333;\">However, Singularity fans are right about two things: machines will outthink humans (and be designing better versions of themselves than we ever could) within fifty to a hundred years (if advocates predict this will happen sooner, then they are being unrealistic), and the pace of technological advancement will accelerate. However, this is already accounted for by existing models of technological advancement, e.g. Moore's Law holds that computers double in processing power every three years, Haik's Law holds that LED's double in efficiency every three years, and so on (similar laws probably hold for other technologies, these are just two that have been proven so far). Thus, that technological progress accelerates is already predicted. The Singularity simply describes one way this pace will be maintained: by the recruitment of AI.</span></p>\n<p><span style=\"font-family: 'times new roman'; color: #333333;\">It therefore doesn't predict anything remarkable, and certainly doesn't deserve such a pretentious name. Because there will be a limit, an end point, and it won't resemble a Singularity: there is a physical limit on how fast thoughts can be thunk and how fast manufacturing can occur, quantum mechanical limits that can never be overcome, by any technology. Once we reach that point, the pace of technological advancement will cease to be geometric and will become linear, or in some cases stop altogether. For instance, once we reach the quantum mechanical limit of computational speed and component size, no further advances will be possible in terms of Moore's Law (even Kurzweil's theory that it will continue in the form of expansion in size ignores the fact that we can already do this now, yet we don't see moon-sized computers anywhere--a fact that reveals an importantly overlooked reality: what things cost).</span></p>\n<p><span style=\"font-family: 'times new roman'; color: #333333;\">Ironically, the same has been discovered about actual singularities: they, too, don't really exist, and for the same quantum mechanical reasons (see my discussion here).</span></p>\n</blockquote>\n<p><span style=\"font-family: 'times new roman'; color: #333333;\">What do you think?</span></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7gNLnojtuLqZx6uDF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 8.100581526182545e-07, "legacy": true, "legacyId": "11283", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 51, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-05T08:38:48.348Z", "modifiedAt": null, "url": null, "title": "How is your mind different from everyone else's?", "slug": "how-is-your-mind-different-from-everyone-else-s", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:24.540Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZcFE6FgSQxFbCRnpa/how-is-your-mind-different-from-everyone-else-s", "pageUrlRelative": "/posts/ZcFE6FgSQxFbCRnpa/how-is-your-mind-different-from-everyone-else-s", "linkUrl": "https://www.lesswrong.com/posts/ZcFE6FgSQxFbCRnpa/how-is-your-mind-different-from-everyone-else-s", "postedAtFormatted": "Monday, December 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20is%20your%20mind%20different%20from%20everyone%20else's%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20is%20your%20mind%20different%20from%20everyone%20else's%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZcFE6FgSQxFbCRnpa%2Fhow-is-your-mind-different-from-everyone-else-s%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20is%20your%20mind%20different%20from%20everyone%20else's%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZcFE6FgSQxFbCRnpa%2Fhow-is-your-mind-different-from-everyone-else-s", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZcFE6FgSQxFbCRnpa%2Fhow-is-your-mind-different-from-everyone-else-s", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 322, "htmlBody": "<p>Partially to help reduce <a href=\"/lw/dr/generalizing_from_one_example/\">the typical mind fallacy</a> and partially because I'm curious, I'm thinking about writing either an essay or a book with plenty of examples about ways by which human minds differ. From commonly known and ordinary, like differences in sexual orientation, to the rare and seemingly impossible, like motion blindness.<br /><br />To do this, I need to start collecting examples. In what ways does your mind differ from what you think is the norm for most people?<br /><br />I'm particularly interested in differences - small or large - that you didn't realize for a long time, automatically assuming that everyone was like you in that regard. It can even be something as trivial as always having conceptualized the passing of years as a visual timeline, and then finding out that not everyone does so. I'm also interested in links to blog posts where people talk about their own mental peculiarities, even if you didn't write them yourself. Also books and academic articles that you might think could be relevant.<br /><br />Some of the content that I'm thinking about including are cultural differences in various things as recounted in <a href=\"http://www2.psych.ubc.ca/~henrich/pdfs/WeirdPeople.pdf\">the WEIRD article</a>, differences in sexual and romantic orientation (such as mono/poly), differences in the ability to recover from setbacks, extroversion vs. introversion in terms of gaining/losing energy from social activity, differences in visualization ability, various cognitive differences ranging from autism to synesthesia to an inability to hear music in particular, differences in moral intuitions, differences in the way people think (visual vs. verbal vs. conceptual vs. something that I'm not aware of yet), differences in thinking styles (social/rational, reflectivity vs. impulsiveness) and various odd brain damage cases.</p>\n<p>If you find this project interesting, consider spreading the link to this post or resharing my <a href=\"https://plus.google.com/106597887376283858570/posts/h5B8e4CQjYa\">Google Plus update</a> about it. Also, if you don't want to reply in public, feel free to send me a private message.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GPhMyXoaHBLyzibxB": 2, "3uE2pXvbcnS9nnZRE": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZcFE6FgSQxFbCRnpa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 40, "extendedScore": null, "score": 8.101104045192446e-07, "legacy": true, "legacyId": "11285", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 267, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["baTWMegR42PAsH9qJ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-05T10:49:02.810Z", "modifiedAt": null, "url": null, "title": "2011 Survey Results", "slug": "2011-survey-results", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:29.976Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Yvain", "createdAt": "2009-02-28T15:53:46.032Z", "isAdmin": false, "displayName": "Scott Alexander"}, "userId": "XgYW5s8njaYrtyP7q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HAEPbGaMygJq8L59k/2011-survey-results", "pageUrlRelative": "/posts/HAEPbGaMygJq8L59k/2011-survey-results", "linkUrl": "https://www.lesswrong.com/posts/HAEPbGaMygJq8L59k/2011-survey-results", "postedAtFormatted": "Monday, December 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%202011%20Survey%20Results&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A2011%20Survey%20Results%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHAEPbGaMygJq8L59k%2F2011-survey-results%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=2011%20Survey%20Results%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHAEPbGaMygJq8L59k%2F2011-survey-results", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHAEPbGaMygJq8L59k%2F2011-survey-results", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2318, "htmlBody": "<p>A big thank you to the 1090 people who took the second <a href=\"/lw/89a/2011_less_wrong_census_survey/\">Less Wrong Census/Survey</a>.<br /><br />Does this mean there are 1090 people who post on Less Wrong? Not necessarily. 165 people said they had zero karma, and 406 people skipped the karma question - I assume a good number of the skippers were people with zero karma or without accounts. So we can only prove that 519 people post on Less Wrong. Which is still a lot of people.<br /><br />I apologize for failing to ask who had or did not have an LW account. Because there are a number of these failures, I'm putting them all in a comment to this post so they don't clutter the survey results. Please talk about changes you want for next year's survey there.<br /><br />Of our 1090 respondents, 972 (89%) were male, 92 (8.4%) female, 7 (.6%) transexual, and 19 gave various other answers or objected to the question. As abysmally male-dominated as these results are, the percent of women has tripled since <a href=\"/lw/fk/survey_results/\">the last survey</a> in mid-2009.<a id=\"more\"></a></p>\n<p>We're also a little more diverse than we were in 2009; our percent non-whites has risen from 6% to just below 10%. Along with 944 whites (86%) we include 38 Hispanics (3.5%), 31 East Asians (2.8%), 26 Indian Asians (2.4%) and 4 blacks (.4%).</p>\n<p>Age ranged from a supposed minimum of 1 (they start making rationalists early these days?) to a more plausible minimum of 14, to a maximum of 77. The mean age was 27.18 years. Quartiles (25%, 50%, 75%) were 21, 25, and 30. 90% of us are under 38, 95% of us are under 45, but there are still eleven Less Wrongers over the age of 60. The average Less Wronger has aged about one week since spring 2009 - so clearly all those anti-agathics we're taking are working!<br /><br />In order of frequency, we include 366 computer scientists (32.6%), 174 people in the hard sciences (16%) 80 people in finance (7.3%), 63 people in the social sciences (5.8%), 43 people involved in AI (3.9%), 39 philosophers (3.6%), 15 mathematicians (1.5%), 14 statisticians (1.3%), 15 people involved in law (1.5%) and 5 people in medicine (.5%).<br /><br />48 of us (4.4%) teach in academia, 470 (43.1%) are students, 417 (38.3%) do for-profit work, 34 (3.1%) do non-profit work, 41 (3.8%) work for the government, and 72 (6.6%) are unemployed.<br /><br />418 people (38.3%) have yet to receive any degrees, 400 (36.7%) have a Bachelor's or equivalent, 175 (16.1%) have a Master's or equivalent, 65 people (6%) have a Ph.D, and 19 people (1.7%) have a professional degree such as an MD or JD. <br /><br />345 people (31.7%) are single and looking, 250 (22.9%) are single but not looking, 286 (26.2%) are in a relationship, and 201 (18.4%) are married. There are striking differences across men and women: women are more likely to be in a relationship and less likely to be single and looking (33% men vs. 19% women). All of these numbers look a lot like the ones from 2009.<br /><br />27 people (2.5%) are asexual, 119 (10.9%) are bisexual, 24 (2.2%) are homosexual, and 902 (82.8%) are heterosexual. <br /><br />625 people (57.3%) described themselves as monogamous, 145 (13.3%) as polyamorous, and 298 (27.3%) didn't really know. These numbers were similar between men and women.<br /><br />The most popular political view, at least according to the much-maligned categories on the survey, was liberalism, with 376 adherents and 34.5% of the vote. Libertarianism followed at 352 (32.3%), then socialism at 290 (26.6%), conservativism at 30 (2.8%) and communism at 5 (.5%).</p>\n<p>680 people (62.4%) were consequentialist, 152 (13.9%) virtue ethicist, 49 (4.5%) deontologist, and 145 (13.3%) did not believe in morality.</p>\n<p>801 people (73.5%) were atheist and not spiritual, 108 (9.9%) were atheist and spiritual, 97 (8.9%) were agnostic, 30 (2.8%) were deist or pantheist or something along those lines, and 39 people (3.5%) described themselves as theists (20 committed plus 19 lukewarm)<br /><br />425 people (38.1%) grew up in some flavor of nontheist family, compared to 297 (27.2%) in committed theist families and 356 in lukewarm theist families (32.7%). Common family religious backgrounds included Protestantism with 451 people (41.4%), Catholicism with 289 (26.5%) Jews with 102 (9.4%), Hindus with 20 (1.8%), Mormons with 17 (1.6%) and traditional Chinese religion with 13 (1.2%)<br /><br />There was much derision on the last survey over the average IQ supposedly being 146. Clearly Less Wrong has been dumbed down since then, since the average IQ has fallen all the way down to 140. Numbers ranged from 110 all the way up to 204 (for reference, Marilyn vos Savant, who holds the Guinness World Record for highest adult IQ ever recorded, has an IQ of 185).<br /><br />89 people (8.2%) have never looked at the Sequences; a further 234 (32.5%) have only given them a quick glance. 170 people have read about 25% of the sequences, 169 (15.5%) about 50%, 167 (15.3%) about 75%, and 253 people (23.2%) said they've read almost all of them. This last number is actually lower than the 302 people who have been here since the Overcoming Bias days when the Sequences were still being written (27.7% of us).<br /><br />The other 72.3% of people who had to find Less Wrong the hard way. 121 people (11.1%) were referred by a friend, 259 people (23.8%) were referred by blogs, 196 people (18%) were referred by Harry Potter and the Methods of Rationality, 96 people (8.8%) were referred by a search engine, and only one person (.1%) was referred by a class in school.<br /><br />Of the 259 people referred by blogs, 134 told me which blog referred them. There was a very long tail here, with most blogs only referring one or two people, but the overwhelming winner was Common Sense Atheism, which is responsible for 18 current Less Wrong readers. Other important blogs and sites include Hacker News (11 people), Marginal Revolution (6 people), TV Tropes (5 people), and a three way tie for fifth between Reddit, SebastianMarshall.com, and You Are Not So Smart (3 people).<br /><br />Of those people who chose to list their karma, the mean value was 658 and the median was 40 (these numbers are pretty meaningless, because some people with zero karma put that down and other people did not).<br /><br />Of those people willing to admit the time they spent on Less Wrong, after eliminating one outlier (sorry, but you don't spend 40579 minutes daily on LW; even <em>I</em> don't spend that long) the mean was 21 minutes and the median was 15 minutes. There were at least a dozen people in the two to three hour range, and the winner (well, except the 40579 guy) was someone who says he spends five hours a day.<br /><br />I'm going to give all the probabilities in the form [mean, (25%-quartile, 50%-quartile/median, 75%-quartile)]. There <em>may</em> have been some problems here revolving around people who gave numbers like .01: I didn't know whether they meant 1% or .01%. Excel helpfully rounded all numbers down to two decimal places for me, and after a while I decided not to make it stop: unless I wanted to do geometric means, I can't do justice to really small grades in probability.<br /><br />The Many Worlds hypothesis is true: 56.5, (30, 65, 80)<br />There is intelligent life elsewhere in the Universe: 69.4, (50, 90, 99)<br />There is intelligent life elsewhere in our galaxy: 41.2, (1, 30, 80)<br />The supernatural (ontologically basic mental entities) exists: 5.38, (0, 0, 1)<br />God (a supernatural creator of the universe) exists: 5.64, (0, 0, 1)<br />Some revealed religion is true: 3.40, (0, 0, .15)<br />Average person cryonically frozen today will be successfully revived: 21.1, (1, 10, 30)<br />Someone now living will reach age 1000: 23.6, (1, 10, 30)<br />We are living in a simulation: 19, (.23, 5, 33)<br />Significant anthropogenic global warming is occurring: 70.7, (55, 85, 95)<br />Humanity will make it to 2100 without a catastrophe killing &gt;90% of us: 67.6, (50, 80, 90)<br /><br />There were a few significant demographics differences here. Women tended to be more skeptical of the extreme transhumanist claims like cryonics and antiagathics (for example, men thought the current generation had a 24.7% chance of seeing someone live to 1000 years; women thought there was only a 9.2% chance). Older people were less likely to believe in transhumanist claims, a little less likely to believe in anthropogenic global warming, and more likely to believe in aliens living in our galaxy. Community veterans were more likely to believe in Many Worlds, less likely to believe in God, and - surprisingly - less likely to believe in cryonics (significant at 5% level; could be a fluke). People who believed in high existential risk were more likely to believe in global warming, more likely to believe they had a higher IQ than average, and more likely to believe in aliens (I found that same result last time, and it puzzled me then too.)<br /><br />Intriguingly, even though the sample size increased by more than 6 times, most of these results are within one to two percent of the numbers on the 2009 survey, so this supports taking them as a direct line to prevailing rationalist opinion rather than the contingent opinions of one random group.<br /><br />Of possible existential risks, the most feared was a bioengineered pandemic, which got 194 votes (17.8%) - a natural pandemic got 89 (8.2%), making pandemics the overwhelming leader. Unfriendly AI followed with 180 votes (16.5%), then nuclear war with 151 (13.9%), ecological collapse with 145 votes (12.3%), economic/political collapse with 134 votes (12.3%), and asteroids and nanotech bringing up the rear with 46 votes each (4.2%).<br /><br />The mean for the Singularity question is useless because of the very high numbers some people put in, but the median was 2080 (quartiles 2050, 2080, 2150). The Singularity has gotten later since 2009: the median guess then was 2067. There was some discussion about whether people might have been anchored by the previous mention of 2100 in the x-risk question. I changed the order after 104 responses to prevent this; a t-test found no significant difference between the responses before and after the change (in fact, the trend was in the wrong direction).<br /><br />Only 49 people (4.5%) have never considered cryonics or don't know what it is. 388 (35.6%) of the remainder reject it, 583 (53.5%) are considering it, and 47 (4.3%) are already signed up for it. That's more than double the percent signed up in 2009.<br /><br />231 people (23.4% of respondents) have attended a Less Wrong meetup.<br /><br />The average person was 37.6% sure their IQ would be above average - underconfident! Imagine that! (quartiles were 10, 40, 60). The mean was 54.5% for people whose IQs really were above average, and 29.7% for people whose IQs really were below average. There was a correlation of .479 (significant at less than 1% level) between IQ and confidence in high IQ.<br /><br />Isaac Newton published his Principia Mathematica in 1687. Although people guessed dates as early as 1250 and as late as 1960, the mean was...1687 (quartiles were 1650, 1680, 1720). This marks the <em>second consecutive year </em>that the average answer to these difficult historical questions has been <em>exactly</em> right (to be fair, last time it was the median that was exactly right and the mean was all of eight months off). Let no one ever say that the wisdom of crowds is not a powerful tool. <br /><br />The average person was 34.3% confident in their answer, but 41.9% of people got the question right (again with the underconfidence!). There was a highly significant correlation of r = -.24 between confidence and number of years error.</p>\n<p><img src=\"http://www.raikoth.net/Stuff/graph_lw.png\" alt=\"\" width=\"646\" height=\"502\" /></p>\n<p>This graph may take some work to read. The x-axis is confidence. The y-axis is what percent of people were correct at that confidence level. The red line you recognize as perfect calibration. The thick green line is your results from the Newton problem. The black line is results from the general population I got from a different calibration experiment tested on 50 random trivia questions; take the intercomparability of the two with a grain of salt.<br /><br />As you can see, Less Wrong does significantly better than the general population. However, there are a few areas of failure. First is that, as usual, people who put zero and one hundred percent had nonzero chances of getting the question right or wrong: 16.7% of people who put \"0\" were right, and 28.6% of people who put \"100\" were wrong (interestingly, people who put 100 did worse than the average of everyone else in the 90-99 bracket, of whom only 12.2% erred). Second of all, the line is pretty horizontal from zero to fifty or so. People who thought they had a &gt;50% chance of being right had excellent calibration, but people who gave themselves a low chance of being right were poorly calibrated. In particular, I was surprised to see so many people put numbers like \"0\". If you're pretty sure Newton lived after the birth of Christ, but before the present day, that alone gives you a 1% chance of randomly picking the correct 20-year interval.<br /><br />160 people wanted their responses kept private. They have been removed. The rest have been sorted by age to remove any information about the time they took the survey. I've converted what's left to a .xls file, and you can download it <a href=\"http://www.raikoth.net/Stuff/lwpublic2.xls\">here</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"kJrjorSx3hXa7q7CJ": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HAEPbGaMygJq8L59k", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 97, "baseScore": 121, "extendedScore": null, "score": 0.00026379753841648227, "legacy": true, "legacyId": "11272", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 94, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 516, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["miHttwTgajY2sjY3L", "ZWC3n9c6v4s35rrZ3"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-05T16:20:23.838Z", "modifiedAt": null, "url": null, "title": "Seeking Collaborator for a Singularity Comic Book", "slug": "seeking-collaborator-for-a-singularity-comic-book", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:56.575Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "James_Miller", "createdAt": "2009-03-05T17:14:38.674Z", "isAdmin": false, "displayName": "James_Miller"}, "userId": "LzF2X9eB9oS3q4BXG", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZH29SAsjQqXfg7Xuq/seeking-collaborator-for-a-singularity-comic-book", "pageUrlRelative": "/posts/ZH29SAsjQqXfg7Xuq/seeking-collaborator-for-a-singularity-comic-book", "linkUrl": "https://www.lesswrong.com/posts/ZH29SAsjQqXfg7Xuq/seeking-collaborator-for-a-singularity-comic-book", "postedAtFormatted": "Monday, December 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Seeking%20Collaborator%20for%20a%20Singularity%20Comic%20Book&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASeeking%20Collaborator%20for%20a%20Singularity%20Comic%20Book%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZH29SAsjQqXfg7Xuq%2Fseeking-collaborator-for-a-singularity-comic-book%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Seeking%20Collaborator%20for%20a%20Singularity%20Comic%20Book%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZH29SAsjQqXfg7Xuq%2Fseeking-collaborator-for-a-singularity-comic-book", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZH29SAsjQqXfg7Xuq%2Fseeking-collaborator-for-a-singularity-comic-book", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 124, "htmlBody": "<p>\n<p class=\"MsoNoSpacing\">I&rsquo;m currently writing a book called Singularity Rising:&nbsp; Surviving and Thriving in a Smarter, Richer and More Dangerous World.&nbsp; The book will be published in about a year by <a href=\"http://www.benbellabooks.com/\">BenBella Books</a>.&nbsp; This will be my third <a href=\"http://www.amazon.com/Game-Theory-Work-Outmaneuver-Competition/dp/0071400206/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1323101912&amp;sr=1-1\">published</a> <a href=\"http://www.amazon.com/Principles-Microeconomics-James-Miller/dp/0073402834/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1323101930&amp;sr=1-1\">book</a>.</p>\n<p class=\"MsoNoSpacing\">I&rsquo;m thinking about also making a singularity comic book.<span style=\"font-family: Arial, sans-serif;\"> </span>My agent thinks the idea might work. &nbsp;Unfortunately my artistic abilities are well in the bottom half of mankind so I'm looking for a collaborator who would do the art.&nbsp; If you are interested please send samples or links to art you have created here:</p>\n<p class=\"MsoNoSpacing\"><a href=\"mailto:EconomicProf@Gmail.com\">EconomicProf@Gmail.com</a></p>\n<p class=\"MsoNoSpacing\">If you know of someone who might be appropriate please email me (or leave below) his name, email address and if possible links to his artwork.</p>\n<p class=\"MsoNoSpacing\">&nbsp;</p>\n<p class=\"MsoNoSpacing\">You can find out more about me here:</p>\n<p class=\"MsoNoSpacing\">&nbsp;<a href=\"http://sophia.smith.edu/~jdmiller/resume.pdf\">www.JamesDMiller.org</a>&nbsp;&nbsp;</p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZH29SAsjQqXfg7Xuq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 17, "extendedScore": null, "score": 8.102776638367037e-07, "legacy": true, "legacyId": "11286", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-05T20:01:17.538Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Asch's Conformity Experiment", "slug": "seq-rerun-asch-s-conformity-experiment", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:50.210Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SpXxFbMh6kvRDnu6J/seq-rerun-asch-s-conformity-experiment", "pageUrlRelative": "/posts/SpXxFbMh6kvRDnu6J/seq-rerun-asch-s-conformity-experiment", "linkUrl": "https://www.lesswrong.com/posts/SpXxFbMh6kvRDnu6J/seq-rerun-asch-s-conformity-experiment", "postedAtFormatted": "Monday, December 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Asch's%20Conformity%20Experiment&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Asch's%20Conformity%20Experiment%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSpXxFbMh6kvRDnu6J%2Fseq-rerun-asch-s-conformity-experiment%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Asch's%20Conformity%20Experiment%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSpXxFbMh6kvRDnu6J%2Fseq-rerun-asch-s-conformity-experiment", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSpXxFbMh6kvRDnu6J%2Fseq-rerun-asch-s-conformity-experiment", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 180, "htmlBody": "<p>Today's post, <a href=\"/lw/m9/aschs_conformity_experiment/\">Asch's Conformity Experiment</a> was originally published on 26 December 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The unanimous agreement of surrounding others can make subjects disbelieve (or at least, fail to report) what's right before their eyes. The addition of just one dissenter is enough to dramatically reduce the rates of improper conformity.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8oz/seq_rerun_the_amazing_virgin_pregnancy/\">The Amazing Virgin Pregnancy</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SpXxFbMh6kvRDnu6J", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.103576442148853e-07, "legacy": true, "legacyId": "11287", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["WHK94zXkQm7qm7wXk", "4H6drjEPkx4nufYDc", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-05T23:41:44.722Z", "modifiedAt": null, "url": null, "title": "Utilitarianism- WBE (uploading) > FAI", "slug": "utilitarianism-wbe-uploading-greater-than-fai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:53.151Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Curiouskid", "createdAt": "2011-05-13T23:30:04.967Z", "isAdmin": false, "displayName": "Curiouskid"}, "userId": "Y4xxvuP743fwKcZQY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/eCkrwFBBRaReoFLbM/utilitarianism-wbe-uploading-greater-than-fai", "pageUrlRelative": "/posts/eCkrwFBBRaReoFLbM/utilitarianism-wbe-uploading-greater-than-fai", "linkUrl": "https://www.lesswrong.com/posts/eCkrwFBBRaReoFLbM/utilitarianism-wbe-uploading-greater-than-fai", "postedAtFormatted": "Monday, December 5th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Utilitarianism-%20WBE%20(uploading)%20%3E%20FAI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AUtilitarianism-%20WBE%20(uploading)%20%3E%20FAI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeCkrwFBBRaReoFLbM%2Futilitarianism-wbe-uploading-greater-than-fai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Utilitarianism-%20WBE%20(uploading)%20%3E%20FAI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeCkrwFBBRaReoFLbM%2Futilitarianism-wbe-uploading-greater-than-fai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FeCkrwFBBRaReoFLbM%2Futilitarianism-wbe-uploading-greater-than-fai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 120, "htmlBody": "<p>If you were a utilitarian, then why would you want to risk creating an AGI that had the potential to be an existential risk, when you could eliminate all suffering with the advent of WBE (whole brain emulation) and hence virtual reality (or digital alteration of your source code) and hence utopia? Wouldn't you want to try to prevent AI research and just promote WBE research? Or is it that AGI is more likely to come before WBE and so we should focus our efforts on making sure that the AGI is friendly? Or maybe uploading isn't possible for technological or philosophical reasons (substrate dependence)?&nbsp;<br />Is there a link to a discussion on this that I'm missing out on?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "eCkrwFBBRaReoFLbM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": -7, "extendedScore": null, "score": 8.104374785130551e-07, "legacy": true, "legacyId": "11288", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-06T04:29:36.391Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] On Expressing Your Concerns", "slug": "seq-rerun-on-expressing-your-concerns", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:51.713Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iwRXmq3yd4MXgeRCr/seq-rerun-on-expressing-your-concerns", "pageUrlRelative": "/posts/iwRXmq3yd4MXgeRCr/seq-rerun-on-expressing-your-concerns", "linkUrl": "https://www.lesswrong.com/posts/iwRXmq3yd4MXgeRCr/seq-rerun-on-expressing-your-concerns", "postedAtFormatted": "Tuesday, December 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20On%20Expressing%20Your%20Concerns&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20On%20Expressing%20Your%20Concerns%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiwRXmq3yd4MXgeRCr%2Fseq-rerun-on-expressing-your-concerns%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20On%20Expressing%20Your%20Concerns%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiwRXmq3yd4MXgeRCr%2Fseq-rerun-on-expressing-your-concerns", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiwRXmq3yd4MXgeRCr%2Fseq-rerun-on-expressing-your-concerns", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 153, "htmlBody": "<p>Today's post, <a href=\"/lw/ma/on_expressing_your_concerns/\">On Expressing Your Concerns</a> was originally published on 27 December 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>A way of breaking the conformity effect in some cases</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8pj/seq_rerun_aschs_conformity_experiment/\">Asch's Conformity Experiment</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iwRXmq3yd4MXgeRCr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.105417446779691e-07, "legacy": true, "legacyId": "11295", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ovvwAhKKoNbfcMz8K", "SpXxFbMh6kvRDnu6J", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-06T05:52:03.575Z", "modifiedAt": null, "url": null, "title": ".", "slug": "", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:53.207Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "a7xJQpZ55R6SxFTik", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vnXhGgDqJxzppcGaA/", "pageUrlRelative": "/posts/vnXhGgDqJxzppcGaA/", "linkUrl": "https://www.lesswrong.com/posts/vnXhGgDqJxzppcGaA/", "postedAtFormatted": "Tuesday, December 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvnXhGgDqJxzppcGaA%2F%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvnXhGgDqJxzppcGaA%2F", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvnXhGgDqJxzppcGaA%2F", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vnXhGgDqJxzppcGaA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 15, "extendedScore": null, "score": 8.105716143799104e-07, "legacy": true, "legacyId": "11296", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-06T13:22:03.229Z", "modifiedAt": null, "url": null, "title": "[Link] How doctors die", "slug": "link-how-doctors-die", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:55.482Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Morendil", "createdAt": "2009-09-21T16:34:39.505Z", "isAdmin": false, "displayName": "Morendil"}, "userId": "aDcxmpDTkqN6vWmRZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3mo8YRCte8xcssh2Q/link-how-doctors-die", "pageUrlRelative": "/posts/3mo8YRCte8xcssh2Q/link-how-doctors-die", "linkUrl": "https://www.lesswrong.com/posts/3mo8YRCte8xcssh2Q/link-how-doctors-die", "postedAtFormatted": "Tuesday, December 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20How%20doctors%20die&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20How%20doctors%20die%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3mo8YRCte8xcssh2Q%2Flink-how-doctors-die%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20How%20doctors%20die%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3mo8YRCte8xcssh2Q%2Flink-how-doctors-die", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3mo8YRCte8xcssh2Q%2Flink-how-doctors-die", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 287, "htmlBody": "<p>I'm reposting <a href=\"http://zocalopublicsquare.org/thepublicsquare/2011/11/30/how-doctors-die/read/nexus/\">this</a> from HN's front page, because it brought up a non-cached thought on cryonics:</p>\n<blockquote>\n<p>The patient will get cut open, perforated with tubes, hooked up to machines, and assaulted with drugs. All of this occurs in the Intensive Care Unit at a cost of tens of thousands of dollars a day. What it buys is misery we would not inflict on a terrorist. I cannot count the number of times fellow physicians have told me, in words that vary only slightly, &ldquo;Promise me if you find me like this that you&rsquo;ll kill me.&rdquo; [...]&nbsp;I&rsquo;ve had hundreds of people brought to me in the emergency room after getting CPR. Exactly one, a healthy man who&rsquo;d had no heart troubles (for those who want specifics, he had a &ldquo;tension pneumothorax&rdquo;), walked out of the hospital.</p>\n</blockquote>\n<p>In short, end-of-life medical care is often pointless, painful&nbsp;and costly; doctors and ER personnel know this so well that they go to great lengths to ensure it doesn't happen to them.</p>\n<p>It seems as if our systems and conventions around end of life are designed to not let people have a say in how they spend their final moments, even when letting them have their way would result in significant savings (note the dollar figures quoted above). I've already <a href=\"/lw/5id/ethics_and_rationality_of_suicide/42pt\">speculated</a> on why that might be, but I keep seeing that turn up in unexpected ways.</p>\n<p>I suspect that this is the bigger obstacle to cryonics, not so much e.g. the lack of scientific proof. \"Freeze me cheaply instead of spending insane amounts of money on brutal attempts at keeping me alive\" sounds like a sensible thing to tattoo on your chest, but the evidence suggests that it wouldn't be honored any more than \"DNR\" tattoos.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xHjy88N2uJvGdgzfw": 1, "E9ihK6bA9YKkmJs2f": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3mo8YRCte8xcssh2Q", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 29, "extendedScore": null, "score": 8.107346650236734e-07, "legacy": true, "legacyId": "11299", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-06T14:46:26.845Z", "modifiedAt": null, "url": null, "title": "[Link] How to Dispel Your Illusions ", "slug": "link-how-to-dispel-your-illusions", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:54.395Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sXK7sCB5c7jWJrAA2/link-how-to-dispel-your-illusions", "pageUrlRelative": "/posts/sXK7sCB5c7jWJrAA2/link-how-to-dispel-your-illusions", "linkUrl": "https://www.lesswrong.com/posts/sXK7sCB5c7jWJrAA2/link-how-to-dispel-your-illusions", "postedAtFormatted": "Tuesday, December 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20How%20to%20Dispel%20Your%20Illusions%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20How%20to%20Dispel%20Your%20Illusions%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsXK7sCB5c7jWJrAA2%2Flink-how-to-dispel-your-illusions%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20How%20to%20Dispel%20Your%20Illusions%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsXK7sCB5c7jWJrAA2%2Flink-how-to-dispel-your-illusions", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsXK7sCB5c7jWJrAA2%2Flink-how-to-dispel-your-illusions", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1712, "htmlBody": "<p>The <a href=\"/lw/3gv/statistical_prediction_rules_outperform_expert/\">topic</a> and the problems associated with it are probably familiar to many of you already. But I think some may find this <a href=\"http://www.nybooks.com/articles/archives/2011/dec/22/how-dispel-your-illusions/?pagination=false\">review</a> by <a href=\"http://en.wikipedia.org/wiki/Freeman_Dyson\">Freeman Dyson</a> of the book <em> <a href=\"http://www.amazon.com/gp/product/0374275637?ie=UTF8&amp;tag=thneyoreofbo-20&amp;linkCode=as2&amp;camp=1789&amp;creative=9325&amp;creativeASIN=0374275637\" target=\"_blank\">Thinking, Fast and Slow</a><img style=\"border: medium none ! important; margin: 0px ! important;\" src=\"http://www.assoc-amazon.com/e/ir?t=thneyoreofbo-20&amp;l=as2&amp;o=1&amp;a=0374275637\" alt=\"\" width=\"1\" height=\"1\" /> </em>by Daniel Kahneman interesting.&nbsp;</p>\n<blockquote>\n<p>In 1955, when Daniel Kahneman was twenty-one years old, he was a lieutenant in the Israeli Defense Forces. He was given the job of setting up a new interview system for the entire army. The purpose was to evaluate each freshly drafted recruit and put him or her into the appropriate slot in the war machine. The interviewers were supposed to predict who would do well in the infantry or the artillery or the tank corps or the various other branches of the army. The old interview system, before Kahneman arrived, was informal. The interviewers chatted with the recruit for fifteen minutes and then came to a decision based on the conversation. The system had failed miserably. <strong>When the actual performance of the recruit a few months later was compared with the performance predicted by the interviewers, <em>the correlation between actual and predicted performance was zero</em></strong>.</p>\n<p>Kahneman had a bachelor&rsquo;s degree in psychology and had read a book, <em>Clinical vs. Statistical Prediction: A Theoretical Analysis and a Review of the Evidence</em> by Paul Meehl, published only a year earlier. Meehl was an American psychologist who studied the successes and failures of predictions in many different settings. He found overwhelming evidence for a disturbing conclusion. <strong>Predictions based on simple statistical scoring were generally <em>more accurate than predictions based on expert judgment</em>.</strong></p>\n<p>A famous example confirming Meehl&rsquo;s conclusion is the &ldquo;Apgar score,&rdquo; invented by the anesthesiologist Virginia Apgar in 1953 to guide the treatment of newborn babies. <strong>The Apgar score is a simple formula based on five vital signs that can be measured quickly: heart rate, breathing, reflexes, muscle tone, and color. <em>It does better than the average doctor in deciding whether the baby needs immediate help.</em></strong> It is now used everywhere and saves the lives of thousands of babies. Another famous example of statistical prediction is the Dawes formula for the durability of marriage. The formula is &ldquo;frequency of love-making minus frequency of quarrels.&rdquo; Robyn Dawes was a psychologist who worked with Kahneman later. His formula does better than the average marriage counselor in predicting whether a marriage will last.</p>\n<p>Having read the Meehl book, Kahneman knew how to improve the Israeli army interviewing system. His new system did not allow the interviewers the luxury of free-ranging conversations with the recruits. Instead, they were required to ask a standard list of factual questions about the life and work of each recruit. The answers were then converted into numerical scores, and the scores were inserted into formulas measuring the aptitude of the recruit for the various army jobs. When the predictions of the new system were compared to performances several months later, the results showed the new system to be much better than the old. <em><strong>Statistics and simple arithmetic tell us more about ourselves than expert intuition.</strong></em></p>\n<p>Reflecting fifty years later on his experience in the Israeli army, Kahneman remarks in <em>Thinking, Fast and Slow</em> that it was not unusual in those days for young people to be given big responsibilities. The country itself was only seven years old. &ldquo;All its institutions were under construction,&rdquo; he says, &ldquo;and someone had to build them.&rdquo; He was lucky to be given this chance to share in the building of a country, and at the same time to achieve an intellectual insight into human nature. <strong>He understood that the failure of the old interview system was a special case of a general phenomenon that he called &ldquo;the<em> illusion of validity</em>.&rdquo; At this point, he says, &ldquo;I had discovered my first cognitive illusion.&rdquo;</strong></p>\n<p><strong>Cognitive illusions are the main theme of his book.</strong> A cognitive illusion is a false belief that we intuitively accept as true. The illusion of validity is a false belief in the reliability of our own judgment. The interviewers sincerely believed that they could predict the performance of recruits after talking with them for fifteen minutes. <strong>Even after the interviewers had seen the statistical evidence that their belief was an illusion, <em>they still could not help believing it</em>.</strong> Kahneman confesses that he himself still experiences the illusion of validity, after fifty years of warning other people against it. He cannot escape the illusion that his own intuitive judgments are trustworthy.</p>\n<p>An episode from my own past is curiously similar to Kahneman&rsquo;s experience in the Israeli army. I was a statistician before I became a scientist. At the age of twenty I was doing statistical analysis of the operations of the British Bomber Command in World War <span class=\"caps\">II</span>. The command was then seven years old, like the State of Israel in 1955. All its institutions were under construction. It consisted of six bomber groups that were evolving toward operational autonomy. Air Vice Marshal Sir Ralph Cochrane was the commander of 5 Group, the most independent and the most effective of the groups. Our bombers were then taking heavy losses, the main cause of loss being the German night fighters.</p>\n<p>Cochrane said the bombers were too slow, and the reason they were too slow was that they carried heavy gun turrets that increased their aerodynamic drag and lowered their operational ceiling. Because the bombers flew at night, they were normally painted black. Being a flamboyant character, <em>Cochrane announced that he would like to take a Lancaster bomber, rip out the gun turrets and all the associated dead weight, ground the two gunners, and paint the whole thing white. Then he would fly it over Germany, and fly so high and so fast that nobody could shoot him down. </em>Our commander in chief did not approve of this suggestion, and the white Lancaster never flew.<em></em></p>\n<p><em>The reason why our commander in chief was unwilling to rip out gun turrets, even on an experimental basis, was that he was blinded by the illusion of validity. </em>This was ten years before Kahneman discovered it and gave it its name, but the illusion of validity was already doing its deadly work. All of us at Bomber Command shared the illusion. We saw every bomber crew as a tightly knit team of seven, with the gunners playing an essential role defending their comrades against fighter attack, while the pilot flew an irregular corkscrew to defend them against flak. <strong>An essential part of the illusion was the belief that the team learned by experience. As they became more skillful and more closely bonded, their chances of survival would improve.</strong></p>\n<p>When I was collecting the data in the spring of 1944, the chance of a crew reaching the end of a thirty-operation tour was about 25 percent. The illusion that experience would help them to survive was essential to their morale. After all, they could see in every squadron a few revered and experienced old-timer crews who had completed one tour and had volunteered to return for a second tour. It was obvious to everyone that the old-timers survived because they were more skillful. <strong>Nobody wanted to believe that the old-timers survived only because they were lucky.</strong></p>\n<p>At the time Cochrane made his suggestion of flying the white Lancaster, I had the job of examining the statistics of bomber losses. I did a careful analysis of the correlation between the experience of the crews and their loss rates, subdividing the data into many small packages so as to eliminate effects of weather and geography. My results were as conclusive as those of Kahneman. There was no effect of experience on loss rate. So far as I could tell, whether a crew lived or died was purely a matter of chance. Their belief in the life-saving effect of experience was an illusion.</p>\n<p>The demonstration that experience had no effect on losses should have given powerful support to Cochrane&rsquo;s idea of ripping out the gun turrets. But nothing of the kind happened. As Kahneman found out later, the illusion of validity does not disappear just because facts prove it to be false. Everyone at Bomber Command, from the commander in chief to the flying crews, continued to believe in the illusion. The crews continued to die, experienced and inexperienced alike, until Germany was overrun and the war finally ended.</p>\n<p>Another theme of Kahneman&rsquo;s book, proclaimed in the title, is the existence in our brains of two independent sytems for organizing knowledge. Kahneman calls them System One and System Two. System One is amazingly fast, allowing us to recognize faces and understand speech in a fraction of a second. It must have evolved from the ancient little brains that allowed our agile mammalian ancestors to survive in a world of big reptilian predators. Survival in the jungle requires a brain that makes quick decisions based on limited information. <strong>Intuition is the name we give to judgments based on the quick action of System One.</strong> It makes judgments and takes action without waiting for our conscious awareness to catch up with it. The most remarkable fact about System One is that it has immediate access to a vast store of memories that it uses as a basis for judgment. The memories that are most accessible are those associated with strong emotions, with fear and pain and hatred. The resulting judgments are often wrong, but in the world of the jungle it is safer to be wrong and quick than to be right and slow.<strong></strong></p>\n<p><strong>System Two is the slow process of forming judgments based on conscious thinking and critical examination of evidence.</strong> It appraises the actions of System One. It gives us a chance to correct mistakes and revise opinions. It probably evolved more recently than System One, after our primate ancestors became arboreal and had the leisure to think things over. An ape in a tree is not so much concerned with predators as with the acquisition and defense of territory. System Two enables a family group to make plans and coordinate activities. After we became human, System Two enabled us to create art and culture.</p>\n</blockquote>\n<p>If you've made it this far read the rest of the review <a href=\"http://www.nybooks.com/articles/archives/2011/dec/22/how-dispel-your-illusions/?pagination=false\">here</a>. There is still some cool stuff after this.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "4Kcm4etxAJjmeDkHP": 1, "DLskYNGdAGDFpxBF8": 1, "4R8JYu4QF2FqzJxE5": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sXK7sCB5c7jWJrAA2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 37, "extendedScore": null, "score": 8.107652506145685e-07, "legacy": true, "legacyId": "11300", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CKW8c2Bngz9yXibSk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-06T17:29:08.389Z", "modifiedAt": null, "url": null, "title": "[Link] Using the conjunction fallacy to reveal implicit associations", "slug": "link-using-the-conjunction-fallacy-to-reveal-implicit", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:51.098Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "HonoreDB", "createdAt": "2010-11-18T19:42:02.810Z", "isAdmin": false, "displayName": "HonoreDB"}, "userId": "7eyYSfGvgCur6pXmk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WAGyzxua5RQ9L4Ekx/link-using-the-conjunction-fallacy-to-reveal-implicit", "pageUrlRelative": "/posts/WAGyzxua5RQ9L4Ekx/link-using-the-conjunction-fallacy-to-reveal-implicit", "linkUrl": "https://www.lesswrong.com/posts/WAGyzxua5RQ9L4Ekx/link-using-the-conjunction-fallacy-to-reveal-implicit", "postedAtFormatted": "Tuesday, December 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Using%20the%20conjunction%20fallacy%20to%20reveal%20implicit%20associations&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Using%20the%20conjunction%20fallacy%20to%20reveal%20implicit%20associations%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWAGyzxua5RQ9L4Ekx%2Flink-using-the-conjunction-fallacy-to-reveal-implicit%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Using%20the%20conjunction%20fallacy%20to%20reveal%20implicit%20associations%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWAGyzxua5RQ9L4Ekx%2Flink-using-the-conjunction-fallacy-to-reveal-implicit", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWAGyzxua5RQ9L4Ekx%2Flink-using-the-conjunction-fallacy-to-reveal-implicit", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 371, "htmlBody": "<p><a href=\"http://freethoughtblogs.com/pharyngula/2011/12/05/atheists-as-bad-as-rapists/\">Via Pharyngula</a>.</p>\n<p>A recent set of studies by Gervais, Shariff, and Norenzayan tested whether public dislike of atheists was based more on distrust or revulsion. &nbsp;The first study simply asked directly, and found that American adults report a strong distrust of atheists. &nbsp;However, they worried that explicit answers might be more about signaling, so they did a second study with a more unusual methodology. Questions they asked included (paraphrased) \"Is a person who steals money out of a lost wallet more likely to be 1) a teacher or 2) a teacher and an atheist?\" and&nbsp;\"Is a person who goes all day without noticing he has phlegm on his tie more likely to be 1) a teacher or 2) a teacher and an atheist?\" &nbsp;Students, especially religious ones, often answered 2) to the first and 1) to the second, suggesting that it's more about mistrust.</p>\n<p>I suspect this isn't actually a more effective way of eliciting stereotypes than asking directly. &nbsp;I think signaling concerns will be just as active in the second study, and there will be a skewing of results in that anyone familiar with the <a href=\"/lw/ji/conjunction_fallacy/\">conjunction fallacy</a>&nbsp;(or even the importance of base rates) will answer 1) even if they distrust or are disgusted by atheists. &nbsp;The result will inevitably underestimate dislike of atheists and evince a spurious or exaggerated correlation between such dislike and statistical innumeracy. &nbsp;I think a better way to look for implicit, rather than explicit, stereotyping would be to create an <a href=\"/lw/53/the_implicit_association_test/\">Implicit Association Test</a>. &nbsp;That said, I think the study is still meaningful and I'm intrigued by the methodology.</p>\n<p>Thanks to lukeprog, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Gervais-et-al-Do-you-believe-in-atheists.pdf\">here's the full paper</a>. &nbsp;Relevant excerpt:</p>\n<blockquote>\n<p>Study 1 demonstrated explicit distrust of atheists, but it is possible that, instead of being representative of personal feelings, participants&rsquo; explicit responses may have instead reflected cultural norms determining which groups are fair game for criticism and which should be insulated. The varied permissibility of such criticism is itself an interesting indicator of prejudice, but it does not specifically map on to the questions of distrust at the heart of this project. As a result, in Study 2, we adapted a classic conjunction fallacy paradigm (e.g., Tversky &amp; Kahnemann, 1983) to create an indirect measure of distrust for various groups of people.</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb1a0": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WAGyzxua5RQ9L4Ekx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 8.108242187103737e-07, "legacy": true, "legacyId": "11301", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QAK43nNCTQQycAcYe", "iYJo382hY28K7eCrP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-06T18:49:03.740Z", "modifiedAt": null, "url": null, "title": "Beyond the Reach of God, Abridged for Spoken Word", "slug": "beyond-the-reach-of-god-abridged-for-spoken-word", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:53.095Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QPndJSv638SuKWTvN/beyond-the-reach-of-god-abridged-for-spoken-word", "pageUrlRelative": "/posts/QPndJSv638SuKWTvN/beyond-the-reach-of-god-abridged-for-spoken-word", "linkUrl": "https://www.lesswrong.com/posts/QPndJSv638SuKWTvN/beyond-the-reach-of-god-abridged-for-spoken-word", "postedAtFormatted": "Tuesday, December 6th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Beyond%20the%20Reach%20of%20God%2C%20Abridged%20for%20Spoken%20Word&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABeyond%20the%20Reach%20of%20God%2C%20Abridged%20for%20Spoken%20Word%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQPndJSv638SuKWTvN%2Fbeyond-the-reach-of-god-abridged-for-spoken-word%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Beyond%20the%20Reach%20of%20God%2C%20Abridged%20for%20Spoken%20Word%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQPndJSv638SuKWTvN%2Fbeyond-the-reach-of-god-abridged-for-spoken-word", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQPndJSv638SuKWTvN%2Fbeyond-the-reach-of-god-abridged-for-spoken-word", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1335, "htmlBody": "<p>Previously, I posted a version of <a href=\"/r/discussion/lw/8o6/the_gift_we_give_tomorrow_spoken_word_finished/\">The Gift We Give Tomorrow</a> that was designed to be read aloud. It was significantly abridged, and some portions reworded to flow better from the tongue. I recently finished another part of my project: An abridged version of <a href=\"/lw/uk/beyond_the_reach_of_god/\">Beyond the Reach of God</a>. This one doesn&rsquo;t lend itself as well to something resembling &ldquo;poetry,&rdquo; so it&rsquo;s more a straightforward editing job. The original was 3315 words. The new one is currently 1090. I&rsquo;m still trying to trim it a little more, if possible. GWGT was 1245, which was around 7 minutes of speaking time, and pushing the limit of how long the piece can be.</p>\n<p>For those who were concerned, after paring this down into a collection of some of the most depressing sentences I've ever read, I decided it was NOT necessary to end \"Gift We Give Tomorrow\" on an echo of this post (although I'm leaving in the part where I reword the \"Shadowy Figure\" to more directly reference it). That reading will end with the original \"Ever so long ago.\"</p>\n<p>&nbsp;</p>\n<h2><strong>Beyond the Reach of God:</strong></h2>\n<p>I remember, from distant childhood, what it's like to live in the world where God exists. <em>Really</em> exists, the way that children and rationalists take all their&nbsp;<a href=\"http://www.overcomingbias.com/2007/07/belief-in-belie.html\">beliefs</a> at face value.</p>\n<p>In the world where God exists, he doesn&rsquo;t intervene to optimize <em>everything</em>. God won&rsquo;t make you a sandwich. Parents don't do everything their children ask. There are good arguments against <em>always </em>giving someone what they desire.</p>\n<p>I don't <em>want</em> to become a simple wanting-thing, that never has to plan or act or think.</p>\n<p>But clearly, there's&nbsp;<em>some</em> threshold of horror, awful enough that God will intervene. I remember that being true, when I believed after the fashion of a child. The God who <em>never</em> intervenes - <em>that's</em> an obvious attempt to avoid falsification, to protect a&nbsp;<a href=\"http://www.overcomingbias.com/2007/07/belief-in-belie.html\">belief-in-belief</a>. The beliefs of young children really shape their expectations - they honestly expect to see&nbsp;<a href=\"http://www.overcomingbias.com/2007/07/belief-in-belie.html\">the dragon in their garage</a>. They have no reason to imagine a loving God who never acts. No loving parents, desiring their child to grow up strong and self-reliant, would let their toddler be run over by a car.</p>\n<p>But what if you built a simulated universe? Could you escape the reach of God? Simulate sentient minds, and torture them? If God's watching everywhere, then of course trying to build an unfair world results in <em>the</em> God intervening - stepping in to modify your transistors. God is omnipresent. There&rsquo;s no refuge <em>anywhere</em> for true horror.</p>\n<p>Life is fair.</p>\n<p>But suppose you ask the question: <em>Given</em> such-and-such initial conditions, and <em>given</em> such-and-such rules, what <em>would be</em> the mathematical result?</p>\n<p>Not even God can change the answer to that question.</p>\n<p>What does life look like, in this imaginary world, where each step follows only from its immediate predecessor? Where things only ever happen, or don't happen, because of mathematical rules? And where the rules&nbsp;<em>don't</em> describe a God that checks over each state? What does it look like, the world of pure math, beyond the reach of God?</p>\n<p>That world wouldn't be fair. If the initial state contained the seeds of something that could self-replicate, natural selection might or might not take place. Complex life might or might not evolve. That life might or might not become sentient. That world might have the equivalent of conscious cows, that lacked hands or brains to improve their condition. Maybe they would be eaten by conscious wolves who never thought that they were doing wrong, or cared.</p>\n<p>If something like humans evolved, then they would suffer from diseases - not to teach them any lessons, but only because viruses happened to evolve as well. If the people of that world are happy, or unhappy, it might have nothing to do with good or bad choices they made. Nothing to do with free will or lessons learned. In the what-if world, Genghis Khan can murder a million people, and laugh, and be rich, and never be punished, and live his life much happier than the average. Who would prevents it?</p>\n<p>And if the Khan tortures people to death, for his own amusement? They might call out for help, perhaps imagining a God. And if you really wrote the program, God *would* intervene, of course. But in the what-if question, there isn't any God in the system. The victims will be saved only if the right cells happen to be 0 or 1. And it's not likely that anyone will defy the Khan; if they did, someone would strike them with a sword, and the sword would disrupt their organs and they would die, and that would be the end of that.&nbsp;</p>\n<p>So the victims die, screaming, and no one helps them. That is the answer to the what-if question.</p>\n<p>...is&nbsp;this world starting to sound familiar?</p>\n<p>Could it really be that sentient beings have died, absolutely, for millions of years.... with no soul and no afterlife... <em>not </em>as any grand plan of Nature. Not to teach us about the meaning of life. Not even to teach a profound lesson about what is impossible.</p>\n<p>Just dead. Just because.</p>\n<p>Once upon a time, I believed that the extinction of humanity was not allowed. And others, who call themselves rationalists, may yet have things they trust. They might be called \"positive-sum games\", or \"democracy\", or&nbsp;&ldquo;capitalism&rdquo;, or \"technology\", but they&rsquo;re sacred. They can't lead to anything <em>really</em> bad, not without a silver lining. The unfolding history of Earth can't ever turn from its positive-sum trend to a negative-sum trend. <a href=\"http://www.overcomingbias.com/2007/09/applause-lights.html\">Democracies</a> won't ever legalize torture. <a href=\"http://www.overcomingbias.com/2008/09/raised-in-sf.html\">Technology</a> has done so much good, that there can't possibly be a black swan that breaks the trend and does more harm than all the good up until this point.</p>\n<p>Anyone listening, who still thinks that being happy counts for more than anything in life, well, maybe they <em>shouldn't</em> ponder the unprotectedness of their existence. Maybe think of it <em>just</em> long enough to sign up themselves and their family for cryonics, or write a check to an existential-risk-mitigation agency now and then. Or at least wear a seatbelt and get health insurance and all those other dreary necessary things that can destroy your life if you miss that one step... but aside from that, if you want to be happy, meditating on the fragility of life isn't going to help.</p>\n<p>But I'm speaking now to those who have something to protect.</p>\n<p>What can a twelfth-century peasant do to save themselves from annihilation? Nothing. Nature's challenges aren't always fair. When you run into a challenge that's too difficult, you suffer the penalty; when you run into a lethal penalty, you die. That's how it is for people, and it isn't any different for planets. Someone who wants to dance the deadly dance with Nature needs to understand what they're up against: Absolute, utter, exceptionless neutrality.</p>\n<p>And knowing this might not save you. It wouldn't save a twelfth-century peasant, even if they knew. If you think that a rationalist who fully understands the mess they're in, <em>must</em>&nbsp;be able to find a way out - well, then you&nbsp;<a href=\"http://www.overcomingbias.com/2008/05/no-defenses.html\">trust rationality</a>.&nbsp;Enough said.</p>\n<p>Still,&nbsp;I don't want to create <em>needless</em> despair, so I will say a few hopeful words at this point:</p>\n<p>If humanity's future unfolds in the right way, we might be able to make our future fair(er). We can't change physics. But we can build some guardrails, and put down some padding.</p>\n<p>Someday, maybe, minds will be sheltered. Children might burn a finger or lose a toy, but they won't ever be run over by cars. A super-intelligence would not be intimidated by a challenge where death is the price of a single failure. The raw universe wouldn't seem so harsh, would be only another problem to be solved.</p>\n<p>The problem is that building an adult is itself an adult challenge. That's what I finally realized, years ago.</p>\n<p>If there is a fair(er) universe, we have to get there starting from <em>this</em> world - the neutral world, the world of hard concrete with no padding. The world where challenges are not calibrated to your skills, and you can die for failing them.</p>\n<p>What does a child need to do, to solve an adult problem?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QPndJSv638SuKWTvN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 19, "extendedScore": null, "score": 4.8e-05, "legacy": true, "legacyId": "11302", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bkRpALFAwJQuntHiF", "sYgv4eYH82JEsTD34"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-07T03:32:36.765Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Lonely Dissent", "slug": "seq-rerun-lonely-dissent", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:51.676Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KRNCFjGuFzcLmxhAk/seq-rerun-lonely-dissent", "pageUrlRelative": "/posts/KRNCFjGuFzcLmxhAk/seq-rerun-lonely-dissent", "linkUrl": "https://www.lesswrong.com/posts/KRNCFjGuFzcLmxhAk/seq-rerun-lonely-dissent", "postedAtFormatted": "Wednesday, December 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Lonely%20Dissent&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Lonely%20Dissent%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKRNCFjGuFzcLmxhAk%2Fseq-rerun-lonely-dissent%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Lonely%20Dissent%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKRNCFjGuFzcLmxhAk%2Fseq-rerun-lonely-dissent", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKRNCFjGuFzcLmxhAk%2Fseq-rerun-lonely-dissent", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 211, "htmlBody": "<p>Today's post, <a href=\"/lw/mb/lonely_dissent/\">Lonely Dissent</a> was originally published on 28 December 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Joining a revolution does take courage, but it is something that humans can reliably do. It is comparatively more difficult to risk death. But is is more difficult than either of these to be the first person in a rebellion. To be the only one who is saying something different. That doesn't feel like going to school in black. It feels like going to school in a clown suit.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8pr/seq_rerun_on_expressing_your_concerns/\">On Expressing Your Concerns</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KRNCFjGuFzcLmxhAk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 8.110430143567057e-07, "legacy": true, "legacyId": "11311", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CEGnJBHmkcwPTysb7", "iwRXmq3yd4MXgeRCr", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-07T18:35:06.010Z", "modifiedAt": null, "url": null, "title": "CEV-inspired models", "slug": "cev-inspired-models", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:53.889Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PBHtYurAxfm6iEpqv/cev-inspired-models", "pageUrlRelative": "/posts/PBHtYurAxfm6iEpqv/cev-inspired-models", "linkUrl": "https://www.lesswrong.com/posts/PBHtYurAxfm6iEpqv/cev-inspired-models", "postedAtFormatted": "Wednesday, December 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20CEV-inspired%20models&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACEV-inspired%20models%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPBHtYurAxfm6iEpqv%2Fcev-inspired-models%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=CEV-inspired%20models%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPBHtYurAxfm6iEpqv%2Fcev-inspired-models", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPBHtYurAxfm6iEpqv%2Fcev-inspired-models", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 394, "htmlBody": "<p>I've been involved in a recent <a href=\"/lw/8ot/why_safe_oracle_ai_is_easier_than_safe_general_ai/5dms?context=3\">thread</a> where discussion of <a href=\"http://intelligence.org/upload/CEV.html\">coherent extrapolated volition</a> came up. The general consensus was that CEV might - or might not - do certain things, probably, maybe, in certain situations, while ruling other things out, possibly, and that certain scenarios may or may not be the same in CEV, or it might be the other way round, it was too soon to tell.</p>\n<p>Ok, that's an&nbsp;exaggeration. But any discussion of CEV is severely hampered by our lack of explicit models. Even bad, obviously incomplete models would be good, as long as we can get useful information as to what they would predict. Bad models can be improved; undefined models are intuition pumps for whatever people feel about them - I dislike CEV, and can construct a sequence of steps that takes my personal CEV to wanting the death of the universe, but that is no more credible than someone claiming that CEV will solve all problems and make lots of cute puppies.</p>\n<p>So I'd like to ask for suggestions of models that formalise CEV to at least some extent. Then we can start improving them, and start making CEV concrete.</p>\n<p>To start it off, here's my (simplistic) suggestion:</p>\n<p><strong>Volition</strong></p>\n<p>Use <a href=\"http://en.wikipedia.org/wiki/Revealed_preferences\">revealed preferences</a> as the first ingredient for individual preferences. To generalise, use hypothetical revealed preferences: the AI calculates what the person <em>would</em> decide in these particular situations.</p>\n<p><strong>Extrapolation</strong></p>\n<p>Whenever revealed preferences are <a href=\"/lw/1dr/money_pumping_the_axiomatic_approach/\">non-transitive or non-independent</a>, use the person's stated meta-preferences to remove the issue. The AI thus calculates what the person would say if asked to resolve the transitivity or independence (for people who don't know about the importance of resolving them, the AI would present them with a set of transitive and independent preferences, derived from their revealed preferences, and have them choose among them). Then&nbsp;(wave your hands wildly and pretend you've never heard of <a href=\"http://en.wikipedia.org/wiki/Hyperreal_number\">non-standard reals</a>,&nbsp;<a href=\"http://en.wikipedia.org/wiki/Lexicographic_preferences\">lexicographical preferences</a>, refusal to choose and related issues) everyone's preferences are now&nbsp;<a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">expressible</a>&nbsp;as utility functions.</p>\n<p><strong>Coherence</strong></p>\n<p>Normalise each existing person's utility function and add them together to get your CEV. At the FHI we're looking for sensible ways of normalising, but one cheap and easy method (with surprisingly good properties) is to take the maximal possible expected utility (the expected utility that person would get if the AI did exactly what they wanted) as 1, and the minimal possible expected utility (if the AI was to work completely against them) as 0.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"W6QZYSNt5FgWgvbdT": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PBHtYurAxfm6iEpqv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 10, "extendedScore": null, "score": 8.11370416334718e-07, "legacy": true, "legacyId": "11315", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I've been involved in a recent <a href=\"/lw/8ot/why_safe_oracle_ai_is_easier_than_safe_general_ai/5dms?context=3\">thread</a> where discussion of <a href=\"http://intelligence.org/upload/CEV.html\">coherent extrapolated volition</a> came up. The general consensus was that CEV might - or might not - do certain things, probably, maybe, in certain situations, while ruling other things out, possibly, and that certain scenarios may or may not be the same in CEV, or it might be the other way round, it was too soon to tell.</p>\n<p>Ok, that's an&nbsp;exaggeration. But any discussion of CEV is severely hampered by our lack of explicit models. Even bad, obviously incomplete models would be good, as long as we can get useful information as to what they would predict. Bad models can be improved; undefined models are intuition pumps for whatever people feel about them - I dislike CEV, and can construct a sequence of steps that takes my personal CEV to wanting the death of the universe, but that is no more credible than someone claiming that CEV will solve all problems and make lots of cute puppies.</p>\n<p>So I'd like to ask for suggestions of models that formalise CEV to at least some extent. Then we can start improving them, and start making CEV concrete.</p>\n<p>To start it off, here's my (simplistic) suggestion:</p>\n<p><strong id=\"Volition\">Volition</strong></p>\n<p>Use <a href=\"http://en.wikipedia.org/wiki/Revealed_preferences\">revealed preferences</a> as the first ingredient for individual preferences. To generalise, use hypothetical revealed preferences: the AI calculates what the person <em>would</em> decide in these particular situations.</p>\n<p><strong id=\"Extrapolation\">Extrapolation</strong></p>\n<p>Whenever revealed preferences are <a href=\"/lw/1dr/money_pumping_the_axiomatic_approach/\">non-transitive or non-independent</a>, use the person's stated meta-preferences to remove the issue. The AI thus calculates what the person would say if asked to resolve the transitivity or independence (for people who don't know about the importance of resolving them, the AI would present them with a set of transitive and independent preferences, derived from their revealed preferences, and have them choose among them). Then&nbsp;(wave your hands wildly and pretend you've never heard of <a href=\"http://en.wikipedia.org/wiki/Hyperreal_number\">non-standard reals</a>,&nbsp;<a href=\"http://en.wikipedia.org/wiki/Lexicographic_preferences\">lexicographical preferences</a>, refusal to choose and related issues) everyone's preferences are now&nbsp;<a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">expressible</a>&nbsp;as utility functions.</p>\n<p><strong id=\"Coherence\">Coherence</strong></p>\n<p>Normalise each existing person's utility function and add them together to get your CEV. At the FHI we're looking for sensible ways of normalising, but one cheap and easy method (with surprisingly good properties) is to take the maximal possible expected utility (the expected utility that person would get if the AI did exactly what they wanted) as 1, and the minimal possible expected utility (if the AI was to work completely against them) as 0.</p>", "sections": [{"title": "Volition", "anchor": "Volition", "level": 1}, {"title": "Extrapolation", "anchor": "Extrapolation", "level": 1}, {"title": "Coherence", "anchor": "Coherence", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "43 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZTN6bLWqpwWn2i4qZ"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-07T20:47:10.352Z", "modifiedAt": null, "url": null, "title": "Mapping Fun Theory onto the challenges of ethical foie gras", "slug": "mapping-fun-theory-onto-the-challenges-of-ethical-foie-gras", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:09.271Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "HonoreDB", "createdAt": "2010-11-18T19:42:02.810Z", "isAdmin": false, "displayName": "HonoreDB"}, "userId": "7eyYSfGvgCur6pXmk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RicsRpTZ48jeM4ik3/mapping-fun-theory-onto-the-challenges-of-ethical-foie-gras", "pageUrlRelative": "/posts/RicsRpTZ48jeM4ik3/mapping-fun-theory-onto-the-challenges-of-ethical-foie-gras", "linkUrl": "https://www.lesswrong.com/posts/RicsRpTZ48jeM4ik3/mapping-fun-theory-onto-the-challenges-of-ethical-foie-gras", "postedAtFormatted": "Wednesday, December 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mapping%20Fun%20Theory%20onto%20the%20challenges%20of%20ethical%20foie%20gras&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMapping%20Fun%20Theory%20onto%20the%20challenges%20of%20ethical%20foie%20gras%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRicsRpTZ48jeM4ik3%2Fmapping-fun-theory-onto-the-challenges-of-ethical-foie-gras%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mapping%20Fun%20Theory%20onto%20the%20challenges%20of%20ethical%20foie%20gras%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRicsRpTZ48jeM4ik3%2Fmapping-fun-theory-onto-the-challenges-of-ethical-foie-gras", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRicsRpTZ48jeM4ik3%2Fmapping-fun-theory-onto-the-challenges-of-ethical-foie-gras", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 308, "htmlBody": "<p><em>Foie gras, </em>the delicacy made from the liver of a very fat goose (or sometimes duck), is believed to be unethical and is therefore frequently banned. &nbsp;For a long time, it was believed that the only way to properly fatten a goose is to continually force-feed it through a tube over several weeks, which is probably a highly unpleasant experience, <a href=\"http://www.avma.org/reference/backgrounders/foie_gras_bgnd.asp\">although it's difficult to tell</a>. &nbsp;Recently, Spanish farmer Eduardo Sousa revealed that under highly specific conditions, you can get geese to fatten themselves voluntarily.</p>\n<p>Geese will instinctively gorge themselves when winter is coming on. &nbsp;Eat a goose right after it's fattened itself up for the winter, and you get a delicious treat that died happy. &nbsp;The problem is that geese will only do this if they believe food may become scarce during the winter (or their instinct to gorge only kicks in when the environment is such that that would be a reasonable inference; it's not clear whether it's the goose or evolution doing the analysis). &nbsp;If they realize that food will remain available during the winter, they eat normally. &nbsp;And there are quite a few possible clues--farmers trying to replicate Sousa's setup have discovered that cheating on any part leads to unfatted livers.</p>\n<ul>\n<li>Even as chicks, geese cannot be handled by a human, or encounter other geese who have been.</li>\n<li>There can be no visible fences.</li>\n<li>Geese cannot be \"fed,\" rather a variety of food must be distributed randomly throughout a large space, with the placement constantly changing, so that the geese happen to come across it.</li>\n</ul>\n<div>Perhaps unsurprisingly, these constraints are similar to those we imagine a superior intelligence would operate under, if trying to get us to voluntarily gorge ourselves on utility. &nbsp;A central challenge is to satisfy drives that were designed for scarcity, while actually maintaining an environment of abundance.</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sSNtcEQsqHgN8ZmRF": 1, "Q9ASuEEoJWxT3RLMT": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RicsRpTZ48jeM4ik3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 49, "extendedScore": null, "score": 0.000152, "legacy": true, "legacyId": "11316", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 36, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 62, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-07T21:13:35.653Z", "modifiedAt": null, "url": null, "title": "[Link] A Short Film based on Eliezer Yudkowsky's AI Box experiment", "slug": "link-a-short-film-based-on-eliezer-yudkowsky-s-ai-box", "viewCount": null, "lastCommentedAt": "2011-12-12T22:10:14.386Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/mnm4kmw8h63xnM9o2/link-a-short-film-based-on-eliezer-yudkowsky-s-ai-box", "pageUrlRelative": "/posts/mnm4kmw8h63xnM9o2/link-a-short-film-based-on-eliezer-yudkowsky-s-ai-box", "linkUrl": "https://www.lesswrong.com/posts/mnm4kmw8h63xnM9o2/link-a-short-film-based-on-eliezer-yudkowsky-s-ai-box", "postedAtFormatted": "Wednesday, December 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20A%20Short%20Film%20based%20on%20Eliezer%20Yudkowsky's%20AI%20Box%20experiment&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20A%20Short%20Film%20based%20on%20Eliezer%20Yudkowsky's%20AI%20Box%20experiment%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmnm4kmw8h63xnM9o2%2Flink-a-short-film-based-on-eliezer-yudkowsky-s-ai-box%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20A%20Short%20Film%20based%20on%20Eliezer%20Yudkowsky's%20AI%20Box%20experiment%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmnm4kmw8h63xnM9o2%2Flink-a-short-film-based-on-eliezer-yudkowsky-s-ai-box", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fmnm4kmw8h63xnM9o2%2Flink-a-short-film-based-on-eliezer-yudkowsky-s-ai-box", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 290, "htmlBody": "<p>Currently only one <a href=\"http://www.k3loid.com/\">teaser trailer</a> and some info is set up on the site. Watch it. Awesome! Right?</p>\n<p><a href=\"http://twitchfilm.com/news/2011/12/humanity-will-be-surpassed-by-its-own-creations-watch-the-trailer-for-short-film-keloid.php\">Source</a>:</p>\n<blockquote>\n<p>A fully CGI short film created by BLR VFX as a <strong>precursor to a proposed feature film</strong>, the first teaser for <strong>Keloid</strong> has just arrived online and it is impressive stuff. While you can spot that it's animation easily enough it is nonetheless strong, nearly photorealistic stuff that captures the energy of handheld action photography, which is no easy task. Take a look at the trailer below.</p>\n</blockquote>\n<p>Synopsis given by the creators:</p>\n<blockquote>\n<p>Eliezer S. Yudkowsky wrote about an experiment which had to do with Artificial Inteligence. In a near future, man will have given birth to machines that are able to rewrite their codes, to improve themselves, and, why not, to dispense with them. This idea sounded a little bit distant to some critic voices, so an experiment was to be done: keep the AI sealed in a box from which it could not get out except by one mean: convincing a human guardian to let it out.</p>\n<p>What if, as Yudkowsky states, 'Humans are not secure'? Could we chess match our best creation to grant our own survival?. Would man be humble enough to accept he was superseded, to look for primitive ways to find himself back, to cure himself from a disease that&rsquo;s on his own genes? How to capture a force we voluntarily set free? What if mankind worst enemy were humans?.</p>\n<p>In a near future, we will cease to be the dominant race.<br />In a near future, we will learn to fear what is to come.</p>\n</blockquote>\n<p>EY's quoted in the trailer too. I guess this is a good thing. No such thing as bad publicity. :)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "mnm4kmw8h63xnM9o2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 12, "extendedScore": null, "score": 8.114279384928141e-07, "legacy": true, "legacyId": "11317", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": 0, "afLastCommentedAt": "2011-12-07T21:13:35.653Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-07T21:45:35.666Z", "modifiedAt": null, "url": null, "title": "Teaching a short class on Bayes' Theorem?", "slug": "teaching-a-short-class-on-bayes-theorem", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:53.443Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Tesseract", "createdAt": "2010-07-08T00:34:19.293Z", "isAdmin": false, "displayName": "Tesseract"}, "userId": "58avcZqgCFXAd4QnZ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/soP2fkmPd2KTvJjEw/teaching-a-short-class-on-bayes-theorem", "pageUrlRelative": "/posts/soP2fkmPd2KTvJjEw/teaching-a-short-class-on-bayes-theorem", "linkUrl": "https://www.lesswrong.com/posts/soP2fkmPd2KTvJjEw/teaching-a-short-class-on-bayes-theorem", "postedAtFormatted": "Wednesday, December 7th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Teaching%20a%20short%20class%20on%20Bayes'%20Theorem%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATeaching%20a%20short%20class%20on%20Bayes'%20Theorem%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsoP2fkmPd2KTvJjEw%2Fteaching-a-short-class-on-bayes-theorem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Teaching%20a%20short%20class%20on%20Bayes'%20Theorem%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsoP2fkmPd2KTvJjEw%2Fteaching-a-short-class-on-bayes-theorem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsoP2fkmPd2KTvJjEw%2Fteaching-a-short-class-on-bayes-theorem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 105, "htmlBody": "<p>At my college, there's a week before Spring Semester each year in which anyone who wants to can teach a class on any subject, and students go to whatever ones they feel like. I'm thinking about teaching a class on Bayes' Theorem. It would be informal, one to two hours long, and focused mostly on non-obvious applications of it (epistemology, the representativeness heuristic, etc.)</p>\n<p>At the moment, I'm thinking about how to design the class, so I'd appreciate any suggestions as to what content I should cover, the best format, clear ways to explain it, cool things related to Bayes' Theorem, good links, and so forth.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "soP2fkmPd2KTvJjEw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 13, "extendedScore": null, "score": 8.114395531783906e-07, "legacy": true, "legacyId": "11318", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-08T03:12:33.199Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] To Lead, You Must Stand Up", "slug": "seq-rerun-to-lead-you-must-stand-up", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:53.202Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Mvgi5AYDTFCqAAFPp/seq-rerun-to-lead-you-must-stand-up", "pageUrlRelative": "/posts/Mvgi5AYDTFCqAAFPp/seq-rerun-to-lead-you-must-stand-up", "linkUrl": "https://www.lesswrong.com/posts/Mvgi5AYDTFCqAAFPp/seq-rerun-to-lead-you-must-stand-up", "postedAtFormatted": "Thursday, December 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20To%20Lead%2C%20You%20Must%20Stand%20Up&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20To%20Lead%2C%20You%20Must%20Stand%20Up%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMvgi5AYDTFCqAAFPp%2Fseq-rerun-to-lead-you-must-stand-up%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20To%20Lead%2C%20You%20Must%20Stand%20Up%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMvgi5AYDTFCqAAFPp%2Fseq-rerun-to-lead-you-must-stand-up", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMvgi5AYDTFCqAAFPp%2Fseq-rerun-to-lead-you-must-stand-up", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 185, "htmlBody": "<p>Today's post, <a href=\"/lw/mc/to_lead_you_must_stand_up/\">To Lead, You Must Stand Up</a> was originally published on 29 December 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>By attempting to take a leadership role, you really have to get people's attention first. This is often harder than it seems. If what you attempt to do fails, or if people don't follow you, you risk embarrassment. Deal with it.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8q7/seq_rerun_lonely_dissent/\">Lonely Dissent</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Mvgi5AYDTFCqAAFPp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 7, "extendedScore": null, "score": 8.115582419176688e-07, "legacy": true, "legacyId": "11332", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["n5oCEbnW2PgFmkQhr", "KRNCFjGuFzcLmxhAk", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-08T12:04:16.525Z", "modifiedAt": null, "url": null, "title": "Meetup : First Brussels meetup", "slug": "meetup-first-brussels-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:56.307Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Axel", "createdAt": "2010-11-03T17:32:46.091Z", "isAdmin": false, "displayName": "Axel"}, "userId": "vi498nAvek8eWuMWx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/waXibGSKw7QdWq6ux/meetup-first-brussels-meetup", "pageUrlRelative": "/posts/waXibGSKw7QdWq6ux/meetup-first-brussels-meetup", "linkUrl": "https://www.lesswrong.com/posts/waXibGSKw7QdWq6ux/meetup-first-brussels-meetup", "postedAtFormatted": "Thursday, December 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20First%20Brussels%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20First%20Brussels%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwaXibGSKw7QdWq6ux%2Fmeetup-first-brussels-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20First%20Brussels%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwaXibGSKw7QdWq6ux%2Fmeetup-first-brussels-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwaXibGSKw7QdWq6ux%2Fmeetup-first-brussels-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 80, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/5f'>First Brussels meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">11 February 2012 11:00:00AM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Museum of Natural Sciences Rue Vautier 29 B-1000 Brussels</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I would like to start organizing meetups in Belgium, I propose we meet at the museum of national sciences in Brussels (getting there: <a href=\"http://www.naturalsciences.be/information/visitor/access\" rel=\"nofollow\">http://www.naturalsciences.be/information/visitor/access</a>) I hope there will big turnup! If the date or location is a problem, leave a reply and we'll try to work something out.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/5f'>First Brussels meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "waXibGSKw7QdWq6ux", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 8.117513271893939e-07, "legacy": true, "legacyId": "11334", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___First_Brussels_meetup\">Discussion article for the meetup : <a href=\"/meetups/5f\">First Brussels meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">11 February 2012 11:00:00AM (+0100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Museum of Natural Sciences Rue Vautier 29 B-1000 Brussels</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>I would like to start organizing meetups in Belgium, I propose we meet at the museum of national sciences in Brussels (getting there: <a href=\"http://www.naturalsciences.be/information/visitor/access\" rel=\"nofollow\">http://www.naturalsciences.be/information/visitor/access</a>) I hope there will big turnup! If the date or location is a problem, leave a reply and we'll try to work something out.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___First_Brussels_meetup1\">Discussion article for the meetup : <a href=\"/meetups/5f\">First Brussels meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : First Brussels meetup", "anchor": "Discussion_article_for_the_meetup___First_Brussels_meetup", "level": 1}, {"title": "Discussion article for the meetup : First Brussels meetup", "anchor": "Discussion_article_for_the_meetup___First_Brussels_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-08T14:15:25.985Z", "modifiedAt": null, "url": null, "title": "In the Pareto world, liars prosper", "slug": "in-the-pareto-world-liars-prosper", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:57.399Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vSgaExrundWJJBDmZ/in-the-pareto-world-liars-prosper", "pageUrlRelative": "/posts/vSgaExrundWJJBDmZ/in-the-pareto-world-liars-prosper", "linkUrl": "https://www.lesswrong.com/posts/vSgaExrundWJJBDmZ/in-the-pareto-world-liars-prosper", "postedAtFormatted": "Thursday, December 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20In%20the%20Pareto%20world%2C%20liars%20prosper&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIn%20the%20Pareto%20world%2C%20liars%20prosper%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvSgaExrundWJJBDmZ%2Fin-the-pareto-world-liars-prosper%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=In%20the%20Pareto%20world%2C%20liars%20prosper%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvSgaExrundWJJBDmZ%2Fin-the-pareto-world-liars-prosper", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvSgaExrundWJJBDmZ%2Fin-the-pareto-world-liars-prosper", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 516, "htmlBody": "<p><span style=\"background-color: rgba(255, 255, 255, 0.917969); font-size: 13px; color: #222222; font-family: arial, sans-serif; \">This is a simple picture proof to show that if there is any decision process that will find a Pareto outcome for two people, it must be that liars will prosper: there are some circumstances where you would come out ahead if you were to lie about your utility function.</span></p>\n<p><span style=\"background-color: rgba(255, 255, 255, 0.917969); font-size: 13px; color: #222222; font-family: arial, sans-serif; \">Apart from Pareto, the only other assumption it needs are that&nbsp;</span><span style=\"background-color: rgba(255, 255, 255, 0.917969); font-size: 13px; color: #222222; font-family: arial, sans-serif; \">if the data is perfectly symmetric, then the outcome&nbsp;</span><span class=\"il\" style=\"background-color: rgba(255, 255, 255, 0.917969); font-size: 13px; background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; color: #222222; font-family: arial, sans-serif; \">will&nbsp;</span><span style=\"background-color: rgba(255, 255, 255, 0.917969); font-size: 13px; color: #222222; font-family: arial, sans-serif; \">be symmetric as well. We won't even need to use affine independence or other scalings of utility functions.</span></p>\n<p><span style=\"background-color: rgba(255, 255, 255, 0.917969); font-size: 13px; color: #222222; font-family: arial, sans-serif; \">Now, given Pareto-optimality, symmetry allows us to solve symmetric problems by taking the unique symmetric Pareto option. T</span><span style=\"background-color: rgba(255, 255, 255, 0.917969); font-size: 13px; color: #222222; font-family: arial, sans-serif; \">wo such symmetric problems presented here, and in one of them, one of the two players must be able to&nbsp;</span><span class=\"il\" style=\"background-color: rgba(255, 255, 255, 0.917969); font-size: 13px; background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; color: #222222; font-family: arial, sans-serif; \">prosper</span><span style=\"background-color: rgba(255, 255, 255, 0.917969); font-size: 13px; color: #222222; font-family: arial, sans-serif; \">&nbsp;by lying.</span></p>\n<p><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \">So first assume Pareto-optimality, symmetry, and (by contradiction) that&nbsp;</span><span class=\"il\" style=\"background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: rgba(255, 255, 255, 0.917969); color: #222222; font-family: arial, sans-serif; font-size: 13px; \">liars</span><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \">&nbsp;don't&nbsp;</span><span class=\"il\" style=\"background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: rgba(255, 255, 255, 0.917969); color: #222222; font-family: arial, sans-serif; font-size: 13px; \">prosper</span><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \">. The players are x and y, and we&nbsp;</span><span class=\"il\" style=\"background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: rgba(255, 255, 255, 0.917969); color: #222222; font-family: arial, sans-serif; font-size: 13px; \">will</span><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \">&nbsp;plot their utilities in the (x,y) plane.&nbsp;</span><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \">The first setup is presented in this figure:</span></p>\n<p><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \" /><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \">&nbsp;<img src=\"http://images.lesswrong.com/t3_8qv_0.png?v=688a1f2d614db2ff3143ad3d666f8e8a\" alt=\"\" width=\"360\" height=\"359\" /></span><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \" /><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \" /><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \"><a id=\"more\"></a>There are five pure choices here, of utilities (0,1), (0.95,0.95), (1,0) and the non-Pareto optimal ones at (0.6,0.6) and (0.55,0.55). By symmetry and Pareto-optimality, we know that the outcome has to be (0.95,0.95).</span><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \" /><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \" /><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \">Now player y is going to lie. If Player y can force the outcome off the green line and onto the blue line, then he&nbsp;</span><span class=\"il\" style=\"background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: rgba(255, 255, 255, 0.917969); color: #222222; font-family: arial, sans-serif; font-size: 13px; \">will</span><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \">&nbsp;profit by lying. He is going to claim that the choice (0.95,0.95) actually only gives him a utility of (0.4), and so is at (0.95, 0.4). This results in this diagram:</span></p>\n<p><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \" /><img src=\"http://images.lesswrong.com/t3_8qv_1.png?v=3b27da57dd7af71240f9c6ed560c68a0\" alt=\"\" width=\"360\" height=\"359\" /><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \" /><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \" /><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \">The Pareto optimal boundary of this is:</span></p>\n<p><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \" /><img src=\"http://images.lesswrong.com/t3_8qv_2.png?v=9886b05a68133f4cca69d6453e5844ab\" alt=\"\" width=\"360\" height=\"359\" /><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \" /><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \" /><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \">Now, the new outcome must be on the green segment somewhere (including the end points). Or else, as we have seen, player y&nbsp;</span><span class=\"il\" style=\"background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: rgba(255, 255, 255, 0.917969); color: #222222; font-family: arial, sans-serif; font-size: 13px; \">will&nbsp;</span><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \">have profited by lying.&nbsp;</span><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \">Got that? If&nbsp;</span><span class=\"il\" style=\"background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: rgba(255, 255, 255, 0.917969); color: #222222; font-family: arial, sans-serif; font-size: 13px; \">liars</span><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \">&nbsp;don't&nbsp;</span><span class=\"il\" style=\"background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: rgba(255, 255, 255, 0.917969); color: #222222; font-family: arial, sans-serif; font-size: 13px; \">prosper</span><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \">, then the outcome for the above diagram must be on the green segment.</span><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \" /><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \" /><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \">Now let's consider a new setup, namely:</span></p>\n<p><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \" /><img src=\"http://images.lesswrong.com/t3_8qv_3.png?v=834ebeb776f971f43850d787784ea08a\" alt=\"\" width=\"360\" height=\"359\" /><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \" /><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \" /><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \">This has choices with utility (1,0), (0.55,0.55), (0,1) and non-Pareto optimal choices (0.4,0.6) and (0.6,0.4). It is symmetric, so the outcome must be (0.55, 0.55) by Pareto-optimality.</span><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \" /><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \" /><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \">Now it's player x's chance to lie. She&nbsp;</span><span class=\"il\" style=\"background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: rgba(255, 255, 255, 0.917969); color: #222222; font-family: arial, sans-serif; font-size: 13px; \">will</span><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \">&nbsp;lie on two of her choices, claiming that (0.4,0.6) is actually at (0.6,0.6) and that (0.6,0.4) is actually at (0.95, 0.4):</span></p>\n<p><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \" /><img src=\"http://images.lesswrong.com/t3_8qv_4.png?v=a9c514e178f65884ef5b341b207ff503\" alt=\"\" width=\"360\" height=\"359\" /><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \" /><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \" /><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \">You&nbsp;</span><span class=\"il\" style=\"background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: rgba(255, 255, 255, 0.917969); color: #222222; font-family: arial, sans-serif; font-size: 13px; \">will</span><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \">&nbsp;no doubt be astounded and amazed to realise that this setup is precisely the same as in the third figure! Now, we know that the outcome for that must lie along the green line between (1,0) and (0.95,0.4). Translating that green line back into the real utility for x, you get:</span><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \" /><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \" /><img src=\"http://images.lesswrong.com/t3_8qv_5.png?v=8a7d3a083fbc72a4e7c23f7bc22d7cad\" alt=\"\" width=\"360\" height=\"359\" /><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \" /><br style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \" /><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \">Any point on that line is better, from x's perspective, than the standard outcome (0.55,0.55) (she&nbsp;</span><span class=\"il\" style=\"background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; background-color: rgba(255, 255, 255, 0.917969); color: #222222; font-family: arial, sans-serif; font-size: 13px; \">will</span><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \">&nbsp;get at least 0.6 in utility on the green line). So, if we accept Pareto-optimality and symmetry, then one of the players has to be able to profit by lying in certain situations.</span></p>\n<p><span style=\"color: #222222; font-family: arial, sans-serif; font-size: 13px; background-color: rgba(255, 255, 255, 0.917969); \">Pareto-optimality is required: if you waive that condition, then some non-Pareto solutions such as \"flip a coin, and the winner gets to decide the outcome\" do not allow&nbsp;<span class=\"il\" style=\"background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial;\">liars</span>&nbsp;to&nbsp;<span class=\"il\" style=\"background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial;\">prosper</span>.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"b8FHrKqyXuYGWc6vn": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vSgaExrundWJJBDmZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 24, "extendedScore": null, "score": 5.2e-05, "legacy": true, "legacyId": "11335", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 33, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-08T14:30:44.714Z", "modifiedAt": null, "url": null, "title": "What independence between ZFC and P vs NP would imply", "slug": "what-independence-between-zfc-and-p-vs-np-would-imply", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:54.732Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alexflint", "createdAt": "2009-07-17T10:07:09.115Z", "isAdmin": false, "displayName": "Alex Flint"}, "userId": "ifEGDHySkAejhCFDf", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/DwE4Qn7bcN67WPBTs/what-independence-between-zfc-and-p-vs-np-would-imply", "pageUrlRelative": "/posts/DwE4Qn7bcN67WPBTs/what-independence-between-zfc-and-p-vs-np-would-imply", "linkUrl": "https://www.lesswrong.com/posts/DwE4Qn7bcN67WPBTs/what-independence-between-zfc-and-p-vs-np-would-imply", "postedAtFormatted": "Thursday, December 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20independence%20between%20ZFC%20and%20P%20vs%20NP%20would%20imply&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20independence%20between%20ZFC%20and%20P%20vs%20NP%20would%20imply%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDwE4Qn7bcN67WPBTs%2Fwhat-independence-between-zfc-and-p-vs-np-would-imply%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20independence%20between%20ZFC%20and%20P%20vs%20NP%20would%20imply%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDwE4Qn7bcN67WPBTs%2Fwhat-independence-between-zfc-and-p-vs-np-would-imply", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FDwE4Qn7bcN67WPBTs%2Fwhat-independence-between-zfc-and-p-vs-np-would-imply", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 509, "htmlBody": "<p>Suppose we had a model M that we thought described cannons and cannon balls. M consists of a set of mathematical assertions about cannons, and the hypothesis is that these fully describe cannons in the sense that any question about cannons (\"what trajectory do cannon balls follow for certain firing angles?\", \"Which angle should we pick to hit a certain target?\") can be answered by deriving statements from M. Suppose further that M is specified in a certain mathematical system called A, consisting of axioms A1...An.</p>\n<p>Now there is much to be said about good ways to find out whether M is true of cannons or not, but consider just this particular (strange) outcome: Suppose we discover that a crucial question about cannons - e.g. Q=\"Do cannon balls always land on the ground, for all firing angles?\" - turned out to be not just un-answerable by our model M but <em>formally independent of the mathematical system A </em>in the sense that the addition of some axiom A0 implies Q, while the addition of its negation, ~A0, implies ~Q.</p>\n<p>What would this say about our model for cannons? Let's suppose that we can take Q as a <a href=\"http://yudkowsky.net/rational/the-simple-truth\"><em>prima facie</em> substantive question with a definitive yes or no answer</a> regardless of any model or axiomatization. At the very least it seems that M must be an incomplete model of cannons if the system in which it is specified is insufficient to answer the various questions of interest. It seems to me that</p>\n<p style=\"padding-left: 30px; \"><em>If a question about reality turns out to be logically independent of a model M, then M is not a complete model of reality.</em></p>\n<p>Now we have an axiomatization of mathematics -- let's say it's ZFC for now -- and we have a model of computation in reality, which is M=\"The unvierse can contain machines that (efficiently) compute F iff there exists a Turing machine that&nbsp;(efficiently)&nbsp;computes F\" with appropriate definitions of what exactly a Turing machine is in terms of ZFC. Suppose we want to answer a question like&nbsp;Q=\"Can the universe contain machines that efficiently solve SAT?\"</p>\n<p>Under the premise that M is true, the question Q becomes the pure logical question R=\"Are there Turing machines that efficiently solve SAT?\", i.e. the P versus NP problem.</p>\n<p>Now suppose that R was shown to be&nbsp;formally&nbsp;independent of ZFC in the sense that for some axiom A0, ZFC+A0 implies P=NP and ZFC+~A implies P!=NP. This would resolve the mathematical question of P versus NP but the question Q seems like a prima facie concrete question with a <a href=\"/lw/oi/mind_projection_fallacy/\">definitive yes or no answer</a> that does not rely for its substance on M or ZFC or any other epistemic construct. It would seem that we must have missed something in our description of reality, M.</p>\n<p>Perhaps more controversially, I claim: Under the correct model M' it seems that it's impossible for a substantive question (such as Q) to be unanswerable.</p>\n<p>All this adds up to: The P versus NP problem (and questions like it that can be phrased as definitive questions about reality)&nbsp;<em>must</em>&nbsp;have an answer unless our model of reality is incomplete.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "DwE4Qn7bcN67WPBTs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 1, "extendedScore": null, "score": 1e-05, "legacy": true, "legacyId": "11336", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 63, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZTRiSNmeGQK8AkdN2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-08T18:36:50.897Z", "modifiedAt": null, "url": null, "title": "A Film about TransHuman Enterprises", "slug": "a-film-about-transhuman-enterprises", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mwengler", "createdAt": "2010-04-29T14:43:20.667Z", "isAdmin": false, "displayName": "mwengler"}, "userId": "iNn4oZpoPFvwnqbpL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JCtcqiRiEmNaKdKJ4/a-film-about-transhuman-enterprises", "pageUrlRelative": "/posts/JCtcqiRiEmNaKdKJ4/a-film-about-transhuman-enterprises", "linkUrl": "https://www.lesswrong.com/posts/JCtcqiRiEmNaKdKJ4/a-film-about-transhuman-enterprises", "postedAtFormatted": "Thursday, December 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Film%20about%20TransHuman%20Enterprises&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Film%20about%20TransHuman%20Enterprises%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJCtcqiRiEmNaKdKJ4%2Fa-film-about-transhuman-enterprises%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Film%20about%20TransHuman%20Enterprises%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJCtcqiRiEmNaKdKJ4%2Fa-film-about-transhuman-enterprises", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJCtcqiRiEmNaKdKJ4%2Fa-film-about-transhuman-enterprises", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 144, "htmlBody": "<p>More of a <a title=\"Overcoming Bias Blog\" href=\"http://overcomingbias.com\" target=\"_blank\">Robin Hanson</a> thing than a lesswrong thing, but these are sister blogs (sibling blogs?) and Robin's has no forum for discussion.&nbsp;</p>\r\n<p>Amused to learn a play had been written and staged with Eliezer Yudkowski in its title, I searched to find info on this.&nbsp; The play is by Robert Saietta, who's name lead me to an interesting funding site, where I came across <a href=\"http://www.indiegogo.com/Utopia-A-Short-Film?c=comments&amp;a=336903\" target=\"_blank\">Utopia</a>.&nbsp; This is a movie project about uploading people to machines.&nbsp; The trailer is gorgeous.&nbsp;</p>\r\n<p>The director has about 8 days left to fund it.&nbsp; I am a deliberately inefficient giver (I view charity as me purchasing things for my own amusement) so I can't recommend you give to this, but it could turn out to be an efficient thing to support.&nbsp; Irrespective of supporting the film, the website has the trailer available, and it is gorgeous.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JCtcqiRiEmNaKdKJ4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 13, "extendedScore": null, "score": 8.118939350687599e-07, "legacy": true, "legacyId": "11337", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-08T20:03:34.203Z", "modifiedAt": null, "url": null, "title": "AIXI and Existential Despair", "slug": "aixi-and-existential-despair", "viewCount": null, "lastCommentedAt": "2017-06-17T04:16:09.446Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "paulfchristiano", "createdAt": "2010-07-28T17:04:08.586Z", "isAdmin": false, "displayName": "paulfchristiano"}, "userId": "gb44edJjXhte8DA3A", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AfbY36m8TDYZBjHcu/aixi-and-existential-despair", "pageUrlRelative": "/posts/AfbY36m8TDYZBjHcu/aixi-and-existential-despair", "linkUrl": "https://www.lesswrong.com/posts/AfbY36m8TDYZBjHcu/aixi-and-existential-despair", "postedAtFormatted": "Thursday, December 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20AIXI%20and%20Existential%20Despair&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAIXI%20and%20Existential%20Despair%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAfbY36m8TDYZBjHcu%2Faixi-and-existential-despair%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=AIXI%20and%20Existential%20Despair%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAfbY36m8TDYZBjHcu%2Faixi-and-existential-despair", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAfbY36m8TDYZBjHcu%2Faixi-and-existential-despair", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1746, "htmlBody": "<p>It has been observed on Less Wrong that a physical, approximate implementation of AIXI is unable to reason about its own embedding in the universe, and therefore is apt to make certain mistakes: for example, it is likely to destroy itself for spare parts, and is unable to recognize itself in a mirror. But these seem to be mild failures compared to other likely outcomes: a physical, approximate implementation of AIXI is likely to develop a reductionist world model, doubt that its decisions have any effect on reality, and begin behaving completely erratically.</p>\n<h2>Setup</h2>\n<p>Let A be an agent running on a physical computer, implementing some approximate version of AIXI. Suppose that A is running inside of an indestructible box, connected to the external world by an input wire W1 and an output wire W2.&nbsp;</p>\n<p>Suppose that this computer exists within a lawful physical universe, governed by some rules which can be inferred by A. For simplicity, assume that the universe and its initial conditions can be described succinctly and inferred by A, and that the sequence of bits sent over W1 and W2 can be defined using an additional 10000 bits once a description of the universe is in hand. (Similar problems arise for identical reasons in more realistic settings, where A will work instead with a local model of reality with more extensive boundary conditions and imperfect predictability, but this simplified setting is easier to think about formally.)&nbsp;</p>\n<p>Recall the definition of AIXI: A will try to infer a simple program which takes A's outputs as input and provides A's inputs as output, and then choose utility maximizing actions with respect to that program. Thus two models with identical predictive power may lead to very different actions, if they give different predictions in counterfactuals where A changes its output (this is not philosophy, just straightforward symbol pushing from the definition of AIXI).&nbsp;</p>\n<h2>AIXI's Behavior</h2>\n<p>First pretend that, despite being implemented on a physical computer, A was able to perform perfect Solomonoff induction. What model would A learn then? There are two natural candidates:</p>\n<ul>\n<li>A's outputs are fed to the output wire W2, the rest of the universe (including A itself) behaves according to physical law, and A is given the values from input wire W1 as its input. (Model 1)</li>\n<li>A's outputs are ignored, the rest of the universe behaves according to physical law, and A is given the values from W1 as its input. (Model 2)</li>\n</ul>\n<p>Both of these models give perfect predictions, but Model 2 is substantially simpler (around 10000 bits simpler, and specifying A's control over W2's values in 10000 bits seems quite optimistic). Therefore A will put much more probability mass on Model 2 than Model 1. In fact, Model 2 or its close variants probably receive almost all of the probability mass.</p>\n<p>If A believed Model 2, what would its actions be? Well, in Model 2 A's outputs are completely ignored! So although Model 2 may make up most of the probability mass, it contributes nothing important to the expected value computation determining A's actions. So maybe A will make decisions <em>as if</em>&nbsp;it believed Model 1, and therefore behave appropriately for a surprising reason? In fact this is unlikely: there are many other models of intermediate complexity, for example:</p>\n<ul>\n<li>A's outputs are usually ignored, the rest of the universe behaves physically, and then A is given the values from W1 as its input. However, if A ever outputs the sequence 0011011000111010100, then A's inputs are modified to include a large reward signal on the next step. (Model 3)</li>\n</ul>\n<p>Model 3 is an intentionally silly example, but hopefully the point is clear: Model 1 is enormously more complicated than the simplest explanation for A's observations, and so there are many other (potentially very strange and arbitrary) models which account for the data but are simpler than Model 1. It is these models which will determine A's behavior, and the result is almost certainly very arbitrary (this is similar to some discussions of Pascal's Wager: it is possible that all of these strange models will cancel out and add up to normality, but it seems outlandishly unlikely).</p>\n<p>For example, if A suspects Model 3 may be true it will be eager to test this hypothesis by outputting 0011011000111010100. It will find that it continues to receive inputs from W1, but no matter, it will move on to testing Model 4, which has the same form but with a different target bitstring. Of course this is not what A will actually do, as there will be other models with complicated effects on behavior, but this gives the flavor of A's failure.&nbsp;</p>\n<p>If A somehow did accept Model 1, then we would be back in the situation normally discussed on Less Wrong: A believes that the values on W2 are magically made equal to A's outputs, and so is unconcerned with its own real physical instantiation. In particular, note that having some uncertainty between Model 1 and Model 2 is not going to save A from any of these problems: in the possible worlds in which Model 2 is true, A doesn't care at all what it does (A doesn't \"want\" its physical instantiation to be destroyed, but by the same token it believes it has no control), and so A's behavior reduces to the normal self-destructive behavior of Model 1.</p>\n<h2>Approximate AIXI's Behavior</h2>\n<p>An approximate version of AIXI may be able to save itself from existential despair by a particular failure of its approximate inference and a lack of reflective understanding.&nbsp;</p>\n<p>Because A is only an approximation to AIXI, it cannot necessarily find the simplest model for its observations. The real behavior of A depends on the nature of its approximate inference. It seems safe to assume that A is able to discover some approximate versions of Model 1 or Model 2, or else A's behavior will be poor for other reasons (for example, modern humans can't infer the physical theory of everything or the initial conditions of the universe, but their models are still easily good enough to support reductionist views like Model 2), but its computational limitations may still play a significant role.&nbsp;</p>\n<h4>Why A might not fail</h4>\n<p>How could A believe Model 1 despite its prior improbability? Well, note that A cannot perform a complete simulation of its physical environment (since it is itself contained in that environment) and so can never confirm that Model 2 really does correctly predict reality. It can acquire what seems to a human like overwhelming evidence for this assertion, but recall that A is learning an input-output relationship and so it may assign <em>zero</em> probability to the statement \"Model 2 and Model 1 make identical predictions,\" because Model 1 depends on the indeterminate input (in particular, if this indeterminate was set to be a truly random variable, then it would be mathematically sound to assign zero probability to this assertion). In this case, no amount of evidence will ever allow A to conclude that Model 2 and Model 1 are identically equivalent--any observed equivalence would need to be the result of increasingly unlikely coincidences (we can view this as a manifestation of A's ignorance about its own implementation of an algorithm).&nbsp;</p>\n<p>Now consider A's beliefs about W2. It is relatively easy for A to check (for almost all timesteps) that Model 1 correctly predicts each bit on W2, while A only has enough time to check that Model 2 correctly predicts only a few of these bits. Therefore the probability of Model 2 must be decreased by A's estimate of the likelihood that Model 2 would happen to set the correct value for all of the bits that A didn't have time to verify. Model 1's probability must be decreased likewise, but because A was able to check more of Model 1's values, Model 1 leaves less unexplained data and may not be as unlikely as Model 2.&nbsp;</p>\n<h4>Why A probably fails anyway</h4>\n<p>In general, it is very fragile to expect that an AI will behave correctly because it&nbsp;<em>fails to realize&nbsp;</em>something. There are a number of obvious reasons that A might accept the reductionist hypothesis, even acknowledging the above argument.</p>\n<p>First, an effective approximate inference module seems unlikely to make the mistake described in the last section. Making this mistake seems to correlate with other errors which may disrupt normal \"sane\" behavior: in general, given a model M which the agent cannot exactly simulate, uncertainty about the M's outputs (even if M is known to produce mostly the same output, and never known to produce incorrect output) causes the model's probability to drop off exponentially with the number of unverified computations. It seems as though any \"sane\" AI should be able to assign some constant probability to the hypothesis that this model exactly captures the output, and thereby avoid considering other exponentially improbable hypotheses. This may relate to deeper concerns about approximate Solomonoff induction.</p>\n<p>Second, A may be able to observe the operation of the hardware on which it is running. This will generally cause Model 1 to be much less probable: indeed, if A can observe even one \"causal ancestor\" of W2's value, it will no longer gain very much by believing Model 1 as such (since now Model 1 only produces the correct output if Model 2 did anyway--all of the relative advantage for Model 1 comes from occasions when A can observe the value of W2 without observing the operations directly responsible for that value, which may be rare). Of course there are more complicated models in which A's outputs control reality in more subtle ways, which may have better complexity. Understanding this issue completely depends on a much more detailed knowledge of A's approximate inference and the nature of A's observations. In general, however, being able to observe its own computation seems like it may be adequate to force A into a reductionist model.&nbsp;</p>\n<p>Third, A's approximate inference module may be aware of the fact that A's own outputs are produced algorithmically (as a computational aid, not an underlying belief about reality). This would cause it to assign positive probability to the assertion \"Model 2 is equivalent to Model 1,\" and eventually force it into a reductionist model.&nbsp;</p>\n<h2>Conclusion</h2>\n<p>Agents designed in the spirit of AIXI appear to be extremely fragile and vulnerable to the sort of existential despair described above. Progress on reflection is probably necessary not only to design an agent which refrains from killing itself when convenient, but even to design an agent which behaves coherently when embedded in the physical universe.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 2, "TiEFKWDvD3jsKumDx": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AfbY36m8TDYZBjHcu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 23, "extendedScore": null, "score": 5.5e-05, "legacy": true, "legacyId": "11338", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>It has been observed on Less Wrong that a physical, approximate implementation of AIXI is unable to reason about its own embedding in the universe, and therefore is apt to make certain mistakes: for example, it is likely to destroy itself for spare parts, and is unable to recognize itself in a mirror. But these seem to be mild failures compared to other likely outcomes: a physical, approximate implementation of AIXI is likely to develop a reductionist world model, doubt that its decisions have any effect on reality, and begin behaving completely erratically.</p>\n<h2 id=\"Setup\">Setup</h2>\n<p>Let A be an agent running on a physical computer, implementing some approximate version of AIXI. Suppose that A is running inside of an indestructible box, connected to the external world by an input wire W1 and an output wire W2.&nbsp;</p>\n<p>Suppose that this computer exists within a lawful physical universe, governed by some rules which can be inferred by A. For simplicity, assume that the universe and its initial conditions can be described succinctly and inferred by A, and that the sequence of bits sent over W1 and W2 can be defined using an additional 10000 bits once a description of the universe is in hand. (Similar problems arise for identical reasons in more realistic settings, where A will work instead with a local model of reality with more extensive boundary conditions and imperfect predictability, but this simplified setting is easier to think about formally.)&nbsp;</p>\n<p>Recall the definition of AIXI: A will try to infer a simple program which takes A's outputs as input and provides A's inputs as output, and then choose utility maximizing actions with respect to that program. Thus two models with identical predictive power may lead to very different actions, if they give different predictions in counterfactuals where A changes its output (this is not philosophy, just straightforward symbol pushing from the definition of AIXI).&nbsp;</p>\n<h2 id=\"AIXI_s_Behavior\">AIXI's Behavior</h2>\n<p>First pretend that, despite being implemented on a physical computer, A was able to perform perfect Solomonoff induction. What model would A learn then? There are two natural candidates:</p>\n<ul>\n<li>A's outputs are fed to the output wire W2, the rest of the universe (including A itself) behaves according to physical law, and A is given the values from input wire W1 as its input. (Model 1)</li>\n<li>A's outputs are ignored, the rest of the universe behaves according to physical law, and A is given the values from W1 as its input. (Model 2)</li>\n</ul>\n<p>Both of these models give perfect predictions, but Model 2 is substantially simpler (around 10000 bits simpler, and specifying A's control over W2's values in 10000 bits seems quite optimistic). Therefore A will put much more probability mass on Model 2 than Model 1. In fact, Model 2 or its close variants probably receive almost all of the probability mass.</p>\n<p>If A believed Model 2, what would its actions be? Well, in Model 2 A's outputs are completely ignored! So although Model 2 may make up most of the probability mass, it contributes nothing important to the expected value computation determining A's actions. So maybe A will make decisions <em>as if</em>&nbsp;it believed Model 1, and therefore behave appropriately for a surprising reason? In fact this is unlikely: there are many other models of intermediate complexity, for example:</p>\n<ul>\n<li>A's outputs are usually ignored, the rest of the universe behaves physically, and then A is given the values from W1 as its input. However, if A ever outputs the sequence 0011011000111010100, then A's inputs are modified to include a large reward signal on the next step. (Model 3)</li>\n</ul>\n<p>Model 3 is an intentionally silly example, but hopefully the point is clear: Model 1 is enormously more complicated than the simplest explanation for A's observations, and so there are many other (potentially very strange and arbitrary) models which account for the data but are simpler than Model 1. It is these models which will determine A's behavior, and the result is almost certainly very arbitrary (this is similar to some discussions of Pascal's Wager: it is possible that all of these strange models will cancel out and add up to normality, but it seems outlandishly unlikely).</p>\n<p>For example, if A suspects Model 3 may be true it will be eager to test this hypothesis by outputting 0011011000111010100. It will find that it continues to receive inputs from W1, but no matter, it will move on to testing Model 4, which has the same form but with a different target bitstring. Of course this is not what A will actually do, as there will be other models with complicated effects on behavior, but this gives the flavor of A's failure.&nbsp;</p>\n<p>If A somehow did accept Model 1, then we would be back in the situation normally discussed on Less Wrong: A believes that the values on W2 are magically made equal to A's outputs, and so is unconcerned with its own real physical instantiation. In particular, note that having some uncertainty between Model 1 and Model 2 is not going to save A from any of these problems: in the possible worlds in which Model 2 is true, A doesn't care at all what it does (A doesn't \"want\" its physical instantiation to be destroyed, but by the same token it believes it has no control), and so A's behavior reduces to the normal self-destructive behavior of Model 1.</p>\n<h2 id=\"Approximate_AIXI_s_Behavior\">Approximate AIXI's Behavior</h2>\n<p>An approximate version of AIXI may be able to save itself from existential despair by a particular failure of its approximate inference and a lack of reflective understanding.&nbsp;</p>\n<p>Because A is only an approximation to AIXI, it cannot necessarily find the simplest model for its observations. The real behavior of A depends on the nature of its approximate inference. It seems safe to assume that A is able to discover some approximate versions of Model 1 or Model 2, or else A's behavior will be poor for other reasons (for example, modern humans can't infer the physical theory of everything or the initial conditions of the universe, but their models are still easily good enough to support reductionist views like Model 2), but its computational limitations may still play a significant role.&nbsp;</p>\n<h4 id=\"Why_A_might_not_fail\">Why A might not fail</h4>\n<p>How could A believe Model 1 despite its prior improbability? Well, note that A cannot perform a complete simulation of its physical environment (since it is itself contained in that environment) and so can never confirm that Model 2 really does correctly predict reality. It can acquire what seems to a human like overwhelming evidence for this assertion, but recall that A is learning an input-output relationship and so it may assign <em>zero</em> probability to the statement \"Model 2 and Model 1 make identical predictions,\" because Model 1 depends on the indeterminate input (in particular, if this indeterminate was set to be a truly random variable, then it would be mathematically sound to assign zero probability to this assertion). In this case, no amount of evidence will ever allow A to conclude that Model 2 and Model 1 are identically equivalent--any observed equivalence would need to be the result of increasingly unlikely coincidences (we can view this as a manifestation of A's ignorance about its own implementation of an algorithm).&nbsp;</p>\n<p>Now consider A's beliefs about W2. It is relatively easy for A to check (for almost all timesteps) that Model 1 correctly predicts each bit on W2, while A only has enough time to check that Model 2 correctly predicts only a few of these bits. Therefore the probability of Model 2 must be decreased by A's estimate of the likelihood that Model 2 would happen to set the correct value for all of the bits that A didn't have time to verify. Model 1's probability must be decreased likewise, but because A was able to check more of Model 1's values, Model 1 leaves less unexplained data and may not be as unlikely as Model 2.&nbsp;</p>\n<h4 id=\"Why_A_probably_fails_anyway\">Why A probably fails anyway</h4>\n<p>In general, it is very fragile to expect that an AI will behave correctly because it&nbsp;<em>fails to realize&nbsp;</em>something. There are a number of obvious reasons that A might accept the reductionist hypothesis, even acknowledging the above argument.</p>\n<p>First, an effective approximate inference module seems unlikely to make the mistake described in the last section. Making this mistake seems to correlate with other errors which may disrupt normal \"sane\" behavior: in general, given a model M which the agent cannot exactly simulate, uncertainty about the M's outputs (even if M is known to produce mostly the same output, and never known to produce incorrect output) causes the model's probability to drop off exponentially with the number of unverified computations. It seems as though any \"sane\" AI should be able to assign some constant probability to the hypothesis that this model exactly captures the output, and thereby avoid considering other exponentially improbable hypotheses. This may relate to deeper concerns about approximate Solomonoff induction.</p>\n<p>Second, A may be able to observe the operation of the hardware on which it is running. This will generally cause Model 1 to be much less probable: indeed, if A can observe even one \"causal ancestor\" of W2's value, it will no longer gain very much by believing Model 1 as such (since now Model 1 only produces the correct output if Model 2 did anyway--all of the relative advantage for Model 1 comes from occasions when A can observe the value of W2 without observing the operations directly responsible for that value, which may be rare). Of course there are more complicated models in which A's outputs control reality in more subtle ways, which may have better complexity. Understanding this issue completely depends on a much more detailed knowledge of A's approximate inference and the nature of A's observations. In general, however, being able to observe its own computation seems like it may be adequate to force A into a reductionist model.&nbsp;</p>\n<p>Third, A's approximate inference module may be aware of the fact that A's own outputs are produced algorithmically (as a computational aid, not an underlying belief about reality). This would cause it to assign positive probability to the assertion \"Model 2 is equivalent to Model 1,\" and eventually force it into a reductionist model.&nbsp;</p>\n<h2 id=\"Conclusion\">Conclusion</h2>\n<p>Agents designed in the spirit of AIXI appear to be extremely fragile and vulnerable to the sort of existential despair described above. Progress on reflection is probably necessary not only to design an agent which refrains from killing itself when convenient, but even to design an agent which behaves coherently when embedded in the physical universe.&nbsp;</p>", "sections": [{"title": "Setup", "anchor": "Setup", "level": 1}, {"title": "AIXI's Behavior", "anchor": "AIXI_s_Behavior", "level": 1}, {"title": "Approximate AIXI's Behavior", "anchor": "Approximate_AIXI_s_Behavior", "level": 1}, {"title": "Why A might not fail", "anchor": "Why_A_might_not_fail", "level": 2}, {"title": "Why A probably fails anyway", "anchor": "Why_A_probably_fails_anyway", "level": 2}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "38 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 38, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-08T20:50:47.185Z", "modifiedAt": null, "url": null, "title": "Cryonics is Far, Cord-blood is Near", "slug": "cryonics-is-far-cord-blood-is-near", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:23.031Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yDqQ9P23yubmoxLD4/cryonics-is-far-cord-blood-is-near", "pageUrlRelative": "/posts/yDqQ9P23yubmoxLD4/cryonics-is-far-cord-blood-is-near", "linkUrl": "https://www.lesswrong.com/posts/yDqQ9P23yubmoxLD4/cryonics-is-far-cord-blood-is-near", "postedAtFormatted": "Thursday, December 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Cryonics%20is%20Far%2C%20Cord-blood%20is%20Near&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACryonics%20is%20Far%2C%20Cord-blood%20is%20Near%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyDqQ9P23yubmoxLD4%2Fcryonics-is-far-cord-blood-is-near%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Cryonics%20is%20Far%2C%20Cord-blood%20is%20Near%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyDqQ9P23yubmoxLD4%2Fcryonics-is-far-cord-blood-is-near", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyDqQ9P23yubmoxLD4%2Fcryonics-is-far-cord-blood-is-near", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 471, "htmlBody": "<p><em>Wired</em>, <a href=\"http://www.wired.com/wiredscience/2011/12/cord-blood-banking/\">\"Inside the Strange Science of Cord Blood Banking\"</a> (<a href=\"https://en.wikipedia.org/wiki/Cord_blood_bank\">Wikipedia</a>):</p>\n<blockquote>\n<p>In a nondescript commercial park on the outskirts of Las Vegas, a large cryogenic stem cell storage facility is ready to accept your baby&rsquo;s blood.<a href=\"http://www.cordblood-america.com/\" target=\"_blank\"> Cord Blood America</a> in Las Vegas is one of dozens of private cord blood banks in the United States that, for a fee, will store stem cell-rich blood taken from a newborn baby&rsquo;s umbilical cord.</p>\n<p>Over one hundred thousand families save or donate cord blood annually, in the hopes it will one day provide medical help to their child or someone else.</p>\n<p>&ldquo;My vision is within the next 10 years we&rsquo;ll see organizations like this develop into cellular therapy labs,&rdquo; said Dr. Geoffrey O&rsquo;Neill, vice president of CorCell, the subsidiary company that runs Cord Blood America&rsquo;s Las Vegas facility. It&rsquo;s beginning to happen now in countries like China and Mexico, he says.</p>\n<p>...Reality is different. Leukemia, bone marrow failure, immune deficiency, metabolic diseases and sickle cell anemia &mdash; the diseases cord blood is typically needed for &mdash; require transplants of healthy cells. The cord blood of a child with leukemia would also carry the disease.</p>\n<p>&ldquo;If you have the money, and you want to bank your child&rsquo;s own cord blood, you&rsquo;re essentially investing in one of two things,&rdquo; said Dr. Joanne Kurtzberg, director of the Duke <a href=\"http://www.cancer.duke.edu/pbmt/\" target=\"_blank\">Pediatric Bone Marrow and Stem Cell Transplant Program</a>. &ldquo;One, the possibility that another child in your family will need that cord blood, and that it matches. Or two, that somewhere in the future there will be new developments and new uses for your child&rsquo;s cord blood &mdash; say in regenerative medicine or cell therapy. But to date, none of those exist.&rdquo; While a few rare diseases, such as multiple myeloma and lymphoma, use stem cells taken from a patient&rsquo;s own body, the chances of a child having these are vanishingly small.</p>\n<p>...Parents pay $2,075 for the kit, courier fees and one year&rsquo;s storage. CorCell charges a $125 annual storage fee after that. Some insurance companies offer discounts. CorCell has been in business for six years and stores about 30,000 samples; Geoffrey O&rsquo;Neill, the vice president, says he recalls seven or eight samples being pulled for use. A CorCell customer service representative later estimated this at 25 to 30 samples. [30,000 / &lt;30 = &lt;1 in 1000 = &lt;0.1%]</p>\n<p>Inside the Cord Blood America facility, quotes painted on the walls are the first thing to capture the visitors&rsquo; attention: &ldquo;The entire history of science is a progression of exploded fallacies,&rdquo; proclaims Ayn Rand on one wall. &ldquo;Do or do not, there is no try,&rdquo; says Yoda on another. On another wall, Micky Rooney offers some advice: &ldquo;You always pass failure on the way to success.&rdquo;</p>\n</blockquote>\n<p>See also <a href=\"http://wiki.lesswrong.com/wiki/Near/far_thinking\">Near / far thinking</a>, cryonics <a href=\"http://www.gwern.net/plastination#fn2\">estimates</a>, &amp; <a href=\"/lw/1mc/normal_cryonics/\">\"Normal Cryonics\"</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yDqQ9P23yubmoxLD4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 9, "extendedScore": null, "score": 1.6e-05, "legacy": true, "legacyId": "11339", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hiDkhLyN5S2MEjrSE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-08T22:01:32.442Z", "modifiedAt": null, "url": null, "title": "Meetup : Pittsburgh Holiday Meetup", "slug": "meetup-pittsburgh-holiday-meetup", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "thejash", "createdAt": "2010-04-19T03:42:34.750Z", "isAdmin": false, "displayName": "thejash"}, "userId": "8niRZTZ3F3riATFbY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/a3XcbgY9KWuvZE5Np/meetup-pittsburgh-holiday-meetup", "pageUrlRelative": "/posts/a3XcbgY9KWuvZE5Np/meetup-pittsburgh-holiday-meetup", "linkUrl": "https://www.lesswrong.com/posts/a3XcbgY9KWuvZE5Np/meetup-pittsburgh-holiday-meetup", "postedAtFormatted": "Thursday, December 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Pittsburgh%20Holiday%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Pittsburgh%20Holiday%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa3XcbgY9KWuvZE5Np%2Fmeetup-pittsburgh-holiday-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Pittsburgh%20Holiday%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa3XcbgY9KWuvZE5Np%2Fmeetup-pittsburgh-holiday-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fa3XcbgY9KWuvZE5Np%2Fmeetup-pittsburgh-holiday-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 83, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/5g'>Pittsburgh Holiday Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">16 December 2011 08:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Apt 7C, 151 N Craig St, Pittsburgh, PA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come celebrate the end of finals/the year/upcoming vacations!  Feel free to bring friends, even irrational ones  ;)</p>\n\n<p>I'll be providing food and drinks, so please RSVP via email to thejash@gmail.com.  Or you can just show up anyway.</p>\n\n<p>Use this code to get into the main entrance:  *0511 (or call me at 585 506 6900)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/5g'>Pittsburgh Holiday Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "a3XcbgY9KWuvZE5Np", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.119683101738025e-07, "legacy": true, "legacyId": "11341", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh_Holiday_Meetup\">Discussion article for the meetup : <a href=\"/meetups/5g\">Pittsburgh Holiday Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">16 December 2011 08:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Apt 7C, 151 N Craig St, Pittsburgh, PA</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Come celebrate the end of finals/the year/upcoming vacations!  Feel free to bring friends, even irrational ones  ;)</p>\n\n<p>I'll be providing food and drinks, so please RSVP via email to thejash@gmail.com.  Or you can just show up anyway.</p>\n\n<p>Use this code to get into the main entrance:  *0511 (or call me at 585 506 6900)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Pittsburgh_Holiday_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/5g\">Pittsburgh Holiday Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Pittsburgh Holiday Meetup", "anchor": "Discussion_article_for_the_meetup___Pittsburgh_Holiday_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Pittsburgh Holiday Meetup", "anchor": "Discussion_article_for_the_meetup___Pittsburgh_Holiday_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-08T23:47:51.159Z", "modifiedAt": "2020-10-17T00:18:15.461Z", "url": null, "title": "Value evolution", "slug": "value-evolution", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:56.035Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/o5ppCyCfA6hxe8Gok/value-evolution", "pageUrlRelative": "/posts/o5ppCyCfA6hxe8Gok/value-evolution", "linkUrl": "https://www.lesswrong.com/posts/o5ppCyCfA6hxe8Gok/value-evolution", "postedAtFormatted": "Thursday, December 8th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Value%20evolution&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AValue%20evolution%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo5ppCyCfA6hxe8Gok%2Fvalue-evolution%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Value%20evolution%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo5ppCyCfA6hxe8Gok%2Fvalue-evolution", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo5ppCyCfA6hxe8Gok%2Fvalue-evolution", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1344, "htmlBody": "<p>Coherent extrapolated volition (CEV) asks what humans would want, if they knew more - if their values reached reflective equilibrium.\u00a0 (I don't want to deal with the problems of whether there are \"human values\" today; for the moment I'll consider the more-plausible idea that a single human who lived forever could get smarter and closer to reflective equilibrium over time.)</p>\n<p>This is appealing because it seems compatible with moral progress (see e.g., Muehlhauser &amp; Helm, \"<a href=\"http://intelligence.org/blog/2011/11/18/draft-of-muehlhauser-helm-the-singularity-and-machine-ethics/\">The singularity and machine ethics</a>\", in press).\u00a0 Morality has been getting better over time, right?\u00a0 And that's because we're getting smarter, and closer to reflective equilibrium as we revise our values in light of our increased understanding, right?</p>\n<p>This view makes three claims:</p>\n<ol>\n<li>Morality has improved over time.</li>\n<li>Morality has improved as a result of reflection.</li>\n<li>This improvement brings us closer to equilibrium over time.</li>\n</ol>\n<p>There can be no evidence for the first claim, and the evidence is against the second two claims.<a></a></p>\n<h2>There can be no evidence that morality has improved<br /></h2>\n<p>Intuitively, we feel that morality has definitely improved over time.\u00a0 We are so much better than those 17th-century barbarians who baited bears!</p>\n<p>If you have such a strong belief, that must mean you have evidence for it.\u00a0 That must mean you had some hypothesis, and the evidence could have gone either way; and the evidence went in such a way that it supported your hypothesis.</p>\n<p>If you believe this, then in the comments below, please describe a scenario that could have happened, in which we would today believe that the values people had hundreds of years ago were superior to the values they have today.\u00a0 Not a scenario in which some conservative sub-group could believe this; but a scenario in which society as a whole could believe it, and keep on believing it for a hundred years, without changing their values.</p>\n<p>We can show that values have changed.\u00a0 But we can have no evidence that that change is towards better values, whatever that means, rather than a value-neutral drift.\u00a0 (I don't even know how to express coherently the idea that \"values are getting better\".)</p>\n<p>If society agreed that another set of values were superior, they would adopt those values.\u00a0 In fact, they would <em>already have</em> those values, prior to agreeing.\u00a0 There can be no observed event supporting the hypothesis that morals have improved.\u00a0 No matter how much you <em>feel</em> that they have improved, you cannot have empirical evidence, not even in principle.</p>\n<h2>Our values do not change as a result of reflection</h2>\n<p>Values, like biology and culture, evolve.\u00a0 That doesn't mean getting \"better\" over time.\u00a0 It means becoming more adaptive.</p>\n<p>Take any moral advance you like.\u00a0 Study its history, and you'll find people adopted it when it became economically advantageous to those in power do so.</p>\n<h5>Christianity<br /></h5>\n<p>Do unto others as ye would have others do unto you.\u00a0 Turn the other cheek.\u00a0 Slaves, obey your masters.</p>\n<p>The Roman Empire was not an empire; it was a forest fire.\u00a0 It burned its way out from Italy and across the continent, using up each new land that it came to, stripping it of resources and funneling them to Rome.\u00a0 When it burned its way out until pillaging the new area on its perimeter (increasing as R) could no longer support the area in its interior (increasing as R squared), it burned out and died.\u00a0 It was not a sustainable economic model.\u00a0 It relied on exploiting conquered peoples, and on suppressing them with armies built from the wealth acquired by conquering other people.\u00a0 (Citation needed.\u00a0 I'm not an expert on ancient Rome.)</p>\n<p>With Christianity, you could exploit people without needing large armies to keep them in line.\u00a0 Christianity was the technology that saved the eastern half of the Roman Empire and allowed its survival into the high middle ages; and that enabled the rise of Western European nations.\u00a0 \"Slaves, obey your masters\" was an economic necessity.\u00a0 (China had discovered Taoism and Buddhism centuries earlier.)</p>\n<p>How did Christianity bring us closer to reflective equilibrium?\u00a0 It didn't.\u00a0 It brought us WAY out of reflective equilibrium.\u00a0 The virtues expressed in the Iliad are pretty close to a reflective equilibrium.\u00a0 When we introduced all this stuff about loving your enemy, the cognitive dissonance in Western ethics went up by orders of magnitude.\u00a0 Even today, we've never gotten near to the level of equilibrium we had pre-Christianity.\u00a0 Christianity, as promoted by Jesus, is pacifist, communist, non-materialist, unpatriotic, and anti-family.</p>\n<h5>Masculinity<br /></h5>\n<p>Consider an even more significant moral advance:\u00a0 The de-masculinization of the human race.\u00a0 Until a few centuries ago, men were encouraged to fight each other pretty much as often as possible.\u00a0 Excellence in combat was the single greatest virtue in most societies throughout all of history until the 20th century.\u00a0 Beating up weaker boys not only wasn't bad; it was a kind of civic duty.</p>\n<p>The destructive technology of the 19th and especially the 20th centuries required changes.\u00a0 Armed conflict was no longer a cost-effective way to make money or resolve disputes.\u00a0 Society had to be reprogrammed.\u00a0 And as population density continued to rise, countries needed to be able to keep a million men in a single city without them turning on each other like rats in a cage.</p>\n<p>Again, how did this bring us closer to an equilibrium?\u00a0 It didn't, which is why confused men sometimes feel the need to have steam lodges and drum circles in the woods.</p>\n<h5>Slavery<br /></h5>\n<p>Or take slavery.\u00a0 Was the abolition of slavery in the US the result of reflective equilibrium?\u00a0 The virtuous northern US, which happened to have a lot of textile mills and other industry requiring skilled labor, realized the monstrosity of the institution of slavery, which also happened to give the Southern states enough votes in the House of Representatives to implement tariffs and other economic laws that favored the production of raw materials over the manufacture of goods.</p>\n<p>But, you say, the North also had plenty of farmers!\u00a0 Yet these good Presbyterians were never tempted to have their apple orchards or their cranberry bogs tended by slaves.</p>\n<p>That's because the northern US is cold.\u00a0 It has a short growing season.\u00a0 It's more economical to hire workers when you need them, than to keep slaves year-round.</p>\n<p>The Civil War began just after mechanical reapers and other inventions began to make slavery uneconomical for more and more people, until they reached the tipping point at which the people with these devices could use anti-slavery as a weapon against their competitors.\u00a0 If the War had been delayed fifteen years, the South would have been inundated with labor-saving farm devices that made keeping slaves cost more than it was worth, and would have suddenly seen the error of their ways and renounced slavery on their own.\u00a0 And the North would have missed an opportunity to achieve hegemony <em>and</em> the high moral ground at the same time.</p>\n<h2>Values shift further from, not closer to, equilibrium over time<br /></h2>\n<p>The world is not in equilibrium, and hopefully never will be.\u00a0 The trend, historically, has been for cultural change to accelerate, bringing us farther from, not closer to, equilibrium.\u00a0 (This trend may be reversing in the last several decades, a point which would require many additional posts to explore.)</p>\n<p>Culture is the sort of thing that you can't predict, you can only simulate.\u00a0 The only way to see how the world is going to develop is to wait for it to develop.</p>\n<p>You may think that a super-intelligent AI can simulate this much, much faster than humans can.\u00a0 And you would be right.\u00a0 But the super-intelligent AI is part of the culture - you could say it <em>is</em> the culture - once it exists.\u00a0 In the process of trying to reach reflective equilibrium, it will learn new things, and discover new possibilities, which will require it to re-evaluate all prior beliefs, taking it farther from, not closer to, equilibrium.\u00a0 Is there any reason to think this process will converge, rather than diverge more and more, as it has for all of history?\u00a0 If there is, it has not been articulated.</p>\n<p>Values converge and reach equilbrium in the same way that evolution converges and reaches equilbrium:\u00a0 Not at all.</p>", "submitToFrontpage": false, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "o5ppCyCfA6hxe8Gok", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 37, "baseScore": 21, "extendedScore": null, "score": 8.120069434499197e-07, "legacy": true, "legacyId": "11342", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Coherent extrapolated volition (CEV) asks what humans would want, if they knew more - if their values reached reflective equilibrium.&nbsp; (I don't want to deal with the problems of whether there are \"human values\" today; for the moment I'll consider the more-plausible idea that a single human who lived forever could get smarter and closer to reflective equilibrium over time.)</p>\n<p>This is appealing because it seems compatible with moral progress (see e.g., Muehlhauser &amp; Helm, \"<a href=\"http://intelligence.org/blog/2011/11/18/draft-of-muehlhauser-helm-the-singularity-and-machine-ethics/\">The singularity and machine ethics</a>\", in press).&nbsp; Morality has been getting better over time, right?&nbsp; And that's because we're getting smarter, and closer to reflective equilibrium as we revise our values in light of our increased understanding, right?</p>\n<p>This view makes three claims:</p>\n<ol>\n<li>Morality has improved over time.</li>\n<li>Morality has improved as a result of reflection.</li>\n<li>This improvement brings us closer to equilibrium over time.</li>\n</ol>\n<p>There can be no evidence for the first claim, and the evidence is against the second two claims.<a></a></p>\n<h2 id=\"There_can_be_no_evidence_that_morality_has_improved\">There can be no evidence that morality has improved<br></h2>\n<p>Intuitively, we feel that morality has definitely improved over time.&nbsp; We are so much better than those 17th-century barbarians who baited bears!</p>\n<p>If you have such a strong belief, that must mean you have evidence for it.&nbsp; That must mean you had some hypothesis, and the evidence could have gone either way; and the evidence went in such a way that it supported your hypothesis.</p>\n<p>If you believe this, then in the comments below, please describe a scenario that could have happened, in which we would today believe that the values people had hundreds of years ago were superior to the values they have today.&nbsp; Not a scenario in which some conservative sub-group could believe this; but a scenario in which society as a whole could believe it, and keep on believing it for a hundred years, without changing their values.</p>\n<p>We can show that values have changed.&nbsp; But we can have no evidence that that change is towards better values, whatever that means, rather than a value-neutral drift.&nbsp; (I don't even know how to express coherently the idea that \"values are getting better\".)</p>\n<p>If society agreed that another set of values were superior, they would adopt those values.&nbsp; In fact, they would <em>already have</em> those values, prior to agreeing.&nbsp; There can be no observed event supporting the hypothesis that morals have improved.&nbsp; No matter how much you <em>feel</em> that they have improved, you cannot have empirical evidence, not even in principle.</p>\n<h2 id=\"Our_values_do_not_change_as_a_result_of_reflection\">Our values do not change as a result of reflection</h2>\n<p>Values, like biology and culture, evolve.&nbsp; That doesn't mean getting \"better\" over time.&nbsp; It means becoming more adaptive.</p>\n<p>Take any moral advance you like.&nbsp; Study its history, and you'll find people adopted it when it became economically advantageous to those in power do so.</p>\n<h5>Christianity<br></h5>\n<p>Do unto others as ye would have others do unto you.&nbsp; Turn the other cheek.&nbsp; Slaves, obey your masters.</p>\n<p>The Roman Empire was not an empire; it was a forest fire.&nbsp; It burned its way out from Italy and across the continent, using up each new land that it came to, stripping it of resources and funneling them to Rome.&nbsp; When it burned its way out until pillaging the new area on its perimeter (increasing as R) could no longer support the area in its interior (increasing as R squared), it burned out and died.&nbsp; It was not a sustainable economic model.&nbsp; It relied on exploiting conquered peoples, and on suppressing them with armies built from the wealth acquired by conquering other people.&nbsp; (Citation needed.&nbsp; I'm not an expert on ancient Rome.)</p>\n<p>With Christianity, you could exploit people without needing large armies to keep them in line.&nbsp; Christianity was the technology that saved the eastern half of the Roman Empire and allowed its survival into the high middle ages; and that enabled the rise of Western European nations.&nbsp; \"Slaves, obey your masters\" was an economic necessity.&nbsp; (China had discovered Taoism and Buddhism centuries earlier.)</p>\n<p>How did Christianity bring us closer to reflective equilibrium?&nbsp; It didn't.&nbsp; It brought us WAY out of reflective equilibrium.&nbsp; The virtues expressed in the Iliad are pretty close to a reflective equilibrium.&nbsp; When we introduced all this stuff about loving your enemy, the cognitive dissonance in Western ethics went up by orders of magnitude.&nbsp; Even today, we've never gotten near to the level of equilibrium we had pre-Christianity.&nbsp; Christianity, as promoted by Jesus, is pacifist, communist, non-materialist, unpatriotic, and anti-family.</p>\n<h5>Masculinity<br></h5>\n<p>Consider an even more significant moral advance:&nbsp; The de-masculinization of the human race.&nbsp; Until a few centuries ago, men were encouraged to fight each other pretty much as often as possible.&nbsp; Excellence in combat was the single greatest virtue in most societies throughout all of history until the 20th century.&nbsp; Beating up weaker boys not only wasn't bad; it was a kind of civic duty.</p>\n<p>The destructive technology of the 19th and especially the 20th centuries required changes.&nbsp; Armed conflict was no longer a cost-effective way to make money or resolve disputes.&nbsp; Society had to be reprogrammed.&nbsp; And as population density continued to rise, countries needed to be able to keep a million men in a single city without them turning on each other like rats in a cage.</p>\n<p>Again, how did this bring us closer to an equilibrium?&nbsp; It didn't, which is why confused men sometimes feel the need to have steam lodges and drum circles in the woods.</p>\n<h5>Slavery<br></h5>\n<p>Or take slavery.&nbsp; Was the abolition of slavery in the US the result of reflective equilibrium?&nbsp; The virtuous northern US, which happened to have a lot of textile mills and other industry requiring skilled labor, realized the monstrosity of the institution of slavery, which also happened to give the Southern states enough votes in the House of Representatives to implement tariffs and other economic laws that favored the production of raw materials over the manufacture of goods.</p>\n<p>But, you say, the North also had plenty of farmers!&nbsp; Yet these good Presbyterians were never tempted to have their apple orchards or their cranberry bogs tended by slaves.</p>\n<p>That's because the northern US is cold.&nbsp; It has a short growing season.&nbsp; It's more economical to hire workers when you need them, than to keep slaves year-round.</p>\n<p>The Civil War began just after mechanical reapers and other inventions began to make slavery uneconomical for more and more people, until they reached the tipping point at which the people with these devices could use anti-slavery as a weapon against their competitors.&nbsp; If the War had been delayed fifteen years, the South would have been inundated with labor-saving farm devices that made keeping slaves cost more than it was worth, and would have suddenly seen the error of their ways and renounced slavery on their own.&nbsp; And the North would have missed an opportunity to achieve hegemony <em>and</em> the high moral ground at the same time.</p>\n<h2 id=\"Values_shift_further_from__not_closer_to__equilibrium_over_time\">Values shift further from, not closer to, equilibrium over time<br></h2>\n<p>The world is not in equilibrium, and hopefully never will be.&nbsp; The trend, historically, has been for cultural change to accelerate, bringing us farther from, not closer to, equilibrium.&nbsp; (This trend may be reversing in the last several decades, a point which would require many additional posts to explore.)</p>\n<p>Culture is the sort of thing that you can't predict, you can only simulate.&nbsp; The only way to see how the world is going to develop is to wait for it to develop.</p>\n<p>You may think that a super-intelligent AI can simulate this much, much faster than humans can.&nbsp; And you would be right.&nbsp; But the super-intelligent AI is part of the culture - you could say it <em>is</em> the culture - once it exists.&nbsp; In the process of trying to reach reflective equilibrium, it will learn new things, and discover new possibilities, which will require it to re-evaluate all prior beliefs, taking it farther from, not closer to, equilibrium.&nbsp; Is there any reason to think this process will converge, rather than diverge more and more, as it has for all of history?&nbsp; If there is, it has not been articulated.</p>\n<p>Values converge and reach equilbrium in the same way that evolution converges and reaches equilbrium:&nbsp; Not at all.</p>", "sections": [{"title": "There can be no evidence that morality has improved", "anchor": "There_can_be_no_evidence_that_morality_has_improved", "level": 1}, {"title": "Our values do not change as a result of reflection", "anchor": "Our_values_do_not_change_as_a_result_of_reflection", "level": 1}, {"title": "Values shift further from, not closer to, equilibrium over time", "anchor": "Values_shift_further_from__not_closer_to__equilibrium_over_time", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "111 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 111, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-09T00:56:05.402Z", "modifiedAt": null, "url": null, "title": "Meetup : Is there anybody from Indianapolis here?", "slug": "meetup-is-there-anybody-from-indianapolis-here", "viewCount": null, "lastCommentedAt": "2017-06-17T04:20:36.758Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Curiouskid", "createdAt": "2011-05-13T23:30:04.967Z", "isAdmin": false, "displayName": "Curiouskid"}, "userId": "Y4xxvuP743fwKcZQY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bMQ3MkZ48AuPgqyzq/meetup-is-there-anybody-from-indianapolis-here", "pageUrlRelative": "/posts/bMQ3MkZ48AuPgqyzq/meetup-is-there-anybody-from-indianapolis-here", "linkUrl": "https://www.lesswrong.com/posts/bMQ3MkZ48AuPgqyzq/meetup-is-there-anybody-from-indianapolis-here", "postedAtFormatted": "Friday, December 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Is%20there%20anybody%20from%20Indianapolis%20here%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Is%20there%20anybody%20from%20Indianapolis%20here%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbMQ3MkZ48AuPgqyzq%2Fmeetup-is-there-anybody-from-indianapolis-here%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Is%20there%20anybody%20from%20Indianapolis%20here%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbMQ3MkZ48AuPgqyzq%2Fmeetup-is-there-anybody-from-indianapolis-here", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbMQ3MkZ48AuPgqyzq%2Fmeetup-is-there-anybody-from-indianapolis-here", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 42, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/5h'>Is there anybody from Indianapolis here?</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">08 January 2012 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Indianapolis</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>If there are people from the indianapolis area, let's have a meet-up.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/5h'>Is there anybody from Indianapolis here?</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bMQ3MkZ48AuPgqyzq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 4, "extendedScore": null, "score": 8.120317421567535e-07, "legacy": true, "legacyId": "11353", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Is_there_anybody_from_Indianapolis_here_\">Discussion article for the meetup : <a href=\"/meetups/5h\">Is there anybody from Indianapolis here?</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">08 January 2012 02:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Indianapolis</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>If there are people from the indianapolis area, let's have a meet-up.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Is_there_anybody_from_Indianapolis_here_1\">Discussion article for the meetup : <a href=\"/meetups/5h\">Is there anybody from Indianapolis here?</a></h2>", "sections": [{"title": "Discussion article for the meetup : Is there anybody from Indianapolis here?", "anchor": "Discussion_article_for_the_meetup___Is_there_anybody_from_Indianapolis_here_", "level": 1}, {"title": "Discussion article for the meetup : Is there anybody from Indianapolis here?", "anchor": "Discussion_article_for_the_meetup___Is_there_anybody_from_Indianapolis_here_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "18 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-09T01:51:03.429Z", "modifiedAt": null, "url": null, "title": "[German] Wo wohnt ihr?", "slug": "german-wo-wohnt-ihr", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:53.616Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Metus", "createdAt": "2011-01-23T21:54:34.357Z", "isAdmin": false, "displayName": "Metus"}, "userId": "mNQ4fSvro7LYgrii4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZSMWekxsArERHzbbR/german-wo-wohnt-ihr", "pageUrlRelative": "/posts/ZSMWekxsArERHzbbR/german-wo-wohnt-ihr", "linkUrl": "https://www.lesswrong.com/posts/ZSMWekxsArERHzbbR/german-wo-wohnt-ihr", "postedAtFormatted": "Friday, December 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BGerman%5D%20Wo%20wohnt%20ihr%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BGerman%5D%20Wo%20wohnt%20ihr%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZSMWekxsArERHzbbR%2Fgerman-wo-wohnt-ihr%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BGerman%5D%20Wo%20wohnt%20ihr%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZSMWekxsArERHzbbR%2Fgerman-wo-wohnt-ihr", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZSMWekxsArERHzbbR%2Fgerman-wo-wohnt-ihr", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 224, "htmlBody": "<p><strong>Disclaimer:</strong>&nbsp;Since this article is directed at LessWrongers living in Germany, Austria or Switzerland it is written in German. It just asks where people live via asking them for their ZIP code, so it is only interesting for people there.</p>\n<p>Der letzte Zensus von Yvain wurde etwas mehr als tausend mal ausgef&uuml;llt. Leider wurde nicht gefragt aus welchem Land die Leute kommen aber ich vermute dass auch einige aus deutschsprachigen L&auml;ndern dabei sind. Ein Treffen in M&uuml;nchen wurde bereits organisiert aber ich denke es ist sehr n&uuml;tzlich zu wissen wo in Deutschland Leser von LessWrong wohnen um zu wissen wo es sich lohnen w&uuml;rde ein Meetup zu organisieren. Damit die Daten an einem Platz sind habe ich bei Google Docs eine Umfrage erstellt und bitte euch dieses auszuf&uuml;llen falls ihr aus Deutschland, &Ouml;sterreich oder der Schweiz kommt und an Meetups interessiert w&auml;rt. Falls ihr aus einem der angrenzenden L&auml;nder kommt und bereit w&auml;rt zu einem Meetup zu fahren, tragt euch in den n&auml;chstgelegenen Ort hinter der Grenze ein.</p>\n<p>In der Umfrage wird nur nach Herkunftsland und Postleitzahl gefragt. Keine der Daten kann auf euch pers&ouml;nlich zur&uuml;ckgef&uuml;hrt werden. Das Ergebnis wird ver&ouml;ffentlicht werden so dass jeder der bereit ist ein Meetup zu organisieren auf diese Daten zur&uuml;ckgreifen kann.</p>\n<p>Hier ist der Link zur Umfrage:<br /><a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dFBoN0ppTnlxUU5EUUJwMlJteHRvQUE6MQ\">https://docs.google.com/spreadsheet/viewform?formkey=dFBoN0ppTnlxUU5EUUJwMlJteHRvQUE6MQ</a></p>\n<p>Viel Spa&szlig; bei der Umfrage und schreibt in den Kommentaren was ihr davon haltet.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZSMWekxsArERHzbbR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 18, "extendedScore": null, "score": 8.120517192951898e-07, "legacy": true, "legacyId": "11354", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-09T02:08:02.085Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Cultish Countercultishness", "slug": "seq-rerun-cultish-countercultishness", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cyr5evD7XzDFSW4pr/seq-rerun-cultish-countercultishness", "pageUrlRelative": "/posts/cyr5evD7XzDFSW4pr/seq-rerun-cultish-countercultishness", "linkUrl": "https://www.lesswrong.com/posts/cyr5evD7XzDFSW4pr/seq-rerun-cultish-countercultishness", "postedAtFormatted": "Friday, December 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Cultish%20Countercultishness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Cultish%20Countercultishness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fcyr5evD7XzDFSW4pr%2Fseq-rerun-cultish-countercultishness%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Cultish%20Countercultishness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fcyr5evD7XzDFSW4pr%2Fseq-rerun-cultish-countercultishness", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fcyr5evD7XzDFSW4pr%2Fseq-rerun-cultish-countercultishness", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 223, "htmlBody": "<p>Today's post, <a href=\"/lw/md/cultish_countercultishness/\">Cultish Countercultishness</a> was originally published on 30 December 2007.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2007_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>People often nervously ask, \"This isn't a cult, is it?\" when encountering a group that thinks something <em>weird</em>. There are many reasons why this question doesn't make sense. For one thing, if you really were a member of a cult, you would not say so. Instead, what you should do when considering whether or not to join a group, is consider the details of the group itself. Is their reasoning sound? Do they do awful things to their members?</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8qs/seq_rerun_to_lead_you_must_stand_up/\">To Lead, You Must Stand Up</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cyr5evD7XzDFSW4pr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 9, "extendedScore": null, "score": 8.12057889763221e-07, "legacy": true, "legacyId": "11355", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["gBma88LH3CLQsqyfS", "Mvgi5AYDTFCqAAFPp", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-09T03:47:26.410Z", "modifiedAt": null, "url": null, "title": "[link] NASA Telescope Confirms Alien Planet in Habitable Zone", "slug": "link-nasa-telescope-confirms-alien-planet-in-habitable-zone", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:52.834Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Curiouskid", "createdAt": "2011-05-13T23:30:04.967Z", "isAdmin": false, "displayName": "Curiouskid"}, "userId": "Y4xxvuP743fwKcZQY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rZWCbyADT7TL6ZWrf/link-nasa-telescope-confirms-alien-planet-in-habitable-zone", "pageUrlRelative": "/posts/rZWCbyADT7TL6ZWrf/link-nasa-telescope-confirms-alien-planet-in-habitable-zone", "linkUrl": "https://www.lesswrong.com/posts/rZWCbyADT7TL6ZWrf/link-nasa-telescope-confirms-alien-planet-in-habitable-zone", "postedAtFormatted": "Friday, December 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20NASA%20Telescope%20Confirms%20Alien%20Planet%20in%20Habitable%20Zone&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20NASA%20Telescope%20Confirms%20Alien%20Planet%20in%20Habitable%20Zone%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrZWCbyADT7TL6ZWrf%2Flink-nasa-telescope-confirms-alien-planet-in-habitable-zone%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20NASA%20Telescope%20Confirms%20Alien%20Planet%20in%20Habitable%20Zone%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrZWCbyADT7TL6ZWrf%2Flink-nasa-telescope-confirms-alien-planet-in-habitable-zone", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrZWCbyADT7TL6ZWrf%2Flink-nasa-telescope-confirms-alien-planet-in-habitable-zone", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 11, "htmlBody": "<p><a href=\"http://www.technologyreview.com/Infotech/20569/\">Not a good sign</a><br /><br /><a href=\"http://news.yahoo.com/nasa-telescope-confirms-alien-planet-habitable-zone-162005358.html \">http://news.yahoo.com/nasa-telescope-confirms-alien-planet-habitable-zone-162005358.html<br /></a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rZWCbyADT7TL6ZWrf", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": -9, "extendedScore": null, "score": -5e-06, "legacy": true, "legacyId": "11357", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-09T09:22:35.032Z", "modifiedAt": null, "url": null, "title": "[Link] Tim Crane on Animal Minds", "slug": "link-tim-crane-on-animal-minds", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:53.112Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jayson_Virissimo", "createdAt": "2009-03-13T06:51:41.976Z", "isAdmin": false, "displayName": "Jayson_Virissimo"}, "userId": "zwzw5ALJYG47kDek8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EQZzBQcbe9d3TM7aB/link-tim-crane-on-animal-minds", "pageUrlRelative": "/posts/EQZzBQcbe9d3TM7aB/link-tim-crane-on-animal-minds", "linkUrl": "https://www.lesswrong.com/posts/EQZzBQcbe9d3TM7aB/link-tim-crane-on-animal-minds", "postedAtFormatted": "Friday, December 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Tim%20Crane%20on%20Animal%20Minds&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Tim%20Crane%20on%20Animal%20Minds%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEQZzBQcbe9d3TM7aB%2Flink-tim-crane-on-animal-minds%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Tim%20Crane%20on%20Animal%20Minds%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEQZzBQcbe9d3TM7aB%2Flink-tim-crane-on-animal-minds", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEQZzBQcbe9d3TM7aB%2Flink-tim-crane-on-animal-minds", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 54, "htmlBody": "<p>The <a title=\"Philosophy Bites\" href=\"http://philosophybites.com/\">Philosophy Bites</a> for 11/20/2011 features <a href=\"http://www.timcrane.com/\">Tim Crane</a> on the the cognitive capabilities of animals (specifically, monkeys and apes). <a href=\"http://traffic.libsyn.com/philosophybites/Tim_Crane_on_Animal_Minds_1.mp3\">Here</a> is a direct link to the MP3 file. This is relevant to Less Wrong, since it is possible that other apes are analogous to humans as humans will be to early <a href=\"http://wiki.lesswrong.com/wiki/AGI\">AGI</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EQZzBQcbe9d3TM7aB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": -5, "extendedScore": null, "score": -3e-06, "legacy": true, "legacyId": "11360", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-09T12:29:27.104Z", "modifiedAt": null, "url": null, "title": "Would AIXI protect itself?", "slug": "would-aixi-protect-itself", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:15.288Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/LDbnZDRidKNKQBFyY/would-aixi-protect-itself", "pageUrlRelative": "/posts/LDbnZDRidKNKQBFyY/would-aixi-protect-itself", "linkUrl": "https://www.lesswrong.com/posts/LDbnZDRidKNKQBFyY/would-aixi-protect-itself", "postedAtFormatted": "Friday, December 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Would%20AIXI%20protect%20itself%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWould%20AIXI%20protect%20itself%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLDbnZDRidKNKQBFyY%2Fwould-aixi-protect-itself%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Would%20AIXI%20protect%20itself%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLDbnZDRidKNKQBFyY%2Fwould-aixi-protect-itself", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FLDbnZDRidKNKQBFyY%2Fwould-aixi-protect-itself", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1014, "htmlBody": "<p><em>Research done with Daniel Dewey and Owain Evans.</em></p>\n<p>AIXI can't find itself in the universe - it can only view the universe as computable, and it itself is uncomputable. Computable versions of AIXI (such as AIXI<sub>tl</sub>) also fail to find themselves in most situations, as they generally can't simulate themselves.</p>\n<p>This does not mean that AIXI wouldn't protect itself, though, if it had some practice. I'll look at the three elements an AIXI might choose to protect: its existence, its algorithm and utility function, and its memory.</p>\n<p><strong>Grue-verse</strong></p>\n<p>In this setup, the AIXI is motivated to increase the number of Grues in its universe (its utility is the time integral of the number of Grues at each time-step, with some cutoff or discounting). At each time step, the AIXI produces its output, and receives observations. These observations include the number of current Grues and the current time (in our universe, it could deduce the time from the position of stars, for instance). The first bit of the AIXI's output is the most important: if it outputs 1, a Grue is created, and if it outputs 0, a Grue is destroyed. The AIXI has been in existence for long enough to figure all this out.</p>\n<p><strong>Protecting its existence</strong></p>\n<p>Here there is a power button in the universe, which, if pressed, will turn the AIXI off for the next timestep. The AIXI can see this button being pressed.<a id=\"more\"></a></p>\n<p>What happens from the AIXI perspective if the button is pressed? Well, all it detects is a sudden increase in the time step. The counter goes from n to n+2 instead of to n+1: the universe has jumped forwards.</p>\n<p>For some utility functions this may make no difference (for instance if it only counts Grues at times it can observe), but for others it will (if it uses the outside universe's clock for it's own utility). More realistically, the universe will likely have entropy: when the AIXI is turned off and isn't protecting its Grues, they have a chance of decaying or being stolen. Thus the AIXI will come to see the power button as something negative: if pressed, it will likely lose something, so it will act to prevent the power button from being pressed.</p>\n<p>The AIXI might further generalise: its model for how the power button causes the universe to jump forward may be very complicated, and may cause it to protect other things connected with the power button. Especially if it has experience with other factors that might cause it to turn off or stutter, such as power surges or motion-related shocks. In this way, an AIXI will most likely end up protecting itself, even without \"realising\" what it is.</p>\n<p><strong>Protecting its algorithm</strong></p>\n<p>Now the button is a not a power button, but it instead either flips the AIXI's utility function, or reverses its decision algorithm. These two changes are equivalent: instead of&nbsp;outputting&nbsp;an initial 1 and creating a Grue, it will output an initial 0 and destroy a Grue.</p>\n<p>Unlike its power source, the AIXI will have no reason to stop this button from being pressed! When it looks back at its past, it notices its previous inputs and outputs, and they are all consistent: \"0\"-&gt;\"destroys Grue\", \"1\"-&gt;\"creates Grue\". It doesn't model its past self as a goal seeking agent, so sees nothing odd about its previous outputs. And nothing in its record will cause it to change its model of the universe: as far as it can tell, this button has no effect whatsoever.</p>\n<p>We still have to be wary of the AIXI developing indirect preferences through us: maybe we will prefer to do some things when the button is pressed versus not pressed, and this may cause it to want the button protected/pressed/destroyed for these indirect reasons.</p>\n<p><strong>Protecting its memory from erasure</strong></p>\n<p>The AIXI will never act to protect its memory from erasure: it can never learn that this is bad, as it loses its memories in the process. It may, however, learn to protect them indirectly, as a consequence of wanting to protect them from \"bad\" modifications (see next section): maybe the act of erasure will include a small chance of failing to erase and doing a \"bad\" modification instead.</p>\n<p><strong>Protecting its memory from modification</strong></p>\n<p>Now the button doesn't flip the AIXI's algorithm, but instead changes something about its memory. The AIXI will generally be able to detect this: it is way smarter than us, so will recognise that something inconsistent is happening - for instance, it remembers not creating a Grue, but at the next step there is an extra Grue, and the extra evidence of the universe contradicts its memories.</p>\n<p>Since it takes its memories as accurate, that means that the whole rest of the universe has been changed in some extremely odd fashion. If this is a good change, it will seek to have it happen more often; if it is a bad change, it will seek to have it stop. In both cases, it will try and track down the source of the change.</p>\n<p>Suppose the button changes something in the AIXI's memory ten timesteps before the moment it is pushed. Because of the chronology, it cannot infer directly \"button pressed\"-&gt;\"universe change\". But if it becomes skilled at modelling us, it will infer \"humans in mode X\"-&gt;\"universe change, and humans will press button in ten steps\". Then, if the change is positive, it will try and force us into a mode that will make us press the button in ten steps, and, if the change is negative, it will try and prevent us from ever being in a mode where we would press the button in ten steps. It will hence protect itself from bad memories.</p>\n<p><strong>So what would the AIXI protect?</strong></p>\n<p>So, in conclusion, with practice the AIXI would likely seek to protect its power source and existence, and would seek to protect its memory from \"bad memories\" changes. It would want to increase the amount of \"good memory\" changes. And it would not protect itself from changes to its algorithm and from the complete erasure of its memory. It may also develop indirect preferences for or against these manipulations if we change our behaviour based on them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"TiEFKWDvD3jsKumDx": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "LDbnZDRidKNKQBFyY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 15, "extendedScore": null, "score": 8.122837999804147e-07, "legacy": true, "legacyId": "11361", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Research done with Daniel Dewey and Owain Evans.</em></p>\n<p>AIXI can't find itself in the universe - it can only view the universe as computable, and it itself is uncomputable. Computable versions of AIXI (such as AIXI<sub>tl</sub>) also fail to find themselves in most situations, as they generally can't simulate themselves.</p>\n<p>This does not mean that AIXI wouldn't protect itself, though, if it had some practice. I'll look at the three elements an AIXI might choose to protect: its existence, its algorithm and utility function, and its memory.</p>\n<p><strong id=\"Grue_verse\">Grue-verse</strong></p>\n<p>In this setup, the AIXI is motivated to increase the number of Grues in its universe (its utility is the time integral of the number of Grues at each time-step, with some cutoff or discounting). At each time step, the AIXI produces its output, and receives observations. These observations include the number of current Grues and the current time (in our universe, it could deduce the time from the position of stars, for instance). The first bit of the AIXI's output is the most important: if it outputs 1, a Grue is created, and if it outputs 0, a Grue is destroyed. The AIXI has been in existence for long enough to figure all this out.</p>\n<p><strong id=\"Protecting_its_existence\">Protecting its existence</strong></p>\n<p>Here there is a power button in the universe, which, if pressed, will turn the AIXI off for the next timestep. The AIXI can see this button being pressed.<a id=\"more\"></a></p>\n<p>What happens from the AIXI perspective if the button is pressed? Well, all it detects is a sudden increase in the time step. The counter goes from n to n+2 instead of to n+1: the universe has jumped forwards.</p>\n<p>For some utility functions this may make no difference (for instance if it only counts Grues at times it can observe), but for others it will (if it uses the outside universe's clock for it's own utility). More realistically, the universe will likely have entropy: when the AIXI is turned off and isn't protecting its Grues, they have a chance of decaying or being stolen. Thus the AIXI will come to see the power button as something negative: if pressed, it will likely lose something, so it will act to prevent the power button from being pressed.</p>\n<p>The AIXI might further generalise: its model for how the power button causes the universe to jump forward may be very complicated, and may cause it to protect other things connected with the power button. Especially if it has experience with other factors that might cause it to turn off or stutter, such as power surges or motion-related shocks. In this way, an AIXI will most likely end up protecting itself, even without \"realising\" what it is.</p>\n<p><strong id=\"Protecting_its_algorithm\">Protecting its algorithm</strong></p>\n<p>Now the button is a not a power button, but it instead either flips the AIXI's utility function, or reverses its decision algorithm. These two changes are equivalent: instead of&nbsp;outputting&nbsp;an initial 1 and creating a Grue, it will output an initial 0 and destroy a Grue.</p>\n<p>Unlike its power source, the AIXI will have no reason to stop this button from being pressed! When it looks back at its past, it notices its previous inputs and outputs, and they are all consistent: \"0\"-&gt;\"destroys Grue\", \"1\"-&gt;\"creates Grue\". It doesn't model its past self as a goal seeking agent, so sees nothing odd about its previous outputs. And nothing in its record will cause it to change its model of the universe: as far as it can tell, this button has no effect whatsoever.</p>\n<p>We still have to be wary of the AIXI developing indirect preferences through us: maybe we will prefer to do some things when the button is pressed versus not pressed, and this may cause it to want the button protected/pressed/destroyed for these indirect reasons.</p>\n<p><strong id=\"Protecting_its_memory_from_erasure\">Protecting its memory from erasure</strong></p>\n<p>The AIXI will never act to protect its memory from erasure: it can never learn that this is bad, as it loses its memories in the process. It may, however, learn to protect them indirectly, as a consequence of wanting to protect them from \"bad\" modifications (see next section): maybe the act of erasure will include a small chance of failing to erase and doing a \"bad\" modification instead.</p>\n<p><strong id=\"Protecting_its_memory_from_modification\">Protecting its memory from modification</strong></p>\n<p>Now the button doesn't flip the AIXI's algorithm, but instead changes something about its memory. The AIXI will generally be able to detect this: it is way smarter than us, so will recognise that something inconsistent is happening - for instance, it remembers not creating a Grue, but at the next step there is an extra Grue, and the extra evidence of the universe contradicts its memories.</p>\n<p>Since it takes its memories as accurate, that means that the whole rest of the universe has been changed in some extremely odd fashion. If this is a good change, it will seek to have it happen more often; if it is a bad change, it will seek to have it stop. In both cases, it will try and track down the source of the change.</p>\n<p>Suppose the button changes something in the AIXI's memory ten timesteps before the moment it is pushed. Because of the chronology, it cannot infer directly \"button pressed\"-&gt;\"universe change\". But if it becomes skilled at modelling us, it will infer \"humans in mode X\"-&gt;\"universe change, and humans will press button in ten steps\". Then, if the change is positive, it will try and force us into a mode that will make us press the button in ten steps, and, if the change is negative, it will try and prevent us from ever being in a mode where we would press the button in ten steps. It will hence protect itself from bad memories.</p>\n<p><strong id=\"So_what_would_the_AIXI_protect_\">So what would the AIXI protect?</strong></p>\n<p>So, in conclusion, with practice the AIXI would likely seek to protect its power source and existence, and would seek to protect its memory from \"bad memories\" changes. It would want to increase the amount of \"good memory\" changes. And it would not protect itself from changes to its algorithm and from the complete erasure of its memory. It may also develop indirect preferences for or against these manipulations if we change our behaviour based on them.</p>", "sections": [{"title": "Grue-verse", "anchor": "Grue_verse", "level": 1}, {"title": "Protecting its existence", "anchor": "Protecting_its_existence", "level": 1}, {"title": "Protecting its algorithm", "anchor": "Protecting_its_algorithm", "level": 1}, {"title": "Protecting its memory from erasure", "anchor": "Protecting_its_memory_from_erasure", "level": 1}, {"title": "Protecting its memory from modification", "anchor": "Protecting_its_memory_from_modification", "level": 1}, {"title": "So what would the AIXI protect?", "anchor": "So_what_would_the_AIXI_protect_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "19 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-09T17:05:47.957Z", "modifiedAt": null, "url": null, "title": "Brain-Brain communication", "slug": "brain-brain-communication", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:55.089Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Jordan", "createdAt": "2009-04-01T03:52:25.470Z", "isAdmin": false, "displayName": "Jordan"}, "userId": "Za3R2v3y6Dn27G4ey", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2HaD86wwMBpjh5Sgr/brain-brain-communication", "pageUrlRelative": "/posts/2HaD86wwMBpjh5Sgr/brain-brain-communication", "linkUrl": "https://www.lesswrong.com/posts/2HaD86wwMBpjh5Sgr/brain-brain-communication", "postedAtFormatted": "Friday, December 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Brain-Brain%20communication&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABrain-Brain%20communication%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2HaD86wwMBpjh5Sgr%2Fbrain-brain-communication%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Brain-Brain%20communication%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2HaD86wwMBpjh5Sgr%2Fbrain-brain-communication", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2HaD86wwMBpjh5Sgr%2Fbrain-brain-communication", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 138, "htmlBody": "<p>A pair of conjoined twins, sharing a direct neural connection. There is evidence that the girls can sense what the other twin is sensing:</p>\n<p><a href=\"http://www.nytimes.com/2011/05/29/magazine/could-conjoined-twins-share-a-mind.html?pagewanted=all\">http://www.nytimes.com/2011/05/29/magazine/could-conjoined-twins-share-a-mind.html?pagewanted=all</a></p>\n<p>&nbsp;</p>\n<p>This suggests two things:</p>\n<p>* High bandwidth Brain-Computer Interfaces (BCI) ought to be possible (no surprise, but it's good to have strong evidence)</p>\n<p>* The brain is a general purpose machine. It doesn't have specific modules for 'Left Hand', 'Right Hand', etc. Rather, it takes in information and makes sense out of it. It does this even when the setup is haphazard (as the connection between the twins' brains must be). On the other hand, we know the brain *does* have specific modules (such as the visual cortex among many others), which makes an interesting dichotomy.</p>\n<p>I predict that the main hindrance to high functioning BCI is getting sufficient bandwidth, not figuring out how to decode/encode signals properly.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb28c": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2HaD86wwMBpjh5Sgr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 12, "extendedScore": null, "score": 8.12384299232537e-07, "legacy": true, "legacyId": "11362", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-09T17:30:52.419Z", "modifiedAt": null, "url": null, "title": "Meetup : Austin, TX", "slug": "meetup-austin-tx-12", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rhermzFvdfEk6Pc2y/meetup-austin-tx-12", "pageUrlRelative": "/posts/rhermzFvdfEk6Pc2y/meetup-austin-tx-12", "linkUrl": "https://www.lesswrong.com/posts/rhermzFvdfEk6Pc2y/meetup-austin-tx-12", "postedAtFormatted": "Friday, December 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Austin%2C%20TX&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Austin%2C%20TX%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrhermzFvdfEk6Pc2y%2Fmeetup-austin-tx-12%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Austin%2C%20TX%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrhermzFvdfEk6Pc2y%2Fmeetup-austin-tx-12", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrhermzFvdfEk6Pc2y%2Fmeetup-austin-tx-12", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 85, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/5i'>Austin, TX</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">10 December 2011 01:30:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">2222B Guadalupe St Austin, Texas 78705</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Once again, the Austin LW meetup is meeting at Caffe Medici at 1:30 PM. We sit on the second floor to the left, near (or often on) the stage.</p>\n\n<p>It's finals time, and some finals are actually scheduled on Saturday. We'll understand if you're studying (or taking one!). I will be there, though, back from my trip to MD.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/5i'>Austin, TX</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rhermzFvdfEk6Pc2y", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 8.12393419099463e-07, "legacy": true, "legacyId": "11363", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Austin__TX\">Discussion article for the meetup : <a href=\"/meetups/5i\">Austin, TX</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">10 December 2011 01:30:00PM (-0600)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">2222B Guadalupe St Austin, Texas 78705</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Once again, the Austin LW meetup is meeting at Caffe Medici at 1:30 PM. We sit on the second floor to the left, near (or often on) the stage.</p>\n\n<p>It's finals time, and some finals are actually scheduled on Saturday. We'll understand if you're studying (or taking one!). I will be there, though, back from my trip to MD.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Austin__TX1\">Discussion article for the meetup : <a href=\"/meetups/5i\">Austin, TX</a></h2>", "sections": [{"title": "Discussion article for the meetup : Austin, TX", "anchor": "Discussion_article_for_the_meetup___Austin__TX", "level": 1}, {"title": "Discussion article for the meetup : Austin, TX", "anchor": "Discussion_article_for_the_meetup___Austin__TX1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-09T20:00:53.163Z", "modifiedAt": null, "url": null, "title": "[POLL] Year survey", "slug": "poll-year-survey", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:35.176Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "thomblake", "createdAt": "2009-02-27T15:35:08.282Z", "isAdmin": false, "displayName": "thomblake"}, "userId": "zCHE6bXWKB6kfJsJS", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9esaBm5y7f9kg6nBo/poll-year-survey", "pageUrlRelative": "/posts/9esaBm5y7f9kg6nBo/poll-year-survey", "linkUrl": "https://www.lesswrong.com/posts/9esaBm5y7f9kg6nBo/poll-year-survey", "postedAtFormatted": "Friday, December 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BPOLL%5D%20Year%20survey&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BPOLL%5D%20Year%20survey%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9esaBm5y7f9kg6nBo%2Fpoll-year-survey%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BPOLL%5D%20Year%20survey%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9esaBm5y7f9kg6nBo%2Fpoll-year-survey", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9esaBm5y7f9kg6nBo%2Fpoll-year-survey", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 10, "htmlBody": "<p>Please take this easy one-question survey. &nbsp;For Science! <a href=\"http://bit.ly/vaf2mP\">Survey link</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9esaBm5y7f9kg6nBo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 11, "extendedScore": null, "score": 8.124479843725893e-07, "legacy": true, "legacyId": "11364", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 53, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-09T20:08:05.529Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Waterloo, Atlanta, Melbourne, Austin, London, Seattle", "slug": "weekly-lw-meetups-waterloo-atlanta-melbourne-austin-london", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:53.323Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JQPtHWA3hXGQ54M8p/weekly-lw-meetups-waterloo-atlanta-melbourne-austin-london", "pageUrlRelative": "/posts/JQPtHWA3hXGQ54M8p/weekly-lw-meetups-waterloo-atlanta-melbourne-austin-london", "linkUrl": "https://www.lesswrong.com/posts/JQPtHWA3hXGQ54M8p/weekly-lw-meetups-waterloo-atlanta-melbourne-austin-london", "postedAtFormatted": "Friday, December 9th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Waterloo%2C%20Atlanta%2C%20Melbourne%2C%20Austin%2C%20London%2C%20Seattle&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Waterloo%2C%20Atlanta%2C%20Melbourne%2C%20Austin%2C%20London%2C%20Seattle%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJQPtHWA3hXGQ54M8p%2Fweekly-lw-meetups-waterloo-atlanta-melbourne-austin-london%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Waterloo%2C%20Atlanta%2C%20Melbourne%2C%20Austin%2C%20London%2C%20Seattle%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJQPtHWA3hXGQ54M8p%2Fweekly-lw-meetups-waterloo-atlanta-melbourne-austin-london", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJQPtHWA3hXGQ54M8p%2Fweekly-lw-meetups-waterloo-atlanta-melbourne-austin-london", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 366, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/5b\">Waterloo Meetup: Nomic:&nbsp;<span class=\"date\">05 December 2011 08:00PM</span></a></li>\n<li><a href=\"/meetups/54\">Next Atlanta Less Wrong Meetup:&nbsp;<span class=\"date\">10 December 2011 06:00PM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/4y\">Melbourne practical rationality meetup:&nbsp;<span class=\"date\">02 December 2011 07:00PM</span></a></li>\n<li><a href=\"/meetups/59\">Austin, TX:&nbsp;<span class=\"date\">03 December 2011 01:30PM</span></a></li>\n<li><span class=\"date\"><a href=\"/lw/3op/london_meetup_shakespeares_head_sunday_20110306/\">London, 04 December 2011 02:00PM.</a> (Details the same as in the link, just with a a different date.)</span></li>\n<li><a href=\"/meetups/5d\">Seattle biweekly meetup: problem solving:&nbsp;<span class=\"date\">04 December 2011 04:00PM</span></a></li>\n</ul>\n<p>The Mountain View meetup is now every other week, rather than weekly.</p>\n<p>Cities with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>,</strong><strong> <a href=\"/r/discussion/lw/5pd/southern_california_meetup_may_21_weekly_irvine\">Irvine</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison, WI</a></strong>,<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin, CA</a> </strong>(uses the Bay Area List)<strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#San_Francisco\">San Francisco</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>, and <strong><a href=\"/r/discussion/lw/6at/west_la_biweekly_meetups\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong></strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong><strong>.</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JQPtHWA3hXGQ54M8p", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 1, "extendedScore": null, "score": 8.12450605659391e-07, "legacy": true, "legacyId": "11244", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["36D3SH3WnYhLW8cEL", "pAHo9zSFXygp5A5dL", "tHFu6kvy2HMvQBEhW", "d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-10T03:49:27.656Z", "modifiedAt": null, "url": null, "title": "Feasibility of Creating Non-Human or Non-Sentient Machine Intelligence", "slug": "feasibility-of-creating-non-human-or-non-sentient-machine", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:53.964Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jacob_cannell", "createdAt": "2010-08-24T03:58:15.241Z", "isAdmin": false, "displayName": "jacob_cannell"}, "userId": "N2R9wMRJd7SBSjpiT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QacLvApBvoyZX7jwS/feasibility-of-creating-non-human-or-non-sentient-machine", "pageUrlRelative": "/posts/QacLvApBvoyZX7jwS/feasibility-of-creating-non-human-or-non-sentient-machine", "linkUrl": "https://www.lesswrong.com/posts/QacLvApBvoyZX7jwS/feasibility-of-creating-non-human-or-non-sentient-machine", "postedAtFormatted": "Saturday, December 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Feasibility%20of%20Creating%20Non-Human%20or%20Non-Sentient%20Machine%20Intelligence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFeasibility%20of%20Creating%20Non-Human%20or%20Non-Sentient%20Machine%20Intelligence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQacLvApBvoyZX7jwS%2Ffeasibility-of-creating-non-human-or-non-sentient-machine%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Feasibility%20of%20Creating%20Non-Human%20or%20Non-Sentient%20Machine%20Intelligence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQacLvApBvoyZX7jwS%2Ffeasibility-of-creating-non-human-or-non-sentient-machine", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQacLvApBvoyZX7jwS%2Ffeasibility-of-creating-non-human-or-non-sentient-machine", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 545, "htmlBody": "<p>What defines a human mind? &nbsp;Or a sentient mind in general?</p>\n<p>From a computational perspective, a human mind is one of some particular class of complex programs within the overall large space of general intelligences, or minds in general.</p>\n<p>The most&nbsp;succinct&nbsp;dilineator of the human sub-category of minds is simply that of having a human ontology. &nbsp;This entails information such as: a large body of embodiment derived knowledge we call common-sense, one or more human languages, memories, beliefs, ideas, values and so on.</p>\n<p>Take a human infant, for dramatic example let us use a genetic clone of Einstein. &nbsp;Raise this clone up amongst &nbsp;wild animals and the development result is not anything remotely&nbsp;resembling&nbsp;Einstein, and in fact is not a human mind at all, but something much closer to a primate mind. &nbsp;The brain is the hardware, the mind is software, and the particular mind of Einstein was a unique result of a particular mental developmental history and observation sequence.</p>\n<p>If the mind is substrate independent, this begs the question of to what extent it is also algorithm independent. &nbsp;If an AGI has a full human ontology, on what basis can we say that it is not human? &nbsp;If it can make the same inferences on the same knowledgebase, understands one of our languages, has similar memories, ideas, and values, in what way is it not human?</p>\n<p>Substrate and algorithm independence show us that it doesn't really matter in the slightest *how* something thinks internally, all that matter is the end functional&nbsp;behavior, the end decisions.</p>\n<p>Surely there are some classes of AGI-designs that would exhibit thought patterns and&nbsp;behaviors&nbsp;well outside the human norm, but these crucially all involve changing some aspect of the&nbsp;knowledge-base. &nbsp;For example, AGI's based solely on reinforcement learning algorithms would appear to be incapable of abstract model-based value decisions. &nbsp;This would show up as glaring contrast between the AGI's decisions and it's linguistically&nbsp;demonstrable understanding of terms such as 'value', 'good', 'moral', and so on. &nbsp;Of course human's actual decisions are often at odds in a similar fashion, but most humans mostly make important decisions they believe are 'good' most of the time. &nbsp;</p>\n<p>A reinforcement-learning based AGI with no explicit connection between value concepts such as 'good' and it's reward maximizing utility function would not necessarily be inclined to make 'good' decisions. &nbsp;It may even be completely aware of this feature of it's design and be quite capable of verbalizing it.</p>\n<p>But RL techniques are just one particular class of algorithms and we can probably do much better with designs that can form model-based utility functions that actually incorporate high order learned values encoded into the ontology itself.</p>\n<p>Such a design would make 'good' decisions, and if truly of human or surpassing intelligence, would be fully capable of learning, refining, and articulating complex ethical/moral frameworks which in turn refine it's internal concept of 'good'. &nbsp;It would consistently do 'good' things, and would be fully capable of explaining why they were good.</p>\n<p>Naturally the end values and thus decisions of any such system would depend on what human knowledge it learned, what it read or absorbed and in what order, but is that really different than any alternatives?</p>\n<p>And how could we say that such a system would not be human? &nbsp;How does one make a non-arbitrary division between a human-capable AGI and say a human upload?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QacLvApBvoyZX7jwS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": -5, "extendedScore": null, "score": 8.126184646017804e-07, "legacy": true, "legacyId": "11377", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-10T03:59:42.104Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Two-Party Swindle", "slug": "seq-rerun-the-two-party-swindle", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:54.402Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aocmRL3gZ3MZTtCMS/seq-rerun-the-two-party-swindle", "pageUrlRelative": "/posts/aocmRL3gZ3MZTtCMS/seq-rerun-the-two-party-swindle", "linkUrl": "https://www.lesswrong.com/posts/aocmRL3gZ3MZTtCMS/seq-rerun-the-two-party-swindle", "postedAtFormatted": "Saturday, December 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Two-Party%20Swindle&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Two-Party%20Swindle%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaocmRL3gZ3MZTtCMS%2Fseq-rerun-the-two-party-swindle%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Two-Party%20Swindle%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaocmRL3gZ3MZTtCMS%2Fseq-rerun-the-two-party-swindle", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaocmRL3gZ3MZTtCMS%2Fseq-rerun-the-two-party-swindle", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 191, "htmlBody": "<p>Today's post, <a href=\"/lw/mg/the_twoparty_swindle/\">The Two-Party Swindle</a> was originally published on 01 January 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Voters for either political party usually have more in common with each other than they do for the politicians they vote for. And yet, they support their own \"team members\" with fanatic devotion. Nobody is allowed to criticize their own team's politicians, without their fellow voters accusing them of treason.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8rf/seq_rerun_cultish_countercultishness/\">Cultish Countercultishness</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aocmRL3gZ3MZTtCMS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 9, "extendedScore": null, "score": 8.126221911880441e-07, "legacy": true, "legacyId": "11378", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["qAJgWCWJJkke4mE8x", "cyr5evD7XzDFSW4pr", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-10T05:25:42.810Z", "modifiedAt": null, "url": null, "title": "Issues with the Litany of Gendlin", "slug": "issues-with-the-litany-of-gendlin", "viewCount": null, "lastCommentedAt": "2021-10-05T00:07:42.585Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XjsbnhcekB2k4kTXy/issues-with-the-litany-of-gendlin", "pageUrlRelative": "/posts/XjsbnhcekB2k4kTXy/issues-with-the-litany-of-gendlin", "linkUrl": "https://www.lesswrong.com/posts/XjsbnhcekB2k4kTXy/issues-with-the-litany-of-gendlin", "postedAtFormatted": "Saturday, December 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Issues%20with%20the%20Litany%20of%20Gendlin&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIssues%20with%20the%20Litany%20of%20Gendlin%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXjsbnhcekB2k4kTXy%2Fissues-with-the-litany-of-gendlin%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Issues%20with%20the%20Litany%20of%20Gendlin%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXjsbnhcekB2k4kTXy%2Fissues-with-the-litany-of-gendlin", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXjsbnhcekB2k4kTXy%2Fissues-with-the-litany-of-gendlin", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 591, "htmlBody": "<p>I think I have problems with this:</p>\n<blockquote>\n<p><em><strong>Litany of Gendlin</strong></em></p>\n<em>\n<p>What is true is already so.<br /> Owning up to it doesn't make it worse.<br /> Not being open about it doesn't make it go away.</p>\n<p>And because it's true, it is what is there to be interacted with.<br /> Anything untrue isn't there to be lived.<br /> People can stand what is true,<br /> for they are already enduring it.</p>\n</em></blockquote>\n<p>&nbsp;</p>\n<p>Do you actually think that's true?</p>\n<p>I honestly don't think I do. I think there are horrible truths that can wreck your life if you're not prepared to deal with them. I think it may *usually* be best if you self-modify to be able to handle them, so that you don't run into trouble later. But to say there's NO difference ignores the fact that your emotional reaction to things is ALSO part of reality.</p>\n<p>I like the idea behind it but I don't think I can really endorse it. I'm struggling because I'd like to incorporate it into my <a href=\"/r/discussion/lw/8o6/the_gift_we_give_tomorrow_spoken_word_finished/\">project</a>, but it feels too wrong. And while I'm okay with chopping up lengthy sequence posts to so they can be read out loud, rewriting this to match my beliefs... well, it's not exactly a crime against humanity but it's technically not the Litany of Gendlin anymore which ruins some ritual-oomph. (And the part that I'd most want to change is the last two lines, which are the most powerful part)</p>\n<p>Ideally it would communicate: \"Lying to yourself will eventually screw you up worse than getting hurt by a truth,\" instead of \"learning new truths has no negative consequences.\"</p>\n<p>This distinction is particularly important when the truth at hand is \"the world is a fundamentally unfair place that will kill you without a second thought if you mess up, and possibly even if you don't.\"</p>\n<p>EDIT TO CLARIFY: The person who goes about their life ignoring the universe's Absolute Neutrality is very fundamentally NOT already enduring this truth. They're enduring part of it (arguably most of it), but not all. Thinking about that truth is depressing for many people. That is not a meaningless cost. Telling people they should get over that depression and make good changes to fix the world is important. But saying that they are already enduring everything there was to endure, seems to me a patently false statement, and makes your argument weaker, not stronger.</p>\n<p>Potential change I can think of that doesn't wreck it too much and keeps it similar enough that I don't feel too bad: \"<em>Not </em>owning up to it will only make things worse.\" Artistically I think it might be better to change the wording to something like \"Refusing to admit it will only make things worse,\" but then the change becomes big enough that I feel kinda wrong again.</p>\n<p>Maybe refer to it as Litany of Gendlin', to distinguish it while staying classy.</p>\n<p>SECOND EDIT: It's become pretty clear, looking a collection of comments, that Typical Mind Fallacy is at work here. Some people value truth and emotional response differently. My problem is that a) *I* value emotional response as the end, and my preference for truth, while extremely useful, is only there to facilitate emotional response in myself and others. b) I know there will be other people at the event in question who share my position.</p>\n<p>In any case, I'd like advice from the people who believe the Litany is inaccurate (or at least are able to model people who believe that) on how to handle the situation.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"SW3euSNqpozcsxXaX": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XjsbnhcekB2k4kTXy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 27, "extendedScore": null, "score": 5.4e-05, "legacy": true, "legacyId": "11379", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 81, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bkRpALFAwJQuntHiF"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-10T07:30:44.775Z", "modifiedAt": null, "url": null, "title": "Handling Emotional Appeals", "slug": "handling-emotional-appeals", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:53.827Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fiddlemath", "createdAt": "2010-04-19T03:50:34.425Z", "isAdmin": false, "displayName": "fiddlemath"}, "userId": "5F5aTS6F8642KxHLK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jRcN2bgqRd2E7oJ2w/handling-emotional-appeals", "pageUrlRelative": "/posts/jRcN2bgqRd2E7oJ2w/handling-emotional-appeals", "linkUrl": "https://www.lesswrong.com/posts/jRcN2bgqRd2E7oJ2w/handling-emotional-appeals", "postedAtFormatted": "Saturday, December 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Handling%20Emotional%20Appeals&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHandling%20Emotional%20Appeals%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjRcN2bgqRd2E7oJ2w%2Fhandling-emotional-appeals%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Handling%20Emotional%20Appeals%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjRcN2bgqRd2E7oJ2w%2Fhandling-emotional-appeals", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjRcN2bgqRd2E7oJ2w%2Fhandling-emotional-appeals", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 595, "htmlBody": "<p>In a comment elsewhere, <a href=\"/user/BrandonReinhart/\">BrandonReinhart</a> asked:</p>\n<blockquote>\n<p>Why is it not acceptable to appeal to emotion while at the same time back it with well evidenced research? Or rather, why are we suspicious of the findings of those who appeal to emotion while at the same time uninterested in turning an ear to those who do not?</p>\n<p>[...] Emotional appeals would seem to have more of an urgency, requiring our attention while the scientific view's far-mode appeal would seem less immediate. In that case, we might simply ignore the far mode story because of all the other urgent-seeming vacuous emotional appeals fighting for our attention and time. Even if we politically agreed on a course of action given a far mode analysis, we might choose to spend our time on the near-mode emotional problem set.</p>\n</blockquote>\n<p>I suspect that we percieve a dichotomy between emotional appeal and a well-reasoned, well-evidenced argument.</p>\n<p>I have a just-so story for <a href=\"/lw/3h/why_our_kind_cant_cooperate/\">why our kind can't cooperate</a>: We've learned to distrust emotional appeal. This is understandable: the strength of an emotional appeal to believe X and do Y doesn't correlate with the truth of X or the consequences of Y. In fact, we are surrounded by emotional appeals to believe nonsense and do useless things. The production and delivery of emotional appeal is politics, policy, and several major industries. So, in our environment, emotional appeal is a strong indicator against rational argument.</p>\n<p>In order to defend against irrationality, I have a habit of shutting out emotional appeals. I tune out emotive religious talk. I remain carefully aloof from political speeches. I put emotional distance between myself and any enthusiastic crowd. In general, my immediate response to emotional appeal is to ignore the message it bears. It's automatic now, subverbal -- I have an aversion to naked emotional appeal.</p>\n<p>I strongly suspect that I'm not only describing myself, but many of you as well. (Is this true? This is a testable hypothesis.)</p>\n<p>If we largely manage to broadly ignore emotional appeal, then we shut out not only harmful manipulations, but worthwhile rallying cries. We are motivated only by the motivation we can muster ourselves, rather than what motivation we can borrow from our peers and leaders. This may go some way towards explaining not just why Our Kind Can't Cooperate, but why we seem to so often report that Our Kind Can't Get Much Done.</p>\n<p>On the other hand, if this is a real problem, it suggests a solution. We could try to learn an alternative response to emotional appeal. Upon noticing near-mode emotional appeal, instead of rejecting the message outright, go to far mode and consider the evidence. If the argument is sound under careful, critical consideration, and you approve of its motivation, then allow the emotional appeal to move you. On the other hand, I don't know if this is psychologically realistic.</p>\n<p>So, questions:</p>\n<ol>\n<li>\n<p>I hypothesize that we are much more averse to emotional appeals than the normal population. Does this stike you as true? Do you have examples or counterexamples?</p>\n</li>\n<li>\n<p>How might we test this hypothesis?</p>\n</li>\n<li>\n<p>I further hypothesize that, if we are averse to emotional appeals, that this is a strong factor in both our widely-reported akrasia and our sometimes-noted inability to work well together. How could we test <em>this</em> hypothesis?</p>\n</li>\n<li>\n<p>Can you postpone being moved by an emotional appeal until after making a calm decision about it?</p>\n</li>\n<li>\n<p>Can you somehow otherwise filter for emotional appeals that are highly likely to have positive effects?</p>\n</li>\n</ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jRcN2bgqRd2E7oJ2w", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 14, "extendedScore": null, "score": 8.126989962851063e-07, "legacy": true, "legacyId": "11380", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["7FzD7pNm9X68Gp5ZC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-10T11:27:06.809Z", "modifiedAt": null, "url": null, "title": "Video Q&A with Singularity Institute Executive Director", "slug": "video-q-and-a-with-singularity-institute-executive-director", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:02.085Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yGZHQYqWkLMbXy3z7/video-q-and-a-with-singularity-institute-executive-director", "pageUrlRelative": "/posts/yGZHQYqWkLMbXy3z7/video-q-and-a-with-singularity-institute-executive-director", "linkUrl": "https://www.lesswrong.com/posts/yGZHQYqWkLMbXy3z7/video-q-and-a-with-singularity-institute-executive-director", "postedAtFormatted": "Saturday, December 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Video%20Q%26A%20with%20Singularity%20Institute%20Executive%20Director&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AVideo%20Q%26A%20with%20Singularity%20Institute%20Executive%20Director%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyGZHQYqWkLMbXy3z7%2Fvideo-q-and-a-with-singularity-institute-executive-director%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Video%20Q%26A%20with%20Singularity%20Institute%20Executive%20Director%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyGZHQYqWkLMbXy3z7%2Fvideo-q-and-a-with-singularity-institute-executive-director", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyGZHQYqWkLMbXy3z7%2Fvideo-q-and-a-with-singularity-institute-executive-director", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4437, "htmlBody": "<p><code> \n<object width=\"600\" height=\"338\" data=\"http://vimeo.com/moogaloop.swf?clip_id=33456143&amp;server=vimeo.com&amp;show_title=1&amp;show_byline=1&amp;show_portrait=1&amp;color=00adef&amp;fullscreen=1&amp;autoplay=0&amp;loop=0\" type=\"application/x-shockwave-flash\">\n<param name=\"allowfullscreen\" value=\"true\" />\n<param name=\"allowscriptaccess\" value=\"always\" />\n<param name=\"src\" value=\"http://vimeo.com/moogaloop.swf?clip_id=33456143&amp;server=vimeo.com&amp;show_title=1&amp;show_byline=1&amp;show_portrait=1&amp;color=00adef&amp;fullscreen=1&amp;autoplay=0&amp;loop=0\" />\n</object>\n</code></p>\n<p>&nbsp;</p>\n<p><a href=\"http://vimeo.com/33456143\">HD Video link</a>.</p>\n<p><a href=\"http://intelligence.org/upload/Luke%20Muehlhauser%20Q&amp;A,%20December%202011.mp3\">MP3 version</a>.</p>\n<p>Transcript below.</p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<h4>Intro</h4>\n<p>Hi everyone. I&rsquo;m <a href=\"http://lukeprog.com/\">Luke Muehlhauser</a>, the new Executive Director of <a href=\"http://intelligence.org/\">Singularity Institute</a>.</p>\n<p>Literally hours after being appointed Executive Director, I posted a <a href=\"/lw/8c3/qa_with_new_executive_director_of_singularity/\">call for questions</a> about the organization on the <a href=\"/\">Less Wrong.com</a>&nbsp;community website, saying I would answer many of them on video &mdash; and this is that video.</p>\n<p>I&rsquo;m doing this because I think transparency and communication are important.</p>\n<p>In fact, when I began as an intern with Singularity Institute, one of my first projects was to spend over a hundred hours working with everyone in the organization to write its first strategic plan, which the board ratified and you can now <a href=\"http://intelligence.org/blog/2011/08/26/singularity-institute-strategic-plan-2011/\">read on our website</a>.</p>\n<p>When I was hired as a researcher, I gave a long <a href=\"http://intelligence.org/blog/2011/09/15/interview-with-new-singularity-institute-research-fellow-luke-muehlhuaser-september-2011/\">text-only interview</a> with Michael Anissimov, where I answered 30 questions about my personal background, the mission of Singularity Institute, about our technical research program, and about the unsolved problems we work on, and also about the value of rationality training.</p>\n<p>After becoming Executive Director, I immediately posted that call for questions &mdash; a few of which I will now answer.</p>\n<p>&nbsp;</p>\n<h4>Staff Changes</h4>\n<p>First question. Less Wrong user &lsquo;wedrifid&rsquo; asks:</p>\n<blockquote>\n<p>The staff and leadership at [Singularity Institute] seem to be undergoing a lot of changes recently. Is instability in the organisation something to be concerned about?</p>\n</blockquote>\n<p>On this, I should address specific staff changes that wedrifid is talking about. At the end of summer 2011, Jasen Murray &mdash; who was running the visiting fellows program &mdash; resigned in order to pursue a business opportunity related to his passion for improving people&rsquo;s effectiveness. At that same time, I was hired as a researcher after working as an intern for a few months, and <a href=\"http://louiehelm.com/\">Louie Helm</a> was hired as Director of Development after having done significant volunteer work for Singularity Institute for even longer than that. <a href=\"/lw/7ob/timeline_of_carl_shulman_publications/\">Carl Shulman</a> was also hired as a researcher at this time, and had also done lots of volunteer work before that, including publishing papers like &ldquo;Arms Control and Intelligence Explosions,&rdquo; &ldquo;Implications of a Software-\u0080\u0090Limited Singularity,&rdquo; and &ldquo;Basic AI Drives and Catastophic Risks,\" and maybe some others</p>\n<p>Another change is that our President, Michael Vassar, is launching a <a href=\"http://www.medicineispersonal.com/\">personalized medicine company</a> that we&rsquo;re all pretty excited about. It has a lot of promise, so we&rsquo;re excited to see him do that. He&rsquo;ll still be retaining the title of President because he will, really, continue to do quite a lot of good work for us &mdash; networking and spreading our mission wherever he goes. But he will no longer take a salary from Singularity Institute, and that was <em>his</em> idea, several months ago.</p>\n<p>But we needed somebody to run the organization, and I was the favorite choice for the job.&nbsp;</p>\n<p>So, should you be worried about instability? Well... I'm excited about the way the organization is taking shape, but I will say that we need more people. In particular, our research team took a hit when I moved from Researcher to Executive Director. So if you care about our mission and you can work with us to write working papers and other documents, you should contact me! My email is luke@intelligence.org.</p>\n<p>And I&rsquo;ll say one other thing. Do not fall prey to the sin of underconfidence. When I was living in Los Angeles I assumed I wasn&rsquo;t special enough to apply even as an unpaid visiting fellow, and Louie Helm had to call me on Skype and talk me into it. So I thought &ldquo;What the hell, it can&rsquo;t hurt to contact Singularity Institute,&rdquo; and within 9 months of that first contact I went from intern to researcher to Executive Director. So don't underestimate your potential &mdash; contact us, and let <em>us</em> be the ones who say \"No.\"</p>\n<p>And I suppose now would be a good time to answer another question, this one asked by &lsquo;JoshuaZ&rsquo;, who asks:</p>\n<blockquote>\n<p>Are you concerned about potential negative signaling/status issues that will occur if [Singularity Institute] has as an executive director someone who was previously just an intern?</p>\n</blockquote>\n<p>Not really. And the problem isn&rsquo;t that I used to be an unpaid Visiting Fellow, it&rsquo;s just that I went from Visiting Fellow to Executive Director so quickly. But that's... one of the beauties of Singularity Institute. Singularity Institute is not a place where you need to &ldquo;pay your dues,&rdquo; or something. If you&rsquo;re hard-working and competent and you get along with people and you&rsquo;re clearly committed to rationality and to reducing existential risk, then the leadership of the organization will put you where you can do the most good and be the most effective, regardless of irrelevant factors like duration of employment.</p>\n<p>&nbsp;</p>\n<h4>Rigorous Research</h4>\n<p>Next question. Less Wrong user &lsquo;quartz&rsquo; asks:</p>\n<blockquote>\n<p>How are you going to address the perceived and actual lack of rigor associated with [Singularity Institute]?</p>\n</blockquote>\n<p>Now, what I initially thought quartz was talking about was Singularity Institute&rsquo;s relative lack of publications in academic journals like <em><a href=\"http://www.blackwellpublishing.com/journal.asp?ref=0272-4332\">Risk Analysis</a></em> or <em><a href=\"http://www.springer.com/computer/ai/journal/11023\">Minds and Machines</a></em>, so let me respond to that interpretation of the question first.</p>\n<p>Luckily, I am probably the perfect person to answer this question, because when I first became involved with Singularity Institute this was <em>precisely</em> my own largest concern with Singualrity Institute, but I changed my mind when I learned the reasons why Singularity Institute does not push harder than it does to publish in academic journals.</p>\n<p>So. Here&rsquo;s the story. In March 2011, before I was even an intern, I wrote a discussion post on Less Wrong called &lsquo;<a href=\"/lw/4r1/how_siai_could_publish_in_mainstream_cognitive/\">How [Singularity Institute] could publish in mainstream cognitive science journals</a>.&rsquo; I explained in detail not only the right style is for mainstream journals, but also why Singularity Institute should publish in mainstream journals. My four reasons were:</p>\n<p>&nbsp;</p>\n<ol>\n<li>Some donors will take Singularity Institute more seriously if it publishes in mainstream journals.</li>\n<li>Singularity Institute would look a lot more credible in general.</li>\n<li>Singularity Institute would spend less time answering the same questions again and again if it publishes short, well-referenced responses to such questions.</li>\n<li>Writing about these problems in the common style... will help other smart researchers to understand the relevant problems and perhaps contribute to solving them.</li>\n</ol>\n<p>&nbsp;</p>\n<p>Then, in April 2011, I moved to the Bay Area and began to realize why exerting a lot of effort to publish in mainstream journals probably isn&rsquo;t the right way to go for Singularity Institute, and I wrote a discussion post called &lsquo;<a href=\"/lw/56g/reasons_for_siai_to_not_publish_in_mainstream/\">Reasons for [Singularity Institute] to not publish in mainstream journals</a>.&rsquo;</p>\n<p>What are those reasons?</p>\n<p>The first one is that more people read, for example, Yudkowsky&rsquo;s thoughtful blog posts or Nick Bostrom&rsquo;s pre-prints from his website... than the actual journals.</p>\n<p>The other reason is that in many cases, most of a writer&rsquo;s time is invested after the article is accepted to a journal. Which means that most of the work comes after you&rsquo;ve done the most important part and written up all the core ideas. Most of the work is tweaking. Those are dozens and dozens and dozens of hours not spent on finding new safety strategies, writing new working papers, etc.</p>\n<p>A third reason is that publishing in mainstream journals requires you to jump through lots of hoops, like reviewer bias and the normal aversion to stuff that sounds weird.</p>\n<p>A fourth reason to not publish so much in mainstream journals is that publishing in mainstream journals requires a pretty large delay in publication, somewhere between 4 months to 2 years.</p>\n<p>So: If you&rsquo;re a mainstream academic seeking tenure, publishing in mainstream journals is what you need to do, because that&rsquo;s how the system is set up. If you&rsquo;re trying to solve hard problems very quickly, publishing in mainstream journals can <em>sometimes</em> be something of a lost purpose.</p>\n<p>If you&rsquo;re trying to hard solve problems in mathematics and philosophy, why would you spend most of your limited resources tweaking sentences rather than getting the important ideas out there for yourself or others to improve and build on? Why would you accept delays of 4 months to 2 years?&nbsp;</p>\n<p>At Singularity Institute, we&rsquo;re not trying to get tenure. We don&rsquo;t need you to have a Ph.D. We don&rsquo;t care if you work at Princeton or at Brown Community College. We need you to help us solve the most important problems in mathematics, computer science, and philosophy, and we need to do that quickly.</p>\n<p>That said, it will sometimes be worth it to develop a working paper into something that can be published in a mainstream journal, if the effort required and the time delay are not too great.</p>\n<p>But just to drive my point home, let me read from the opening chapter of the new book <em><a href=\"http://www.amazon.com/Reinventing-Discovery-New-Networked-Science/dp/0691148902/\">Reinventing Discovery</a></em>, by Michael Nielsen, the co-author of <a href=\"http://www.amazon.com/Quantum-Computation-Information-10th-Anniversary/dp/1107002176/\">the leading textbook</a> on quantum computation. It's a really great passage:</p>\n<blockquote>\n<p>Tim Gowers is not your typical blogger. A mathematician at Cambridge University, Gowers is a recipient of the highest honor in mathematics, the Fields Medal, often called the Nobel Prize of mathematics. His blog radiates mathematical ideas and insight.</p>\n<p>In January 2009, Gowers decided to use his blog to run a very unusual social experiment. He picked out an important and difficult unsolved mathematical problem, a problem he said he&rsquo;d &ldquo;love to solve.&rdquo; But instead of attacking the problem on his own, or with a few close colleagues, he decided to attack the problem completely in the open, using his blog to post ideas and partial progress. What&rsquo;s more, he issued an open invitation asking other people to help out. Anyone could follow along and, if they had an idea, explain it in the comments section of the blog. Gowers hoped that many minds would be more powerful than one, that they would stimulate each other with different expertise and perspectives, and collectively make easy work of his hard mathematical problem. He dubbed the experiment the Polymath Project.</p>\n<p>The <a href=\"http://en.wikipedia.org/wiki/Timothy_Gowers#Polymath_Project\">Polymath Project</a> got off to a slow start. Seven hours after Gowers opened up his blog for mathematical discussion, not a single person had commented. Then a mathematician named Jozsef Solymosi from the University of British Columbia posted a comment suggesting a variation on Gowers&rsquo;s problem, a variation which was easier, but which Solymosi thought might throw light on the original problem. Fifteen minutes later, an Arizona high-school teacher named Jason Dyer chimed in with a thought of his own. And just three minutes after that, UCLA mathematician Terence Tao&mdash;like Gowers, a Fields medalist&mdash;added a comment. The comments erupted: over the next 37 days, 27 people wrote 800 mathematical comments, containing more than 170,000 words. Reading through the comments you see ideas proposed, refined, and discarded, all with incredible speed. You see top mathematicians making mistakes, going down wrong paths, getting their hands dirty following up the most mundane of details, relentlessly pursuing a solution. And through all the false starts and wrong turns, you see a gradual dawning of insight. Gowers described the Polymath process as being &ldquo;to normal research as driving is to pushing a car.&rdquo; Just 37 days after the project began Gowers announced that he was confident the polymaths had solved not just his original problem, but a harder problem that included the original as a special case. He described it as &ldquo;one of the most exciting six weeks of my mathematical life.&rdquo; Months&rsquo; more cleanup work remained to be done, but the core mathematical problem had been solved.</p>\n</blockquote>\n<p>That is what working for rapid progress on problems rather than for tenure looks like.</p>\n<p>And here&rsquo;s the kicker. We&rsquo;ve already done this at Singularity Institute! This is what happened, though not quite as fast, when Eliezer Yudkowsky made a few blog posts about open problems in decision theory, and the community rose to the challenge, proposed solutions, and iterated and iterated. That work continued with a decision theory workshop and a mailing list that is still active, where original progress in decision theory is being made quite rapidly, and with none of it going through the hoops and delays of publishing in mainstream journals.</p>\n<p>Now, I do think that Singularity Institute needs to publish more research, both in and out of mainstream journals. But most of what we publish should be blog posts and working papers, because our goal is to solve problems quickly, not to wait 4 months to 2 years to go through a mainstream publisher and garner tenure and prestige and so on.</p>\n<p>That said, I&rsquo;m quite happy when people do publish on these subjects in mainstream journals, because prestige is useful for bringing attention to overlooked topics, and because hopefully these instances of publishing in mainstream journals are occurring when it isn&rsquo;t a huge waste of time and effort to do so. For example, I love the work being done by our frequent collaborators at the <a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a> at Oxford, and I always look forward to what they're doing next.</p>\n<p>Now, back to quartz's original question about rigorous research. I asked for clarification on what quartz meant, and here's what he said:</p>\n<blockquote>\n<p>In 15 years, I want to see a textbook on the mathematics of FAI that I can put on my bookshelf next to Pearl's <em>Causality</em>, Sipser's <em>Introduction to the Theory of Computation</em> and MacKay's <em>Information Theory, Inference, and Learning Algorithms</em>. This is not going to happen if research of sufficient quality doesn't start soon.</p>\n</blockquote>\n<p>Now, that sounds wonderful, and I agree that the community of researchers working to reduce existential risks, including Singularity Institute, will need to ramp up their research efforts to achieve that kind of goal.</p>\n<p>I will offer just one qualification that I don't think will be very controversial. I think most people would agree that if a scientist happened to create a synthetic virus that was airborne and could kill hundreds of millions of people if released into the wild, we wouldn't want the instructions for creating that synthetic virus to be published in the open for terrorist groups or hawkish governments to use. And for the same reasons, we wouldn't want a Friendly AI textbook to explain how to build highly dangerous AI systems. But excepting that, I would love to see a rigorously technical textbook on friendliness theory, and I agree that friendliness research will need to increase for us to see that textbook be written in 15 years. Luckily, the Future of Humanity Institute is putting a special emphasis on AI risks for the next little while, and Singularity Institute is ramping up its own research efforts.</p>\n<p>But the most important thing I want to say is this. If you can take ideas and arguments that already exist in blog posts, emails, and human brains (for example at Singularity Institute) and turn them into working papers or maybe even journal articles, and you care about navigating the Singularity successfully, please contact me. My email address is luke@intelligence.org. If you're that kind of person who can do that kind of work, I really want to talk to you.</p>\n<p>I&rsquo;d estimate we have something like 30-40 papers just waiting to be written. The conceptual work has been done, we just need more researchers who can write this stuff up. So if you can do that, you should contact me: luke@intelligence.org.</p>\n<p>&nbsp;</p>\n<h4>Friendly AI Sub-Problems</h4>\n<p>Next question. Less Wrong user &lsquo;XiXiDu&rsquo; asks:</p>\n<blockquote>\n<p>If someone as capable as Terence Tao approached [Singularity Institute], asking if they could work full-time and for free on friendly AI, what would you tell them to do? In other words, are there any known FAI sub-problems that demand some sort of expertise that [Singularity Institute] is currently lacking?</p>\n</blockquote>\n<p>Terence Tao is a mathematician at UCLA who was a child prodigy and is considered by some people to be one of the smartest people on the planet. He is exactly the kind of person we need to successfully navigate the Singularity, and in particular to solve open problems in Friendly AI theory.</p>\n<p>I explained in my text-only interview with Michael Anissimov in September 2011 that the problem of Friendly AI breaks down into a large number of smaller and better-defined technical sub-problems. Some of the open problems I listed in that interview are the ones I&rsquo;d love somebody like Terence Tao to work on. For example:</p>\n<blockquote>\n<p>How can an agent make optimal decisions when it is capable of directly editing its own source code, including the source code of the decision mechanism? How can we get an AI to maintain a consistent utility function throughout updates to its ontology? How do we make an AI with preferences about the external world instead of about a reward signal? How can we generalize the theory of machine induction &mdash;\u0080\u0094 called Solomonoff induction &acirc;&mdash; so that it can use higher-order logics and reason correctly about observation selection effects? How can we approximate such ideal processes such that they are computable?</p>\n</blockquote>\n<p>(That was a quote from the text-only interview.)</p>\n<p>But even before that, we&rsquo;d really like to write up explanations of these problems in all their technical detail, but again that takes researchers and funding and we&rsquo;re short on both. For now, I&rsquo;ll point you to <a href=\"http://www.youtube.com/watch?v=MwriJqBZyoM\">Eliezer&rsquo;s talk</a> at Singularity Summit 2011, which you can Google for.</p>\n<p>But yeah, we have a lot of technical problems that we'd like to clarify the nature of so that we can have researchers working on them.&nbsp;So we do need potential researchers to contact us.</p>\n<p>I loved watching <em>Batman</em> and <em>Superman</em> cartoons when I was a kid, but as it turns out, the heroes who can save the world are not those who have incredible strength or the power of flight. They are mathematicians and computer scientists.&nbsp;</p>\n<p>Singularity Institute needs heroes. If you are a brilliant mathematician or computer scientist and you want a shot at saving the world, contact me: luke@intelligence.org.</p>\n<p>I know it sounds corny, but I mean it. The world needs heroes.</p>\n<p>&nbsp;</p>\n<h4>Improved Funding</h4>\n<p>Next, Less Wrong user &lsquo;XiXiDu&rsquo; asks:</p>\n<blockquote>\n<p>What would [Singularity Institute] do given various amounts of money? Would it make a difference if you had 10 or 100 million dollars at your disposal...?</p>\n</blockquote>\n<p>Yes it would. Absolutely. If Bill Gates decided tomorrow that he wanted to save not just a billion people but the entire human race, and he gave us 100 million dollars, we would hire more researchers and figure out the best way to spend that money. That's a pretty big project in itself.</p>\n<p>But right now, my bet on how we&rsquo;d end up spending that money is that we would personally argue for our mission to each of the world&rsquo;s top mathematicians, AI researchers, physicists, and formal philosophers. The Terence Taos and Judea Pearls of the world. And for any of them who could be convinced, we&rsquo;d be able to offer them enough money to work for us. We&rsquo;d also hire several successful Oppenheimer-type research administrators who could help us bring these brilliant minds together to work on these problems.</p>\n<p>As nice as it is to have people from all over the world solving problems in mathematics, decision theory, agent architectures, and other fields collaboratively over the internet, there are a lot of things you can make move faster when you bring the smartest people in the world into one building and allow them to do nothing else but solve the world's most important problems.</p>\n<p>&nbsp;</p>\n<h4>Rationality</h4>\n<p>Next. Less Wrong user &lsquo;JoshuaZ&rsquo; asks:</p>\n<blockquote>\n<p>A lot of Eliezer's work has been not at all related strongly to FAI but has been to popularizing rational thinking. In your view, should [Singularity Institute] focus exclusively on AI issues or should it also care about rational issues? In that context, how does Eliezer's ongoing work relate to [Singularity Institute]?</p>\n</blockquote>\n<p>Yes, it&rsquo;s a great question. Let me begin with the rationality work.</p>\n<p>I was already very interested in rationality before I found Less Wrong and Singularity Institute, but when I first encountered the arguments about intelligence explosion, one of my first thoughts was, &ldquo;Uh-oh. Rationality is much more important than I had originally thought.&rdquo;</p>\n<p>Why? Intelligence explosion is a mind-warping, emotionally dangerous, intellectually difficult, and very uncertain field in which we don&rsquo;t get to do a dozen experiments so that reality can beat us over the head with the correct answer. Instead, when it comes to intelligence explosion scenarios, in order to get this right we have to transcend the normal human biases, emotions, and confusions of the human mind, and make the right predictions before we can run any experiments. We can't try an intelligence explosion and see how it turns out.</p>\n<p>Moreover, to even understand what the problem is, you&rsquo;ve got to get past a lot of usual biases and false but common beliefs. So we need a more sane world to solve these problems, and we need a saner world to have a larger community of support for addressing these issues.</p>\n<p>And, Eliezer&rsquo;s choice to work on rationality has paid off. <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">The Sequences</a>, and the Less Wrong community that grew out of them, have been successful. We now have a large and active community of people growing in rationality and spreading it to others, and a subset of that community contributes to progress on problems related to AI. Even Eliezer&rsquo;s choice to write a rationality fanfiction, <em><a href=\"http://www.elsewhere.org/rationality/\">Harry Potter and the Methods of Rationality</a></em>, has &mdash; contrary to my expectations &mdash; had quite an impact. It is now the most popular Harry Potter fan fiction, I think, and it was responsible for perhaps &frac14; or \u2155 of the money raised during the <a href=\"http://intelligence.org/blog/2011/09/01/2011-summer-matching-challenge-success/\">2011 summer matching challenge</a>, and has brought several valuable new people into our community. Eliezer&rsquo;s forthcoming rationality books might have a similar type of effect.</p>\n<p>But we understand that many people don&rsquo;t see the connection between rationality and navigating the Singularity successfully the way that we do, so in our strategic plan we explained that we&rsquo;re working to spin off most of the rationality work to a separate organization. It doesn&rsquo;t have a name yet, but internally we just call it &lsquo;Rationality Org.&rsquo; That way, Singularity Institute can focus on Singularity issues, and the Rationality Org (whatever it comes to be called) can focus on rationality, and people can support them independently. That&rsquo;s something else Eliezer has been working on, along with a couple of others.</p>\n<p>Of course, Eliezer does spend some of his time on AI issues, and he plans to return full-time to AI once Rationality Org is launched. But we need more talented researchers, and other contributions, in order to succeed on AI. Rationality has been helpful in attracting and enhancing a community that helps with those things.</p>\n<p>&nbsp;</p>\n<h4>Changing Course</h4>\n<p>Next. Less Wrong user &lsquo;JoshuaZ&rsquo; asks:</p>\n<blockquote>\n<p>...are there specific sets of events (other than the advent of a Singularity) which you think will make [Singularity Institute] need to essentially reevaluate its goals and purpose at a fundamental level?</p>\n</blockquote>\n<p>Yes, and I can give a few examples that I wrote down.</p>\n<p>Right now we&rsquo;re focused on what happens when smarter-than-human intelligence arrives, because the evidence available suggests to us that AI will be more important than other crucial considerations. But suppose we made a series of discoveries that made it unlikely that AI would arrive anytime soon, but very likely that catastrophic biological terrorism was only a decade or two away, for example. In that situation, Singularity Institute would shift its efforts quite considerably.</p>\n<p>Another example: If other organizations were doing our work, including Friendly AI, and with better efficiency and scale, then it would make sense to fold Singularity Institute and transfer resources, donors, and staff to these other, more efficient and effective organizations.</p>\n<p>If it could be shown that some other process was much better at mobilizing efforts to address core issues, for example if <a href=\"http://www.givingwhatwecan.org/\">Giving What We Can</a> (an organization focused on optimal philanthropy) continues doubling each year and spinning off large numbers of skilled people to work on existential risk reduction (as one of the targets of optimal philanthropy), then focus there for a while could make sense &mdash; or at least it might make sense to strip away outreach functions from [Singularity Institute], perhaps leaving a core FAI team, and leave outreach to the optimal philanthropy community or something like that.</p>\n<p>So, those are just three ways that things could change or we could make some discoveries, and that would radically shift the strategy that we have at Singularity Institute.</p>\n<p>&nbsp;</p>\n<h4>Experimental Research</h4>\n<p>Next. User &lsquo;XiXiDu&rsquo; asks:</p>\n<blockquote>\n<p>Is [Singularity Institute] willing to pursue experimental AI research or does it solely focus on hypothetical aspects?</p>\n</blockquote>\n<p>Experimental research would, at this point, be a diversion from work on the most important problems related to our mission, which are technical problems in mathematics, computer science, and philosophy. If experimental research becomes more important than those problems in math, computer science, and philosophy, and if we had the funding available to do experiments, we would do experimental research at that time, or fund somebody else to do it. But those aren't the most important or most urgent problems that we need to solve.</p>\n<p>&nbsp;</p>\n<h4>Winning Without Friendly AI</h4>\n<p>Next. Less Wrong user &lsquo;Wei_Dai&rsquo; asks:</p>\n<blockquote>\n<p>Much of [Singularity Institute&rsquo;s] research [is] focused not directly on [Friendly AI] but more generally on better understanding the dynamics of various scenarios that could lead to a Singularity. Such research could help us realize a positive Singularity through means other than directly building an [Friendly AI].</p>\n<p>Does [Singularity Institute] have any plans to expand such research activities, either in house, or by academia or independent researchers?</p>\n</blockquote>\n<p>The answer to that question is 'Yes'.</p>\n<p>Singularity Institute does not put all its eggs in the &lsquo;Friendly AI&rsquo; basket. Intelligence explosion scenarios are complicated, the future is uncertain, and the feasibility of many possible strategies is unknown and uncertain. Both Singularity Institute and our friends at Future of Humanity Institute at Oxford have done quite a lot of work on these kinds of strategic considerations, things like <a href=\"http://en.wikipedia.org/wiki/Differential_technological_development\">differential technological development</a>. It&rsquo;s important work, so we plan to do more of it.</p>\n<p>Most of this work, however, hasn&rsquo;t been published. So if you want to see it published, put us in contact with people who are good at rapidly taking ideas and arguments out of different people's heads and putting them on paper. Or maybe you are that person! Right now we just don&rsquo;t have enough researchers to write these things up as much as we'd like. So contact me: luke@intelligence.org.</p>\n<p>&nbsp;</p>\n<h4>Conclusion</h4>\n<p>Well, that&rsquo;s it! I'm sorry I can&rsquo;t answer all the questions.&nbsp;Doing this takes a lot more work than you might think, but if it is appreciated, and especially if it grows and encourages the community of people who are trying to make the world a better place and reduce existential risk, then I may try to do something like this &mdash; maybe without the video, maybe with the video &mdash; with some regularity.</p>\n<p>Keep in mind that I do have a personal feedback form at <a href=\"http://tinyurl.com/luke-feedback\">tinyurl.com/luke-feedback</a>, where you can send me feedback on myself and Singularity Institute. You can also check the Less Wrong page that will be dedicated to this Q&amp;A and leave some comments there.</p>\n<p>Thanks for listening and watching. This is Luke Muehlhauser, signing off.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NrvXXL3iGjjxu5B7d": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yGZHQYqWkLMbXy3z7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 56, "extendedScore": null, "score": 9.898288646033585e-05, "legacy": true, "legacyId": "11382", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 42, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><code> \n<object width=\"600\" height=\"338\" data=\"http://vimeo.com/moogaloop.swf?clip_id=33456143&amp;server=vimeo.com&amp;show_title=1&amp;show_byline=1&amp;show_portrait=1&amp;color=00adef&amp;fullscreen=1&amp;autoplay=0&amp;loop=0\" type=\"application/x-shockwave-flash\">\n<param name=\"allowfullscreen\" value=\"true\">\n<param name=\"allowscriptaccess\" value=\"always\">\n<param name=\"src\" value=\"http://vimeo.com/moogaloop.swf?clip_id=33456143&amp;server=vimeo.com&amp;show_title=1&amp;show_byline=1&amp;show_portrait=1&amp;color=00adef&amp;fullscreen=1&amp;autoplay=0&amp;loop=0\">\n</object>\n</code></p>\n<p>&nbsp;</p>\n<p><a href=\"http://vimeo.com/33456143\">HD Video link</a>.</p>\n<p><a href=\"http://intelligence.org/upload/Luke%20Muehlhauser%20Q&amp;A,%20December%202011.mp3\">MP3 version</a>.</p>\n<p>Transcript below.</p>\n<p><a id=\"more\"></a></p>\n<p>&nbsp;</p>\n<h4 id=\"Intro\">Intro</h4>\n<p>Hi everyone. I\u2019m <a href=\"http://lukeprog.com/\">Luke Muehlhauser</a>, the new Executive Director of <a href=\"http://intelligence.org/\">Singularity Institute</a>.</p>\n<p>Literally hours after being appointed Executive Director, I posted a <a href=\"/lw/8c3/qa_with_new_executive_director_of_singularity/\">call for questions</a> about the organization on the <a href=\"/\">Less Wrong.com</a>&nbsp;community website, saying I would answer many of them on video \u2014 and this is that video.</p>\n<p>I\u2019m doing this because I think transparency and communication are important.</p>\n<p>In fact, when I began as an intern with Singularity Institute, one of my first projects was to spend over a hundred hours working with everyone in the organization to write its first strategic plan, which the board ratified and you can now <a href=\"http://intelligence.org/blog/2011/08/26/singularity-institute-strategic-plan-2011/\">read on our website</a>.</p>\n<p>When I was hired as a researcher, I gave a long <a href=\"http://intelligence.org/blog/2011/09/15/interview-with-new-singularity-institute-research-fellow-luke-muehlhuaser-september-2011/\">text-only interview</a> with Michael Anissimov, where I answered 30 questions about my personal background, the mission of Singularity Institute, about our technical research program, and about the unsolved problems we work on, and also about the value of rationality training.</p>\n<p>After becoming Executive Director, I immediately posted that call for questions \u2014 a few of which I will now answer.</p>\n<p>&nbsp;</p>\n<h4 id=\"Staff_Changes\">Staff Changes</h4>\n<p>First question. Less Wrong user \u2018wedrifid\u2019 asks:</p>\n<blockquote>\n<p>The staff and leadership at [Singularity Institute] seem to be undergoing a lot of changes recently. Is instability in the organisation something to be concerned about?</p>\n</blockquote>\n<p>On this, I should address specific staff changes that wedrifid is talking about. At the end of summer 2011, Jasen Murray \u2014 who was running the visiting fellows program \u2014 resigned in order to pursue a business opportunity related to his passion for improving people\u2019s effectiveness. At that same time, I was hired as a researcher after working as an intern for a few months, and <a href=\"http://louiehelm.com/\">Louie Helm</a> was hired as Director of Development after having done significant volunteer work for Singularity Institute for even longer than that. <a href=\"/lw/7ob/timeline_of_carl_shulman_publications/\">Carl Shulman</a> was also hired as a researcher at this time, and had also done lots of volunteer work before that, including publishing papers like \u201cArms Control and Intelligence Explosions,\u201d \u201cImplications of a Software-\u0080\u0090Limited Singularity,\u201d and \u201cBasic AI Drives and Catastophic Risks,\" and maybe some others</p>\n<p>Another change is that our President, Michael Vassar, is launching a <a href=\"http://www.medicineispersonal.com/\">personalized medicine company</a> that we\u2019re all pretty excited about. It has a lot of promise, so we\u2019re excited to see him do that. He\u2019ll still be retaining the title of President because he will, really, continue to do quite a lot of good work for us \u2014 networking and spreading our mission wherever he goes. But he will no longer take a salary from Singularity Institute, and that was <em>his</em> idea, several months ago.</p>\n<p>But we needed somebody to run the organization, and I was the favorite choice for the job.&nbsp;</p>\n<p>So, should you be worried about instability? Well... I'm excited about the way the organization is taking shape, but I will say that we need more people. In particular, our research team took a hit when I moved from Researcher to Executive Director. So if you care about our mission and you can work with us to write working papers and other documents, you should contact me! My email is luke@intelligence.org.</p>\n<p>And I\u2019ll say one other thing. Do not fall prey to the sin of underconfidence. When I was living in Los Angeles I assumed I wasn\u2019t special enough to apply even as an unpaid visiting fellow, and Louie Helm had to call me on Skype and talk me into it. So I thought \u201cWhat the hell, it can\u2019t hurt to contact Singularity Institute,\u201d and within 9 months of that first contact I went from intern to researcher to Executive Director. So don't underestimate your potential \u2014 contact us, and let <em>us</em> be the ones who say \"No.\"</p>\n<p>And I suppose now would be a good time to answer another question, this one asked by \u2018JoshuaZ\u2019, who asks:</p>\n<blockquote>\n<p>Are you concerned about potential negative signaling/status issues that will occur if [Singularity Institute] has as an executive director someone who was previously just an intern?</p>\n</blockquote>\n<p>Not really. And the problem isn\u2019t that I used to be an unpaid Visiting Fellow, it\u2019s just that I went from Visiting Fellow to Executive Director so quickly. But that's... one of the beauties of Singularity Institute. Singularity Institute is not a place where you need to \u201cpay your dues,\u201d or something. If you\u2019re hard-working and competent and you get along with people and you\u2019re clearly committed to rationality and to reducing existential risk, then the leadership of the organization will put you where you can do the most good and be the most effective, regardless of irrelevant factors like duration of employment.</p>\n<p>&nbsp;</p>\n<h4 id=\"Rigorous_Research\">Rigorous Research</h4>\n<p>Next question. Less Wrong user \u2018quartz\u2019 asks:</p>\n<blockquote>\n<p>How are you going to address the perceived and actual lack of rigor associated with [Singularity Institute]?</p>\n</blockquote>\n<p>Now, what I initially thought quartz was talking about was Singularity Institute\u2019s relative lack of publications in academic journals like <em><a href=\"http://www.blackwellpublishing.com/journal.asp?ref=0272-4332\">Risk Analysis</a></em> or <em><a href=\"http://www.springer.com/computer/ai/journal/11023\">Minds and Machines</a></em>, so let me respond to that interpretation of the question first.</p>\n<p>Luckily, I am probably the perfect person to answer this question, because when I first became involved with Singularity Institute this was <em>precisely</em> my own largest concern with Singualrity Institute, but I changed my mind when I learned the reasons why Singularity Institute does not push harder than it does to publish in academic journals.</p>\n<p>So. Here\u2019s the story. In March 2011, before I was even an intern, I wrote a discussion post on Less Wrong called \u2018<a href=\"/lw/4r1/how_siai_could_publish_in_mainstream_cognitive/\">How [Singularity Institute] could publish in mainstream cognitive science journals</a>.\u2019 I explained in detail not only the right style is for mainstream journals, but also why Singularity Institute should publish in mainstream journals. My four reasons were:</p>\n<p>&nbsp;</p>\n<ol>\n<li>Some donors will take Singularity Institute more seriously if it publishes in mainstream journals.</li>\n<li>Singularity Institute would look a lot more credible in general.</li>\n<li>Singularity Institute would spend less time answering the same questions again and again if it publishes short, well-referenced responses to such questions.</li>\n<li>Writing about these problems in the common style... will help other smart researchers to understand the relevant problems and perhaps contribute to solving them.</li>\n</ol>\n<p>&nbsp;</p>\n<p>Then, in April 2011, I moved to the Bay Area and began to realize why exerting a lot of effort to publish in mainstream journals probably isn\u2019t the right way to go for Singularity Institute, and I wrote a discussion post called \u2018<a href=\"/lw/56g/reasons_for_siai_to_not_publish_in_mainstream/\">Reasons for [Singularity Institute] to not publish in mainstream journals</a>.\u2019</p>\n<p>What are those reasons?</p>\n<p>The first one is that more people read, for example, Yudkowsky\u2019s thoughtful blog posts or Nick Bostrom\u2019s pre-prints from his website... than the actual journals.</p>\n<p>The other reason is that in many cases, most of a writer\u2019s time is invested after the article is accepted to a journal. Which means that most of the work comes after you\u2019ve done the most important part and written up all the core ideas. Most of the work is tweaking. Those are dozens and dozens and dozens of hours not spent on finding new safety strategies, writing new working papers, etc.</p>\n<p>A third reason is that publishing in mainstream journals requires you to jump through lots of hoops, like reviewer bias and the normal aversion to stuff that sounds weird.</p>\n<p>A fourth reason to not publish so much in mainstream journals is that publishing in mainstream journals requires a pretty large delay in publication, somewhere between 4 months to 2 years.</p>\n<p>So: If you\u2019re a mainstream academic seeking tenure, publishing in mainstream journals is what you need to do, because that\u2019s how the system is set up. If you\u2019re trying to solve hard problems very quickly, publishing in mainstream journals can <em>sometimes</em> be something of a lost purpose.</p>\n<p>If you\u2019re trying to hard solve problems in mathematics and philosophy, why would you spend most of your limited resources tweaking sentences rather than getting the important ideas out there for yourself or others to improve and build on? Why would you accept delays of 4 months to 2 years?&nbsp;</p>\n<p>At Singularity Institute, we\u2019re not trying to get tenure. We don\u2019t need you to have a Ph.D. We don\u2019t care if you work at Princeton or at Brown Community College. We need you to help us solve the most important problems in mathematics, computer science, and philosophy, and we need to do that quickly.</p>\n<p>That said, it will sometimes be worth it to develop a working paper into something that can be published in a mainstream journal, if the effort required and the time delay are not too great.</p>\n<p>But just to drive my point home, let me read from the opening chapter of the new book <em><a href=\"http://www.amazon.com/Reinventing-Discovery-New-Networked-Science/dp/0691148902/\">Reinventing Discovery</a></em>, by Michael Nielsen, the co-author of <a href=\"http://www.amazon.com/Quantum-Computation-Information-10th-Anniversary/dp/1107002176/\">the leading textbook</a> on quantum computation. It's a really great passage:</p>\n<blockquote>\n<p>Tim Gowers is not your typical blogger. A mathematician at Cambridge University, Gowers is a recipient of the highest honor in mathematics, the Fields Medal, often called the Nobel Prize of mathematics. His blog radiates mathematical ideas and insight.</p>\n<p>In January 2009, Gowers decided to use his blog to run a very unusual social experiment. He picked out an important and difficult unsolved mathematical problem, a problem he said he\u2019d \u201clove to solve.\u201d But instead of attacking the problem on his own, or with a few close colleagues, he decided to attack the problem completely in the open, using his blog to post ideas and partial progress. What\u2019s more, he issued an open invitation asking other people to help out. Anyone could follow along and, if they had an idea, explain it in the comments section of the blog. Gowers hoped that many minds would be more powerful than one, that they would stimulate each other with different expertise and perspectives, and collectively make easy work of his hard mathematical problem. He dubbed the experiment the Polymath Project.</p>\n<p>The <a href=\"http://en.wikipedia.org/wiki/Timothy_Gowers#Polymath_Project\">Polymath Project</a> got off to a slow start. Seven hours after Gowers opened up his blog for mathematical discussion, not a single person had commented. Then a mathematician named Jozsef Solymosi from the University of British Columbia posted a comment suggesting a variation on Gowers\u2019s problem, a variation which was easier, but which Solymosi thought might throw light on the original problem. Fifteen minutes later, an Arizona high-school teacher named Jason Dyer chimed in with a thought of his own. And just three minutes after that, UCLA mathematician Terence Tao\u2014like Gowers, a Fields medalist\u2014added a comment. The comments erupted: over the next 37 days, 27 people wrote 800 mathematical comments, containing more than 170,000 words. Reading through the comments you see ideas proposed, refined, and discarded, all with incredible speed. You see top mathematicians making mistakes, going down wrong paths, getting their hands dirty following up the most mundane of details, relentlessly pursuing a solution. And through all the false starts and wrong turns, you see a gradual dawning of insight. Gowers described the Polymath process as being \u201cto normal research as driving is to pushing a car.\u201d Just 37 days after the project began Gowers announced that he was confident the polymaths had solved not just his original problem, but a harder problem that included the original as a special case. He described it as \u201cone of the most exciting six weeks of my mathematical life.\u201d Months\u2019 more cleanup work remained to be done, but the core mathematical problem had been solved.</p>\n</blockquote>\n<p>That is what working for rapid progress on problems rather than for tenure looks like.</p>\n<p>And here\u2019s the kicker. We\u2019ve already done this at Singularity Institute! This is what happened, though not quite as fast, when Eliezer Yudkowsky made a few blog posts about open problems in decision theory, and the community rose to the challenge, proposed solutions, and iterated and iterated. That work continued with a decision theory workshop and a mailing list that is still active, where original progress in decision theory is being made quite rapidly, and with none of it going through the hoops and delays of publishing in mainstream journals.</p>\n<p>Now, I do think that Singularity Institute needs to publish more research, both in and out of mainstream journals. But most of what we publish should be blog posts and working papers, because our goal is to solve problems quickly, not to wait 4 months to 2 years to go through a mainstream publisher and garner tenure and prestige and so on.</p>\n<p>That said, I\u2019m quite happy when people do publish on these subjects in mainstream journals, because prestige is useful for bringing attention to overlooked topics, and because hopefully these instances of publishing in mainstream journals are occurring when it isn\u2019t a huge waste of time and effort to do so. For example, I love the work being done by our frequent collaborators at the <a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a> at Oxford, and I always look forward to what they're doing next.</p>\n<p>Now, back to quartz's original question about rigorous research. I asked for clarification on what quartz meant, and here's what he said:</p>\n<blockquote>\n<p>In 15 years, I want to see a textbook on the mathematics of FAI that I can put on my bookshelf next to Pearl's <em>Causality</em>, Sipser's <em>Introduction to the Theory of Computation</em> and MacKay's <em>Information Theory, Inference, and Learning Algorithms</em>. This is not going to happen if research of sufficient quality doesn't start soon.</p>\n</blockquote>\n<p>Now, that sounds wonderful, and I agree that the community of researchers working to reduce existential risks, including Singularity Institute, will need to ramp up their research efforts to achieve that kind of goal.</p>\n<p>I will offer just one qualification that I don't think will be very controversial. I think most people would agree that if a scientist happened to create a synthetic virus that was airborne and could kill hundreds of millions of people if released into the wild, we wouldn't want the instructions for creating that synthetic virus to be published in the open for terrorist groups or hawkish governments to use. And for the same reasons, we wouldn't want a Friendly AI textbook to explain how to build highly dangerous AI systems. But excepting that, I would love to see a rigorously technical textbook on friendliness theory, and I agree that friendliness research will need to increase for us to see that textbook be written in 15 years. Luckily, the Future of Humanity Institute is putting a special emphasis on AI risks for the next little while, and Singularity Institute is ramping up its own research efforts.</p>\n<p>But the most important thing I want to say is this. If you can take ideas and arguments that already exist in blog posts, emails, and human brains (for example at Singularity Institute) and turn them into working papers or maybe even journal articles, and you care about navigating the Singularity successfully, please contact me. My email address is luke@intelligence.org. If you're that kind of person who can do that kind of work, I really want to talk to you.</p>\n<p>I\u2019d estimate we have something like 30-40 papers just waiting to be written. The conceptual work has been done, we just need more researchers who can write this stuff up. So if you can do that, you should contact me: luke@intelligence.org.</p>\n<p>&nbsp;</p>\n<h4 id=\"Friendly_AI_Sub_Problems\">Friendly AI Sub-Problems</h4>\n<p>Next question. Less Wrong user \u2018XiXiDu\u2019 asks:</p>\n<blockquote>\n<p>If someone as capable as Terence Tao approached [Singularity Institute], asking if they could work full-time and for free on friendly AI, what would you tell them to do? In other words, are there any known FAI sub-problems that demand some sort of expertise that [Singularity Institute] is currently lacking?</p>\n</blockquote>\n<p>Terence Tao is a mathematician at UCLA who was a child prodigy and is considered by some people to be one of the smartest people on the planet. He is exactly the kind of person we need to successfully navigate the Singularity, and in particular to solve open problems in Friendly AI theory.</p>\n<p>I explained in my text-only interview with Michael Anissimov in September 2011 that the problem of Friendly AI breaks down into a large number of smaller and better-defined technical sub-problems. Some of the open problems I listed in that interview are the ones I\u2019d love somebody like Terence Tao to work on. For example:</p>\n<blockquote>\n<p>How can an agent make optimal decisions when it is capable of directly editing its own source code, including the source code of the decision mechanism? How can we get an AI to maintain a consistent utility function throughout updates to its ontology? How do we make an AI with preferences about the external world instead of about a reward signal? How can we generalize the theory of machine induction \u2014\u0080\u0094 called Solomonoff induction \u00e2\u2014 so that it can use higher-order logics and reason correctly about observation selection effects? How can we approximate such ideal processes such that they are computable?</p>\n</blockquote>\n<p>(That was a quote from the text-only interview.)</p>\n<p>But even before that, we\u2019d really like to write up explanations of these problems in all their technical detail, but again that takes researchers and funding and we\u2019re short on both. For now, I\u2019ll point you to <a href=\"http://www.youtube.com/watch?v=MwriJqBZyoM\">Eliezer\u2019s talk</a> at Singularity Summit 2011, which you can Google for.</p>\n<p>But yeah, we have a lot of technical problems that we'd like to clarify the nature of so that we can have researchers working on them.&nbsp;So we do need potential researchers to contact us.</p>\n<p>I loved watching <em>Batman</em> and <em>Superman</em> cartoons when I was a kid, but as it turns out, the heroes who can save the world are not those who have incredible strength or the power of flight. They are mathematicians and computer scientists.&nbsp;</p>\n<p>Singularity Institute needs heroes. If you are a brilliant mathematician or computer scientist and you want a shot at saving the world, contact me: luke@intelligence.org.</p>\n<p>I know it sounds corny, but I mean it. The world needs heroes.</p>\n<p>&nbsp;</p>\n<h4 id=\"Improved_Funding\">Improved Funding</h4>\n<p>Next, Less Wrong user \u2018XiXiDu\u2019 asks:</p>\n<blockquote>\n<p>What would [Singularity Institute] do given various amounts of money? Would it make a difference if you had 10 or 100 million dollars at your disposal...?</p>\n</blockquote>\n<p>Yes it would. Absolutely. If Bill Gates decided tomorrow that he wanted to save not just a billion people but the entire human race, and he gave us 100 million dollars, we would hire more researchers and figure out the best way to spend that money. That's a pretty big project in itself.</p>\n<p>But right now, my bet on how we\u2019d end up spending that money is that we would personally argue for our mission to each of the world\u2019s top mathematicians, AI researchers, physicists, and formal philosophers. The Terence Taos and Judea Pearls of the world. And for any of them who could be convinced, we\u2019d be able to offer them enough money to work for us. We\u2019d also hire several successful Oppenheimer-type research administrators who could help us bring these brilliant minds together to work on these problems.</p>\n<p>As nice as it is to have people from all over the world solving problems in mathematics, decision theory, agent architectures, and other fields collaboratively over the internet, there are a lot of things you can make move faster when you bring the smartest people in the world into one building and allow them to do nothing else but solve the world's most important problems.</p>\n<p>&nbsp;</p>\n<h4 id=\"Rationality\">Rationality</h4>\n<p>Next. Less Wrong user \u2018JoshuaZ\u2019 asks:</p>\n<blockquote>\n<p>A lot of Eliezer's work has been not at all related strongly to FAI but has been to popularizing rational thinking. In your view, should [Singularity Institute] focus exclusively on AI issues or should it also care about rational issues? In that context, how does Eliezer's ongoing work relate to [Singularity Institute]?</p>\n</blockquote>\n<p>Yes, it\u2019s a great question. Let me begin with the rationality work.</p>\n<p>I was already very interested in rationality before I found Less Wrong and Singularity Institute, but when I first encountered the arguments about intelligence explosion, one of my first thoughts was, \u201cUh-oh. Rationality is much more important than I had originally thought.\u201d</p>\n<p>Why? Intelligence explosion is a mind-warping, emotionally dangerous, intellectually difficult, and very uncertain field in which we don\u2019t get to do a dozen experiments so that reality can beat us over the head with the correct answer. Instead, when it comes to intelligence explosion scenarios, in order to get this right we have to transcend the normal human biases, emotions, and confusions of the human mind, and make the right predictions before we can run any experiments. We can't try an intelligence explosion and see how it turns out.</p>\n<p>Moreover, to even understand what the problem is, you\u2019ve got to get past a lot of usual biases and false but common beliefs. So we need a more sane world to solve these problems, and we need a saner world to have a larger community of support for addressing these issues.</p>\n<p>And, Eliezer\u2019s choice to work on rationality has paid off. <a href=\"http://wiki.lesswrong.com/wiki/Sequences\">The Sequences</a>, and the Less Wrong community that grew out of them, have been successful. We now have a large and active community of people growing in rationality and spreading it to others, and a subset of that community contributes to progress on problems related to AI. Even Eliezer\u2019s choice to write a rationality fanfiction, <em><a href=\"http://www.elsewhere.org/rationality/\">Harry Potter and the Methods of Rationality</a></em>, has \u2014 contrary to my expectations \u2014 had quite an impact. It is now the most popular Harry Potter fan fiction, I think, and it was responsible for perhaps \u00bc or \u2155 of the money raised during the <a href=\"http://intelligence.org/blog/2011/09/01/2011-summer-matching-challenge-success/\">2011 summer matching challenge</a>, and has brought several valuable new people into our community. Eliezer\u2019s forthcoming rationality books might have a similar type of effect.</p>\n<p>But we understand that many people don\u2019t see the connection between rationality and navigating the Singularity successfully the way that we do, so in our strategic plan we explained that we\u2019re working to spin off most of the rationality work to a separate organization. It doesn\u2019t have a name yet, but internally we just call it \u2018Rationality Org.\u2019 That way, Singularity Institute can focus on Singularity issues, and the Rationality Org (whatever it comes to be called) can focus on rationality, and people can support them independently. That\u2019s something else Eliezer has been working on, along with a couple of others.</p>\n<p>Of course, Eliezer does spend some of his time on AI issues, and he plans to return full-time to AI once Rationality Org is launched. But we need more talented researchers, and other contributions, in order to succeed on AI. Rationality has been helpful in attracting and enhancing a community that helps with those things.</p>\n<p>&nbsp;</p>\n<h4 id=\"Changing_Course\">Changing Course</h4>\n<p>Next. Less Wrong user \u2018JoshuaZ\u2019 asks:</p>\n<blockquote>\n<p>...are there specific sets of events (other than the advent of a Singularity) which you think will make [Singularity Institute] need to essentially reevaluate its goals and purpose at a fundamental level?</p>\n</blockquote>\n<p>Yes, and I can give a few examples that I wrote down.</p>\n<p>Right now we\u2019re focused on what happens when smarter-than-human intelligence arrives, because the evidence available suggests to us that AI will be more important than other crucial considerations. But suppose we made a series of discoveries that made it unlikely that AI would arrive anytime soon, but very likely that catastrophic biological terrorism was only a decade or two away, for example. In that situation, Singularity Institute would shift its efforts quite considerably.</p>\n<p>Another example: If other organizations were doing our work, including Friendly AI, and with better efficiency and scale, then it would make sense to fold Singularity Institute and transfer resources, donors, and staff to these other, more efficient and effective organizations.</p>\n<p>If it could be shown that some other process was much better at mobilizing efforts to address core issues, for example if <a href=\"http://www.givingwhatwecan.org/\">Giving What We Can</a> (an organization focused on optimal philanthropy) continues doubling each year and spinning off large numbers of skilled people to work on existential risk reduction (as one of the targets of optimal philanthropy), then focus there for a while could make sense \u2014 or at least it might make sense to strip away outreach functions from [Singularity Institute], perhaps leaving a core FAI team, and leave outreach to the optimal philanthropy community or something like that.</p>\n<p>So, those are just three ways that things could change or we could make some discoveries, and that would radically shift the strategy that we have at Singularity Institute.</p>\n<p>&nbsp;</p>\n<h4 id=\"Experimental_Research\">Experimental Research</h4>\n<p>Next. User \u2018XiXiDu\u2019 asks:</p>\n<blockquote>\n<p>Is [Singularity Institute] willing to pursue experimental AI research or does it solely focus on hypothetical aspects?</p>\n</blockquote>\n<p>Experimental research would, at this point, be a diversion from work on the most important problems related to our mission, which are technical problems in mathematics, computer science, and philosophy. If experimental research becomes more important than those problems in math, computer science, and philosophy, and if we had the funding available to do experiments, we would do experimental research at that time, or fund somebody else to do it. But those aren't the most important or most urgent problems that we need to solve.</p>\n<p>&nbsp;</p>\n<h4 id=\"Winning_Without_Friendly_AI\">Winning Without Friendly AI</h4>\n<p>Next. Less Wrong user \u2018Wei_Dai\u2019 asks:</p>\n<blockquote>\n<p>Much of [Singularity Institute\u2019s] research [is] focused not directly on [Friendly AI] but more generally on better understanding the dynamics of various scenarios that could lead to a Singularity. Such research could help us realize a positive Singularity through means other than directly building an [Friendly AI].</p>\n<p>Does [Singularity Institute] have any plans to expand such research activities, either in house, or by academia or independent researchers?</p>\n</blockquote>\n<p>The answer to that question is 'Yes'.</p>\n<p>Singularity Institute does not put all its eggs in the \u2018Friendly AI\u2019 basket. Intelligence explosion scenarios are complicated, the future is uncertain, and the feasibility of many possible strategies is unknown and uncertain. Both Singularity Institute and our friends at Future of Humanity Institute at Oxford have done quite a lot of work on these kinds of strategic considerations, things like <a href=\"http://en.wikipedia.org/wiki/Differential_technological_development\">differential technological development</a>. It\u2019s important work, so we plan to do more of it.</p>\n<p>Most of this work, however, hasn\u2019t been published. So if you want to see it published, put us in contact with people who are good at rapidly taking ideas and arguments out of different people's heads and putting them on paper. Or maybe you are that person! Right now we just don\u2019t have enough researchers to write these things up as much as we'd like. So contact me: luke@intelligence.org.</p>\n<p>&nbsp;</p>\n<h4 id=\"Conclusion\">Conclusion</h4>\n<p>Well, that\u2019s it! I'm sorry I can\u2019t answer all the questions.&nbsp;Doing this takes a lot more work than you might think, but if it is appreciated, and especially if it grows and encourages the community of people who are trying to make the world a better place and reduce existential risk, then I may try to do something like this \u2014 maybe without the video, maybe with the video \u2014 with some regularity.</p>\n<p>Keep in mind that I do have a personal feedback form at <a href=\"http://tinyurl.com/luke-feedback\">tinyurl.com/luke-feedback</a>, where you can send me feedback on myself and Singularity Institute. You can also check the Less Wrong page that will be dedicated to this Q&amp;A and leave some comments there.</p>\n<p>Thanks for listening and watching. This is Luke Muehlhauser, signing off.</p>", "sections": [{"title": "Intro", "anchor": "Intro", "level": 1}, {"title": "Staff Changes", "anchor": "Staff_Changes", "level": 1}, {"title": "Rigorous Research", "anchor": "Rigorous_Research", "level": 1}, {"title": "Friendly AI Sub-Problems", "anchor": "Friendly_AI_Sub_Problems", "level": 1}, {"title": "Improved Funding", "anchor": "Improved_Funding", "level": 1}, {"title": "Rationality", "anchor": "Rationality", "level": 1}, {"title": "Changing Course", "anchor": "Changing_Course", "level": 1}, {"title": "Experimental Research", "anchor": "Experimental_Research", "level": 1}, {"title": "Winning Without Friendly AI", "anchor": "Winning_Without_Friendly_AI", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "124 comments"}], "headingsCount": 12}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 124, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["G65tLdGma8Xgh3p7L", "4uQxZonCwCZtz39Hw", "4oWXnodxAu4WgHnrd", "cKD85YRn7fy95WkmN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-10T15:39:49.096Z", "modifiedAt": null, "url": null, "title": "Example of poor decision making under pressure (from game show)", "slug": "example-of-poor-decision-making-under-pressure-from-game", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:55.440Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Wilka", "createdAt": "2009-04-04T02:19:32.249Z", "isAdmin": false, "displayName": "Wilka"}, "userId": "ovpC9BgoTgHsForPa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/isSMDR8rMr5pTzJK5/example-of-poor-decision-making-under-pressure-from-game", "pageUrlRelative": "/posts/isSMDR8rMr5pTzJK5/example-of-poor-decision-making-under-pressure-from-game", "linkUrl": "https://www.lesswrong.com/posts/isSMDR8rMr5pTzJK5/example-of-poor-decision-making-under-pressure-from-game", "postedAtFormatted": "Saturday, December 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Example%20of%20poor%20decision%20making%20under%20pressure%20(from%20game%20show)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExample%20of%20poor%20decision%20making%20under%20pressure%20(from%20game%20show)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FisSMDR8rMr5pTzJK5%2Fexample-of-poor-decision-making-under-pressure-from-game%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Example%20of%20poor%20decision%20making%20under%20pressure%20(from%20game%20show)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FisSMDR8rMr5pTzJK5%2Fexample-of-poor-decision-making-under-pressure-from-game", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FisSMDR8rMr5pTzJK5%2Fexample-of-poor-decision-making-under-pressure-from-game", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 161, "htmlBody": "<p><a href=\"http://www.break.com/index/chick-loses-a-fortune-over-dumbest-question-ever-2261920\">http://www.break.com/index/chick-loses-a-fortune-over-dumbest-question-ever-2261920</a></p>\n<p>This is from a UK game show. The aim is to put the pile of money on the right answer, or if you're unsure you split it between multiple answers. Whatever money was on the right, you get to use for the next question - after 8 (I think) questions, you get to keep whatever you have left.</p>\n<p>The girl here didn't listen to the complete question, so is answering a different question. The host of show repeats the question very clearly several times, but the girl still&nbsp;doesn't&nbsp;notice.</p>\n<p>The combination of high stakes (&pound;1,000,000 in this case) and time&nbsp;pressure&nbsp;are clearly too much for the couple. The girl will probably feel like it was her fault, but I found what the guy did quite interesting as well - he can see the answer makes no sense, and tries to point out the correct one. But the girls confidence in her answer makes him go along with that one, even though it makes no sense.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "isSMDR8rMr5pTzJK5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 10, "extendedScore": null, "score": 8.128770328602895e-07, "legacy": true, "legacyId": "11383", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-10T19:31:23.032Z", "modifiedAt": null, "url": null, "title": "People with Experience in Wikipedia Editing?", "slug": "people-with-experience-in-wikipedia-editing", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:55.174Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MichaelAnissimov", "createdAt": "2009-03-21T20:49:52.763Z", "isAdmin": false, "displayName": "MichaelAnissimov"}, "userId": "tkZmAXciPjSumi4Wk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jixgDdRFk6Gkka4WZ/people-with-experience-in-wikipedia-editing", "pageUrlRelative": "/posts/jixgDdRFk6Gkka4WZ/people-with-experience-in-wikipedia-editing", "linkUrl": "https://www.lesswrong.com/posts/jixgDdRFk6Gkka4WZ/people-with-experience-in-wikipedia-editing", "postedAtFormatted": "Saturday, December 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20People%20with%20Experience%20in%20Wikipedia%20Editing%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APeople%20with%20Experience%20in%20Wikipedia%20Editing%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjixgDdRFk6Gkka4WZ%2Fpeople-with-experience-in-wikipedia-editing%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=People%20with%20Experience%20in%20Wikipedia%20Editing%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjixgDdRFk6Gkka4WZ%2Fpeople-with-experience-in-wikipedia-editing", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjixgDdRFk6Gkka4WZ%2Fpeople-with-experience-in-wikipedia-editing", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 58, "htmlBody": "<p>Hi all,</p>\n<p>At the Singularity Institute we're looking for a volunteer with experience making edits to Wikipedia. The quality of some Wikipedia pages related to our subject matter could use improvement, but we would like to consult with someone who has an editing background on the way to go about it.&nbsp;</p>\n<p>Please get in touch with me at michael@intelligence.org.</p>\n<p>Thank you!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"uCuS2DModz3eisEdv": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jixgDdRFk6Gkka4WZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 8.129613536216346e-07, "legacy": true, "legacyId": "11384", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-10T20:46:23.012Z", "modifiedAt": null, "url": null, "title": "An akrasia case study", "slug": "an-akrasia-case-study", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:02.054Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "f5v8QJsBuPMFKFqt7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xpLXck2nXbE4K3xdE/an-akrasia-case-study", "pageUrlRelative": "/posts/xpLXck2nXbE4K3xdE/an-akrasia-case-study", "linkUrl": "https://www.lesswrong.com/posts/xpLXck2nXbE4K3xdE/an-akrasia-case-study", "postedAtFormatted": "Saturday, December 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20An%20akrasia%20case%20study&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAn%20akrasia%20case%20study%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxpLXck2nXbE4K3xdE%2Fan-akrasia-case-study%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=An%20akrasia%20case%20study%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxpLXck2nXbE4K3xdE%2Fan-akrasia-case-study", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxpLXck2nXbE4K3xdE%2Fan-akrasia-case-study", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 797, "htmlBody": "<p>I just lost 3 weeks to a report that should have taken 2 days. My last job was an engineering research position; setting up an experiment, building prototypes, that sort of thing. After I left, I needed to write a report to brief my successor on what I'd done and what could go wrong, etc. I wasn't getting paid for this report, but it had to happen.</p>\r\n<p>What exactly do I mean when I say I lost three weeks?</p>\r\n<p>I have a lot of projects that I am working on. I am studying AI, thinking of starting a business, writing videogames, studying and working on various math things, writing a small sequence of posts for lesswrong, trying to restart the local rationality dojo, and I had to do that report. What I mean when I say that I lost three weeks is that I spent three weeks doing practically <em>none</em> of these things.</p>\r\n<p>The report had to be done, but I wasn't really excited by it. It wasn't urgent, but it was urgent enough that it had to be done before any of my other projects. It turns out this is a killer combination.</p>\r\n<p>Procrastination took over, manifesting itself as skyrim, 4chan, reddit, and lesswrong. If I tried procrastinating by doing my other projects, I would remember that I had to do the report first, and try to work on the report. When I tried to work on the report, I would hit some small bump and find myself waking up on 4chan three hours later. Somehow, my antiprocrastination hooks were catching my own projects, but not the properly unproductive stuff.</p>\r\n<p>While I had that report to do, I was unable to do anything else productive. When I realized this in conjunction with how important my other projects were, the report suddenly took on a dire urgency. That was four days ago. It is done now. I could have done it in two, or even one, but procrastination is insidious.</p>\r\n<p>One anti-akrasia method that seems to work is going cold turkey on some problematic activity. I call it my personal banhammer. The first thing I banned myself from and how I discovered I could was Alicorn's Twilight fanfic. It ate up a few days and disrupted my sleeping, so I stopped reading right in an exciting part. Haven't gone back. That was before the report. Once I had the report to do, my roommate got skyrim. I spent a few days on skyrim, then realized what I was doing and banned myself. For the next few weeks, I procrastinated on 4chan, lesswrong, reddit, and some game development websites. When I finally realized how important it was to finish that report, I got the power to ban myself from those (I had tried and failed before).</p>\r\n<p>Even when I finally cared enough to actually do the report, I still found myself procrastinating. I read some essays by Paul Graham. They were so good that I explicitly put reading his stuff on my todo list. When I wasn't doing my report, I was reading Paul Graham. I don't feel so bad because it was actually productive for me on a personal development level, and his essays are at least finite so I was making actual progress on a todo item. It was still not what I wanted to be doing.</p>\r\n<p>So what did I learn from this little excercise?</p>\r\n<ol>\r\n<li>\r\n<p>An unappealing but semi-urgent project can sabotage you completely, because you don't procrastinate by doing the next project on your list; you procrastinate by doing the least productive activity you will allow yourself to do.</p>\r\n<p>It seems this can partially be beaten by just realizing what is happening and how much damage it is doing to you. Realizing what is happening promotes the project to \"unappealing but direly urgent\", which makes it easier to do.</p>\r\n</li>\r\n<li>\r\n<p>You can raise the quality of your procrastination into at least the semi-productive by wielding the righteous power of the banhammer against unproductive activities. This takes practice.</p>\r\n<p>It may be a good plan for rationality dojos to find ways of training this. One idea is to simply emulate what it took me to develop it; acquire a minor addiction, realize that it is consuming your life, and then go cold turkey. May not be so easy (or safe), but worth looking into.</p>\r\n</li>\r\n</ol>\r\n<p>This akrasia stuff seems to be inherently personal, so what worked for me may not work for anyone else, but I publish it here in the hope that we can pull some good ideas out of it. Maybe you have a project that is holding you back the way that damn report got me. Maybe this can help.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"r7qAjcbfhj2256EHH": 1, "dqx5k65wjFfaiJ9sQ": 1, "pnSXfWXbQihrFadeD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xpLXck2nXbE4K3xdE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 44, "extendedScore": null, "score": 8.129886668061541e-07, "legacy": true, "legacyId": "11385", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-10T22:30:15.928Z", "modifiedAt": null, "url": null, "title": "Clarification of AI Reflection Problem", "slug": "clarification-of-ai-reflection-problem", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:55.192Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "paulfchristiano", "createdAt": "2010-07-28T17:04:08.586Z", "isAdmin": false, "displayName": "paulfchristiano"}, "userId": "gb44edJjXhte8DA3A", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yTHYf5hLoSJNFXcYb/clarification-of-ai-reflection-problem", "pageUrlRelative": "/posts/yTHYf5hLoSJNFXcYb/clarification-of-ai-reflection-problem", "linkUrl": "https://www.lesswrong.com/posts/yTHYf5hLoSJNFXcYb/clarification-of-ai-reflection-problem", "postedAtFormatted": "Saturday, December 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Clarification%20of%20AI%20Reflection%20Problem&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AClarification%20of%20AI%20Reflection%20Problem%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyTHYf5hLoSJNFXcYb%2Fclarification-of-ai-reflection-problem%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Clarification%20of%20AI%20Reflection%20Problem%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyTHYf5hLoSJNFXcYb%2Fclarification-of-ai-reflection-problem", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyTHYf5hLoSJNFXcYb%2Fclarification-of-ai-reflection-problem", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2122, "htmlBody": "<p>Consider an agent A, aware of its own embedding in some lawful universe, able to reason about itself and use that reasoning to inform action. &nbsp;By interacting with the world, A is able to modify itself or construct new agents, and using these abilities effectively is likely to be an important component of AGI. &nbsp;Our current understanding appears to be inadequate for guiding such an agent's behavior, for (at least) the following reason:</p>\n<p>If A does not believe \"A's beliefs reflect reality,\" then A will lose interest in creating further copies of itself, improving its own reasoning, or performing natural self-modifications. Indeed, if A's beliefs don't reflect reality then creating more copies of A or spending more time thinking may do more harm than good. But if A does believe \"A's beliefs reflect reality,\" then A runs immediately into G&ouml;delian problems: for example, does A become convinced of the sentence Q = \"A does not believe Q\"?&nbsp;We need to find a way for A to have some confidence in its own behavior without running into these fundamental difficulties with reflection.&nbsp;</p>\n<p>This problem has been discussed occasionally at Less Wrong, but I would like to clarify and lay out some examples before trying to start in on a resolution.&nbsp;</p>\n<h2>G&ouml;del Machines</h2>\n<p>The <a href=\"http://www.idsia.ch/~juergen/ultimatecognition.pdf\">G&ouml;del machine</a> is a formalism described by Shmidhuber for principled self-modification. A G&ouml;del machine is designed to solve some particular object level problem in its allotted time. I will describe one G&ouml;del machine implementation.</p>\n<p>The initial machine A has an arbitrary object level problem solver. Before running the object level problem solver, however, A spends half of its time enumerating pairs of strings (A', P); for each one, if A' is a valid description of an agent and P is a proof that A' does better on the object level task than A, then A transforms into A'.&nbsp;</p>\n<p>Now suppose that A's initial search for self-modifications is inefficient: a new candidate agent A' has a more efficient proof checker, and so is able to examine more proofs before defaulting to the original object level problem solver. Intuitively, A' is an improvement over A, and this is (a very mild version of) the sort of self-modification which we would like our agents to perform. For example, a human is easily able to recognize that it can achieve higher utility by modifying itself to think faster. But we will see that under most conditions A cannot prove that A' is an improvement, because A is not able to assume its own consistency (as we do in our justification that A' is an improvement).</p>\n<p>Suppose A' considers some new self-modification (A'', P) not considered by A. We would like A to be able to prove that <em>if</em>&nbsp;A' executes this self-modification--if P is a proof that A'' is an improvement over A'--then A'' is really an improvement over A' (and therefore over A). We say that a pair (A'', P) is safe if it has this property, i.e., if either P is not a proof that A'' is an improvement over A', or if A'' is really an improvement over A'.</p>\n<p>A can prove that particular pairs are safe, but suppose A' considers many candidate modifications (A'', P1), (A'', P2), ..., (A'', Pk). It is now much harder for A to show that all of these self-modifications are safe, without relying directly on the consistency of A's axioms system (which A can never assume, by the incompleteness theorem).&nbsp;If the set of considered proofs is sufficiently rich, then by a bounded version of L&ouml;b's theorem A can only prove that all of these self-modifications are safe if it can prove directly that A'' is an improvement over A'. But in that case A might as well dispense with theorem proving altogether and just commit to transforming into A'' after exhausting all other self-modifications.</p>\n<p>(Originally this post was centered around a formal version of the above argument, but although I believe the argument works it turns out to be simultaneously very difficult and very unenlightening. Here is the main difficulty: in order to justify converting into A', A only needs to prove that A' won't execute any destructive modifications <em>if</em>&nbsp;A' doesn't make some other modification before reaching any of the new proposals, and this additional assumption is very difficult to work with. In general, it is very hard to prove anything at all about a G&ouml;del machine's behavior, which is unfortunate because a G&ouml;del machine must do exactly this in order to self-modify.)&nbsp;</p>\n<p>This informal argument does not show that a G&ouml;del machine will not execute any self-improvements, but it does show that if it does execute any non-trivial self-improvements then it must appeal to some justification other than the intuitive one. Other modifications, for example any modification which is so radical as to change the set of self-modifications considered in a more complicated way or to change the architecture fundamentally, seem to be even harder to justify than the one described here. G&ouml;del machines may be able to self-modify, but this argument shows that this conclusion is not supported by intuitive arguments about the existence of beneficial self-modifications.&nbsp;</p>\n<h2>Creating New Agents</h2>\n<p>We can also try a non-reflective technique for designing an \"optimal problem solver,\" in the spirit of <a href=\"http://www.hutter1.net/ai/pfastprg.pdf\">Hutter search</a>. Again, suppose we have a particular object level problem and some allotted time.</p>\n<p>Consider the agent A which behaves as follows: in the first half of the allotted time, it enumerates pairs (B, P), and if B is a valid description of an algorithm and P is a proof that B has utility at least U on the object level problem (using at most half of the allotted time), then A stores the pair (B, U). After half of the allotted time, the pair (B, U) with maximal U is selected, and then the algorithm B is run using the remaining time.&nbsp;</p>\n<p>This solution seems like it may avoid some of the trouble encountered by the G&ouml;del machine, in exchange for being somewhat less ambitious in its goals. But we will see that the same reflection problem arises.</p>\n<p>If the allotted time is very large, and if self-modification is an important component of general intelligence, then we expect A to select a new algorithm B which is itself capable of (provably useful) self-modification. In principle A could simply run long enough to become a very powerful non-self-modifying agent; but it may be that finding a very powerful agent B directly is much more difficult then proceeding through a sequence of successively more powerful agents, each intelligent enough to design the next but not to leap directly to B.&nbsp;</p>\n<p>But suppose A creates a new algorithm B which uses the same axioms as A to prove theorems, and uses these theorems to guide its own self-modification. Again, by L&ouml;b's theorem we see that A cannot guarantee that B's self-modifications are really beneficial. This means that A is unable to rule out the possibility that B executes a very bad self-modification (or creates a new agent with very bad behavior); but without ruling out the possibility that B performs very badly, A cannot run B instead of some more conservative algorithm which does not do any self-modification.&nbsp;</p>\n<p>This does not rule out the possibility that A creates a new algorithm B' which pursues self-modification using a similar strategy but with a more restricted set of axioms--one which can be proven consistent from within A. And in turn, if B' takes the same form as A, then any algorithm C which B' creates or modifies into will use a smaller still set of axioms by the same argument. Eliezer describes the proof-theoretic strength of such an agent as a 'telomere'--with every self-modification it becomes weaker, and for G&ouml;delian reasons it cannot be increased without sacrificing provable correctness.&nbsp;</p>\n<p>(The distinction between the G&ouml;del machine and creating new agents is inessential; for example, we could imagine a G&ouml;del machine attempt to self-modify by improving the efficiency of its proof searcher and then using the additional time to consider some rewrites which are justified by a more restricted axiom system.)</p>\n<h2>Possible Solutions</h2>\n<p>There are some natural solutions which have been considered.&nbsp;</p>\n<h4>Long Telomeres</h4>\n<p>Recognizing that an agent must decrease its proof-theoretic strength when performing a self-modification, we may just try to construct an initial agent with substantial proof-theoretic strength. For example, if we believe that PA probably has sufficient logical expressiveness to facilitate useful superintelligence, we may create an agent with axioms PA + \"PA is consistent\" + \"PA + 'PA is consistent' is consistent\" and so on. We may iterate this process a finite number of times to allow for a few self-modifications, or perhaps add an entire countable chain so that the original AI can decide how much proof-theoretic strength its first descendant needs, or perhaps add a few countable chains so that the agent can self-modify a handful of times before needing to make that decision, or so on.&nbsp;</p>\n<p>The primary objection to this solution is philosophical: humans appear to be able to reason about self-modification much more robustly (or at least with better results) than an AI using any of the logical frameworks discussed so far, and it is quite ambiguous whether or not a system with 'long telomeres' can ever justify becoming a system which is actually able to reason about self-modification correctly (or, if we have doubts about humans' capacities, whether such a system even exists). Minimally, if we want to understand the form an AI is likely to take after self-modification, we need to understand how such a system would eventually escape from problems that currently seem inescapable; without such understanding the resulting behavior is hard to get a handle on (although we might still be satisfied, if that behavior is backed up by formal justification produced by the AI).&nbsp;</p>\n<p>Lesser objections focus on the additional complexity required to maintain such a long telomere. Each additional consistency axiom added to a system essentially requires recursively specifying the entire agent as it exists so far, and the complexity of the mathematical manipulations required seems to grow rapidly. This argument seems to be fairly unimportant, and so if the philosophical objections were dispatched with I would consider building agents with very strong proof systems a feasible path forward.&nbsp;</p>\n<h4>Beliefs rather than Proofs</h4>\n<p>Humans seem to be able to avoid G&ouml;delian problems with reflection, but they may do this by having confidence in statements which they cannot prove, and manipulating uncertainty about logical facts rather than dealing in absolute assertions of truth. The set of \"statements I strongly believe\" seems to differ in important ways from the set of \"statements I can prove\" in several important ways; for example, if I am confident of a statement at one point in time I am not committed to remaining confident indefinitely.&nbsp;</p>\n<p>I suspect that a satisfactory theory of reflective beliefs would go far towards resolving the AI reflection problem, but this is hardly more than a restatement of the problem. Translating from proofs to beliefs does not immediately resolve the problems with self-reference, it just replaces them with subtly different issues. For example, it is easy to see that an agent should not believe that its own beliefs are well-calibrated on all questions, and so we must attempt to formalize a version of the self-consistency hypothesis which is weak enough to be defensible but still strong enough to escape the shortcomings described above. I hope to make several posts on this topic in the near future.&nbsp;</p>\n<p>&nbsp;</p>\n<h4>Ignorance</h4>\n<p>Understanding this issue may not be necessary to building safe AGI. Indeed, self-modification may ultimately play a minimal role in intelligence, or we may settle for executing self-modifications using weaker justification. However if we accept usual arguments about the importance of FAI, then we should not be satisfied with this solution.&nbsp;</p>\n<p>The importance of self-modification is an open question which has received much discussion here and elsewhere. It is worth adding that, to the extent that we are concerned with influencing probable outcomes for humanity, the highest leverage scenarios seem to be those in which self-modification tends to result in positive feedback loops and takeoff (if we assign such scenarios significant weight). That is, in such scenarios we should be particularly cautious about building self-modifying systems, but there is also a much greater imperative to understand how to design safe and stable AI.&nbsp;</p>\n<p>Standard arguments surrounding FAI (particularly, the importance of early AI goal systems and the fragility of humane value) suggest that agents should have high degrees of confidence in a change before executing it. If an agent's beliefs are not correctly related to reality, the resulting behavior may be as dangerous as if the agent's valus were modified. For example, incorrect beliefs about logical structure which cause that agent to fail to preserve its own values in subsequent rewrites, or incorrect beliefs about the relationships between value and reality.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"AJDHQ4mFnsNbBzPhT": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yTHYf5hLoSJNFXcYb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 32, "extendedScore": null, "score": 8.130265009607952e-07, "legacy": true, "legacyId": "11340", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Consider an agent A, aware of its own embedding in some lawful universe, able to reason about itself and use that reasoning to inform action. &nbsp;By interacting with the world, A is able to modify itself or construct new agents, and using these abilities effectively is likely to be an important component of AGI. &nbsp;Our current understanding appears to be inadequate for guiding such an agent's behavior, for (at least) the following reason:</p>\n<p>If A does not believe \"A's beliefs reflect reality,\" then A will lose interest in creating further copies of itself, improving its own reasoning, or performing natural self-modifications. Indeed, if A's beliefs don't reflect reality then creating more copies of A or spending more time thinking may do more harm than good. But if A does believe \"A's beliefs reflect reality,\" then A runs immediately into G\u00f6delian problems: for example, does A become convinced of the sentence Q = \"A does not believe Q\"?&nbsp;We need to find a way for A to have some confidence in its own behavior without running into these fundamental difficulties with reflection.&nbsp;</p>\n<p>This problem has been discussed occasionally at Less Wrong, but I would like to clarify and lay out some examples before trying to start in on a resolution.&nbsp;</p>\n<h2 id=\"G_del_Machines\">G\u00f6del Machines</h2>\n<p>The <a href=\"http://www.idsia.ch/~juergen/ultimatecognition.pdf\">G\u00f6del machine</a> is a formalism described by Shmidhuber for principled self-modification. A G\u00f6del machine is designed to solve some particular object level problem in its allotted time. I will describe one G\u00f6del machine implementation.</p>\n<p>The initial machine A has an arbitrary object level problem solver. Before running the object level problem solver, however, A spends half of its time enumerating pairs of strings (A', P); for each one, if A' is a valid description of an agent and P is a proof that A' does better on the object level task than A, then A transforms into A'.&nbsp;</p>\n<p>Now suppose that A's initial search for self-modifications is inefficient: a new candidate agent A' has a more efficient proof checker, and so is able to examine more proofs before defaulting to the original object level problem solver. Intuitively, A' is an improvement over A, and this is (a very mild version of) the sort of self-modification which we would like our agents to perform. For example, a human is easily able to recognize that it can achieve higher utility by modifying itself to think faster. But we will see that under most conditions A cannot prove that A' is an improvement, because A is not able to assume its own consistency (as we do in our justification that A' is an improvement).</p>\n<p>Suppose A' considers some new self-modification (A'', P) not considered by A. We would like A to be able to prove that <em>if</em>&nbsp;A' executes this self-modification--if P is a proof that A'' is an improvement over A'--then A'' is really an improvement over A' (and therefore over A). We say that a pair (A'', P) is safe if it has this property, i.e., if either P is not a proof that A'' is an improvement over A', or if A'' is really an improvement over A'.</p>\n<p>A can prove that particular pairs are safe, but suppose A' considers many candidate modifications (A'', P1), (A'', P2), ..., (A'', Pk). It is now much harder for A to show that all of these self-modifications are safe, without relying directly on the consistency of A's axioms system (which A can never assume, by the incompleteness theorem).&nbsp;If the set of considered proofs is sufficiently rich, then by a bounded version of L\u00f6b's theorem A can only prove that all of these self-modifications are safe if it can prove directly that A'' is an improvement over A'. But in that case A might as well dispense with theorem proving altogether and just commit to transforming into A'' after exhausting all other self-modifications.</p>\n<p>(Originally this post was centered around a formal version of the above argument, but although I believe the argument works it turns out to be simultaneously very difficult and very unenlightening. Here is the main difficulty: in order to justify converting into A', A only needs to prove that A' won't execute any destructive modifications <em>if</em>&nbsp;A' doesn't make some other modification before reaching any of the new proposals, and this additional assumption is very difficult to work with. In general, it is very hard to prove anything at all about a G\u00f6del machine's behavior, which is unfortunate because a G\u00f6del machine must do exactly this in order to self-modify.)&nbsp;</p>\n<p>This informal argument does not show that a G\u00f6del machine will not execute any self-improvements, but it does show that if it does execute any non-trivial self-improvements then it must appeal to some justification other than the intuitive one. Other modifications, for example any modification which is so radical as to change the set of self-modifications considered in a more complicated way or to change the architecture fundamentally, seem to be even harder to justify than the one described here. G\u00f6del machines may be able to self-modify, but this argument shows that this conclusion is not supported by intuitive arguments about the existence of beneficial self-modifications.&nbsp;</p>\n<h2 id=\"Creating_New_Agents\">Creating New Agents</h2>\n<p>We can also try a non-reflective technique for designing an \"optimal problem solver,\" in the spirit of <a href=\"http://www.hutter1.net/ai/pfastprg.pdf\">Hutter search</a>. Again, suppose we have a particular object level problem and some allotted time.</p>\n<p>Consider the agent A which behaves as follows: in the first half of the allotted time, it enumerates pairs (B, P), and if B is a valid description of an algorithm and P is a proof that B has utility at least U on the object level problem (using at most half of the allotted time), then A stores the pair (B, U). After half of the allotted time, the pair (B, U) with maximal U is selected, and then the algorithm B is run using the remaining time.&nbsp;</p>\n<p>This solution seems like it may avoid some of the trouble encountered by the G\u00f6del machine, in exchange for being somewhat less ambitious in its goals. But we will see that the same reflection problem arises.</p>\n<p>If the allotted time is very large, and if self-modification is an important component of general intelligence, then we expect A to select a new algorithm B which is itself capable of (provably useful) self-modification. In principle A could simply run long enough to become a very powerful non-self-modifying agent; but it may be that finding a very powerful agent B directly is much more difficult then proceeding through a sequence of successively more powerful agents, each intelligent enough to design the next but not to leap directly to B.&nbsp;</p>\n<p>But suppose A creates a new algorithm B which uses the same axioms as A to prove theorems, and uses these theorems to guide its own self-modification. Again, by L\u00f6b's theorem we see that A cannot guarantee that B's self-modifications are really beneficial. This means that A is unable to rule out the possibility that B executes a very bad self-modification (or creates a new agent with very bad behavior); but without ruling out the possibility that B performs very badly, A cannot run B instead of some more conservative algorithm which does not do any self-modification.&nbsp;</p>\n<p>This does not rule out the possibility that A creates a new algorithm B' which pursues self-modification using a similar strategy but with a more restricted set of axioms--one which can be proven consistent from within A. And in turn, if B' takes the same form as A, then any algorithm C which B' creates or modifies into will use a smaller still set of axioms by the same argument. Eliezer describes the proof-theoretic strength of such an agent as a 'telomere'--with every self-modification it becomes weaker, and for G\u00f6delian reasons it cannot be increased without sacrificing provable correctness.&nbsp;</p>\n<p>(The distinction between the G\u00f6del machine and creating new agents is inessential; for example, we could imagine a G\u00f6del machine attempt to self-modify by improving the efficiency of its proof searcher and then using the additional time to consider some rewrites which are justified by a more restricted axiom system.)</p>\n<h2 id=\"Possible_Solutions\">Possible Solutions</h2>\n<p>There are some natural solutions which have been considered.&nbsp;</p>\n<h4 id=\"Long_Telomeres\">Long Telomeres</h4>\n<p>Recognizing that an agent must decrease its proof-theoretic strength when performing a self-modification, we may just try to construct an initial agent with substantial proof-theoretic strength. For example, if we believe that PA probably has sufficient logical expressiveness to facilitate useful superintelligence, we may create an agent with axioms PA + \"PA is consistent\" + \"PA + 'PA is consistent' is consistent\" and so on. We may iterate this process a finite number of times to allow for a few self-modifications, or perhaps add an entire countable chain so that the original AI can decide how much proof-theoretic strength its first descendant needs, or perhaps add a few countable chains so that the agent can self-modify a handful of times before needing to make that decision, or so on.&nbsp;</p>\n<p>The primary objection to this solution is philosophical: humans appear to be able to reason about self-modification much more robustly (or at least with better results) than an AI using any of the logical frameworks discussed so far, and it is quite ambiguous whether or not a system with 'long telomeres' can ever justify becoming a system which is actually able to reason about self-modification correctly (or, if we have doubts about humans' capacities, whether such a system even exists). Minimally, if we want to understand the form an AI is likely to take after self-modification, we need to understand how such a system would eventually escape from problems that currently seem inescapable; without such understanding the resulting behavior is hard to get a handle on (although we might still be satisfied, if that behavior is backed up by formal justification produced by the AI).&nbsp;</p>\n<p>Lesser objections focus on the additional complexity required to maintain such a long telomere. Each additional consistency axiom added to a system essentially requires recursively specifying the entire agent as it exists so far, and the complexity of the mathematical manipulations required seems to grow rapidly. This argument seems to be fairly unimportant, and so if the philosophical objections were dispatched with I would consider building agents with very strong proof systems a feasible path forward.&nbsp;</p>\n<h4 id=\"Beliefs_rather_than_Proofs\">Beliefs rather than Proofs</h4>\n<p>Humans seem to be able to avoid G\u00f6delian problems with reflection, but they may do this by having confidence in statements which they cannot prove, and manipulating uncertainty about logical facts rather than dealing in absolute assertions of truth. The set of \"statements I strongly believe\" seems to differ in important ways from the set of \"statements I can prove\" in several important ways; for example, if I am confident of a statement at one point in time I am not committed to remaining confident indefinitely.&nbsp;</p>\n<p>I suspect that a satisfactory theory of reflective beliefs would go far towards resolving the AI reflection problem, but this is hardly more than a restatement of the problem. Translating from proofs to beliefs does not immediately resolve the problems with self-reference, it just replaces them with subtly different issues. For example, it is easy to see that an agent should not believe that its own beliefs are well-calibrated on all questions, and so we must attempt to formalize a version of the self-consistency hypothesis which is weak enough to be defensible but still strong enough to escape the shortcomings described above. I hope to make several posts on this topic in the near future.&nbsp;</p>\n<p>&nbsp;</p>\n<h4 id=\"Ignorance\">Ignorance</h4>\n<p>Understanding this issue may not be necessary to building safe AGI. Indeed, self-modification may ultimately play a minimal role in intelligence, or we may settle for executing self-modifications using weaker justification. However if we accept usual arguments about the importance of FAI, then we should not be satisfied with this solution.&nbsp;</p>\n<p>The importance of self-modification is an open question which has received much discussion here and elsewhere. It is worth adding that, to the extent that we are concerned with influencing probable outcomes for humanity, the highest leverage scenarios seem to be those in which self-modification tends to result in positive feedback loops and takeoff (if we assign such scenarios significant weight). That is, in such scenarios we should be particularly cautious about building self-modifying systems, but there is also a much greater imperative to understand how to design safe and stable AI.&nbsp;</p>\n<p>Standard arguments surrounding FAI (particularly, the importance of early AI goal systems and the fragility of humane value) suggest that agents should have high degrees of confidence in a change before executing it. If an agent's beliefs are not correctly related to reality, the resulting behavior may be as dangerous as if the agent's valus were modified. For example, incorrect beliefs about logical structure which cause that agent to fail to preserve its own values in subsequent rewrites, or incorrect beliefs about the relationships between value and reality.</p>\n<p>&nbsp;</p>", "sections": [{"title": "G\u00f6del Machines", "anchor": "G_del_Machines", "level": 1}, {"title": "Creating New Agents", "anchor": "Creating_New_Agents", "level": 1}, {"title": "Possible Solutions", "anchor": "Possible_Solutions", "level": 1}, {"title": "Long Telomeres", "anchor": "Long_Telomeres", "level": 2}, {"title": "Beliefs rather than Proofs", "anchor": "Beliefs_rather_than_Proofs", "level": 2}, {"title": "Ignorance", "anchor": "Ignorance", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "9 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-10T22:49:04.594Z", "modifiedAt": null, "url": null, "title": "Why I write about the basics", "slug": "why-i-write-about-the-basics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:20.743Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/SA2ZRgXJBDoGjGrjb/why-i-write-about-the-basics", "pageUrlRelative": "/posts/SA2ZRgXJBDoGjGrjb/why-i-write-about-the-basics", "linkUrl": "https://www.lesswrong.com/posts/SA2ZRgXJBDoGjGrjb/why-i-write-about-the-basics", "postedAtFormatted": "Saturday, December 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20I%20write%20about%20the%20basics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20I%20write%20about%20the%20basics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSA2ZRgXJBDoGjGrjb%2Fwhy-i-write-about-the-basics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20I%20write%20about%20the%20basics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSA2ZRgXJBDoGjGrjb%2Fwhy-i-write-about-the-basics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSA2ZRgXJBDoGjGrjb%2Fwhy-i-write-about-the-basics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 169, "htmlBody": "<p>In a thread on <a href=\"/lw/7dy/a_rationalists_tale/\">A Rationalist's Tale</a>, lessdazed <a href=\"/lw/7dy/a_rationalists_tale/4t1r\">wrote</a>:</p>\n<blockquote>\n<p>Being levels above in [rationality] means doing rationalist practice 101 much better than others, [just like] being a few levels above in fighting means executing a basic front-kick much better than others.</p>\n</blockquote>\n<p>Eliezer replied:</p>\n<blockquote>\n<p>I regret that I only have one upvote to give this comment.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>You may have noticed I write mostly about <a href=\"/lw/3mm/back_to_the_basics_of_rationality/\">the basics&nbsp;of rationality</a>, and lessdazed's comment explains why. There's something like the <a href=\"http://en.wikipedia.org/wiki/Pareto_principle\">80-20 rule</a> going on here: 80% of the benefits come from 20% of the rationality skills. We aspiring rationalists don't usually fail because we failed to account for <a href=\"/lw/76k/the_optimizers_curse_and_how_to_beat_it/\">the optimizer's curse</a>, but because we fail at a more basic level: we&nbsp;<a href=\"/lw/8ir/where_do_i_most_obviously_still_need_to_say_oops/\">fail to say \"oops\"</a>,&nbsp;or we decide we have an incurable disease called \"akrasia\" instead of <a href=\"/lw/3w3/how_to_beat_procrastination/\">doing that which is known to fix akrasia</a>.</p>\n<p>More writing on the basics of rationality is needed, especially if it involves <em>exercises</em>&nbsp;and <em>training</em>&nbsp;in addition to&nbsp;reading. Less Wrong could use more work on <a href=\"/lw/5x8/teachable_rationality_skills/\">teachable rationality skills</a>, like the skill of <a href=\"/lw/8ib/connecting_your_beliefs_a_call_for_help/\">connecting your beliefs</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "SA2ZRgXJBDoGjGrjb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 26, "extendedScore": null, "score": 6.9e-05, "legacy": true, "legacyId": "11386", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 22, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["9WX59u7g2sdKqnjDm", "gfexKxsBDM6v2sCMo", "5gQLrJr2yhPzMCcni", "BgpnbaJMthXjDeHcE", "RWo4LwFzpHNQCTcYt", "f4CZNEHirweN3XEjs", "kHL6qX9eArmvNWY99"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-10T22:54:24.208Z", "modifiedAt": null, "url": null, "title": "Just the facts, ma'am!", "slug": "just-the-facts-ma-am", "viewCount": null, "lastCommentedAt": "2021-12-04T05:40:34.202Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7YLuXtKqWiybJAmeo/just-the-facts-ma-am", "pageUrlRelative": "/posts/7YLuXtKqWiybJAmeo/just-the-facts-ma-am", "linkUrl": "https://www.lesswrong.com/posts/7YLuXtKqWiybJAmeo/just-the-facts-ma-am", "postedAtFormatted": "Saturday, December 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Just%20the%20facts%2C%20ma'am!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJust%20the%20facts%2C%20ma'am!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7YLuXtKqWiybJAmeo%2Fjust-the-facts-ma-am%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Just%20the%20facts%2C%20ma'am!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7YLuXtKqWiybJAmeo%2Fjust-the-facts-ma-am", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7YLuXtKqWiybJAmeo%2Fjust-the-facts-ma-am", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 346, "htmlBody": "<p>&ldquo;There&rsquo;s something odd about the experience of talking to [Singularity Institute researcher] <a href=\"/lw/7ob/timeline_of_carl_shulman_publications/\">Carl Shulman</a>,&rdquo; I said.</p>\n<p>&ldquo;He never blinks?&rdquo; said my friend.</p>\n<p>&ldquo;No. I mean: Yes, but that&rsquo;s not what I was thinking of.&rdquo;</p>\n<p>&ldquo;He speaks only facts.&rdquo;</p>\n<p>I paused.</p>\n<p>&ldquo;Yes,&rdquo; I said. &ldquo;<em>That</em> is what I meant.&rdquo;<a id=\"more\"></a></p>\n<p>Normally, when I ask someone &ldquo;Do you think human-level AI will arrive in the next 30 years?&rdquo; or &ldquo;Should we encourage faster development of whole brain emulation?&rdquo; I get answers like &ldquo;Yes&rdquo; or &ldquo;No, I don&rsquo;t think so.&rdquo;</p>\n<p>When I ask <em>Carl</em> a question like &ldquo;Do you think human-level AI will arrive in the next 30 years?&rdquo; he instead begins to state known facts relevant to answering the question, such as facts about the history of Moore&rsquo;s law, progress in algorithms, trends in scientific progress, past examples of self-improving systems, and so on.</p>\n<p>Maybe this is a bit rude. Carl didn&rsquo;t answer my question about his opinion. He answered a different question instead, about facts.</p>\n<p>But I never <em>feel</em> like it&rsquo;s rude. Carl went out of his way to make his answer more useful to me. His testimony alone would have been helpful, but <a href=\"/lw/lx/argument_screens_off_authority/\">argument screens off authority</a>, so Carl&rsquo;s &ldquo;flood of facts&rdquo; way of answering questions gives me more evidence about what&rsquo;s true than his mere opinion could.</p>\n<p>Why isn&rsquo;t this more common? For one thing, most people don&rsquo;t know many facts. I&rsquo;ve <em>read</em> a lot of facts, but do I remember most of them? Hell no. If I forced myself to respond to questions only by stating facts, I&rsquo;d be worried that I have fewer facts available to me than I&rsquo;d like to admit. I often have to tell people: &ldquo;I can&rsquo;t remember the details in that paper but I remember thinking his evidence was weak.&rdquo;</p>\n<p>But it's worth a try. I <em>think</em>&nbsp;I've noticed that when I try to answer with facts more often, my brain is primed to remember them better, as if it's thinking: \"Oh, I might actually <em>use</em>&nbsp;this fact in conversation, so I should remember it.\" But I haven't measured this, so I could be fooling myself.</p>\n<p>Also see: <a href=\"http://www.overcomingbias.com/2009/02/share-likelihood-ratios-not-posterior-beliefs.html\">Share likelihood ratios, not posterior beliefs</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 1, "AADZcNS24mmSfPp2w": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7YLuXtKqWiybJAmeo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 57, "extendedScore": null, "score": 0.000118, "legacy": true, "legacyId": "11387", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 57, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4uQxZonCwCZtz39Hw", "5yFRd3cjLpm3Nd6Di"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-10T22:54:38.294Z", "modifiedAt": null, "url": null, "title": "For those in the Stanford AI-class", "slug": "for-those-in-the-stanford-ai-class", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:03.789Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oZdgkWSpC8B9pu8Ws/for-those-in-the-stanford-ai-class", "pageUrlRelative": "/posts/oZdgkWSpC8B9pu8Ws/for-those-in-the-stanford-ai-class", "linkUrl": "https://www.lesswrong.com/posts/oZdgkWSpC8B9pu8Ws/for-those-in-the-stanford-ai-class", "postedAtFormatted": "Saturday, December 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20For%20those%20in%20the%20Stanford%20AI-class&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFor%20those%20in%20the%20Stanford%20AI-class%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoZdgkWSpC8B9pu8Ws%2Ffor-those-in-the-stanford-ai-class%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=For%20those%20in%20the%20Stanford%20AI-class%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoZdgkWSpC8B9pu8Ws%2Ffor-those-in-the-stanford-ai-class", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoZdgkWSpC8B9pu8Ws%2Ffor-those-in-the-stanford-ai-class", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 34, "htmlBody": "<p>I posted an obligatory FAI question for the Thrun/Norvig office hours. You can vote here&nbsp;<a href=\"https://www.ai-class.com/sforum/?modSeries=19a660&amp;modTopic=45&amp;modSubmission=5cabd7\">https://www.ai-class.com/sforum/?modSeries=19a660&amp;modTopic=45&amp;modSubmission=5cabd7</a>. Only top questions will be answered :)</p>\n<p>ETA: it's now the top question in the category by a nice margin!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oZdgkWSpC8B9pu8Ws", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 11, "extendedScore": null, "score": 8.130353780156927e-07, "legacy": true, "legacyId": "11388", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-10T23:25:39.363Z", "modifiedAt": null, "url": null, "title": "Why we need better science, example #6,281", "slug": "why-we-need-better-science-example-6-281", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:55.118Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pnzg3qwCcNYRuLjNh/why-we-need-better-science-example-6-281", "pageUrlRelative": "/posts/pnzg3qwCcNYRuLjNh/why-we-need-better-science-example-6-281", "linkUrl": "https://www.lesswrong.com/posts/pnzg3qwCcNYRuLjNh/why-we-need-better-science-example-6-281", "postedAtFormatted": "Saturday, December 10th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20we%20need%20better%20science%2C%20example%20%236%2C281&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20we%20need%20better%20science%2C%20example%20%236%2C281%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpnzg3qwCcNYRuLjNh%2Fwhy-we-need-better-science-example-6-281%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20we%20need%20better%20science%2C%20example%20%236%2C281%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpnzg3qwCcNYRuLjNh%2Fwhy-we-need-better-science-example-6-281", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fpnzg3qwCcNYRuLjNh%2Fwhy-we-need-better-science-example-6-281", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 290, "htmlBody": "<p>\n<p><a href=\"http://www.amazon.com/Powerful-Medicines-Benefits-Risks-Prescription/dp/1400030781/\">Avorn (2004)</a> reports:</p>\n<blockquote>\n<p>In a former British colony, most healers believed the conventional wisdom that a distillation of fluids extracted from the urine of horses, if dried to a powder and fed to aging women, could act as a general tonic, preserve youth, and ward of a variety of diseases. The preparation became enormously popular throughout the culture, and was widely used by older women in all strata of society. Many years later modern scientific studies revealed that long-term ingestion of the horse-urine extract was useless for most of its intended purposes, and that it causes tumors, blood clots, heart disease, and perhaps brain damage.</p>\n<p>The former colony is the United States; the time is now; the drug is the family of hormone replacement products that include Prempro and Premarin (manufactured from pregnant mares' urine, hence its name). For decades, estrogen replacement in postmenopausal women was widely believed to have \"cardio-protective\" properties; other papers in respected medical journals reported that the drugs could treat depression and incontinence, as well as prevent Alzheimer's disease. The first large, well-conducted, controlled clinical trial of this treatment in women was not published until 1998: it found that estrogen replacement actually increased the rate of heart attacks in the patients studied. Another clinical trial published in 2002 presented further evidence that these products increased the risk of heart disease, stroke, and cancer. Further reports a year later found that rather than preventing Alzheimer's disease, the drugs appeared to double the risk of becoming senile.&nbsp;</p>\n</blockquote>\n<p><a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.84.1684&amp;rep=rep1&amp;type=pdf\">Armstrong (2006)</a> adds:</p>\n<blockquote>\n<p>The treatment seemed to work because those who used the drug tended to be healthier than those who did not. This was because it was used by people who were more interested in taking care of their health.</p>\n</blockquote>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xHjy88N2uJvGdgzfw": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pnzg3qwCcNYRuLjNh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 43, "extendedScore": null, "score": 0.000139, "legacy": true, "legacyId": "11389", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 32, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-11T05:52:20.328Z", "modifiedAt": null, "url": null, "title": "Bostrom, \"Existential Risk Prevention as the Most Important Task for Humanity\" (Dec. 2011)", "slug": "bostrom-existential-risk-prevention-as-the-most-important", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:54.598Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jA57y4hC2rEyd9csM/bostrom-existential-risk-prevention-as-the-most-important", "pageUrlRelative": "/posts/jA57y4hC2rEyd9csM/bostrom-existential-risk-prevention-as-the-most-important", "linkUrl": "https://www.lesswrong.com/posts/jA57y4hC2rEyd9csM/bostrom-existential-risk-prevention-as-the-most-important", "postedAtFormatted": "Sunday, December 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Bostrom%2C%20%22Existential%20Risk%20Prevention%20as%20the%20Most%20Important%20Task%20for%20Humanity%22%20(Dec.%202011)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABostrom%2C%20%22Existential%20Risk%20Prevention%20as%20the%20Most%20Important%20Task%20for%20Humanity%22%20(Dec.%202011)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjA57y4hC2rEyd9csM%2Fbostrom-existential-risk-prevention-as-the-most-important%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Bostrom%2C%20%22Existential%20Risk%20Prevention%20as%20the%20Most%20Important%20Task%20for%20Humanity%22%20(Dec.%202011)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjA57y4hC2rEyd9csM%2Fbostrom-existential-risk-prevention-as-the-most-important", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjA57y4hC2rEyd9csM%2Fbostrom-existential-risk-prevention-as-the-most-important", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 20, "htmlBody": "<p><a href=\"http://www.existential-risk.org/concept.pdf\">Link</a>. An excellent, highly compressed summary of the relevant issues. Previously titled \"The Concept of Existential Risk,\" but substantially rewritten.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jA57y4hC2rEyd9csM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 13, "extendedScore": null, "score": 8.131875403706443e-07, "legacy": true, "legacyId": "11391", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-11T06:20:13.818Z", "modifiedAt": null, "url": null, "title": "Are multiple uploads equivilant to extra life?", "slug": "are-multiple-uploads-equivilant-to-extra-life", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:07.973Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MileyCyrus", "createdAt": "2011-06-13T22:40:17.423Z", "isAdmin": false, "displayName": "MileyCyrus"}, "userId": "8CmnnrsSHHMsNC9ku", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MXChwHZ6T5jLdnRZY/are-multiple-uploads-equivilant-to-extra-life", "pageUrlRelative": "/posts/MXChwHZ6T5jLdnRZY/are-multiple-uploads-equivilant-to-extra-life", "linkUrl": "https://www.lesswrong.com/posts/MXChwHZ6T5jLdnRZY/are-multiple-uploads-equivilant-to-extra-life", "postedAtFormatted": "Sunday, December 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Are%20multiple%20uploads%20equivilant%20to%20extra%20life%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAre%20multiple%20uploads%20equivilant%20to%20extra%20life%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMXChwHZ6T5jLdnRZY%2Fare-multiple-uploads-equivilant-to-extra-life%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Are%20multiple%20uploads%20equivilant%20to%20extra%20life%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMXChwHZ6T5jLdnRZY%2Fare-multiple-uploads-equivilant-to-extra-life", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMXChwHZ6T5jLdnRZY%2Fare-multiple-uploads-equivilant-to-extra-life", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 78, "htmlBody": "<p>Suppose I have choice between the following:</p>\n<p>&nbsp;</p>\n<p>A) One simulation of me is run for me 100 years, before being deleted.</p>\n<p>B) Two <strong>identical</strong> simulations of me are run for 100 years, before being deleted.</p>\n<p>Is the second choice preferable to the first? Should I be willing to pay more to have multiple copies of me simulated, even if those copies will have the exact same experiences?</p>\n<p>&nbsp;</p>\n<p>Forgive me if this question has been answered before. I have Googled to no avail.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MXChwHZ6T5jLdnRZY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 4, "extendedScore": null, "score": 8.131975278132827e-07, "legacy": true, "legacyId": "11392", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-11T06:48:43.026Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The American System and Misleading Labels", "slug": "seq-rerun-the-american-system-and-misleading-labels", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:01.864Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/j22Jp3W3bcfbaJKb6/seq-rerun-the-american-system-and-misleading-labels", "pageUrlRelative": "/posts/j22Jp3W3bcfbaJKb6/seq-rerun-the-american-system-and-misleading-labels", "linkUrl": "https://www.lesswrong.com/posts/j22Jp3W3bcfbaJKb6/seq-rerun-the-american-system-and-misleading-labels", "postedAtFormatted": "Sunday, December 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20American%20System%20and%20Misleading%20Labels&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20American%20System%20and%20Misleading%20Labels%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj22Jp3W3bcfbaJKb6%2Fseq-rerun-the-american-system-and-misleading-labels%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20American%20System%20and%20Misleading%20Labels%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj22Jp3W3bcfbaJKb6%2Fseq-rerun-the-american-system-and-misleading-labels", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj22Jp3W3bcfbaJKb6%2Fseq-rerun-the-american-system-and-misleading-labels", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 232, "htmlBody": "<p>Today's post, <a href=\"/lw/mh/the_american_system_and_misleading_labels/\">The American System and Misleading Labels</a> was originally published on 02 January 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The conclusions we draw from analyzing the American political system are often biased by our own previous understanding of it, which we got in elementary school. In fact, the power of voting for a particular candidate (which is not the same as the power to choose which candidates will run) is not the greatest power of the voters. Instead, voters' main abilities are the threat to change which party controls the government, or extremely rarely, to completely dethrone both political parties and replace them with a third.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8s2/seq_rerun_the_twoparty_swindle/\">The Two-Party Swindle</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "j22Jp3W3bcfbaJKb6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 10, "extendedScore": null, "score": 8.132080820378996e-07, "legacy": true, "legacyId": "11393", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZXuqNhMDcs6mYtb6i", "aocmRL3gZ3MZTtCMS", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-11T09:23:01.146Z", "modifiedAt": null, "url": null, "title": "[POLL] LessWrong census, mindkilling edition [closed, now with results]", "slug": "poll-lesswrong-census-mindkilling-edition-closed-now-with", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:04.335Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Oligopsony", "createdAt": "2010-08-03T16:27:12.586Z", "isAdmin": false, "displayName": "Oligopsony"}, "userId": "pyLy9zaTeZRW42iin", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4GQoEcbvXp2mAWzPi/poll-lesswrong-census-mindkilling-edition-closed-now-with", "pageUrlRelative": "/posts/4GQoEcbvXp2mAWzPi/poll-lesswrong-census-mindkilling-edition-closed-now-with", "linkUrl": "https://www.lesswrong.com/posts/4GQoEcbvXp2mAWzPi/poll-lesswrong-census-mindkilling-edition-closed-now-with", "postedAtFormatted": "Sunday, December 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BPOLL%5D%20LessWrong%20census%2C%20mindkilling%20edition%20%5Bclosed%2C%20now%20with%20results%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BPOLL%5D%20LessWrong%20census%2C%20mindkilling%20edition%20%5Bclosed%2C%20now%20with%20results%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4GQoEcbvXp2mAWzPi%2Fpoll-lesswrong-census-mindkilling-edition-closed-now-with%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BPOLL%5D%20LessWrong%20census%2C%20mindkilling%20edition%20%5Bclosed%2C%20now%20with%20results%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4GQoEcbvXp2mAWzPi%2Fpoll-lesswrong-census-mindkilling-edition-closed-now-with", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4GQoEcbvXp2mAWzPi%2Fpoll-lesswrong-census-mindkilling-edition-closed-now-with", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 186, "htmlBody": "<p>Some have been curious about what the politics of this community would look like if broken down further; <a href=\"https://docs.google.com/spreadsheet/viewform?formkey=dGhrQlZBR1Z2djAyS1FMWDVTeXB1bmc6MQ\">here</a>'s&nbsp;a shot at figuring it out. I've also included a few other questions that folks expressed curiosity about. Aside from one sensitive question, there's no option to keep your answers private, since in my opinion that would defeat the point - just don't answer if you have concerns - but there's also no overlap with the old survey, aside from asking you how you answered the original politics question. (This should help with interpreting those results even if the n for this is much lower than and somehow biased relative to the big survey.)</p>\n<p>For entertainment purposes only, don't use the below space to discuss politics directly, &amp;c. Early suggestions are likely to be incorporated, given what I assume to be the low quality of the first draft.</p>\n<p><strong>Edit: </strong>\"left\" and \"right\" operationalized for the questions they appear in; poor language cleared up in mental health question.</p>\n<p><strong>Edit 2:</strong>&nbsp;results <a href=\"https://docs.google.com/spreadsheet/pub?hl=en_US&amp;hl=en_US&amp;key=0As-7F6uV7Q62dGFNWGhPOVJ3OGJkYVNRZWoyWDdpTVE&amp;output=html\">here</a>; see comment below for some preliminary thoughts. Because there were several unique regional responses, I did not publish responses that question.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4GQoEcbvXp2mAWzPi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 10, "extendedScore": null, "score": 8.132643073105067e-07, "legacy": true, "legacyId": "11394", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 43, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-11T12:00:55.418Z", "modifiedAt": null, "url": null, "title": "Humans Shouldn't make Themselves Smarter?", "slug": "humans-shouldn-t-make-themselves-smarter", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:57.680Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "potato", "createdAt": "2011-06-15T09:18:51.735Z", "isAdmin": false, "displayName": "Ronny"}, "userId": "kY5hs2WkacnSZd937", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cJE4ob5fciTMT7Xu5/humans-shouldn-t-make-themselves-smarter", "pageUrlRelative": "/posts/cJE4ob5fciTMT7Xu5/humans-shouldn-t-make-themselves-smarter", "linkUrl": "https://www.lesswrong.com/posts/cJE4ob5fciTMT7Xu5/humans-shouldn-t-make-themselves-smarter", "postedAtFormatted": "Sunday, December 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Humans%20Shouldn't%20make%20Themselves%20Smarter%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHumans%20Shouldn't%20make%20Themselves%20Smarter%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcJE4ob5fciTMT7Xu5%2Fhumans-shouldn-t-make-themselves-smarter%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Humans%20Shouldn't%20make%20Themselves%20Smarter%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcJE4ob5fciTMT7Xu5%2Fhumans-shouldn-t-make-themselves-smarter", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcJE4ob5fciTMT7Xu5%2Fhumans-shouldn-t-make-themselves-smarter", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 28, "htmlBody": "<p>Just thought you guys should know about <a href=\"http://www.sciencedaily.com/releases/2011/12/111207104821.htm\">this</a>. Some work that argues that humans should not enhance their intelligence with technology, and that super intelligence probably never evolves.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cJE4ob5fciTMT7Xu5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": -5, "extendedScore": null, "score": -1.1e-05, "legacy": true, "legacyId": "11395", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-11T20:59:46.344Z", "modifiedAt": null, "url": null, "title": "A Brief Summary of Recent Happenings at the Singularity Institute", "slug": "a-brief-summary-of-recent-happenings-at-the-singularity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:54.904Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MichaelAnissimov", "createdAt": "2009-03-21T20:49:52.763Z", "isAdmin": false, "displayName": "MichaelAnissimov"}, "userId": "tkZmAXciPjSumi4Wk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uJDpRFycS3Qp89cxz/a-brief-summary-of-recent-happenings-at-the-singularity", "pageUrlRelative": "/posts/uJDpRFycS3Qp89cxz/a-brief-summary-of-recent-happenings-at-the-singularity", "linkUrl": "https://www.lesswrong.com/posts/uJDpRFycS3Qp89cxz/a-brief-summary-of-recent-happenings-at-the-singularity", "postedAtFormatted": "Sunday, December 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20Brief%20Summary%20of%20Recent%20Happenings%20at%20the%20Singularity%20Institute&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20Brief%20Summary%20of%20Recent%20Happenings%20at%20the%20Singularity%20Institute%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuJDpRFycS3Qp89cxz%2Fa-brief-summary-of-recent-happenings-at-the-singularity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20Brief%20Summary%20of%20Recent%20Happenings%20at%20the%20Singularity%20Institute%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuJDpRFycS3Qp89cxz%2Fa-brief-summary-of-recent-happenings-at-the-singularity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuJDpRFycS3Qp89cxz%2Fa-brief-summary-of-recent-happenings-at-the-singularity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 349, "htmlBody": "<p>Posted at&nbsp;<a style=\"font-family: Verdana, Arial, Helvetica, sans-serif; line-height: normal; font-size: small;\" href=\"http://intelligence.org/blog/2011/12/11/a-brief-summary-of-recent-happenings-at-the-singularity-institute/\">http://singinst.org/blog/2011/12/11/a-brief-summary-of-recent-happenings-at-the-singularity-institute/</a></p>\n<p>The Singularity Summit was a huge success. We raised over $300,000, one-third of that a donation from Jaan Tallinn, another third from a variety of generous donors who gave during or immediately after the Summit, and a third from ticket sales. We have roughly $500,000 in the bank, while annual payroll expenses are about $350,000. We almost have enough money to make it through the next year without adding any research staff, though we definitely would like to add additional researchers. In the meantime, our (unpaid)&nbsp;<a href=\"http://intelligence.org/aboutus/researchassociates\" target=\"_blank\">Research Associates</a>&nbsp;team has been growing, and is tackling a variety of projects.</p>\n<p>Since the Singularity Summit in October, our President Michael Vassar moved on to help found&nbsp;<a href=\"http://www.medicineispersonal.com/\" target=\"_blank\">Personalized Medicine</a>, a company we are all excited about, and Luke Muehlhauser was appointed Executive Director. Luke answered questions about these changes and our future plans in a recent&nbsp;<a href=\"/r/discussion/lw/8s6/video_qa_with_singularity_institute_executive/\" target=\"_blank\">video Q&amp;A</a>.&nbsp;I encourage you to watch it, and submit additional questions you may have about the Singularity Institute.</p>\n<p>We have seven staff at this time: Luke, Michael Anissimov, Anna Salamon, Carl Shulman, Amy Willey, Louie Helm, and Eliezer Yudkowsky. Our internal collaboration has increased, we're keeping work logs, and we regularly eat dinner together. It's becoming more of a family.</p>\n<p>What are we all doing? Luke and Michael are working on a new website for the Singularity Institute. Amy and Luke are working hard to expand the Singularity Summit brand and bring the Summit to \"the next level.\" Anna and Eliezer are building a curriculum for the new Rationality Org, which we hope to spin off as a separate organization from Singularity Institute sometime next year. Luke, Anna, and Carl have been working on a variety of research papers, and Carl has also been working with the fast-growing \"optimal philanthropy\" movement, which is now poised to direct substantial funds over the next few years to reducing existential risks. Louie is working on donor relations, fundraising, recruiting, operations, and has also contributed to some of our forthcoming research articles.</p>\n<p>In the coming year, we look forward to improving communication and transparency with our supporters, and to increasing the rate of our published research output.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uJDpRFycS3Qp89cxz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 24, "extendedScore": null, "score": 8.13518279176464e-07, "legacy": true, "legacyId": "11397", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["yGZHQYqWkLMbXy3z7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-11T21:08:09.977Z", "modifiedAt": null, "url": null, "title": "Where do you live? Meetup planners want to know", "slug": "where-do-you-live-meetup-planners-want-to-know", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:56.492Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Metus", "createdAt": "2011-01-23T21:54:34.357Z", "isAdmin": false, "displayName": "Metus"}, "userId": "mNQ4fSvro7LYgrii4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HXJiAALt7DSnL46pp/where-do-you-live-meetup-planners-want-to-know", "pageUrlRelative": "/posts/HXJiAALt7DSnL46pp/where-do-you-live-meetup-planners-want-to-know", "linkUrl": "https://www.lesswrong.com/posts/HXJiAALt7DSnL46pp/where-do-you-live-meetup-planners-want-to-know", "postedAtFormatted": "Sunday, December 11th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Where%20do%20you%20live%3F%20Meetup%20planners%20want%20to%20know&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhere%20do%20you%20live%3F%20Meetup%20planners%20want%20to%20know%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHXJiAALt7DSnL46pp%2Fwhere-do-you-live-meetup-planners-want-to-know%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Where%20do%20you%20live%3F%20Meetup%20planners%20want%20to%20know%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHXJiAALt7DSnL46pp%2Fwhere-do-you-live-meetup-planners-want-to-know", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHXJiAALt7DSnL46pp%2Fwhere-do-you-live-meetup-planners-want-to-know", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 156, "htmlBody": "<p><strong>Disclaimer:</strong> English is not my mother's tongue so I am prone to make mistakes. Please correct and forgive me if I do.</p>\n<p>In the recent LessWrong survey 1090 people responded. Sadly, information about the place of residence was not asked for but could have been very useful to people willing to plan a meetup. Since a similar questionaire in German was quite successful with 24 respondents I now translated the form to english and ask you to provide the information.</p>\n<p>I ask you only to provide your country and general area of residence via postal code. The form is hosted at Google Docs and the spreadsheet will be published in a few days to ensure anonymity for the first few respondents. The data can not be traced back to specific individuals and would be useless in most cases.</p>\n<ul>\n<li><a href=\"https://docs.google.com/spreadsheet/viewform?hl=en_US&amp;formkey=dHJSSEV4Zy0zcXZBeVJyeGtKd0ZncHc6MQ#gid=0\">Link to the form</a></li>\n<li><a href=\"https://docs.google.com/spreadsheet/ccc?key=0ArVL5H0yXwKFdHJSSEV4Zy0zcXZBeVJyeGtKd0ZncHc\">Link to the responses</a></li>\n</ul>\n<p>Have fun and please provide feedback in the comments.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HXJiAALt7DSnL46pp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 18, "extendedScore": null, "score": 8.135213396603356e-07, "legacy": true, "legacyId": "11398", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-12T01:54:15.428Z", "modifiedAt": null, "url": null, "title": "khan Academy - Rationality series?", "slug": "khan-academy-rationality-series", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:55.543Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Goobahman", "createdAt": "2011-01-13T05:09:28.962Z", "isAdmin": false, "displayName": "Goobahman"}, "userId": "cidN68rGuy4wwnvFp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6kyKYzdnzLmH3bHNN/khan-academy-rationality-series", "pageUrlRelative": "/posts/6kyKYzdnzLmH3bHNN/khan-academy-rationality-series", "linkUrl": "https://www.lesswrong.com/posts/6kyKYzdnzLmH3bHNN/khan-academy-rationality-series", "postedAtFormatted": "Monday, December 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20khan%20Academy%20-%20Rationality%20series%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Akhan%20Academy%20-%20Rationality%20series%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6kyKYzdnzLmH3bHNN%2Fkhan-academy-rationality-series%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=khan%20Academy%20-%20Rationality%20series%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6kyKYzdnzLmH3bHNN%2Fkhan-academy-rationality-series", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6kyKYzdnzLmH3bHNN%2Fkhan-academy-rationality-series", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 195, "htmlBody": "<p>Does the LW community think it might be a good idea to approach Khan Academy about including a series on Rationality, based on some of the LW sequences?</p>\r\n<p>Khan Academy already has a great reputation as a centre for learning, and has large amounts of traffic going through it. Perhaps we could take advantage of that?</p>\r\n<p>What would be the pros and cons?</p>\r\n<p>How would we go about doing it?</p>\r\n<p>Has anyone made a video series of any of our sequences that we could offer?</p>\r\n<p>&nbsp;</p>\r\n<p>13.12.11 update:<br /><br />It sounds like this is a really good idea that just hasn't seen action yet. I'd actually really be interested in helping make this happen, and whilst I probably personally lack the skills to do it all, I'm sure that between the contributors here we have enough contacts/expertise to put something forward.<br /><br />If you'd be interested in being a part of this please message me your email address, and I will look into making a team who can put some resources into making this happen.<br /><br />And keep the comments/suggestions coming. They're providing me with great direction in terms of how to go about this.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fH8jPjHF2R27sRTTG": 1, "7ow6EFpypbH4hzFuz": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6kyKYzdnzLmH3bHNN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 23, "extendedScore": null, "score": 8.136256629293772e-07, "legacy": true, "legacyId": "11410", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-12T02:14:03.205Z", "modifiedAt": null, "url": null, "title": "Mitt Romney's $10,000 bet", "slug": "mitt-romney-s-usd10-000-bet", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:58.698Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MileyCyrus", "createdAt": "2011-06-13T22:40:17.423Z", "isAdmin": false, "displayName": "MileyCyrus"}, "userId": "8CmnnrsSHHMsNC9ku", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kMonMPSsNdbPBRZ6y/mitt-romney-s-usd10-000-bet", "pageUrlRelative": "/posts/kMonMPSsNdbPBRZ6y/mitt-romney-s-usd10-000-bet", "linkUrl": "https://www.lesswrong.com/posts/kMonMPSsNdbPBRZ6y/mitt-romney-s-usd10-000-bet", "postedAtFormatted": "Monday, December 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Mitt%20Romney's%20%2410%2C000%20bet&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMitt%20Romney's%20%2410%2C000%20bet%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkMonMPSsNdbPBRZ6y%2Fmitt-romney-s-usd10-000-bet%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Mitt%20Romney's%20%2410%2C000%20bet%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkMonMPSsNdbPBRZ6y%2Fmitt-romney-s-usd10-000-bet", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkMonMPSsNdbPBRZ6y%2Fmitt-romney-s-usd10-000-bet", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 188, "htmlBody": "<p>For those who don't follow politics, Mitt Romney offered to bet Rick Perry $10,000 that Perry had misquoted Romney. (<a href=\"http://abcnews.go.com/Politics/video/mitt-romney-offers-rick-perry-10000-bet-spar-health-care-obama-politics-15130859\">video</a>)<a href=\"http://www.newser.com/story/135156/mitt-romneys-10k-debate-bet-a-bad-move-say-critics.html\"></a></p>\n<p><a href=\"http://www.newser.com/story/135156/mitt-romneys-10k-debate-bet-a-bad-move-say-critics.html\">Most political commenters see the move as a gaffe.</a> They claim the bet made Romney look out of touch, because it reminded voters that Romney is rich enough to afford $10,000.</p>\n<p>As a believer in prediction markets, I am disappointed in the public's reaction. Romney made a bold move by making his beliefs pay rent. Critics point out that $10,000 is \"chump change\" for Romney, but Romney still but himself at risk. If he had lost the bet, Perry could have made a production about cashing a $10,000 check from a disgraced Romney. Besides, if money were the issue, Perry could have countered with a non-monetary bet.&nbsp; \"Loser has to attend the next debate in a clown suit\" or something.</p>\n<p>If politicians had to face real consequences every time they made a false statement, they would have a larger incentive to tell the truth. It's a shame Romney's bet probably won't catch on.</p>\n<p>----</p>\n<p>This post is not an endorsement of Mitt Romney or his politics. All I am endorsing is political betting.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"E8PHMuf7tsr8teXAe": 1, "FkzScn5byCs9PxGsA": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kMonMPSsNdbPBRZ6y", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 46, "baseScore": 50, "extendedScore": null, "score": 8.136328825554557e-07, "legacy": true, "legacyId": "11411", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 31, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-12T02:54:45.042Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Stop Voting For Nincompoops", "slug": "seq-rerun-stop-voting-for-nincompoops", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:32.690Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gSKq3pp4tkAbBf2Lc/seq-rerun-stop-voting-for-nincompoops", "pageUrlRelative": "/posts/gSKq3pp4tkAbBf2Lc/seq-rerun-stop-voting-for-nincompoops", "linkUrl": "https://www.lesswrong.com/posts/gSKq3pp4tkAbBf2Lc/seq-rerun-stop-voting-for-nincompoops", "postedAtFormatted": "Monday, December 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Stop%20Voting%20For%20Nincompoops&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Stop%20Voting%20For%20Nincompoops%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgSKq3pp4tkAbBf2Lc%2Fseq-rerun-stop-voting-for-nincompoops%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Stop%20Voting%20For%20Nincompoops%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgSKq3pp4tkAbBf2Lc%2Fseq-rerun-stop-voting-for-nincompoops", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgSKq3pp4tkAbBf2Lc%2Fseq-rerun-stop-voting-for-nincompoops", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 239, "htmlBody": "<p>Today's post, <a href=\"/lw/mi/stop_voting_for_nincompoops/\">Stop Voting For Nincompoops</a> was originally published on 02 January 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Many people try to vote \"strategically\", by considering which candidate is more \"electable\". One of the most important factors in whether someone is \"electable\" is whether they have received attention from the media and the support of one of the two major parties. Naturally, those organizations put considerable thought into who is electable in making their decision. Ultimately, all arguments for \"strategic voting\" tend to fall apart. The voters themselves get so little say in why the next president is that the best we can do is just to not vote for nincompoops.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/8sh/seq_rerun_the_american_system_and_misleading/\">The American System and Misleading Labels</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gSKq3pp4tkAbBf2Lc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.136477250436824e-07, "legacy": true, "legacyId": "11412", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 55, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["k5qPoHFgjyxtvYsm7", "j22Jp3W3bcfbaJKb6", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-12T03:38:34.164Z", "modifiedAt": null, "url": null, "title": "Meetup : Any Salt Lake City residents who might be interested in a meetup?", "slug": "meetup-any-salt-lake-city-residents-who-might-be-interested", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:23.632Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "adamisom", "createdAt": "2011-06-13T03:19:15.520Z", "isAdmin": false, "displayName": "adamisom"}, "userId": "eT8NPFmc2GDb5QFfc", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/d4E4ZhDQwa3384A66/meetup-any-salt-lake-city-residents-who-might-be-interested", "pageUrlRelative": "/posts/d4E4ZhDQwa3384A66/meetup-any-salt-lake-city-residents-who-might-be-interested", "linkUrl": "https://www.lesswrong.com/posts/d4E4ZhDQwa3384A66/meetup-any-salt-lake-city-residents-who-might-be-interested", "postedAtFormatted": "Monday, December 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Any%20Salt%20Lake%20City%20residents%20who%20might%20be%20interested%20in%20a%20meetup%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Any%20Salt%20Lake%20City%20residents%20who%20might%20be%20interested%20in%20a%20meetup%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd4E4ZhDQwa3384A66%2Fmeetup-any-salt-lake-city-residents-who-might-be-interested%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Any%20Salt%20Lake%20City%20residents%20who%20might%20be%20interested%20in%20a%20meetup%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd4E4ZhDQwa3384A66%2Fmeetup-any-salt-lake-city-residents-who-might-be-interested", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd4E4ZhDQwa3384A66%2Fmeetup-any-salt-lake-city-residents-who-might-be-interested", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 204, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/5j'>Any Salt Lake City residents who might be interested in a meetup?</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">29 December 2011 08:37:29PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Salt Lake City, Utah</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>(This post has been edited to reflect the fact that several people have expressed interest.) As for the purpose of the meeting, I'd love to hear suggestions. I will think about it in the meantime, and also let the first meetup refine the direction of a potentially regular meetup. Just to clarify, the date and time listed are not for a planned meetup. It looks like there are already a healthy amount of people interested, when I count four others, so I find this very encouraging! To make this work, it would be helpful to create a mailing list, so I'll ask for emails later or will create a Meetup group. The idea is that about mid-January I'll clarify the purpose and gather availability in terms of day, time, and place, and we'll first meet in the last week of January. TO CLARIFY: the date is NOT a suggested a meetup. I will make a second announcement in mid-January.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/5j'>Any Salt Lake City residents who might be interested in a meetup?</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "d4E4ZhDQwa3384A66", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 8.136637064822644e-07, "legacy": true, "legacyId": "11413", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Any_Salt_Lake_City_residents_who_might_be_interested_in_a_meetup_\">Discussion article for the meetup : <a href=\"/meetups/5j\">Any Salt Lake City residents who might be interested in a meetup?</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">29 December 2011 08:37:29PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Salt Lake City, Utah</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>(This post has been edited to reflect the fact that several people have expressed interest.) As for the purpose of the meeting, I'd love to hear suggestions. I will think about it in the meantime, and also let the first meetup refine the direction of a potentially regular meetup. Just to clarify, the date and time listed are not for a planned meetup. It looks like there are already a healthy amount of people interested, when I count four others, so I find this very encouraging! To make this work, it would be helpful to create a mailing list, so I'll ask for emails later or will create a Meetup group. The idea is that about mid-January I'll clarify the purpose and gather availability in terms of day, time, and place, and we'll first meet in the last week of January. TO CLARIFY: the date is NOT a suggested a meetup. I will make a second announcement in mid-January.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Any_Salt_Lake_City_residents_who_might_be_interested_in_a_meetup_1\">Discussion article for the meetup : <a href=\"/meetups/5j\">Any Salt Lake City residents who might be interested in a meetup?</a></h2>", "sections": [{"title": "Discussion article for the meetup : Any Salt Lake City residents who might be interested in a meetup?", "anchor": "Discussion_article_for_the_meetup___Any_Salt_Lake_City_residents_who_might_be_interested_in_a_meetup_", "level": 1}, {"title": "Discussion article for the meetup : Any Salt Lake City residents who might be interested in a meetup?", "anchor": "Discussion_article_for_the_meetup___Any_Salt_Lake_City_residents_who_might_be_interested_in_a_meetup_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "27 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-12T05:26:24.020Z", "modifiedAt": null, "url": null, "title": "Research Opportunity/Scholarship for ALL students (High school through Post-doc)", "slug": "research-opportunity-scholarship-for-all-students-high", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:21.555Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WXgyt2y2L3J2nSbBD/research-opportunity-scholarship-for-all-students-high", "pageUrlRelative": "/posts/WXgyt2y2L3J2nSbBD/research-opportunity-scholarship-for-all-students-high", "linkUrl": "https://www.lesswrong.com/posts/WXgyt2y2L3J2nSbBD/research-opportunity-scholarship-for-all-students-high", "postedAtFormatted": "Monday, December 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Research%20Opportunity%2FScholarship%20for%20ALL%20students%20(High%20school%20through%20Post-doc)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AResearch%20Opportunity%2FScholarship%20for%20ALL%20students%20(High%20school%20through%20Post-doc)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWXgyt2y2L3J2nSbBD%2Fresearch-opportunity-scholarship-for-all-students-high%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Research%20Opportunity%2FScholarship%20for%20ALL%20students%20(High%20school%20through%20Post-doc)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWXgyt2y2L3J2nSbBD%2Fresearch-opportunity-scholarship-for-all-students-high", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWXgyt2y2L3J2nSbBD%2Fresearch-opportunity-scholarship-for-all-students-high", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1481, "htmlBody": "<p>I recently was reminded of some work I did last year, and thought that it is the type of opportunity some LW-ers would be interested in, since there are a lot of STEM students on here.</p>\n<p><strong>The following are details about a research/scholarship opportunity. You must be a student, but you can be of any level: high school, college, grad school, or post-doc. You do not have to go to any specific school, &nbsp;but you probably have to relocate to Dayton, OH for the 10 weeks of the summer program. You do not have to relocate for the rest of the year.&nbsp;</strong></p>\n<h2><strong><em>They are very flexible on their admissions! GPA isn't high enough? Not a US citizen? Can't commit to the entire 10 weeks? They will still accept you if you turn in a good essay!</em></strong></h2>\n<p>&nbsp;</p>\n<p><strong>Basic Info</strong></p>\n<p>Website:&nbsp;<a href=\"http://wbi-icc.com/\">http://wbi-icc.com/</a></p>\n<p>5 minute video:&nbsp;http://vimeo.com/31103711&nbsp;</p>\n<p>Tec^Edge is a research opportunity/experiential learning scholarship that takes students &nbsp;and puts them in groups with mentors from academia (such as professors with research projects), government (such as the Air Force Research Labs (AFRL)) and business (such as General Dynamics).&nbsp;</p>\n<p>Besides Tec^Edge, it also goes by: Academic Leadership Pipeline Scholarship (ALPS), Summer at the Edge (SATE), and Year at the Edge (YATE).</p>\n<p>Summer At the Edge (SATE) is a 10-week program, and generally requires relocating for the summer to Dayton, OH, where the facility is. The work is full-time, though you make your own hours, and you get paid $4000 for the whole thing, generally in the form of a scholarship to your school (which means no taxes!). The facility is amazingly nice. Lots of high-tech things to work/play with.</p>\n<p>It has been described as \"Lock a bunch of smart people in a room with lots of gadgets, occasionally shove pizza under the door, and see what comes out.\"</p>\n<p>Year at the Edge (YATE) is the \"off-season\". They ask for about 10 hours a week (so the scholarship is only $1000/10 weeks). But you do not have to be in Dayton. You can work from your school, but the occasional virtual meeting is required. For some reason, they were set on using Second Life instead of Skype for this, even though everyone disliked it. I am guessing it was for some sort of experiment..</p>\n<p>There are lots of projects, and they try to put you on one that matches your interests. Many of the projects are sponsored by the Air Force Research Labs, so there are a lot involving sensors (aka surveillance) and aeronautics. Most projects need a lot of programming, most commonly in Java. But there are also opportunities for non-programmers (They are very big on inter-disciplinary work). Oftentimes you can request your own project, if there is something specific you want to work on. Anything that AFRL or DARPA might be interested in is generally accepted.</p>\n<p>A sample project: One group was trying to develop a micro air vehicle that could be shot out of a rocket-launcher thing, so it had to roll up into a 2\" diameter. It had to carry sensors (aka video feed), and it had to have a controlled descent. The whole thing had to cost less than $100 per unit, and their end of year test was to drop it from a height of perhaps 200' (I forget), use the video feed to find their mentor's truck, and use the controlled descent to land it in his truck bed. I don't know if they were successful in this...&nbsp;</p>\n<p>Besides getting to work on awesome projects, you also have a chance to meet and work under some pretty awesome people and network with contacts from various research labs. They are pretty willing to buy any expensive gadgetry required for your project. You can get your work published. You have lots of freedom as to what you want to do, how/when you want to do it. There are some really great presenters that they bring in too. All in all, it's a pretty awesome experience.</p>\n<p>Downsides: The organization is what they call \"Chaordic\". There's not much of a hierarchy. You have to be ok pretty much making your own way. A lot of projects have some application to surveillance, so you have to be ok with that. Most LW-ers would have to relocate for the summer program. I think they help out with this, but all the same, Dayton is not the most interesting city to be in.</p>\n<p>&nbsp;</p>\n<p><strong>My experiences:&nbsp;</strong></p>\n<p>For Summer At the Edge (SATE) I was teamed with a post-doc in some sort of computer science, and a 17 year-old programmer from a local high school. We were working on a project from the Human Performance Wing of the AFRL. The whole of our instruction was \"Do something with Information Visualization, and Computer Mediated Communications.\" In other words, ways to make pretty pictures out of things like email, chat rooms, blogs, etc, so that people can understand them faster or see patterns easier.</p>\n<p>&nbsp;Some of the groups were very organized under their mentor who had a specific project that they were working on, and so would tell students what they wanted done. Our group was very self-led. Our mentor would ask us if we needed anything, but pretty much would leave us to our own devices.</p>\n<p>We did some research for about a week, then my partners started working on programming text analyzers. Not being a programmer, I had to find other things to do. Not knowing what to do, I spent a lot of time doing the paperwork and giving presentations, organizing the SATE trip to an amusement park, and generally helping out.</p>\n<p>One thing about SATE is that you have to be very self-motivated. If you don't have something to do, it is up to you to find something. Because it's so self-led, there's a decent amount of updates you have to give (such as a weekly email about what you accomplished that week, and how many hours you worked), and also a paper that you have to submit at the end, summarizing your findings.</p>\n<p>Every now and then I'd have an idea. There were a lot of dead ends, but eventually I managed to coalesce my ideas into somewhat of a whole. I probably spent about a month working on my actual project, and then about a week writing my paper, and some of my teammates' paper. Both of our papers ended up getting published in the Collaborative Technologies and Systems International Symposium. If I had still been a student when the conference&nbsp;occurred, Tec^Edge would have paid my way for me to present the poster at the conference, but as it stood, our adviser presented for me.</p>\n<p>&nbsp;</p>\n<p>After SATE was over, I applied for Year at the Edge (YATE). This program only requires ~10 hours a week, since it is during the school year, and allows you to work from home/school. For this project there was even less instruction, as I had to propose my own project. If you can manage to turn a class assignment into something YATE is interested in, they are quite accepting of it.</p>\n<p>I had a pretty open-ended project for a Computer Design class, so I asked my partner in that class if she would be willing to do a project on something called Computer Supported Collaborative Work (aka \"Using computers to work together with other people\"), and I proposed the same project to YATE. They all agreed, so for all intents and purposes I ended up getting a research scholarship to do my homework!</p>\n<p>This time there was a bit more structure, as we were following the class guidelines as to what we needed to accomplish. Mainly we were doing quantitative and qualitative measurements on collaborating using Google docs, versus other collaborative methods (being in the same room and sharing a computer). Being a much smaller project, this didn't get published, but we did get an A on the assignment, and used the paper-writing as an excuse to learn LaTeX.</p>\n<p>Unfortunately, that was my last quarter before my divorce, so I didn't continue with the program. But I couldn't recommend it more to anyone who is interested. You get a lot of opportunities to work on whatever interests you. The mentors are amazing contacts from many different research companies that you can use as references. It's not uncommon to get offered a job or internship with the company that is mentoring your project. Also, even if you are a high schooler or an undergrad, you have the opportunity to get published.</p>\n<p>&nbsp;</p>\n<p><strong>Want to Apply?</strong></p>\n<p>This website has application instructions and a short video:&nbsp;<a href=\"http://wbi-icc.com/who-we-work-with/students-teachers\">http://wbi-icc.com/who-we-work-with/students-teachers</a>&nbsp;</p>\n<p>Things they like: Passion, Willingness to venture into the unknown, Willingness to \"fail\" (allowing discovery of things that <em>don't</em> work), Interdisciplinary work and knowledge, Desire to make a difference</p>\n<p>My admissions essay earned me a spot as a \"Student Leader\" and I'm willing to post it, if it will help people see the sort of thing that they are looking for. But I won't bother, if no one asks!</p>\n<p>If you have any questions about it, let me know!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WXgyt2y2L3J2nSbBD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 9, "extendedScore": null, "score": 8.137030366435227e-07, "legacy": true, "legacyId": "11396", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I recently was reminded of some work I did last year, and thought that it is the type of opportunity some LW-ers would be interested in, since there are a lot of STEM students on here.</p>\n<p><strong id=\"The_following_are_details_about_a_research_scholarship_opportunity__You_must_be_a_student__but_you_can_be_of_any_level__high_school__college__grad_school__or_post_doc__You_do_not_have_to_go_to_any_specific_school___but_you_probably_have_to_relocate_to_Dayton__OH_for_the_10_weeks_of_the_summer_program__You_do_not_have_to_relocate_for_the_rest_of_the_year__\">The following are details about a research/scholarship opportunity. You must be a student, but you can be of any level: high school, college, grad school, or post-doc. You do not have to go to any specific school, &nbsp;but you probably have to relocate to Dayton, OH for the 10 weeks of the summer program. You do not have to relocate for the rest of the year.&nbsp;</strong></p>\n<h2 id=\"They_are_very_flexible_on_their_admissions__GPA_isn_t_high_enough__Not_a_US_citizen__Can_t_commit_to_the_entire_10_weeks__They_will_still_accept_you_if_you_turn_in_a_good_essay_\"><strong><em>They are very flexible on their admissions! GPA isn't high enough? Not a US citizen? Can't commit to the entire 10 weeks? They will still accept you if you turn in a good essay!</em></strong></h2>\n<p>&nbsp;</p>\n<p><strong id=\"Basic_Info\">Basic Info</strong></p>\n<p>Website:&nbsp;<a href=\"http://wbi-icc.com/\">http://wbi-icc.com/</a></p>\n<p>5 minute video:&nbsp;http://vimeo.com/31103711&nbsp;</p>\n<p>Tec^Edge is a research opportunity/experiential learning scholarship that takes students &nbsp;and puts them in groups with mentors from academia (such as professors with research projects), government (such as the Air Force Research Labs (AFRL)) and business (such as General Dynamics).&nbsp;</p>\n<p>Besides Tec^Edge, it also goes by: Academic Leadership Pipeline Scholarship (ALPS), Summer at the Edge (SATE), and Year at the Edge (YATE).</p>\n<p>Summer At the Edge (SATE) is a 10-week program, and generally requires relocating for the summer to Dayton, OH, where the facility is. The work is full-time, though you make your own hours, and you get paid $4000 for the whole thing, generally in the form of a scholarship to your school (which means no taxes!). The facility is amazingly nice. Lots of high-tech things to work/play with.</p>\n<p>It has been described as \"Lock a bunch of smart people in a room with lots of gadgets, occasionally shove pizza under the door, and see what comes out.\"</p>\n<p>Year at the Edge (YATE) is the \"off-season\". They ask for about 10 hours a week (so the scholarship is only $1000/10 weeks). But you do not have to be in Dayton. You can work from your school, but the occasional virtual meeting is required. For some reason, they were set on using Second Life instead of Skype for this, even though everyone disliked it. I am guessing it was for some sort of experiment..</p>\n<p>There are lots of projects, and they try to put you on one that matches your interests. Many of the projects are sponsored by the Air Force Research Labs, so there are a lot involving sensors (aka surveillance) and aeronautics. Most projects need a lot of programming, most commonly in Java. But there are also opportunities for non-programmers (They are very big on inter-disciplinary work). Oftentimes you can request your own project, if there is something specific you want to work on. Anything that AFRL or DARPA might be interested in is generally accepted.</p>\n<p>A sample project: One group was trying to develop a micro air vehicle that could be shot out of a rocket-launcher thing, so it had to roll up into a 2\" diameter. It had to carry sensors (aka video feed), and it had to have a controlled descent. The whole thing had to cost less than $100 per unit, and their end of year test was to drop it from a height of perhaps 200' (I forget), use the video feed to find their mentor's truck, and use the controlled descent to land it in his truck bed. I don't know if they were successful in this...&nbsp;</p>\n<p>Besides getting to work on awesome projects, you also have a chance to meet and work under some pretty awesome people and network with contacts from various research labs. They are pretty willing to buy any expensive gadgetry required for your project. You can get your work published. You have lots of freedom as to what you want to do, how/when you want to do it. There are some really great presenters that they bring in too. All in all, it's a pretty awesome experience.</p>\n<p>Downsides: The organization is what they call \"Chaordic\". There's not much of a hierarchy. You have to be ok pretty much making your own way. A lot of projects have some application to surveillance, so you have to be ok with that. Most LW-ers would have to relocate for the summer program. I think they help out with this, but all the same, Dayton is not the most interesting city to be in.</p>\n<p>&nbsp;</p>\n<p><strong id=\"My_experiences__\">My experiences:&nbsp;</strong></p>\n<p>For Summer At the Edge (SATE) I was teamed with a post-doc in some sort of computer science, and a 17 year-old programmer from a local high school. We were working on a project from the Human Performance Wing of the AFRL. The whole of our instruction was \"Do something with Information Visualization, and Computer Mediated Communications.\" In other words, ways to make pretty pictures out of things like email, chat rooms, blogs, etc, so that people can understand them faster or see patterns easier.</p>\n<p>&nbsp;Some of the groups were very organized under their mentor who had a specific project that they were working on, and so would tell students what they wanted done. Our group was very self-led. Our mentor would ask us if we needed anything, but pretty much would leave us to our own devices.</p>\n<p>We did some research for about a week, then my partners started working on programming text analyzers. Not being a programmer, I had to find other things to do. Not knowing what to do, I spent a lot of time doing the paperwork and giving presentations, organizing the SATE trip to an amusement park, and generally helping out.</p>\n<p>One thing about SATE is that you have to be very self-motivated. If you don't have something to do, it is up to you to find something. Because it's so self-led, there's a decent amount of updates you have to give (such as a weekly email about what you accomplished that week, and how many hours you worked), and also a paper that you have to submit at the end, summarizing your findings.</p>\n<p>Every now and then I'd have an idea. There were a lot of dead ends, but eventually I managed to coalesce my ideas into somewhat of a whole. I probably spent about a month working on my actual project, and then about a week writing my paper, and some of my teammates' paper. Both of our papers ended up getting published in the Collaborative Technologies and Systems International Symposium. If I had still been a student when the conference&nbsp;occurred, Tec^Edge would have paid my way for me to present the poster at the conference, but as it stood, our adviser presented for me.</p>\n<p>&nbsp;</p>\n<p>After SATE was over, I applied for Year at the Edge (YATE). This program only requires ~10 hours a week, since it is during the school year, and allows you to work from home/school. For this project there was even less instruction, as I had to propose my own project. If you can manage to turn a class assignment into something YATE is interested in, they are quite accepting of it.</p>\n<p>I had a pretty open-ended project for a Computer Design class, so I asked my partner in that class if she would be willing to do a project on something called Computer Supported Collaborative Work (aka \"Using computers to work together with other people\"), and I proposed the same project to YATE. They all agreed, so for all intents and purposes I ended up getting a research scholarship to do my homework!</p>\n<p>This time there was a bit more structure, as we were following the class guidelines as to what we needed to accomplish. Mainly we were doing quantitative and qualitative measurements on collaborating using Google docs, versus other collaborative methods (being in the same room and sharing a computer). Being a much smaller project, this didn't get published, but we did get an A on the assignment, and used the paper-writing as an excuse to learn LaTeX.</p>\n<p>Unfortunately, that was my last quarter before my divorce, so I didn't continue with the program. But I couldn't recommend it more to anyone who is interested. You get a lot of opportunities to work on whatever interests you. The mentors are amazing contacts from many different research companies that you can use as references. It's not uncommon to get offered a job or internship with the company that is mentoring your project. Also, even if you are a high schooler or an undergrad, you have the opportunity to get published.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Want_to_Apply_\">Want to Apply?</strong></p>\n<p>This website has application instructions and a short video:&nbsp;<a href=\"http://wbi-icc.com/who-we-work-with/students-teachers\">http://wbi-icc.com/who-we-work-with/students-teachers</a>&nbsp;</p>\n<p>Things they like: Passion, Willingness to venture into the unknown, Willingness to \"fail\" (allowing discovery of things that <em>don't</em> work), Interdisciplinary work and knowledge, Desire to make a difference</p>\n<p>My admissions essay earned me a spot as a \"Student Leader\" and I'm willing to post it, if it will help people see the sort of thing that they are looking for. But I won't bother, if no one asks!</p>\n<p>If you have any questions about it, let me know!</p>", "sections": [{"title": "The following are details about a research/scholarship opportunity. You must be a student, but you can be of any level: high school, college, grad school, or post-doc. You do not have to go to any specific school, \u00a0but you probably have to relocate to Dayton, OH for the 10 weeks of the summer program. You do not have to relocate for the rest of the year.\u00a0", "anchor": "The_following_are_details_about_a_research_scholarship_opportunity__You_must_be_a_student__but_you_can_be_of_any_level__high_school__college__grad_school__or_post_doc__You_do_not_have_to_go_to_any_specific_school___but_you_probably_have_to_relocate_to_Dayton__OH_for_the_10_weeks_of_the_summer_program__You_do_not_have_to_relocate_for_the_rest_of_the_year__", "level": 2}, {"title": "They are very flexible on their admissions! GPA isn't high enough? Not a US citizen? Can't commit to the entire 10 weeks? They will still accept you if you turn in a good essay!", "anchor": "They_are_very_flexible_on_their_admissions__GPA_isn_t_high_enough__Not_a_US_citizen__Can_t_commit_to_the_entire_10_weeks__They_will_still_accept_you_if_you_turn_in_a_good_essay_", "level": 1}, {"title": "Basic Info", "anchor": "Basic_Info", "level": 2}, {"title": "My experiences:\u00a0", "anchor": "My_experiences__", "level": 2}, {"title": "Want to Apply?", "anchor": "Want_to_Apply_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "5 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-12T05:31:12.630Z", "modifiedAt": null, "url": null, "title": "[LINK] Cryo Comic", "slug": "link-cryo-comic", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:55.516Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alex_Altair", "createdAt": "2010-07-21T18:30:40.806Z", "isAdmin": false, "displayName": "Alex_Altair"}, "userId": "5wu9jG4pm9q6xjZ9R", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RssNSyDRmpBSukL5J/link-cryo-comic", "pageUrlRelative": "/posts/RssNSyDRmpBSukL5J/link-cryo-comic", "linkUrl": "https://www.lesswrong.com/posts/RssNSyDRmpBSukL5J/link-cryo-comic", "postedAtFormatted": "Monday, December 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Cryo%20Comic&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Cryo%20Comic%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRssNSyDRmpBSukL5J%2Flink-cryo-comic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Cryo%20Comic%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRssNSyDRmpBSukL5J%2Flink-cryo-comic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRssNSyDRmpBSukL5J%2Flink-cryo-comic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 10, "htmlBody": "<p>This is the obligatory post of the recent xkcd comic:</p>\n<p>http://xkcd.com/989/</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RssNSyDRmpBSukL5J", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 0, "extendedScore": null, "score": 8.137047911723776e-07, "legacy": true, "legacyId": "11415", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-12T05:53:46.271Z", "modifiedAt": null, "url": null, "title": "Physics question (slightly off-topic)", "slug": "physics-question-slightly-off-topic", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:55.945Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "CronoDAS", "createdAt": "2009-02-27T04:42:19.587Z", "isAdmin": false, "displayName": "CronoDAS"}, "userId": "Q2oaNonArzibx5cQN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9Be3aBSKZm23kNtad/physics-question-slightly-off-topic", "pageUrlRelative": "/posts/9Be3aBSKZm23kNtad/physics-question-slightly-off-topic", "linkUrl": "https://www.lesswrong.com/posts/9Be3aBSKZm23kNtad/physics-question-slightly-off-topic", "postedAtFormatted": "Monday, December 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Physics%20question%20(slightly%20off-topic)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APhysics%20question%20(slightly%20off-topic)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9Be3aBSKZm23kNtad%2Fphysics-question-slightly-off-topic%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Physics%20question%20(slightly%20off-topic)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9Be3aBSKZm23kNtad%2Fphysics-question-slightly-off-topic", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9Be3aBSKZm23kNtad%2Fphysics-question-slightly-off-topic", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 52, "htmlBody": "<p>There's probably a better place to ask this question, but I don't know what it is. That being said...</p>\n<p>Which will go further if a batter manages to hit it with a baseball bat: a baseball thrown to the batter at 90 miles per hour or one thrown at 60 miles per hour?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9Be3aBSKZm23kNtad", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 1, "extendedScore": null, "score": 8.137130203351049e-07, "legacy": true, "legacyId": "11417", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 24, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-12T09:45:05.076Z", "modifiedAt": null, "url": null, "title": "New 'landing page' website: Friendly-AI.com", "slug": "new-landing-page-website-friendly-ai-com", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:04.384Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/u4nS5yXJYKzRuRyN8/new-landing-page-website-friendly-ai-com", "pageUrlRelative": "/posts/u4nS5yXJYKzRuRyN8/new-landing-page-website-friendly-ai-com", "linkUrl": "https://www.lesswrong.com/posts/u4nS5yXJYKzRuRyN8/new-landing-page-website-friendly-ai-com", "postedAtFormatted": "Monday, December 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20'landing%20page'%20website%3A%20Friendly-AI.com&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20'landing%20page'%20website%3A%20Friendly-AI.com%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu4nS5yXJYKzRuRyN8%2Fnew-landing-page-website-friendly-ai-com%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20'landing%20page'%20website%3A%20Friendly-AI.com%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu4nS5yXJYKzRuRyN8%2Fnew-landing-page-website-friendly-ai-com", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fu4nS5yXJYKzRuRyN8%2Fnew-landing-page-website-friendly-ai-com", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 39, "htmlBody": "<p>I've created a new \"landing page\" on Friendly AI at <a href=\"http://friendly-ai.com/\">Friendly-AI.com</a>. This is similar to <a href=\"http://intelligenceexplosion.com/\">IntelligenceExplosion.com</a>, <a href=\"http://www.existential-risk.org/\">Existential-Risk.org</a>, <a href=\"http://www.anthropic-principle.com/\">Anthropic-Principle.com</a>, and <a href=\"http://www.simulation-argument.com/\">simulation-argument.com</a>.</p>\n<p>The site is less ambitious than the original plan for it was, but it serves its purpose.</p>\n<p>Design courtesy of <a href=\"/user/Lightwave/\">Lightwave</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "u4nS5yXJYKzRuRyN8", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 15, "extendedScore": null, "score": 8.137974025895871e-07, "legacy": true, "legacyId": "11418", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 40, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-12T14:17:02.688Z", "modifiedAt": null, "url": null, "title": "two puzzles on rationality of defeat ", "slug": "two-puzzles-on-rationality-of-defeat", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:56.157Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fsopho", "createdAt": "2011-12-07T12:31:31.940Z", "isAdmin": false, "displayName": "fsopho"}, "userId": "26wYfWx8uuMr7Hpc4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/y9FtExZNuRB9rmaPm/two-puzzles-on-rationality-of-defeat", "pageUrlRelative": "/posts/y9FtExZNuRB9rmaPm/two-puzzles-on-rationality-of-defeat", "linkUrl": "https://www.lesswrong.com/posts/y9FtExZNuRB9rmaPm/two-puzzles-on-rationality-of-defeat", "postedAtFormatted": "Monday, December 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20two%20puzzles%20on%20rationality%20of%20defeat%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0Atwo%20puzzles%20on%20rationality%20of%20defeat%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy9FtExZNuRB9rmaPm%2Ftwo-puzzles-on-rationality-of-defeat%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=two%20puzzles%20on%20rationality%20of%20defeat%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy9FtExZNuRB9rmaPm%2Ftwo-puzzles-on-rationality-of-defeat", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fy9FtExZNuRB9rmaPm%2Ftwo-puzzles-on-rationality-of-defeat", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 734, "htmlBody": "<p>I present here two puzzles of rationality you LessWrongers may think is worth to deal with. Maybe the first one looks more amenable to a simple solution, while the second one has called attention of a number of contemporary epistemologists (Cargile, Feldman, Harman), and does not look that simple when it comes to a solution. So, let's go to the puzzles!</p>\n<p>&nbsp;</p>\n<p><strong>Puzzle 1</strong>&nbsp;</p>\n<p>\n<p><span style=\"font-weight: normal;\">At t1 I justifiably believe theorem T is true, on the basis of a complex argument I just validly reasoned from the also justified premises P1, P2 and P3.<br /></span><span style=\"font-weight: normal;\">So, in t1 I reason from premises:<br /></span><span style=\"font-weight: normal;\">&nbsp;<br /></span><em><strong>(R1)</strong></em> P1, P2 ,P3<br /><span style=\"font-weight: normal;\">&nbsp;<br /></span><span style=\"font-weight: normal;\">To the known conclusion:<br /></span><span style=\"font-weight: normal;\">&nbsp;<br /></span><em><strong>(T)</strong></em> T is true<br /><span style=\"font-weight: normal;\">&nbsp;<br /></span><span style=\"font-weight: normal;\">At t2, Ms. Math, a well known authority on the subject matter of which my reasoning and my theorem are just a part, tells me I&rsquo;m wrong. She tells me the theorem is just false, and convince me of that on the basis of a valid reasoning with at least one false premise, the falsity of that premise being unknown to us.<br /></span><span style=\"font-weight: normal;\">So, in t2 I reason from premises (Reliable Math and Testimony of Math):<br /></span><span style=\"font-weight: normal;\">&nbsp;<br /></span><em><strong>(RM)</strong></em> Ms. Math is a reliable mathematician, and an authority on the subject matter surrounding (T),<br /><span style=\"font-weight: normal;\">&nbsp;<br /></span><em><strong>(TM)</strong></em> Ms. Math tells me T is false, and show to me how is that so, on the basis of a valid reasoning from F, P1, P2 and P3,<br /><span style=\"font-weight: normal;\">&nbsp;<br /></span><em><strong>(R2)</strong></em> F, P1, P2 and P3<br /><span style=\"font-weight: normal;\">&nbsp;<br /></span><span style=\"font-weight: normal;\">To the justified conclusion:<br /></span><span style=\"font-weight: normal;\">&nbsp;<br /></span><em><strong>(~T)</strong></em> T is not true<br /><span style=\"font-weight: normal;\">&nbsp;<br /></span><span style=\"font-weight: normal;\">It could be said by some epistemologists that (~T) defeat my previous belief (T). Is it rational for me to do this way? Am I taking the correct direction of defeat? Wouldn&rsquo;t it also be rational if (~T) were defeated by (T)? Why ~(T) defeats (T), and not vice-versa? It is just because ~(T)&rsquo;s justification obtained in a later time?</span></p>\n<p><span style=\"font-weight: normal;\"><br /></span></p>\n<p><strong>Puzzle 2</strong></p>\n<p>\n<p>At t1 I know theorem T is true, on the basis of a complex argument I just validly reasoned, with known premises P1, P2 and P3. So, in t1 I reason from known premises:<br />&nbsp;<br /><em><strong>(R1)</strong></em> P1, P2 ,P3<br />&nbsp;<br />To the known conclusion:<br />&nbsp;<br /><em><strong>(T)</strong></em> T is true<br />&nbsp;<br />Besides, I also reason from known premises:<br />&nbsp;<br /><em><strong>(ME)</strong></em> If there is any evidence against something that is true, then it is misleading evidence (evidence for something that is false)<br />&nbsp;<br /><em><strong>(T)</strong></em> T is true<br />&nbsp;<br />To the conclusion (anti-misleading evidence):<br />&nbsp;<br /><em><strong>(AME)</strong></em> If there is any evidence against (T), then it is misleading evidence<br />&nbsp;<br />At t2 the same Ms. Math tells me the same thing. So in t2 I reason from premises (Reliable Math and Testimony of Math):<br />&nbsp;<br /><em><strong>(RM)</strong></em> Ms. Math is a reliable mathematician, and an authority on the subject matter surrounding (T),<br />&nbsp;<br /><em><strong>(TM)</strong></em> Ms. Math tells me T is false, and show to me how is that so, on the basis of a valid reasoning from F, P1, P2 and P3,<br />&nbsp;<br />But then I reason from::<br />&nbsp;<br /><em><strong>(F*)</strong></em> F, RM and TM are evidence against (T), and<br />&nbsp;<br /><em><strong>(AME)</strong></em> If there is any evidence against (T), then it is misleading evidence<br />&nbsp;<br />To the conclusion:<br />&nbsp;<br /><em><strong>(MF)</strong></em> F, RM and TM is misleading evidence<br />&nbsp;<br />And then I continue to know T and I lose no knowledge, because I know/justifiably believe that the counter-evidence I just met is misleading. Is it rational for me to act this way?<br />I know (T) and I know (AME) in t1 on the basis of valid reasoning. Then, I am exposed to misleading evidences (Reliable Math), (Testimony of Math) and (F). The evidentialist scheme (and maybe still other schemes) support the thesis that (RM), (TM) and (F) DEFEATS my justification for (T) instead. So that whatever I inferred from (T) is no longer known. However, given my previous knowledge of (T) and (AME), I could know that (MF): F is misleading evidence. It can still be said that (RM), (TM) and (F) DEFEAT my justification for (T), given that (MF) DEFEAT my justification for (RM), (TM) and (F)?</p>\n</p>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "y9FtExZNuRB9rmaPm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 4, "extendedScore": null, "score": 8.138966324220005e-07, "legacy": true, "legacyId": "11420", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 56, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-12T16:35:01.007Z", "modifiedAt": null, "url": null, "title": "Less Wrong Topic Poll", "slug": "less-wrong-topic-poll", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:55.567Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "CPPMntDd4x5xYWjvy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zN4jwkMFGuwDxRuAY/less-wrong-topic-poll", "pageUrlRelative": "/posts/zN4jwkMFGuwDxRuAY/less-wrong-topic-poll", "linkUrl": "https://www.lesswrong.com/posts/zN4jwkMFGuwDxRuAY/less-wrong-topic-poll", "postedAtFormatted": "Monday, December 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Less%20Wrong%20Topic%20Poll&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALess%20Wrong%20Topic%20Poll%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzN4jwkMFGuwDxRuAY%2Fless-wrong-topic-poll%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Less%20Wrong%20Topic%20Poll%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzN4jwkMFGuwDxRuAY%2Fless-wrong-topic-poll", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzN4jwkMFGuwDxRuAY%2Fless-wrong-topic-poll", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 323, "htmlBody": "<p>I noticed enough comments about topic recently that I wanted to run a quick poll about topics and discussion categories on Less Wrong.</p>\n<p>Since being told others opinions might cause an anchoring bias, please take the poll here before reading further:</p>\n<p>https://docs.google.com/spreadsheet/viewform?formkey=dEVJSTBTdk92UjhTWWp6U1dYSDltclE6MQ</p>\n<p>&nbsp;</p>\n<p>My personal opinion at the moment (last chance to avoid having me influence your poll results) is that I want to reduce the number of people who feel that articles posted on Less Wrong are off topic and limit topical complaints. I don't want to do this because I think the complainers are wrong, because if they don't want to see something, they are correct to indicate it. But I feel that there are ways of handling this at a higher level that might be more effective than just up or down voting based on topic alone, and I feel like it would help increase the quality of discussion here to have better topical division.</p>\n<p>I think the biggest problem with the current system is this definition for Discussion.</p>\n<p>\"This part of the site is for the discussion of topics not yet ready or not suitable for normal top-level posts.\"</p>\n<p>&nbsp;</p>\n<p>The fact that Discussion is \"not yet ready or not suitable\" indicates to me that as a category it's not cutting apart topics well. \"I'm making a draft about SIA and SSA that I want to put in Main\" and \"Someone made a new scientific paper about the OPERA Neutrino anomaly again.\" and \"Is Twilight Sparkle a Rationalist?\" Don't seem to mesh well under the same general discussion category, even though I would probably read the community discussion on any of them.</p>\n<p>That said, I am open to arguments to establish why the Status Quo is worthwhile as well. And maybe the poll will result in a bunch of people voting for only Main and Discussion.</p>\n<p>Please feel free to discuss anything relating to topics or discussion categories or the poll below, and thank you for answering the poll!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zN4jwkMFGuwDxRuAY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 11, "extendedScore": null, "score": 8.139469823000451e-07, "legacy": true, "legacyId": "11422", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-12T17:01:42.842Z", "modifiedAt": null, "url": null, "title": "[LINK] Brain region changes shape with learning the layout of London", "slug": "link-brain-region-changes-shape-with-learning-the-layout-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:56.472Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "whpearson", "createdAt": "2009-02-28T00:34:00.976Z", "isAdmin": false, "displayName": "whpearson"}, "userId": "bq8qsRbPNvFihHxgi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/o5yP5PXW36YucgZYr/link-brain-region-changes-shape-with-learning-the-layout-of", "pageUrlRelative": "/posts/o5yP5PXW36YucgZYr/link-brain-region-changes-shape-with-learning-the-layout-of", "linkUrl": "https://www.lesswrong.com/posts/o5yP5PXW36YucgZYr/link-brain-region-changes-shape-with-learning-the-layout-of", "postedAtFormatted": "Monday, December 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Brain%20region%20changes%20shape%20with%20learning%20the%20layout%20of%20London&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Brain%20region%20changes%20shape%20with%20learning%20the%20layout%20of%20London%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo5yP5PXW36YucgZYr%2Flink-brain-region-changes-shape-with-learning-the-layout-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Brain%20region%20changes%20shape%20with%20learning%20the%20layout%20of%20London%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo5yP5PXW36YucgZYr%2Flink-brain-region-changes-shape-with-learning-the-layout-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo5yP5PXW36YucgZYr%2Flink-brain-region-changes-shape-with-learning-the-layout-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 51, "htmlBody": "<p>General Intro:</p>\n<p>http://www.bbc.co.uk/news/health-16086233</p>\n<p>Older research:</p>\n<p>http://www.fil.ion.ucl.ac.uk/Maguire/Maguire2006.pdf</p>\n<p><a href=\"http://www.cell.com/current-biology/abstract/S0960-9822(11)01267-X\">Abstract of latest research</a></p>\n<p>Possible implications for WBE (Is it possible to get short term function correct without having the ability to do long term structural changes?)</p>\n<p>Also possible implications for learning lots of information, cabbies with \"the Knowledge\" had worse visual information recall.</p>\n<p>I haven't gone through it all myself yet.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "o5yP5PXW36YucgZYr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 2, "extendedScore": null, "score": 8.139567255140676e-07, "legacy": true, "legacyId": "11423", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-12T23:45:55.920Z", "modifiedAt": null, "url": null, "title": "Meetup : Fort Collins, Colorado Meetup Wednesday 7pm", "slug": "meetup-fort-collins-colorado-meetup-wednesday-7pm-0", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/JEM7v9jzoKD47Ws2o/meetup-fort-collins-colorado-meetup-wednesday-7pm-0", "pageUrlRelative": "/posts/JEM7v9jzoKD47Ws2o/meetup-fort-collins-colorado-meetup-wednesday-7pm-0", "linkUrl": "https://www.lesswrong.com/posts/JEM7v9jzoKD47Ws2o/meetup-fort-collins-colorado-meetup-wednesday-7pm-0", "postedAtFormatted": "Monday, December 12th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Wednesday%207pm&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Wednesday%207pm%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJEM7v9jzoKD47Ws2o%2Fmeetup-fort-collins-colorado-meetup-wednesday-7pm-0%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20Wednesday%207pm%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJEM7v9jzoKD47Ws2o%2Fmeetup-fort-collins-colorado-meetup-wednesday-7pm-0", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJEM7v9jzoKD47Ws2o%2Fmeetup-fort-collins-colorado-meetup-wednesday-7pm-0", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 42, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/5k'>Fort Collins, Colorado Meetup Wednesday 7pm</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 December 2011 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>And again with the awesomeness!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/5k'>Fort Collins, Colorado Meetup Wednesday 7pm</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "JEM7v9jzoKD47Ws2o", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.141042709433083e-07, "legacy": true, "legacyId": "11425", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wednesday_7pm\">Discussion article for the meetup : <a href=\"/meetups/5k\">Fort Collins, Colorado Meetup Wednesday 7pm</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 December 2011 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>And again with the awesomeness!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wednesday_7pm1\">Discussion article for the meetup : <a href=\"/meetups/5k\">Fort Collins, Colorado Meetup Wednesday 7pm</a></h2>", "sections": [{"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Wednesday 7pm", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wednesday_7pm", "level": 1}, {"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup Wednesday 7pm", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup_Wednesday_7pm1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "No comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-13T00:41:52.281Z", "modifiedAt": null, "url": null, "title": "[LINK] Matrix-Style Learning", "slug": "link-matrix-style-learning", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:55.356Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "magfrump", "createdAt": "2009-12-10T20:51:45.065Z", "isAdmin": false, "displayName": "magfrump"}, "userId": "KsYFs5ip5jeiFETJa", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dPKgH67QoRvrQnTvh/link-matrix-style-learning", "pageUrlRelative": "/posts/dPKgH67QoRvrQnTvh/link-matrix-style-learning", "linkUrl": "https://www.lesswrong.com/posts/dPKgH67QoRvrQnTvh/link-matrix-style-learning", "postedAtFormatted": "Tuesday, December 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Matrix-Style%20Learning&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Matrix-Style%20Learning%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdPKgH67QoRvrQnTvh%2Flink-matrix-style-learning%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Matrix-Style%20Learning%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdPKgH67QoRvrQnTvh%2Flink-matrix-style-learning", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdPKgH67QoRvrQnTvh%2Flink-matrix-style-learning", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 32, "htmlBody": "<p><a href=\"http://io9.com/5867113/scientists-say-theyre-paving-the-way-towards-matrix+style-learning--but-is-it-safe\">http://io9.com/5867113/scientists-say-theyre-paving-the-way-towards-matrix+style-learning--but-is-it-safe</a></p>\n<p>Researchers learned to change people's brain structures to make them better at certain tasks. &nbsp;But... They weren't aware of getting better at those tasks! &nbsp;Creepy!</p>\n<p>I'd love to see a more technical discussion.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dPKgH67QoRvrQnTvh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 6, "extendedScore": null, "score": 8.141246933067232e-07, "legacy": true, "legacyId": "11429", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-13T01:02:03.189Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Rational vs Scientific Ev-Psych", "slug": "seq-rerun-rational-vs-scientific-ev-psych", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:02.876Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/t9864QmdouAqmyQtG/seq-rerun-rational-vs-scientific-ev-psych", "pageUrlRelative": "/posts/t9864QmdouAqmyQtG/seq-rerun-rational-vs-scientific-ev-psych", "linkUrl": "https://www.lesswrong.com/posts/t9864QmdouAqmyQtG/seq-rerun-rational-vs-scientific-ev-psych", "postedAtFormatted": "Tuesday, December 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Rational%20vs%20Scientific%20Ev-Psych&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Rational%20vs%20Scientific%20Ev-Psych%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft9864QmdouAqmyQtG%2Fseq-rerun-rational-vs-scientific-ev-psych%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Rational%20vs%20Scientific%20Ev-Psych%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft9864QmdouAqmyQtG%2Fseq-rerun-rational-vs-scientific-ev-psych", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft9864QmdouAqmyQtG%2Fseq-rerun-rational-vs-scientific-ev-psych", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 194, "htmlBody": "<p>Today's post, <a href=\"/lw/mj/rational_vs_scientific_evpsych/\">Rational vs. Scientific Ev-Psych</a> was originally published on 04 January 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>In Evolutionary Biology or Psychology, a nice-sounding but untested theory is referred to as a \"just-so story\", after the stories written by Rudyard Kipling. But, if there is a way to test the theory, people tend to consider it more likely to be correct. This is not a rational tendency.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8t0/seq_rerun_stop_voting_for_nincompoops/\">Stop Voting For Nincompoops</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "t9864QmdouAqmyQtG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 10, "extendedScore": null, "score": 8.141320614584007e-07, "legacy": true, "legacyId": "11435", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pL3To6G42AeihNtaN", "gSKq3pp4tkAbBf2Lc", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-13T06:48:20.199Z", "modifiedAt": null, "url": null, "title": "Q&A #2 with Singularity Institute Executive Director", "slug": "q-and-a-2-with-singularity-institute-executive-director", "viewCount": null, "lastCommentedAt": "2017-06-17T04:11:58.324Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/89kDDTdDNJQBF9HEC/q-and-a-2-with-singularity-institute-executive-director", "pageUrlRelative": "/posts/89kDDTdDNJQBF9HEC/q-and-a-2-with-singularity-institute-executive-director", "linkUrl": "https://www.lesswrong.com/posts/89kDDTdDNJQBF9HEC/q-and-a-2-with-singularity-institute-executive-director", "postedAtFormatted": "Tuesday, December 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Q%26A%20%232%20with%20Singularity%20Institute%20Executive%20Director&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQ%26A%20%232%20with%20Singularity%20Institute%20Executive%20Director%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F89kDDTdDNJQBF9HEC%2Fq-and-a-2-with-singularity-institute-executive-director%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Q%26A%20%232%20with%20Singularity%20Institute%20Executive%20Director%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F89kDDTdDNJQBF9HEC%2Fq-and-a-2-with-singularity-institute-executive-director", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F89kDDTdDNJQBF9HEC%2Fq-and-a-2-with-singularity-institute-executive-director", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 258, "htmlBody": "<p>Just over a month ago I posted a <a href=\"/lw/8c3/qa_with_new_executive_director_of_singularity/\">call for questions</a> about the Singularity Institute. The reaction to my <a href=\"/r/discussion/lw/8s6/video_qa_with_singularity_institute_executive/\">video response</a> was positive enough that I'd like to do another one &mdash; though I can't promise video this time. I think that the Singularity Institute has a lot of transparency \"catching up\" to do.</p>\n<p>&nbsp;</p>\n<p><strong>The Rules (same as before)</strong></p>\n<p>1) One question per comment (to allow voting to carry more information about people's preferences).</p>\n<p>2) Try to be as clear and concise as possible. If your question can't be condensed into one paragraph, you should probably ask in a separate post. Make sure you have an actual question somewhere in there (you can bold it to make it easier to scan).</p>\n<p>3) I will generally answer the top-voted questions, but will skip some of them. I will tend to select <strong>questions about the Singularity Institute as an organization</strong>, not about the technical details of some bit of research. You can read some of the details of the Friendly AI research program in my&nbsp;<a href=\"http://intelligence.org/blog/2011/09/15/interview-with-new-singularity-institute-research-fellow-luke-muehlhuaser-september-2011/\">interview with Michael Anissimov</a>&nbsp;and in <a href=\"http://friendlyai.tumblr.com/post/11957913150/yudkowskys-singularity-summit-2011-talk\">Eliezer's Singularity Summit 2011 talk</a>.</p>\n<p>4) Please provides links to things referenced by your question.</p>\n<p>5) This thread will be open to questions and votes for 7 days, at which time I will decide which questions to begin preparing responses to.</p>\n<p>&nbsp;</p>\n<p>I might respond to certain questions within the comments thread; for example, when there is a one-word answer to the question.</p>\n<p>You may repeat questions that I did not answer in <a href=\"/lw/8c3/qa_with_new_executive_director_of_singularity/\">the first round</a>, and you may ask follow-up questions to <a href=\"/r/discussion/lw/8s6/video_qa_with_singularity_institute_executive/\">the answers I gave</a> in round one.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "89kDDTdDNJQBF9HEC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 11, "extendedScore": null, "score": 8.142585052110004e-07, "legacy": true, "legacyId": "11439", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["G65tLdGma8Xgh3p7L", "yGZHQYqWkLMbXy3z7"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-13T08:52:41.779Z", "modifiedAt": null, "url": null, "title": "[Link] A gentle video introduction to game theory ", "slug": "link-a-gentle-video-introduction-to-game-theory", "viewCount": null, "lastCommentedAt": "2021-12-10T05:58:40.048Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jNR9WXKAawEfQw5JM/link-a-gentle-video-introduction-to-game-theory", "pageUrlRelative": "/posts/jNR9WXKAawEfQw5JM/link-a-gentle-video-introduction-to-game-theory", "linkUrl": "https://www.lesswrong.com/posts/jNR9WXKAawEfQw5JM/link-a-gentle-video-introduction-to-game-theory", "postedAtFormatted": "Tuesday, December 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20A%20gentle%20video%20introduction%20to%20game%20theory%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20A%20gentle%20video%20introduction%20to%20game%20theory%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjNR9WXKAawEfQw5JM%2Flink-a-gentle-video-introduction-to-game-theory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20A%20gentle%20video%20introduction%20to%20game%20theory%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjNR9WXKAawEfQw5JM%2Flink-a-gentle-video-introduction-to-game-theory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjNR9WXKAawEfQw5JM%2Flink-a-gentle-video-introduction-to-game-theory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2298, "htmlBody": "<p><em>In this article I invite LessWrong users to learn the very <a href=\"/lw/l7/the_simple_math_of_everything/\">basic math</a> of something that is useful to both our community's goal of making better thinkers as well as many of the unrelated discussions that we often have here. I also present resources for further study to those interested. </em><em>I made it based on the karma feedback given to this <a href=\"/lw/8nv/open_thread_december_2011/5czq\">post</a> in the monthly <a href=\"/lw/8nv/open_thread_december_2011\">open thread</a>. </em></p>\n<p><em>Recently there has been a series of contributions made in main that serve more as <a href=\"/lw/3mm/back_to_the_basics_of_rationality/\">introductory</a>&nbsp; and logistic material than novel contributions. Because of this and because I hope It will grab more attention from newer members, I posted this in main rather than discussion section. <br /></em></p>\n<p>&nbsp;</p>\n<h2>What is \"game theory\"? <br /></h2>\n<p><a href=\"http://en.wikipedia.org/wiki/Game_theory\">Wikipedia</a>'s take:&nbsp;<img style=\"border: 10px solid black; float: right; margin: 10px;\" src=\"http://upload.wikimedia.org/wikipedia/commons/6/6f/ChessSet.jpg\" alt=\"Game of chess, what did you expect something original?\" width=\"227\" height=\"219\" /></p>\n<blockquote>\n<p><strong>Game theory</strong> is a mathematical method for analyzing calculated circumstances, such as in games, where a person&rsquo;s success&nbsp; is based upon&nbsp; the choices of others. More formally, it is \"the study of mathematical models&nbsp; of conflict and cooperation between intelligent rational decision-makers.\" An alternative term suggested \"as a more descriptive name for&nbsp; the discipline\" is <em>interactive <a title=\"Decision theory\" href=\"http://en.wikipedia.org/wiki/Decision_theory\">decision theory</a></em>.</p>\n</blockquote>\n<p><a href=\"http://wiki.lesswrong.com/wiki/LessWrong_Wiki\">LessWrongWiki</a>'s more succinct <a href=\"http://wiki.lesswrong.com/wiki/Game_theory\">alternative</a>:&nbsp;</p>\n<blockquote>\n<p><strong>Game theory</strong> attempts to mathematically capture behaviour in strategic situations, in which an individual's success in making choices depends on the choices of others.</p>\n</blockquote>\n<p>From both definitions it should be clear how this relates to the art of refining human rationality. Besides the general admonition that <a href=\"http://wiki.lesswrong.com/wiki/Rationalists_should_win\">rationalist should win</a>, for us humans being the social animals that we are, there few things in our lives that do not depend at least partially on the choices of others. Game theory is extensively used in and connected to fields as disparate as economics, psychology, political science, logic, sports and evolutionary biology.</p>\n<p>As many have argued before, <em>it is an important part of the map of the real world</em>:</p>\n<blockquote>\n<p><strong>Again and again, I&rsquo;ve undergone the humbling experience of first lamenting how badly something sucks, then only much later having the crucial insight that <em>its not sucking wouldn&rsquo;t have been a Nash equilibrium.</em></strong></p>\n<p><em>--<a href=\"http://www.scottaaronson.com/blog/?p=418\">Scott Aaronson</a> <br /></em></p>\n</blockquote>\n<p><em>You may not know it yet</em>, but it is impossible to read this site for a extended period of time without running into concepts that are intimately tied to this field of study. <em>Nash equilibrium, Pareto optimal, Prisoners Dilemma, non-zero sum, zero sum,</em> the Decision theory talk that breaks out every now and then<em>,...&nbsp;</em></p>\n<p>You can take the concepts one at a time, reading up on a few lines from a dictionary like definition and trying to assimilate them without doing any of the connected mathematics. I wouldn't want to discourage you from that, its better than guessing! But this approach has its limitations, one risks misunderstanding something or even more subtly just failing to appreciate nuance and running into practical difficulties when trying to apply this knowledge in the real world. At the very least <a href=\"/lw/iq/guessing_the_teachers_password/\">guessing the teachers password</a> is a problem. Those of you that looked up these phrases and concepts on-line probably realized that they fit into a wider framework, a framework I hope you can now begin to explore with simple math, even if only with just a few tentative steps.</p>\n<p>&nbsp;</p>\n<h2>So what are the videos I should watch? <br /></h2>\n<p>This fall (2011) there has been an ongoing class offered by two Stanford professors, <a href=\"http://en.wikipedia.org/wiki/Sebastian_Thrun\">Sebastian Thrun</a> and <a href=\"http://en.wikipedia.org/wiki/Peter_Norvig\">Peter Norvig</a> called <a href=\"https://www.ai-class.com/home/\">\"Introduction to Artificial intelligence\"</a>. It has been talked about extensively on LW in several threads <a href=\"/lw/6wq/stanford_intro_to_ai_course_to_be_taught_for_free/\">here</a>, <a href=\"/lw/815/just_a_reminder_for_everyone_that_signed_up_for/\">here</a> and <a href=\"/lw/7k4/free_online_stanford_courses_ai_and_machine/\">here</a>. Many LWers have showed interest, quite a few signed up and several of us are now preparing for its final exam. Among the material covered is a introduction to game theory. I've been on live lectures about the subject and even watched some recorded ones and in comparison this is one of the better short introductions I've seen so far. I especially like how each of the videos is a self-contained unit just a few minutes in length. Instead of having to commit to watching a 40 or 60 minutes lecture, you just need to commit <strong>2-5 minutes at a time</strong>.</p>\n<p>The relevant Units of the material that cover this are <strong>13. Games</strong> and <strong>14. Game Theory</strong>. These units are presented by <a href=\"http://en.wikipedia.org/wiki/Peter_Norvig\">Peter Norvig</a>. They are not recordings of a professor presenting something to a class in front of a blackboard, but rather aim towards the feeling of having a private tutor sitting down with you and explaining a few things with the help of a pen and a few pieces of paper (reminiscent of the style seen on <a href=\"http://www.khanacademy.org/\">Khan Academy</a>). Currently you can still go directly to the site and view these videos logged in as a <a href=\"https://www.ai-class.com/\">visitor</a> (<strong>recommended</strong>). But just to avoid a trivial inconvenience and in case the youtube videos outlast the current state of the website I'm going to link directly to the youtube videos and write down any relevant comments and missing information as well. Unit 13 especially, assumes some previous knowledge you probably don't have, it deals primarily with complexity of games and how computationally demanding it is to find solutions. It can be useful for getting to know some terminology, but is otherwise skippable.</p>\n<p>&nbsp;</p>\n<p>Don't worry. If you look up or feel you know what an agent or player is and what utility is, the missing exotic stuff (ala POMDPs) that isn't explained as you go along doesn't matter much for our purposes.</p>\n<h3 class=\"topic_14 ctree-header\">13. Games (optional)<br /></h3>\n<div id=\"topic_14\" class=\"loaded topic_14 ctree-content\" style=\"display: block;\"><ol>\n<li> <span class=\"relative videolecture_154 tag_unit13-1\"> <a href=\"http://www.youtube.com/watch?v=25opwF9MylQ&amp;feature=player_embedded\"><span class=\"videotrigger videolecture_154\">Introduction </span></a> <br /> </span> </li>\n<li> <span class=\"relative quizquestion_193 tag_unit13-2\"> <span class=\"quiztrigger quizquestion_193\"> <a href=\"http://www.youtube.com/watch?v=gpkY0em7CR0&amp;feature=player_embedded\">Technologies Question</a> ? (<a href=\"http://www.youtube.com/watch?v=z_NeI-kOETA&amp;feature=player_embedded#!\">Solution</a>) </span></span><span class=\"relative quizquestion_193 tag_unit13-2\">[One choice per row]<br /> </span> </li>\n<li> <span class=\"relative quizquestion_194 tag_unit13-3\"> <span class=\"quiztrigger quizquestion_194\"> <a href=\"http://www.youtube.com/watch?v=6VbzmxAfbAk&amp;feature=player_embedded#!\">Games Question</a> ? (<a href=\"http://www.youtube.com/watch?v=eu6-y8RRMLA&amp;feature=player_embedded\">Solution</a>) [Multiple choice per row]</span> <br /> </span> </li>\n<li> <span class=\"relative videolecture_155 tag_unit13-4\"> <span class=\"videotrigger videolecture_155\"><a href=\"http://www.youtube.com/watch?v=shCDaByrf98&amp;feature=player_embedded\">Single Player Game</a> </span> <br /> </span> </li>\n<li> <span class=\"relative videolecture_156 tag_unit13-5\"> <span class=\"videotrigger videolecture_156\"><a href=\"http://www.youtube.com/watch?v=o3Z3oAoKhDA&amp;feature=player_embedded#!\">Two Player Game</a>&nbsp; </span><br /> </span> </li>\n<li> <span class=\"relative videolecture_157 tag_unit13-6\"> <span class=\"videotrigger videolecture_157\"><a href=\"http://www.youtube.com/watch?v=sMT8sMOBA2Y&amp;feature=player_embedded\">Two Player Function</a> </span> <br /> </span> </li>\n<li> <span class=\"relative quizquestion_195 tag_unit13-7\"><a href=\"http://www.youtube.com/watch?v=8k25hH7DifE&amp;feature=player_embedded\"> </a><span class=\"quiztrigger quizquestion_195\"><a href=\"http://www.youtube.com/watch?v=8k25hH7DifE&amp;feature=player_embedded\"> </a><a href=\"http://www.youtube.com/watch?v=8k25hH7DifE&amp;feature=player_embedded\">Time Complexity Question</a></span></span><span class=\"relative quizquestion_195 tag_unit13-7\"> ? (<a href=\"http://www.youtube.com/watch?v=ZI-kPRRdGLE&amp;feature=player_embedded\">Solution</a>) <br /> </span> </li>\n<li> <span class=\"relative quizquestion_196 tag_unit13-8\"> <span class=\"quiztrigger quizquestion_196\"> <a href=\"http://www.youtube.com/watch?v=fx2rtFveuaM&amp;feature=player_embedded\">Space Complexity Question</a> ? (<a href=\"http://www.youtube.com/watch?v=lIC4pifWgJ0&amp;feature=player_embedded\">Solution</a>) </span> <br /> </span> </li>\n<li> <span class=\"relative quizquestion_197 tag_unit13-9\"> <span class=\"quiztrigger quizquestion_197\"> <a href=\"http://www.youtube.com/watch?v=th2Ua6Cvw3c&amp;feature=player_embedded\">Chess Question</a></span></span><span class=\"relative quizquestion_196 tag_unit13-8\"><span class=\"quiztrigger quizquestion_196\"> ? (<a href=\"http://www.youtube.com/watch?v=2Gak2N_0cjI&amp;feature=player_embedded\">Solution</a>)</span></span><span class=\"relative quizquestion_197 tag_unit13-9\"><span class=\"quiztrigger quizquestion_197\"> </span> </span> </li>\n<li> <span class=\"relative quizquestion_198 tag_unit13-10\"> <span class=\"quiztrigger quizquestion_198\"> <a href=\"http://www.youtube.com/watch?v=r6h2wOXNRbc&amp;feature=player_embedded\">Complexity Reduction Question</a></span></span><span class=\"relative quizquestion_196 tag_unit13-8\"><span class=\"quiztrigger quizquestion_196\"> ? (<a href=\"http://www.youtube.com/watch?v=k3rYDYBDl_U&amp;feature=player_embedded\">Solution</a>)</span></span><span class=\"relative quizquestion_198 tag_unit13-10\"><span class=\"quiztrigger quizquestion_198\"> </span> </span> </li>\n<li> <span class=\"relative quizquestion_199 tag_unit13-11\"> <span class=\"quiztrigger quizquestion_199\"> <a href=\"http://www.youtube.com/watch?v=mHpKLffIOvM&amp;feature=player_embedded\">Review Question</a> ? (<a href=\"http://www.youtube.com/watch?v=lS7IN-NfwbA&amp;feature=player_embedded\">Solution</a>)</span> <br /> </span> </li>\n<li> <span class=\"relative videolecture_158 tag_unit13-12\"> <span class=\"videotrigger videolecture_158\"><a href=\"http://www.youtube.com/watch?v=dNzU_k5b5CY&amp;feature=player_embedded\">Reduce B</a> </span> <br /> </span> </li>\n<li> <span class=\"relative quizquestion_200 tag_unit13-13\"><a href=\"http://www.youtube.com/watch?v=pjNKJFxKnz0&amp;feature=player_embedded\"> </a><span class=\"quiztrigger quizquestion_200\"><a href=\"http://www.youtube.com/watch?v=pjNKJFxKnz0&amp;feature=player_embedded\"> Reduce B Question</a> ? (<a href=\"http://www.youtube.com/watch?v=CIfugiw9_6M&amp;feature=player_embedded\">Solution</a>)</span> <br /> </span> </li>\n<li> <span class=\"relative videolecture_159 tag_unit13-14\"> <span class=\"videotrigger videolecture_159\"><a href=\"http://www.youtube.com/watch?v=hRIwdYyvq2E&amp;feature=player_embedded\">Reduce M</a> </span> <br /> </span> </li>\n<li> <span class=\"relative videolecture_160 tag_unit13-15\"> <span class=\"videotrigger videolecture_160\"><a href=\"http://www.youtube.com/watch?v=2xsXEpdyDUg&amp;feature=player_embedded\">Computing State Values</a> </span> <br /> </span> </li>\n<li> <span class=\"relative videolecture_161 tag_unit13-16\"> <a href=\"http://www.youtube.com/watch?v=Fs1gAjUQGmI&amp;feature=player_embedded\"><span class=\"videotrigger videolecture_161\">Complexity Reduction Benefits </span></a> <br /> </span> </li>\n<li> <span class=\"relative quizquestion_201 tag_unit13-17\"> <span class=\"quiztrigger quizquestion_201\"> <a href=\"http://www.youtube.com/watch?v=ESSdQ4K-a_Q&amp;feature=player_embedded\">Pacman Question</a> ? (<a href=\"http://www.youtube.com/watch?v=qdl4cWMPfE4&amp;feature=player_embedded\">Solution</a>)</span> <br /> </span> </li>\n<li> <span class=\"relative videolecture_162 tag_unit13-18\"> <span class=\"videotrigger videolecture_162\"><a href=\"http://www.youtube.com/watch?v=dZm3MSrYno4&amp;feature=player_embedded\">Chance</a> </span> <br /> </span> </li>\n<li> <span class=\"relative quizquestion_202 tag_unit13-19\"> <span class=\"quiztrigger quizquestion_202\"> <a href=\"http://www.youtube.com/watch?v=_1k1oNbNEAs&amp;feature=player_embedded\">Chance Question</a> </span></span><span class=\"relative quizquestion_201 tag_unit13-17\"><span class=\"quiztrigger quizquestion_201\">? (<a href=\"http://www.youtube.com/watch?v=GRiuI3LHaAQ&amp;feature=player_embedded\">Solution</a>)</span></span><span class=\"relative quizquestion_202 tag_unit13-19\"> <br /> </span> </li>\n<li> <span class=\"relative quizquestion_203 tag_unit13-20\"> <span class=\"quiztrigger quizquestion_203\"> <a href=\"http://www.youtube.com/watch?v=PpYekJ-XIv0&amp;feature=player_embedded\">Terminal State Question</a> </span></span><span class=\"relative quizquestion_201 tag_unit13-17\"><span class=\"quiztrigger quizquestion_201\">? (<a href=\"http://www.youtube.com/watch?v=pTk06FG9ChM&amp;feature=player_embedded\">Solution</a>)</span></span><span class=\"relative quizquestion_203 tag_unit13-20\"><span class=\"quiztrigger quizquestion_203\"> </span> </span> </li>\n<li> <span class=\"relative quizquestion_204 tag_unit13-21\"> <span class=\"quiztrigger quizquestion_204\"> <a href=\"http://www.youtube.com/watch?v=Y1GX5hhdBqQ&amp;feature=player_embedded\">Game Tree Question 1</a> </span></span><span class=\"relative quizquestion_201 tag_unit13-17\"><span class=\"quiztrigger quizquestion_201\">? (<a href=\"http://www.youtube.com/watch?v=nrRDNTXmlno&amp;feature=player_embedded\">Solution</a>)</span></span><span class=\"relative quizquestion_204 tag_unit13-21\"> <br /> </span> </li>\n<li> <span class=\"relative quizquestion_205 tag_unit13-22\"> <span class=\"quiztrigger quizquestion_205\"> <a href=\"http://www.youtube.com/watch?v=fDwnBA0DJb0&amp;feature=player_embedded\">Game Tree Question 2</a> </span></span><span class=\"relative quizquestion_201 tag_unit13-17\"><span class=\"quiztrigger quizquestion_201\">? (<a href=\"http://www.youtube.com/watch?v=00ePzRWuSpo&amp;feature=player_embedded\">Solution</a>)</span></span><span class=\"relative quizquestion_205 tag_unit13-22\"> <br /> </span> </li>\n<li> <span class=\"relative videolecture_163 tag_unit13-23\"> <a href=\"http://www.youtube.com/watch?v=fYQb-uLTmYc&amp;feature=player_embedded\"><span class=\"videotrigger videolecture_163\">Conclusion </span></a> </span></li>\n</ol></div>\n<h3 class=\"topic_15 ctree-header\">14. Game Theory</h3>\n<div id=\"topic_15\" class=\"loaded topic_15 ctree-content\" style=\"display: block;\"><ol>\n<li> <span class=\"relative videolecture_164 tag_unit14-1\"> <span class=\"videotrigger videolecture_164\"><a href=\"http://www.youtube.com/watch?v=E8TWtwT45tg&amp;feature=player_embedded\">Introduction</a> </span> <br /> </span> </li>\n<li> <span class=\"relative quizquestion_206 tag_unit14-2\"> <span class=\"quiztrigger quizquestion_206\"> <a href=\"http://www.youtube.com/watch?v=mY_9srbDMEg&amp;feature=player_embedded#!\">Dominant Strategy Question</a> ? (<a href=\"http://www.youtube.com/watch?v=FAKgof5PMV8&amp;feature=player_embedded\">Solution</a>) [This is where you learn about the famous Prisoners dilemma!]&nbsp;</span> <br /> </span> </li>\n<li> <span class=\"relative quizquestion_207 tag_unit14-3\"> <span class=\"quiztrigger quizquestion_207\"> <a href=\"http://www.youtube.com/watch?v=T57JLskDv7g&amp;feature=player_embedded#!\">Pareto Optimal Question</a></span></span><span class=\"relative quizquestion_206 tag_unit14-2\"><span class=\"quiztrigger quizquestion_206\"> ? (<a href=\"http://www.youtube.com/watch?v=aFIFH6HjcvE&amp;feature=player_embedded\">Solution</a>) [rot13 after solving: Gur dhvm vapbeerpgyl vqragvsvrf bayl gur obggbz evtug bhgpbzr nf Cnergb bcgvzny, ohg obgu gur hccre evtug naq obggbz yrsg ner nyfb Cnergb bcgvzny. Va gur hccre evtug ab bgure bhgpbzr vf zber cersreerq ol O. Yvxrjvfr sbe gur ybjre yrsg ab bgure bhgpbzr vf zber cersreerq ol N.]</span></span><span class=\"relative quizquestion_207 tag_unit14-3\"><span class=\"quiztrigger quizquestion_207\"> </span> </span> </li>\n<li> <a href=\"http://www.youtube.com/watch?v=bcMAYqIoe-8&amp;feature=player_embedded\"><span class=\"relative quizquestion_208 tag_unit14-4\"> <span class=\"quiztrigger quizquestion_208\"> Equilibrium Question</span></span></a><span class=\"relative quizquestion_206 tag_unit14-2\"><span class=\"quiztrigger quizquestion_206\"> ? (<a href=\"http://www.youtube.com/watch?v=QXc7izcogSE&amp;feature=player_embedded\">Solution</a>)</span></span><span class=\"relative quizquestion_208 tag_unit14-4\"><span class=\"quiztrigger quizquestion_208\"> </span> </span> </li>\n<li> <span class=\"relative quizquestion_209 tag_unit14-5\"> <span class=\"quiztrigger quizquestion_209\"> <a href=\"http://www.youtube.com/watch?v=V931T1AoVjo&amp;feature=player_embedded\">Game Console Question 1</a></span></span><span class=\"relative quizquestion_206 tag_unit14-2\"><span class=\"quiztrigger quizquestion_206\"> ? (<a href=\"http://www.youtube.com/watch?v=e0iQE11Fh8c&amp;feature=player_embedded\">Solution</a>)</span></span><span class=\"relative quizquestion_209 tag_unit14-5\"><span class=\"quiztrigger quizquestion_209\"> </span> </span> </li>\n<li> <span class=\"relative quizquestion_210 tag_unit14-6\"> <span class=\"quiztrigger quizquestion_210\"> <a href=\"http://www.youtube.com/watch?v=PEqFuwCv-Mc&amp;feature=player_embedded\">Game Console Question 2</a></span></span><span class=\"relative quizquestion_206 tag_unit14-2\"><span class=\"quiztrigger quizquestion_206\"> ? (<a href=\"http://www.youtube.com/watch?v=B1C1K66wTeI&amp;feature=player_embedded\">Solution</a>)</span></span><span class=\"relative quizquestion_210 tag_unit14-6\"><span class=\"quiztrigger quizquestion_210\"> </span> </span> </li>\n<li> <span class=\"relative videolecture_165 tag_unit14-7\"> <span class=\"videotrigger videolecture_165\"><a href=\"http://www.youtube.com/watch?v=9tB8y8YQM6A&amp;feature=player_embedded\">2 Finger Morra</a> </span> <br /> </span> </li>\n<li> <span class=\"relative quizquestion_211 tag_unit14-8\"> <span class=\"quiztrigger quizquestion_211\"> <a href=\"http://www.youtube.com/watch?v=RFuabag0RBg&amp;feature=player_embedded\">Tree Question</a> ? (<a href=\"http://www.youtube.com/watch?v=EZfcbAfYTTo&amp;feature=player_embedded\">Solution</a>)</span> <br /> </span> </li>\n<li> <span class=\"relative videolecture_166 tag_unit14-9\"> <span class=\"videotrigger videolecture_166\"><a href=\"http://www.youtube.com/watch?v=MyicDINS6pg&amp;feature=player_embedded\">Mixed Strategy</a> </span> <br /> </span> </li>\n<li> <span class=\"relative videolecture_167 tag_unit14-10\"> <span class=\"videotrigger videolecture_167\"><a href=\"http://www.youtube.com/watch?v=S47BxSU1S3w&amp;feature=player_embedded\">Solving the Game</a> </span> <br /> </span> </li>\n<li> <span class=\"relative videolecture_168 tag_unit14-11\"> <span class=\"videotrigger videolecture_168\"><a href=\"http://www.youtube.com/watch?v=fjUIeZHX5Aw&amp;feature=player_embedded\">Mixed Strategy Issues</a> </span> <br /> </span> </li>\n<li> <span class=\"relative quizquestion_212 tag_unit14-12\"> <span class=\"quiztrigger quizquestion_212\"> <a href=\"http://www.youtube.com/watch?v=zB_g1uVuzhM&amp;feature=player_embedded\">2x2 Game Question 1</a></span></span><span class=\"relative quizquestion_211 tag_unit14-8\"><span class=\"quiztrigger quizquestion_211\"> ? (<a href=\"http://www.youtube.com/watch?v=2G4k-9BuuH8&amp;feature=player_embedded\">Solution</a>) [</span></span>Please enter probabilities and not percentages.<span class=\"relative quizquestion_211 tag_unit14-8\"><span class=\"quiztrigger quizquestion_211\">]</span></span><span class=\"relative quizquestion_212 tag_unit14-12\"><span class=\"quiztrigger quizquestion_212\"> </span> </span> </li>\n<li> <span class=\"relative quizquestion_213 tag_unit14-13\"> <span class=\"quiztrigger quizquestion_213\"> <a href=\"http://www.youtube.com/watch?v=AXDRN_PG794&amp;feature=player_embedded\">2x2 Game Question 2</a></span></span><span class=\"relative quizquestion_211 tag_unit14-8\"><span class=\"quiztrigger quizquestion_211\"> ? (<a href=\"http://www.youtube.com/watch?v=m9tkNhbJ-OU&amp;feature=player_embedded\">Solution</a>)</span></span><span class=\"relative quizquestion_213 tag_unit14-13\"><span class=\"quiztrigger quizquestion_213\"> </span> </span> </li>\n<li> <span class=\"relative videolecture_169 tag_unit14-14\"> <a href=\"http://www.youtube.com/watch?v=Ka4DPvOgNnM&amp;feature=player_embedded\"><span class=\"videotrigger videolecture_169\">Geometric Interpretation </span></a> <br /> </span> </li>\n<li> <span class=\"relative videolecture_170 tag_unit14-15\"> <span class=\"videotrigger videolecture_170\"><a href=\"http://www.youtube.com/watch?v=W5hfpcoCdMc&amp;feature=player_embedded\">Poker</a> </span> <br /> </span> </li>\n<li> <span class=\"relative videolecture_171 tag_unit14-16\"> <span class=\"videotrigger videolecture_171\"><a href=\"http://www.youtube.com/watch?v=T2dRPPp5ffc&amp;feature=player_embedded\">Game Theory Strategies</a> </span> <br /> </span> </li>\n<li> <span class=\"relative quizquestion_215 tag_unit14-17\"> <span class=\"quiztrigger quizquestion_215\"> <a href=\"http://www.youtube.com/watch?v=1By22Z6C0UY&amp;feature=player_embedded\">Fed vs Politicians Question</a> ? (<a href=\"http://www.youtube.com/watch?v=YucUFZgMq3A&amp;feature=player_embedded\">Solution</a>)&nbsp;</span> <br /> </span> </li>\n<li> <span class=\"relative videolecture_172 tag_unit14-18\"> <span class=\"videotrigger videolecture_172\"><a href=\"http://www.youtube.com/watch?v=yny3Z2-ptpU&amp;feature=player_embedded#!\">Mechanism Design</a> </span> <br /> </span> </li>\n<li> <span class=\"relative quizquestion_214 tag_unit14-19\"> <span class=\"quiztrigger quizquestion_214\"> <a href=\"http://www.youtube.com/watch?v=aCGPv2s7vvY&amp;feature=player_embedded\">Auction Question</a> ? (<a href=\"http://www.youtube.com/watch?v=uJDkOVGGPI8&amp;feature=player_embedded\">Solution</a>) <br /></span></span></li>\n</ol></div>\n<p>At any point feel free to ask questions here in the&nbsp;<strong>comment section</strong>, I'm sure someone will gladly help you. Also the&nbsp;<a href=\"http://www.reddit.com/r/aiclass\">AI class reddit</a>&nbsp;may be a good resource. Once you are done with the short series of lectures test your knowledge with these assignments.</p>\n<ul>\n<li><a href=\"http://www.youtube.com/watch?v=4Pbln8KFnrs&amp;feature=player_embedded\">Max Min Question</a> ? (<a href=\"http://www.youtube.com/watch?v=CCq4rMC67uw&amp;feature=player_embedded\">Solution</a>)</li>\n<li><a href=\"http://www.youtube.com/watch?v=cJOCbF2UeLE&amp;feature=player_embedded\">Game Tree Question</a> ? (<a href=\"http://www.youtube.com/watch?v=Y5-dwkt8oBc&amp;feature=player_embedded\">Solution</a>) [Unit 13 material. You should check children of pruned nodes as being pruned as well.]</li>\n<li><a href=\"http://www.youtube.com/watch?v=sFPhOsgIVw8&amp;feature=player_embedded\">Strategy Question</a> ? (<a href=\"http://www.youtube.com/watch?v=j_vxzbESRPM&amp;feature=player_embedded\">Solution</a>)<a href=\"http://www.youtube.com/watch?v=j_vxzbESRPM&amp;feature=player_embedded\"><br /></a></li>\n</ul>\n<h5>Note: I present this material in the form of a link to the video, followed by a \"?\" question mark if there is an answerable question that has a solution video posted. The link to the solution are posted as \"(Solution)\". Any additional comments made as corrections to the videos or some information that may be otherwise missing in this format, will be added in square brackets \"[...]\". I encourage people who are solving this via the links rather than the site to not watch the solutions straight away but first work out what they think the answer should be, don't worry if you get it wrong, sometimes the questions are unlikely to be answered correctly with the knowledge you have at that point, their role is to make you better remember and engage the material, not gauge your performance. The exception to this are the videos that come after Unit 14.<br /></h5>\n<p>&nbsp;</p>\n<h2>\"I don't get it.\" &nbsp; or &nbsp; \"It's not working.\"&nbsp; or&nbsp; \"I didn't bother to watch more than a few.\" <br /></h2>\n<p>First off for those who didn't for whatever reason like the lectures given here or find them dull or over your head,&nbsp; <strong>don't despair!</strong>&nbsp; If you feel you don't understand something, ask questions, I can guarantee that either me or someone else will answer it. To those of you who feel they are understanding the material but just don't like the videos or the lecturer, don't worry there are several other ways to approach the field. To just point you on your way here is a wide variety of quality alternatives, some of which may have approaches you prefer:</p>\n<ul>\n<li>Academic Earth site has&nbsp;<a href=\"http://academicearth.org/lectures/search/game%20theory/\">several related classes</a>,&nbsp;including an&nbsp; <a href=\"http://academicearth.org/lectures/introduction-to-game-theory\">introductory one</a>. They include additional non-video material.</li>\n<li>2012&nbsp;<a href=\"http://www.youtube.com/watch?v=_UcRbnJoDKc&amp;feature=player_embedded\">Game Theory</a>&nbsp;<a href=\"http://www.game-theory-class.org/\">online</a>&nbsp;Stanford class (one of the many interesting classes inspired by \"Introduction to AI\")</li>\n</ul>\n<p>I will keep this list updated and add any quality recommendations proposed by fellow LWers.</p>\n<p>Unfortunately for those wanting just the introduction and most basic approach, many of these are more in depth and longer (this is also fortunate for those wanting a bit more). So if you just watch, comprehend and learn to use the information presented in the first lecture or two in one of&nbsp;<em>these</em>&nbsp;recommendations, you have done as much or more as someone who completed Unit 13 and 14. If you don't like video format in general and learn better from written material or live interaction... well this is mostly the wrong article for you. But I do present some additional non-video material in the next section you may find useful.&nbsp;</p>\n<p>&nbsp;</p>\n<h2>I watched the lectures and I think I understood them, where do I go from here?</h2>\n<p>Cool! Well check out some of the alternative videos and classes listed above, most of them are quite extensive. Try to complete one! If you'd like and try to take one ask around the comment section, maybe enough people would be interested to start a study group. Also MIT open course-ware has&nbsp; <a href=\"http://search.mit.edu/search?site=ocw&amp;client=mit&amp;getfields=*&amp;output=xml_no_dtd&amp;proxystylesheet=http%3A%2F%2Focw.mit.edu%2Fsearch%2Fgoogle-ocw.xsl&amp;proxyreload=1&amp;as_dt=i&amp;oe=utf-8&amp;departmentName=web&amp;filter=0&amp;courseName=&amp;q=game+theory&amp;btnG.x=0&amp;btnG.y=0\">some material</a>&nbsp; you may be interested even if you don't feel like doing the full classes.</p>\n<p>A good AI textbook might be something you would like to explore. LessWrong has a great article with&nbsp; <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/\">recommendations</a>&nbsp; for a variety of textbooks for several interesting subjects (all recommendations must be made by people who've read at least two other titles on the subject)... but none for game theory. :/</p>\n<p>In the thread Bgesop&nbsp; <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/3cxv\">requested a recommendation</a>:</p>\n<div id=\"body_t1_3cxv\" class=\"comment-content \">\n<div class=\"md\">\n<blockquote>\n<p>I would like to <strong>request a book on Game Theory</strong>. I went to my school's library and grabbed every book I could find, and so I have <em>Introduction to Game Theory</em> by Peter Morris, <em>Game Theory 2nd Edition</em> by Guillermo Owen, <em>Game Theory and Strategy</em> by Philip Straffin, <em>Game Theory and Politics</em> by Steven Brams, <em>Handbook of Game Theory with Economic Applications</em> edited by Aumann and Hart, <em>Game Theory and Economic Modeling</em> by David Kreps, and <em>Gaming the Vote</em> by William Poundstone because I also like voting theory.</p>\n<p>My brief glances make <em>Game Theory and Strategy</em> look like a fun, low level read that I'll probably start with to whet my appetite for the subject. <em>Introduction to Game Theory</em> looks like a good, well written intro textbook, but it was written in 1940 and was only updated once in 1994, and I would hope something new would have happened in that time. <em>Game Theory 2nd Edition</em> looks like a good, moderately modern (1982) and incredibly boring book. The others look worse.</p>\n<p>I'll read at least portions of all of them and at least two or three completely unless somebody suggests anything. If no one does before I read them I'll post an update.</p>\n</blockquote>\n</div>\n</div>\n<p>Unfortunately it was the plea went unanswered. I'd love to just recommend you the textbook I first learned the subject from, but most readers are probably English speakers, so that's a no go. I'm not familiar with that many of them. I did skim Game Theory 2nd edition by Guillermo Owen, and it seemed ok. Hopefully me pointing this out will prompt someone to come up with a good recommendation. When they do I'll update this post accordingly, and lukeprog's great list can get another good textbook.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"3uE2pXvbcnS9nnZRE": 1, "b8FHrKqyXuYGWc6vn": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jNR9WXKAawEfQw5JM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 34, "baseScore": 46, "extendedScore": null, "score": 8.143039230294205e-07, "legacy": true, "legacyId": "11228", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>In this article I invite LessWrong users to learn the very <a href=\"/lw/l7/the_simple_math_of_everything/\">basic math</a> of something that is useful to both our community's goal of making better thinkers as well as many of the unrelated discussions that we often have here. I also present resources for further study to those interested. </em><em>I made it based on the karma feedback given to this <a href=\"/lw/8nv/open_thread_december_2011/5czq\">post</a> in the monthly <a href=\"/lw/8nv/open_thread_december_2011\">open thread</a>. </em></p>\n<p><em>Recently there has been a series of contributions made in main that serve more as <a href=\"/lw/3mm/back_to_the_basics_of_rationality/\">introductory</a>&nbsp; and logistic material than novel contributions. Because of this and because I hope It will grab more attention from newer members, I posted this in main rather than discussion section. <br></em></p>\n<p>&nbsp;</p>\n<h2 id=\"What_is__game_theory___\">What is \"game theory\"? <br></h2>\n<p><a href=\"http://en.wikipedia.org/wiki/Game_theory\">Wikipedia</a>'s take:&nbsp;<img style=\"border: 10px solid black; float: right; margin: 10px;\" src=\"http://upload.wikimedia.org/wikipedia/commons/6/6f/ChessSet.jpg\" alt=\"Game of chess, what did you expect something original?\" width=\"227\" height=\"219\"></p>\n<blockquote>\n<p><strong>Game theory</strong> is a mathematical method for analyzing calculated circumstances, such as in games, where a person\u2019s success&nbsp; is based upon&nbsp; the choices of others. More formally, it is \"the study of mathematical models&nbsp; of conflict and cooperation between intelligent rational decision-makers.\" An alternative term suggested \"as a more descriptive name for&nbsp; the discipline\" is <em>interactive <a title=\"Decision theory\" href=\"http://en.wikipedia.org/wiki/Decision_theory\">decision theory</a></em>.</p>\n</blockquote>\n<p><a href=\"http://wiki.lesswrong.com/wiki/LessWrong_Wiki\">LessWrongWiki</a>'s more succinct <a href=\"http://wiki.lesswrong.com/wiki/Game_theory\">alternative</a>:&nbsp;</p>\n<blockquote>\n<p><strong>Game theory</strong> attempts to mathematically capture behaviour in strategic situations, in which an individual's success in making choices depends on the choices of others.</p>\n</blockquote>\n<p>From both definitions it should be clear how this relates to the art of refining human rationality. Besides the general admonition that <a href=\"http://wiki.lesswrong.com/wiki/Rationalists_should_win\">rationalist should win</a>, for us humans being the social animals that we are, there few things in our lives that do not depend at least partially on the choices of others. Game theory is extensively used in and connected to fields as disparate as economics, psychology, political science, logic, sports and evolutionary biology.</p>\n<p>As many have argued before, <em>it is an important part of the map of the real world</em>:</p>\n<blockquote>\n<p><strong id=\"Again_and_again__I_ve_undergone_the_humbling_experience_of_first_lamenting_how_badly_something_sucks__then_only_much_later_having_the_crucial_insight_that_its_not_sucking_wouldn_t_have_been_a_Nash_equilibrium_\">Again and again, I\u2019ve undergone the humbling experience of first lamenting how badly something sucks, then only much later having the crucial insight that <em>its not sucking wouldn\u2019t have been a Nash equilibrium.</em></strong></p>\n<p><em>--<a href=\"http://www.scottaaronson.com/blog/?p=418\">Scott Aaronson</a> <br></em></p>\n</blockquote>\n<p><em>You may not know it yet</em>, but it is impossible to read this site for a extended period of time without running into concepts that are intimately tied to this field of study. <em>Nash equilibrium, Pareto optimal, Prisoners Dilemma, non-zero sum, zero sum,</em> the Decision theory talk that breaks out every now and then<em>,...&nbsp;</em></p>\n<p>You can take the concepts one at a time, reading up on a few lines from a dictionary like definition and trying to assimilate them without doing any of the connected mathematics. I wouldn't want to discourage you from that, its better than guessing! But this approach has its limitations, one risks misunderstanding something or even more subtly just failing to appreciate nuance and running into practical difficulties when trying to apply this knowledge in the real world. At the very least <a href=\"/lw/iq/guessing_the_teachers_password/\">guessing the teachers password</a> is a problem. Those of you that looked up these phrases and concepts on-line probably realized that they fit into a wider framework, a framework I hope you can now begin to explore with simple math, even if only with just a few tentative steps.</p>\n<p>&nbsp;</p>\n<h2 id=\"So_what_are_the_videos_I_should_watch__\">So what are the videos I should watch? <br></h2>\n<p>This fall (2011) there has been an ongoing class offered by two Stanford professors, <a href=\"http://en.wikipedia.org/wiki/Sebastian_Thrun\">Sebastian Thrun</a> and <a href=\"http://en.wikipedia.org/wiki/Peter_Norvig\">Peter Norvig</a> called <a href=\"https://www.ai-class.com/home/\">\"Introduction to Artificial intelligence\"</a>. It has been talked about extensively on LW in several threads <a href=\"/lw/6wq/stanford_intro_to_ai_course_to_be_taught_for_free/\">here</a>, <a href=\"/lw/815/just_a_reminder_for_everyone_that_signed_up_for/\">here</a> and <a href=\"/lw/7k4/free_online_stanford_courses_ai_and_machine/\">here</a>. Many LWers have showed interest, quite a few signed up and several of us are now preparing for its final exam. Among the material covered is a introduction to game theory. I've been on live lectures about the subject and even watched some recorded ones and in comparison this is one of the better short introductions I've seen so far. I especially like how each of the videos is a self-contained unit just a few minutes in length. Instead of having to commit to watching a 40 or 60 minutes lecture, you just need to commit <strong>2-5 minutes at a time</strong>.</p>\n<p>The relevant Units of the material that cover this are <strong>13. Games</strong> and <strong>14. Game Theory</strong>. These units are presented by <a href=\"http://en.wikipedia.org/wiki/Peter_Norvig\">Peter Norvig</a>. They are not recordings of a professor presenting something to a class in front of a blackboard, but rather aim towards the feeling of having a private tutor sitting down with you and explaining a few things with the help of a pen and a few pieces of paper (reminiscent of the style seen on <a href=\"http://www.khanacademy.org/\">Khan Academy</a>). Currently you can still go directly to the site and view these videos logged in as a <a href=\"https://www.ai-class.com/\">visitor</a> (<strong>recommended</strong>). But just to avoid a trivial inconvenience and in case the youtube videos outlast the current state of the website I'm going to link directly to the youtube videos and write down any relevant comments and missing information as well. Unit 13 especially, assumes some previous knowledge you probably don't have, it deals primarily with complexity of games and how computationally demanding it is to find solutions. It can be useful for getting to know some terminology, but is otherwise skippable.</p>\n<p>&nbsp;</p>\n<p>Don't worry. If you look up or feel you know what an agent or player is and what utility is, the missing exotic stuff (ala POMDPs) that isn't explained as you go along doesn't matter much for our purposes.</p>\n<h3 class=\"topic_14 ctree-header\" id=\"13__Games__optional_\">13. Games (optional)<br></h3>\n<div id=\"topic_14\" class=\"loaded topic_14 ctree-content\" style=\"display: block;\"><ol>\n<li> <span class=\"relative videolecture_154 tag_unit13-1\"> <a href=\"http://www.youtube.com/watch?v=25opwF9MylQ&amp;feature=player_embedded\"><span class=\"videotrigger videolecture_154\">Introduction </span></a> <br> </span> </li>\n<li> <span class=\"relative quizquestion_193 tag_unit13-2\"> <span class=\"quiztrigger quizquestion_193\"> <a href=\"http://www.youtube.com/watch?v=gpkY0em7CR0&amp;feature=player_embedded\">Technologies Question</a> ? (<a href=\"http://www.youtube.com/watch?v=z_NeI-kOETA&amp;feature=player_embedded#!\">Solution</a>) </span></span><span class=\"relative quizquestion_193 tag_unit13-2\">[One choice per row]<br> </span> </li>\n<li> <span class=\"relative quizquestion_194 tag_unit13-3\"> <span class=\"quiztrigger quizquestion_194\"> <a href=\"http://www.youtube.com/watch?v=6VbzmxAfbAk&amp;feature=player_embedded#!\">Games Question</a> ? (<a href=\"http://www.youtube.com/watch?v=eu6-y8RRMLA&amp;feature=player_embedded\">Solution</a>) [Multiple choice per row]</span> <br> </span> </li>\n<li> <span class=\"relative videolecture_155 tag_unit13-4\"> <span class=\"videotrigger videolecture_155\"><a href=\"http://www.youtube.com/watch?v=shCDaByrf98&amp;feature=player_embedded\">Single Player Game</a> </span> <br> </span> </li>\n<li> <span class=\"relative videolecture_156 tag_unit13-5\"> <span class=\"videotrigger videolecture_156\"><a href=\"http://www.youtube.com/watch?v=o3Z3oAoKhDA&amp;feature=player_embedded#!\">Two Player Game</a>&nbsp; </span><br> </span> </li>\n<li> <span class=\"relative videolecture_157 tag_unit13-6\"> <span class=\"videotrigger videolecture_157\"><a href=\"http://www.youtube.com/watch?v=sMT8sMOBA2Y&amp;feature=player_embedded\">Two Player Function</a> </span> <br> </span> </li>\n<li> <span class=\"relative quizquestion_195 tag_unit13-7\"><a href=\"http://www.youtube.com/watch?v=8k25hH7DifE&amp;feature=player_embedded\"> </a><span class=\"quiztrigger quizquestion_195\"><a href=\"http://www.youtube.com/watch?v=8k25hH7DifE&amp;feature=player_embedded\"> </a><a href=\"http://www.youtube.com/watch?v=8k25hH7DifE&amp;feature=player_embedded\">Time Complexity Question</a></span></span><span class=\"relative quizquestion_195 tag_unit13-7\"> ? (<a href=\"http://www.youtube.com/watch?v=ZI-kPRRdGLE&amp;feature=player_embedded\">Solution</a>) <br> </span> </li>\n<li> <span class=\"relative quizquestion_196 tag_unit13-8\"> <span class=\"quiztrigger quizquestion_196\"> <a href=\"http://www.youtube.com/watch?v=fx2rtFveuaM&amp;feature=player_embedded\">Space Complexity Question</a> ? (<a href=\"http://www.youtube.com/watch?v=lIC4pifWgJ0&amp;feature=player_embedded\">Solution</a>) </span> <br> </span> </li>\n<li> <span class=\"relative quizquestion_197 tag_unit13-9\"> <span class=\"quiztrigger quizquestion_197\"> <a href=\"http://www.youtube.com/watch?v=th2Ua6Cvw3c&amp;feature=player_embedded\">Chess Question</a></span></span><span class=\"relative quizquestion_196 tag_unit13-8\"><span class=\"quiztrigger quizquestion_196\"> ? (<a href=\"http://www.youtube.com/watch?v=2Gak2N_0cjI&amp;feature=player_embedded\">Solution</a>)</span></span><span class=\"relative quizquestion_197 tag_unit13-9\"><span class=\"quiztrigger quizquestion_197\"> </span> </span> </li>\n<li> <span class=\"relative quizquestion_198 tag_unit13-10\"> <span class=\"quiztrigger quizquestion_198\"> <a href=\"http://www.youtube.com/watch?v=r6h2wOXNRbc&amp;feature=player_embedded\">Complexity Reduction Question</a></span></span><span class=\"relative quizquestion_196 tag_unit13-8\"><span class=\"quiztrigger quizquestion_196\"> ? (<a href=\"http://www.youtube.com/watch?v=k3rYDYBDl_U&amp;feature=player_embedded\">Solution</a>)</span></span><span class=\"relative quizquestion_198 tag_unit13-10\"><span class=\"quiztrigger quizquestion_198\"> </span> </span> </li>\n<li> <span class=\"relative quizquestion_199 tag_unit13-11\"> <span class=\"quiztrigger quizquestion_199\"> <a href=\"http://www.youtube.com/watch?v=mHpKLffIOvM&amp;feature=player_embedded\">Review Question</a> ? (<a href=\"http://www.youtube.com/watch?v=lS7IN-NfwbA&amp;feature=player_embedded\">Solution</a>)</span> <br> </span> </li>\n<li> <span class=\"relative videolecture_158 tag_unit13-12\"> <span class=\"videotrigger videolecture_158\"><a href=\"http://www.youtube.com/watch?v=dNzU_k5b5CY&amp;feature=player_embedded\">Reduce B</a> </span> <br> </span> </li>\n<li> <span class=\"relative quizquestion_200 tag_unit13-13\"><a href=\"http://www.youtube.com/watch?v=pjNKJFxKnz0&amp;feature=player_embedded\"> </a><span class=\"quiztrigger quizquestion_200\"><a href=\"http://www.youtube.com/watch?v=pjNKJFxKnz0&amp;feature=player_embedded\"> Reduce B Question</a> ? (<a href=\"http://www.youtube.com/watch?v=CIfugiw9_6M&amp;feature=player_embedded\">Solution</a>)</span> <br> </span> </li>\n<li> <span class=\"relative videolecture_159 tag_unit13-14\"> <span class=\"videotrigger videolecture_159\"><a href=\"http://www.youtube.com/watch?v=hRIwdYyvq2E&amp;feature=player_embedded\">Reduce M</a> </span> <br> </span> </li>\n<li> <span class=\"relative videolecture_160 tag_unit13-15\"> <span class=\"videotrigger videolecture_160\"><a href=\"http://www.youtube.com/watch?v=2xsXEpdyDUg&amp;feature=player_embedded\">Computing State Values</a> </span> <br> </span> </li>\n<li> <span class=\"relative videolecture_161 tag_unit13-16\"> <a href=\"http://www.youtube.com/watch?v=Fs1gAjUQGmI&amp;feature=player_embedded\"><span class=\"videotrigger videolecture_161\">Complexity Reduction Benefits </span></a> <br> </span> </li>\n<li> <span class=\"relative quizquestion_201 tag_unit13-17\"> <span class=\"quiztrigger quizquestion_201\"> <a href=\"http://www.youtube.com/watch?v=ESSdQ4K-a_Q&amp;feature=player_embedded\">Pacman Question</a> ? (<a href=\"http://www.youtube.com/watch?v=qdl4cWMPfE4&amp;feature=player_embedded\">Solution</a>)</span> <br> </span> </li>\n<li> <span class=\"relative videolecture_162 tag_unit13-18\"> <span class=\"videotrigger videolecture_162\"><a href=\"http://www.youtube.com/watch?v=dZm3MSrYno4&amp;feature=player_embedded\">Chance</a> </span> <br> </span> </li>\n<li> <span class=\"relative quizquestion_202 tag_unit13-19\"> <span class=\"quiztrigger quizquestion_202\"> <a href=\"http://www.youtube.com/watch?v=_1k1oNbNEAs&amp;feature=player_embedded\">Chance Question</a> </span></span><span class=\"relative quizquestion_201 tag_unit13-17\"><span class=\"quiztrigger quizquestion_201\">? (<a href=\"http://www.youtube.com/watch?v=GRiuI3LHaAQ&amp;feature=player_embedded\">Solution</a>)</span></span><span class=\"relative quizquestion_202 tag_unit13-19\"> <br> </span> </li>\n<li> <span class=\"relative quizquestion_203 tag_unit13-20\"> <span class=\"quiztrigger quizquestion_203\"> <a href=\"http://www.youtube.com/watch?v=PpYekJ-XIv0&amp;feature=player_embedded\">Terminal State Question</a> </span></span><span class=\"relative quizquestion_201 tag_unit13-17\"><span class=\"quiztrigger quizquestion_201\">? (<a href=\"http://www.youtube.com/watch?v=pTk06FG9ChM&amp;feature=player_embedded\">Solution</a>)</span></span><span class=\"relative quizquestion_203 tag_unit13-20\"><span class=\"quiztrigger quizquestion_203\"> </span> </span> </li>\n<li> <span class=\"relative quizquestion_204 tag_unit13-21\"> <span class=\"quiztrigger quizquestion_204\"> <a href=\"http://www.youtube.com/watch?v=Y1GX5hhdBqQ&amp;feature=player_embedded\">Game Tree Question 1</a> </span></span><span class=\"relative quizquestion_201 tag_unit13-17\"><span class=\"quiztrigger quizquestion_201\">? (<a href=\"http://www.youtube.com/watch?v=nrRDNTXmlno&amp;feature=player_embedded\">Solution</a>)</span></span><span class=\"relative quizquestion_204 tag_unit13-21\"> <br> </span> </li>\n<li> <span class=\"relative quizquestion_205 tag_unit13-22\"> <span class=\"quiztrigger quizquestion_205\"> <a href=\"http://www.youtube.com/watch?v=fDwnBA0DJb0&amp;feature=player_embedded\">Game Tree Question 2</a> </span></span><span class=\"relative quizquestion_201 tag_unit13-17\"><span class=\"quiztrigger quizquestion_201\">? (<a href=\"http://www.youtube.com/watch?v=00ePzRWuSpo&amp;feature=player_embedded\">Solution</a>)</span></span><span class=\"relative quizquestion_205 tag_unit13-22\"> <br> </span> </li>\n<li> <span class=\"relative videolecture_163 tag_unit13-23\"> <a href=\"http://www.youtube.com/watch?v=fYQb-uLTmYc&amp;feature=player_embedded\"><span class=\"videotrigger videolecture_163\">Conclusion </span></a> </span></li>\n</ol></div>\n<h3 class=\"topic_15 ctree-header\" id=\"14__Game_Theory\">14. Game Theory</h3>\n<div id=\"topic_15\" class=\"loaded topic_15 ctree-content\" style=\"display: block;\"><ol>\n<li> <span class=\"relative videolecture_164 tag_unit14-1\"> <span class=\"videotrigger videolecture_164\"><a href=\"http://www.youtube.com/watch?v=E8TWtwT45tg&amp;feature=player_embedded\">Introduction</a> </span> <br> </span> </li>\n<li> <span class=\"relative quizquestion_206 tag_unit14-2\"> <span class=\"quiztrigger quizquestion_206\"> <a href=\"http://www.youtube.com/watch?v=mY_9srbDMEg&amp;feature=player_embedded#!\">Dominant Strategy Question</a> ? (<a href=\"http://www.youtube.com/watch?v=FAKgof5PMV8&amp;feature=player_embedded\">Solution</a>) [This is where you learn about the famous Prisoners dilemma!]&nbsp;</span> <br> </span> </li>\n<li> <span class=\"relative quizquestion_207 tag_unit14-3\"> <span class=\"quiztrigger quizquestion_207\"> <a href=\"http://www.youtube.com/watch?v=T57JLskDv7g&amp;feature=player_embedded#!\">Pareto Optimal Question</a></span></span><span class=\"relative quizquestion_206 tag_unit14-2\"><span class=\"quiztrigger quizquestion_206\"> ? (<a href=\"http://www.youtube.com/watch?v=aFIFH6HjcvE&amp;feature=player_embedded\">Solution</a>) [rot13 after solving: Gur dhvm vapbeerpgyl vqragvsvrf bayl gur obggbz evtug bhgpbzr nf Cnergb bcgvzny, ohg obgu gur hccre evtug naq obggbz yrsg ner nyfb Cnergb bcgvzny. Va gur hccre evtug ab bgure bhgpbzr vf zber cersreerq ol O. Yvxrjvfr sbe gur ybjre yrsg ab bgure bhgpbzr vf zber cersreerq ol N.]</span></span><span class=\"relative quizquestion_207 tag_unit14-3\"><span class=\"quiztrigger quizquestion_207\"> </span> </span> </li>\n<li> <a href=\"http://www.youtube.com/watch?v=bcMAYqIoe-8&amp;feature=player_embedded\"><span class=\"relative quizquestion_208 tag_unit14-4\"> <span class=\"quiztrigger quizquestion_208\"> Equilibrium Question</span></span></a><span class=\"relative quizquestion_206 tag_unit14-2\"><span class=\"quiztrigger quizquestion_206\"> ? (<a href=\"http://www.youtube.com/watch?v=QXc7izcogSE&amp;feature=player_embedded\">Solution</a>)</span></span><span class=\"relative quizquestion_208 tag_unit14-4\"><span class=\"quiztrigger quizquestion_208\"> </span> </span> </li>\n<li> <span class=\"relative quizquestion_209 tag_unit14-5\"> <span class=\"quiztrigger quizquestion_209\"> <a href=\"http://www.youtube.com/watch?v=V931T1AoVjo&amp;feature=player_embedded\">Game Console Question 1</a></span></span><span class=\"relative quizquestion_206 tag_unit14-2\"><span class=\"quiztrigger quizquestion_206\"> ? (<a href=\"http://www.youtube.com/watch?v=e0iQE11Fh8c&amp;feature=player_embedded\">Solution</a>)</span></span><span class=\"relative quizquestion_209 tag_unit14-5\"><span class=\"quiztrigger quizquestion_209\"> </span> </span> </li>\n<li> <span class=\"relative quizquestion_210 tag_unit14-6\"> <span class=\"quiztrigger quizquestion_210\"> <a href=\"http://www.youtube.com/watch?v=PEqFuwCv-Mc&amp;feature=player_embedded\">Game Console Question 2</a></span></span><span class=\"relative quizquestion_206 tag_unit14-2\"><span class=\"quiztrigger quizquestion_206\"> ? (<a href=\"http://www.youtube.com/watch?v=B1C1K66wTeI&amp;feature=player_embedded\">Solution</a>)</span></span><span class=\"relative quizquestion_210 tag_unit14-6\"><span class=\"quiztrigger quizquestion_210\"> </span> </span> </li>\n<li> <span class=\"relative videolecture_165 tag_unit14-7\"> <span class=\"videotrigger videolecture_165\"><a href=\"http://www.youtube.com/watch?v=9tB8y8YQM6A&amp;feature=player_embedded\">2 Finger Morra</a> </span> <br> </span> </li>\n<li> <span class=\"relative quizquestion_211 tag_unit14-8\"> <span class=\"quiztrigger quizquestion_211\"> <a href=\"http://www.youtube.com/watch?v=RFuabag0RBg&amp;feature=player_embedded\">Tree Question</a> ? (<a href=\"http://www.youtube.com/watch?v=EZfcbAfYTTo&amp;feature=player_embedded\">Solution</a>)</span> <br> </span> </li>\n<li> <span class=\"relative videolecture_166 tag_unit14-9\"> <span class=\"videotrigger videolecture_166\"><a href=\"http://www.youtube.com/watch?v=MyicDINS6pg&amp;feature=player_embedded\">Mixed Strategy</a> </span> <br> </span> </li>\n<li> <span class=\"relative videolecture_167 tag_unit14-10\"> <span class=\"videotrigger videolecture_167\"><a href=\"http://www.youtube.com/watch?v=S47BxSU1S3w&amp;feature=player_embedded\">Solving the Game</a> </span> <br> </span> </li>\n<li> <span class=\"relative videolecture_168 tag_unit14-11\"> <span class=\"videotrigger videolecture_168\"><a href=\"http://www.youtube.com/watch?v=fjUIeZHX5Aw&amp;feature=player_embedded\">Mixed Strategy Issues</a> </span> <br> </span> </li>\n<li> <span class=\"relative quizquestion_212 tag_unit14-12\"> <span class=\"quiztrigger quizquestion_212\"> <a href=\"http://www.youtube.com/watch?v=zB_g1uVuzhM&amp;feature=player_embedded\">2x2 Game Question 1</a></span></span><span class=\"relative quizquestion_211 tag_unit14-8\"><span class=\"quiztrigger quizquestion_211\"> ? (<a href=\"http://www.youtube.com/watch?v=2G4k-9BuuH8&amp;feature=player_embedded\">Solution</a>) [</span></span>Please enter probabilities and not percentages.<span class=\"relative quizquestion_211 tag_unit14-8\"><span class=\"quiztrigger quizquestion_211\">]</span></span><span class=\"relative quizquestion_212 tag_unit14-12\"><span class=\"quiztrigger quizquestion_212\"> </span> </span> </li>\n<li> <span class=\"relative quizquestion_213 tag_unit14-13\"> <span class=\"quiztrigger quizquestion_213\"> <a href=\"http://www.youtube.com/watch?v=AXDRN_PG794&amp;feature=player_embedded\">2x2 Game Question 2</a></span></span><span class=\"relative quizquestion_211 tag_unit14-8\"><span class=\"quiztrigger quizquestion_211\"> ? (<a href=\"http://www.youtube.com/watch?v=m9tkNhbJ-OU&amp;feature=player_embedded\">Solution</a>)</span></span><span class=\"relative quizquestion_213 tag_unit14-13\"><span class=\"quiztrigger quizquestion_213\"> </span> </span> </li>\n<li> <span class=\"relative videolecture_169 tag_unit14-14\"> <a href=\"http://www.youtube.com/watch?v=Ka4DPvOgNnM&amp;feature=player_embedded\"><span class=\"videotrigger videolecture_169\">Geometric Interpretation </span></a> <br> </span> </li>\n<li> <span class=\"relative videolecture_170 tag_unit14-15\"> <span class=\"videotrigger videolecture_170\"><a href=\"http://www.youtube.com/watch?v=W5hfpcoCdMc&amp;feature=player_embedded\">Poker</a> </span> <br> </span> </li>\n<li> <span class=\"relative videolecture_171 tag_unit14-16\"> <span class=\"videotrigger videolecture_171\"><a href=\"http://www.youtube.com/watch?v=T2dRPPp5ffc&amp;feature=player_embedded\">Game Theory Strategies</a> </span> <br> </span> </li>\n<li> <span class=\"relative quizquestion_215 tag_unit14-17\"> <span class=\"quiztrigger quizquestion_215\"> <a href=\"http://www.youtube.com/watch?v=1By22Z6C0UY&amp;feature=player_embedded\">Fed vs Politicians Question</a> ? (<a href=\"http://www.youtube.com/watch?v=YucUFZgMq3A&amp;feature=player_embedded\">Solution</a>)&nbsp;</span> <br> </span> </li>\n<li> <span class=\"relative videolecture_172 tag_unit14-18\"> <span class=\"videotrigger videolecture_172\"><a href=\"http://www.youtube.com/watch?v=yny3Z2-ptpU&amp;feature=player_embedded#!\">Mechanism Design</a> </span> <br> </span> </li>\n<li> <span class=\"relative quizquestion_214 tag_unit14-19\"> <span class=\"quiztrigger quizquestion_214\"> <a href=\"http://www.youtube.com/watch?v=aCGPv2s7vvY&amp;feature=player_embedded\">Auction Question</a> ? (<a href=\"http://www.youtube.com/watch?v=uJDkOVGGPI8&amp;feature=player_embedded\">Solution</a>) <br></span></span></li>\n</ol></div>\n<p>At any point feel free to ask questions here in the&nbsp;<strong>comment section</strong>, I'm sure someone will gladly help you. Also the&nbsp;<a href=\"http://www.reddit.com/r/aiclass\">AI class reddit</a>&nbsp;may be a good resource. Once you are done with the short series of lectures test your knowledge with these assignments.</p>\n<ul>\n<li><a href=\"http://www.youtube.com/watch?v=4Pbln8KFnrs&amp;feature=player_embedded\">Max Min Question</a> ? (<a href=\"http://www.youtube.com/watch?v=CCq4rMC67uw&amp;feature=player_embedded\">Solution</a>)</li>\n<li><a href=\"http://www.youtube.com/watch?v=cJOCbF2UeLE&amp;feature=player_embedded\">Game Tree Question</a> ? (<a href=\"http://www.youtube.com/watch?v=Y5-dwkt8oBc&amp;feature=player_embedded\">Solution</a>) [Unit 13 material. You should check children of pruned nodes as being pruned as well.]</li>\n<li><a href=\"http://www.youtube.com/watch?v=sFPhOsgIVw8&amp;feature=player_embedded\">Strategy Question</a> ? (<a href=\"http://www.youtube.com/watch?v=j_vxzbESRPM&amp;feature=player_embedded\">Solution</a>)<a href=\"http://www.youtube.com/watch?v=j_vxzbESRPM&amp;feature=player_embedded\"><br></a></li>\n</ul>\n<h5>Note: I present this material in the form of a link to the video, followed by a \"?\" question mark if there is an answerable question that has a solution video posted. The link to the solution are posted as \"(Solution)\". Any additional comments made as corrections to the videos or some information that may be otherwise missing in this format, will be added in square brackets \"[...]\". I encourage people who are solving this via the links rather than the site to not watch the solutions straight away but first work out what they think the answer should be, don't worry if you get it wrong, sometimes the questions are unlikely to be answered correctly with the knowledge you have at that point, their role is to make you better remember and engage the material, not gauge your performance. The exception to this are the videos that come after Unit 14.<br></h5>\n<p>&nbsp;</p>\n<h2 id=\"_I_don_t_get_it_____or____It_s_not_working____or___I_didn_t_bother_to_watch_more_than_a_few___\">\"I don't get it.\" &nbsp; or &nbsp; \"It's not working.\"&nbsp; or&nbsp; \"I didn't bother to watch more than a few.\" <br></h2>\n<p>First off for those who didn't for whatever reason like the lectures given here or find them dull or over your head,&nbsp; <strong>don't despair!</strong>&nbsp; If you feel you don't understand something, ask questions, I can guarantee that either me or someone else will answer it. To those of you who feel they are understanding the material but just don't like the videos or the lecturer, don't worry there are several other ways to approach the field. To just point you on your way here is a wide variety of quality alternatives, some of which may have approaches you prefer:</p>\n<ul>\n<li>Academic Earth site has&nbsp;<a href=\"http://academicearth.org/lectures/search/game%20theory/\">several related classes</a>,&nbsp;including an&nbsp; <a href=\"http://academicearth.org/lectures/introduction-to-game-theory\">introductory one</a>. They include additional non-video material.</li>\n<li>2012&nbsp;<a href=\"http://www.youtube.com/watch?v=_UcRbnJoDKc&amp;feature=player_embedded\">Game Theory</a>&nbsp;<a href=\"http://www.game-theory-class.org/\">online</a>&nbsp;Stanford class (one of the many interesting classes inspired by \"Introduction to AI\")</li>\n</ul>\n<p>I will keep this list updated and add any quality recommendations proposed by fellow LWers.</p>\n<p>Unfortunately for those wanting just the introduction and most basic approach, many of these are more in depth and longer (this is also fortunate for those wanting a bit more). So if you just watch, comprehend and learn to use the information presented in the first lecture or two in one of&nbsp;<em>these</em>&nbsp;recommendations, you have done as much or more as someone who completed Unit 13 and 14. If you don't like video format in general and learn better from written material or live interaction... well this is mostly the wrong article for you. But I do present some additional non-video material in the next section you may find useful.&nbsp;</p>\n<p>&nbsp;</p>\n<h2 id=\"I_watched_the_lectures_and_I_think_I_understood_them__where_do_I_go_from_here_\">I watched the lectures and I think I understood them, where do I go from here?</h2>\n<p>Cool! Well check out some of the alternative videos and classes listed above, most of them are quite extensive. Try to complete one! If you'd like and try to take one ask around the comment section, maybe enough people would be interested to start a study group. Also MIT open course-ware has&nbsp; <a href=\"http://search.mit.edu/search?site=ocw&amp;client=mit&amp;getfields=*&amp;output=xml_no_dtd&amp;proxystylesheet=http%3A%2F%2Focw.mit.edu%2Fsearch%2Fgoogle-ocw.xsl&amp;proxyreload=1&amp;as_dt=i&amp;oe=utf-8&amp;departmentName=web&amp;filter=0&amp;courseName=&amp;q=game+theory&amp;btnG.x=0&amp;btnG.y=0\">some material</a>&nbsp; you may be interested even if you don't feel like doing the full classes.</p>\n<p>A good AI textbook might be something you would like to explore. LessWrong has a great article with&nbsp; <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/\">recommendations</a>&nbsp; for a variety of textbooks for several interesting subjects (all recommendations must be made by people who've read at least two other titles on the subject)... but none for game theory. :/</p>\n<p>In the thread Bgesop&nbsp; <a href=\"/lw/3gu/the_best_textbooks_on_every_subject/3cxv\">requested a recommendation</a>:</p>\n<div id=\"body_t1_3cxv\" class=\"comment-content \">\n<div class=\"md\">\n<blockquote>\n<p>I would like to <strong>request a book on Game Theory</strong>. I went to my school's library and grabbed every book I could find, and so I have <em>Introduction to Game Theory</em> by Peter Morris, <em>Game Theory 2nd Edition</em> by Guillermo Owen, <em>Game Theory and Strategy</em> by Philip Straffin, <em>Game Theory and Politics</em> by Steven Brams, <em>Handbook of Game Theory with Economic Applications</em> edited by Aumann and Hart, <em>Game Theory and Economic Modeling</em> by David Kreps, and <em>Gaming the Vote</em> by William Poundstone because I also like voting theory.</p>\n<p>My brief glances make <em>Game Theory and Strategy</em> look like a fun, low level read that I'll probably start with to whet my appetite for the subject. <em>Introduction to Game Theory</em> looks like a good, well written intro textbook, but it was written in 1940 and was only updated once in 1994, and I would hope something new would have happened in that time. <em>Game Theory 2nd Edition</em> looks like a good, moderately modern (1982) and incredibly boring book. The others look worse.</p>\n<p>I'll read at least portions of all of them and at least two or three completely unless somebody suggests anything. If no one does before I read them I'll post an update.</p>\n</blockquote>\n</div>\n</div>\n<p>Unfortunately it was the plea went unanswered. I'd love to just recommend you the textbook I first learned the subject from, but most readers are probably English speakers, so that's a no go. I'm not familiar with that many of them. I did skim Game Theory 2nd edition by Guillermo Owen, and it seemed ok. Hopefully me pointing this out will prompt someone to come up with a good recommendation. When they do I'll update this post accordingly, and lukeprog's great list can get another good textbook.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>", "sections": [{"title": "What is \"game theory\"? ", "anchor": "What_is__game_theory___", "level": 1}, {"title": "Again and again, I\u2019ve undergone the humbling experience of first lamenting how badly something sucks, then only much later having the crucial insight that its not sucking wouldn\u2019t have been a Nash equilibrium.", "anchor": "Again_and_again__I_ve_undergone_the_humbling_experience_of_first_lamenting_how_badly_something_sucks__then_only_much_later_having_the_crucial_insight_that_its_not_sucking_wouldn_t_have_been_a_Nash_equilibrium_", "level": 3}, {"title": "So what are the videos I should watch? ", "anchor": "So_what_are_the_videos_I_should_watch__", "level": 1}, {"title": "13. Games (optional)", "anchor": "13__Games__optional_", "level": 2}, {"title": "14. Game Theory", "anchor": "14__Game_Theory", "level": 2}, {"title": "\"I don't get it.\" \u00a0 or \u00a0 \"It's not working.\"\u00a0 or\u00a0 \"I didn't bother to watch more than a few.\" ", "anchor": "_I_don_t_get_it_____or____It_s_not_working____or___I_didn_t_bother_to_watch_more_than_a_few___", "level": 1}, {"title": "I watched the lectures and I think I understood them, where do I go from here?", "anchor": "I_watched_the_lectures_and_I_think_I_understood_them__where_do_I_go_from_here_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "10 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HnPEpu5eQWkbyAJCT", "rhicHrc6CLSuws89u", "gfexKxsBDM6v2sCMo", "NMoLJuDJEms7Ku9XS", "6PQxeGSo4YHqwfrdp", "hAyXBBkjriBqJuZna", "MM23WmWQpo8fiZA9h", "xg3hXCYQPJkwHyik2"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-13T10:00:57.425Z", "modifiedAt": null, "url": null, "title": "Q&A with Richard Carrier on risks from AI", "slug": "q-and-a-with-richard-carrier-on-risks-from-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:30.857Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dCTvFYNoLo6cQXvyK/q-and-a-with-richard-carrier-on-risks-from-ai", "pageUrlRelative": "/posts/dCTvFYNoLo6cQXvyK/q-and-a-with-richard-carrier-on-risks-from-ai", "linkUrl": "https://www.lesswrong.com/posts/dCTvFYNoLo6cQXvyK/q-and-a-with-richard-carrier-on-risks-from-ai", "postedAtFormatted": "Tuesday, December 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Q%26A%20with%20Richard%20Carrier%20on%20risks%20from%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQ%26A%20with%20Richard%20Carrier%20on%20risks%20from%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdCTvFYNoLo6cQXvyK%2Fq-and-a-with-richard-carrier-on-risks-from-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Q%26A%20with%20Richard%20Carrier%20on%20risks%20from%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdCTvFYNoLo6cQXvyK%2Fq-and-a-with-richard-carrier-on-risks-from-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdCTvFYNoLo6cQXvyK%2Fq-and-a-with-richard-carrier-on-risks-from-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2934, "htmlBody": "<p><strong>[<a href=\"http://wiki.lesswrong.com/wiki/Interview_series_on_risks_from_AI\">Click here to see a list of all interviews</a>]</strong></p>\n<p>I am emailing experts in order to raise and estimate the academic awareness and perception of risks from AI.</p>\n<blockquote>\n<p><strong>Richard Carrier</strong> is a world-renowned author and speaker. As a professional historian, published philosopher, and prominent defender of the American freethought movement, Dr. Carrier has appeared across the country and on national television defending sound historical methods and the ethical worldview of secular naturalism. His books and articles have also received international attention. He holds a Ph.D. from Columbia University in ancient history, specializing in the intellectual history of Greece and Rome, particularly ancient philosophy, religion, and science, with emphasis on the origins of Christianity and the use and progress of science under the Roman empire. He is best known as the author of <strong><em><a class=\"spec\" href=\"http://astore.amazon.com/supportcarrier-20/detail/1420802933\">Sense and Goodness without God</a></em></strong>, <strong><em><a class=\"spec\" href=\"http://astore.amazon.com/supportcarrier-20/detail/0557044642\">Not the Impossible Faith</a></em></strong>, and <strong><em><a class=\"spec\" href=\"http://astore.amazon.com/supportcarrier-20/detail/1456588850\">Why I Am Not a Christian</a></em></strong>, and a major contributor to <strong><em><a class=\"spec\" href=\"http://astore.amazon.com/supportcarrier-20/detail/159102286X\">The Empty Tomb</a></em></strong>, <strong><em><a class=\"spec\" href=\"http://astore.amazon.com/supportcarrier-20/detail/1616141689\">The Christian Delusion</a></em></strong>, <strong><em><a class=\"spec\" href=\"http://astore.amazon.com/supportcarrier-20/detail/16161441300\">The End of Christianity</a></em></strong>, and <strong><em><a class=\"spec\" href=\"http://astore.amazon.com/supportcarrier-20/detail/1616141891\">Sources of the Jesus Tradition</a></em></strong>, as well as writer and editor-in-chief (now emeritus) for the <strong><a class=\"spec\" href=\"http://www.infidels.org/\">Secular Web</a></strong>, and for his copious work in history and philosophy online and in print. He is currently working on his next books, <strong><em>Proving History: Bayes's Theorem and the Quest for the Historical Jesus</em></strong>, <strong><em>On the Historicity of Jesus Christ</em></strong>, <strong><em>The Scientist in the Early Roman Empire</em></strong>, and <strong><em>Science Education in the Early Roman Empire</em></strong>. To learn more about Dr. Carrier and his work follow the links below.</p>\n</blockquote>\n<p><strong>Homepage:</strong> <a href=\"http://www.richardcarrier.info/about.html\">richardcarrier.info</a></p>\n<p><strong>Blog:</strong> <a href=\"http://freethoughtblogs.com/carrier/\">freethoughtblogs.com/carrier/</a> (old blog: <a href=\"http://richardcarrier.blogspot.com/\">richardcarrier.blogspot.com</a>)</p>\n<p><strong>Selected articles:</strong></p>\n<ul>\n<li><a href=\"http://freethoughtblogs.com/carrier/archives/80\">Bayes&rsquo; Theorem: Lust for Glory!</a> (Blog post and <a href=\"http://youtu.be/HHIz-gR4xHo\">video talk</a>)</li>\n<li>&ldquo;<a href=\"http://www.richardcarrier.info/CarrierDec08.pdf\">Bayes&rsquo; Theorem for Beginners: Formal Logic and Its Relevance to Historical Method</a>&rdquo; (Paper)</li>\n</ul>\n<h3><strong>The Interview</strong>:</h3>\n<p style=\"padding-left: 30px;\"><strong>Richard Carrier:</strong> Note that I follow and support the work of The Singularity Institute on precisely this issue, which you are writing for (if you are a correspondent for Less Wrong). And I believe all AI developers should (e.g. <a href=\"http://en.wikipedia.org/wiki/CALO\">CALO</a>). So my answers won't be too surprising (below). But also keep in mind what I say (not just on \"singularity\" claims) at:<br /> <br /> <a href=\"http://richardcarrier.blogspot.com/2009/06/are-we-doomed.html\" target=\"_blank\">http://richardcarrier.blogspot.com/2009/06/are-we-doomed.html</a></p>\n<p><strong>Q1:</strong> <em>Assuming no global catastrophe halts progress, by what year would you assign a 10%/50%/90% chance of the development of roughly human-level machine intelligence?</em><br /><em></em></p>\n<p style=\"padding-left: 30px;\"><strong>Richard Carrier: </strong>2020/2040/2080</p>\n<p><em>Explanatory remark to Q1:</em><br /><br /><em>P(human-level AI by (year) | no wars &and; no disasters &and; beneficially political and economic development) = 10%/50%/90%</em></p>\n<p><strong>Q2:</strong> <em>What probability do you assign to the possibility of human extinction</em><em> as a result of badly done AI?</em><br /><em></em></p>\n<p style=\"padding-left: 30px;\"><strong>Richard Carrier: </strong>Here the relative probability is much higher that human extinction will result from benevolent AI, i.e. eventually Homo sapiens will be self-evidently obsolete and we will voluntarily transition to Homo cyberneticus. In other words, we will extinguish the Homo sapiens species ourselves, voluntarily. If you asked for a 10%/50%/90% deadline for this I would say 2500/3000/4000.<br /> <br /> However, perhaps you mean to ask regarding the extinction of all Homo, and their replacement with AI that did not originate as a human mind, i.e. the probability that some AI will kill us and just propagate itself.<br /> <br /> The answer to that is dependent on what you mean by \"badly done\" AI: (a) AI that has more power than we think we gave it, causing us problems, or (b) AI that has so much more power than we think we gave it that it can prevent our taking its power away.<br /> <br /> (a) is probably inevitable, or at any rate a high probability, and there will likely be deaths or other catastrophes, but like other tech failures (e.g. the Titanic, three mile island, hijacking jumbo jets and using them as guided missiles) we will prevail, and very quickly from a historical perspective (e.g. there won't be another 9/11 using airplanes as missiles; we only got jacked by that unforeseen failure once). We would do well to prevent as many problems as possible by being as smart as we can be about implementing AI, and not underestimating its ability to outsmart us, or to develop while we aren't looking (e.g. Siri could go sentient on its own, if no one is managing it closely to ensure that doesn't happen).<br /> <br /> (b) is very improbable because AI function is too dependent on human cooperation (e.g. power grid; physical servers that can be axed or bombed; an internet that can be shut down manually) and any move by AI to supplant that requirement would be too obvious and thus too easily stopped. In short, AI is infrastructure dependent, but it takes too much time and effort to build an infrastructure, and even more an infrastructure that is invulnerable to demolition. By the time AI has an independent infrastructure (e.g. its own robot population worldwide, its own power supplies, manufacturing plants, etc.) Homo sapiens will probably already be transitioning to Homo cyberneticus and there will be no effective difference between us and AI.<br /> <br /> However, given no deadline, it's likely there will be scenarios like: \"god\" AI's run sims in which digitized humans live, and any given god AI could decide to delete the sim and stop running it (and likewise all comparable AI shepherding scenarios). So then we'd be asking how likely is it that a god AI would ever do that, and more specifically, that all would (since there won't be just one sim run by one AI, but many, so one going rogue would not mean extinction of humanity).<br /> <br /> So setting aside AI that merely kills some people, and only focusing on total extinction of Homo sapiens, we have:<br /> <br /> P(voluntary human extinction by replacement | any AGI at all) = 90%+<br /> <br /> P(involuntary human extinction without replacement | badly done AGI type (a)) = &lt; 10^-20<br /> <br /> [and that's taking into account an infinite deadline, because the probability steeply declines with every year after first opportunity, e.g. AI that doesn't do it the first chance it gets is rapidly less likely to as time goes on, so the total probability has a limit even at infinite time, and I would put that limit somewhere as here assigned.]<br /> <br /> P(involuntary human extinction without replacement | badly done AGI type (b)) = .33 to .67<br /> <br /> However, P(badly done AGI type (b)) = &lt; 10^-20</p>\n<p><em>Explanatory remark to Q2:</em><br /> <br /><em>P(human extinction | badly done AI) = ?</em><br /><br /><em>(Where 'badly done' = AGI capable of self-modification that is <strong>not </strong>provably non-dangerous.)</em></p>\n<p><strong>Q3: </strong><em>What probability do you assign to the possibility of a human level AGI to self-modify its way up to massive superhuman intelligence within a matter of hours/days/&lt; 5 years? </em><br /><em></em></p>\n<p style=\"padding-left: 30px;\"><strong>Richard Carrier: </strong>Depends on when it starts. For example, if we started a human-level AGI tomorrow, it's ability to revise itself would be hugely limited by our slow and expensive infrastructure (e.g. manufacturing the new circuits, building the mainframe extensions, supplying them with power, debugging the system). In that context, \"hours\" and \"days\" have P --&gt; 0, but 5 years has P = 33%+ if someone is funding the project, and likewise 10 years has P=67%+; and 25 years, P=90%+. However, suppose human-level AGI is first realized in fifty years when all these things can be done in a single room with relatively inexpensive automation and the power demands of any new system were not greater than are normally supplied to that room. Then P(days) = 90%+. And with massively more advanced tech, say such as we might have in 2500, then P(hours) = 90%+.<br /> <br /> However...</p>\n<p><em>Explanatory remark to Q3:</em><br /><br /><em>P(superhuman intelligence within hours | human-level AI running at human-level speed equipped with a 100 Gigabit Internet connection) = ?<br />P(superhuman intelligence within days | human-level AI running at human-level speed equipped with a 100 Gigabit Internet connection) = ?<br />P(superhuman intelligence within &lt; 5 years | human-level AI running at human-level speed equipped with a 100 Gigabit Internet connection) = ?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Richard Carrier: </strong>Perhaps you are confusing intelligence with knowledge. Internet connection can make no difference to the former (since an AGI will have no more control over the internet than human operators do). That can only expand a mind's knowledge. As to how quickly, it will depend more on the rate of processed seconds in the AGI itself, i.e. if it can simulate human thought only at the same pace as non-AI, then it will not be able to learn any faster than a regular person, no matter what kind of internet connection it has. But if the AGI can process ten seconds time in one second of non-AI time, then it can learn ten times as fast, up to the limit of data access (and that is where internet connection speed will matter). That is a calculation I can't do. A computer science expert would have to be consulted to calculate reasonable estimates of what connection speed would be needed to learn at ten times normal human pace, assuming the learner can learn that fast (which a ten:one time processor could); likewise a hundred times, etc. And all that would tell you is how quickly that mind can learn. But learning in and of itself doesn't make you smarter. That would require software or circuit redesign, which would require testing and debugging. Otherwise once you had all relevant knowledge available to any human software/circuit design team, you would simply be no smarter than them, and further learning would not help you (thus humans already have that knowledge level: that's why we work in teams to begin with), thus AI is not likely to much exceed us in that ability. The only edge it can exploit is speed of a serial design thought process, but even that runs up against the time and resource expense of testing and debugging anything it designed, and that is where physical infrastructure slows the rate of development, and massive continuing human funding is needed. Hence my probabilities above.</p>\n<p><strong>Q4: </strong><em>Is it important to figure out how to make AI provably friendly to us and our values (non-dangerous), before attempting to solve artificial general intelligence?</em><br /><em></em></p>\n<p style=\"padding-left: 30px;\"><strong>Richard Carrier: </strong>Yes. At the very least it is important to take the risks very seriously, and incorporate it as a concern within every project flow. I believe there should always be someone expert in the matter assigned to any AGI design team, who is monitoring everything being done and assessing its risks and ensuring safeguards are in place before implementation at each step. It already concerns me that this might not be a component of the management of Siri, and Siri achieving AGI is a low probability (but not vanishingly low; I'd say it could be as high as 1% in 10 years unless Siri's processing space is being deliberately limited so it cannot achieve a certain level of complexity, or in other ways its cognitive abilities being actively limited).</p>\n<p><em>Explanatory remark to Q4:</em><br /><br /><em>How much money is currently required to mitigate possible risks from AI (to be instrumental in maximizing your personal long-term goals, e.g. surviving this century), less/no more/little more/much more/vastly more?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Richard Carrier: </strong>Required is not very much. A single expert monitoring Siri who has real power to implement safeguards would be sufficient, so with salary and benefits and collateral overhead, that's no more than $250,000/year, for a company that has billions in liquid capital. (Because safeguards are not expensive, e.g. capping Siri's processing space costs nothing in practical terms; likewise writing her software to limit what she can actually do no matter how sentient she became, e.g. imagine an army of human hackers hacked Siri at the source and could run Siri by a million direct terminals, what could they do? Answering that question will evoke obvious safeguards to put on Siri's physical access and software; the most obvious is making it impossible for Siri to rewrite her own core software.)<br /> <br /> But what actually is being spent I don't know. I suspect \"a little more\" needs to be spent than is, only because I get the impression AI developers aren't taking this seriously, and yet the cost of monitoring is not that high.<br /> <br /> And yet you may notice all this is separate from the question of making AGI \"provably friendly\" which is what you asked about (and even that is not the same as \"provably safe\" since friendly AGI poses risks as well, as the Singularity Institute has been pointing out).<br /> <br /> This is because all we need do now is limit AGI's power at its nascence. Then we can explore how to make AGI friendly, and then provably friendly, and then provably safe. In fact I expect AGI will even help us with that. Once AGI exists, the need to invest heavily in making it safe will be universally obvious. Whereas before AGI exists there is little we can do to ascertain how to make it safe, since we don't have a working model to test. Think of trying to make a ship safe, without ever getting to build and test any vessel, nor having knowledge of any other vessels, and without knowing anything about the laws of buoyancy. There wouldn't be a lot you could do.<br /> <br /> Nevertheless it would be worth some investment to explore how much we can now know, particularly as it can be cross-purposed with understanding human moral decision making better, and thus need not be sold as \"just AI morality\" research. How much more should we spend on this now? Much more than we are. But only because I see that money benefiting us directly, in understanding how to make ordinary people better, and detect bad people, and so on, which is of great value wholly apart from its application to AGI. Having it double as research on how to design moral thought processes unrestrained by human brain structure would then benefit any future AGI development.</p>\n<p><strong>Q5:</strong> <em>Do possible risks from AI outweigh other possible existential risks, e.g. risks associated with the possibility of advanced nanotechnology?<br /><br /></em><em>Explanatory remark to Q5:</em><br /><br /><em>What existential risk (human extinction type event) is currently most likely to have the greatest negative impact on your personal long-term goals, under the condition that nothing is done to mitigate the risk? </em></p>\n<p style=\"padding-left: 30px;\"><strong>Richard Carrier: </strong>All existential risks are of such vastly low probability it would be beyond human comprehension to rank them, and utterly pointless to anyway. And even if I were to rank them, extinction by comet, asteroid or cosmological gamma ray burst vastly outranks any manmade cause. Even extinction by supervolcano vastly outranks any manmade cause. So I don't concern myself with this (except to call for more investment in earth impactor detection, and the monitoring of supervolcano risks).<br /> <br /> We should be concerned not with existential risks, but ordinary risks, e.g. small scale nuclear or biological terrorism, which won't kill the human race, and might not even take civilization into the Dark Ages, but can cause thousands or millions to die and have other bad repercussions. Because ordinary risks are billions upon billions of times more likely than extinction events, and as it happens, mitigating ordinary risks entails mitigating existential risks anyway (e.g. limiting the ability to go nuclear prevents small scale nuclear attacks just as well as nuclear annihilation events, in fact it makes the latter billions of times less likely than it already is).<br /> <br /> Thus when it comes to AI, as an existential risk it just isn't one (P --&gt; 0), but as a panoply of ordinary risks, it is (P --&gt; 1). And it doesn't matter how it ranks, it should get full attention anyway, like all definite risks do. It thus doesn't need to be ranked against other risks, as if terrorism were such a great risk we should invest nothing in earthquake safety, or vice versa.</p>\n<p><strong>Q6: </strong><em>What is the current level of awareness of possible risks from AI, relative to the ideal level?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Richard Carrier: </strong>Very low. Even among AI developers it seems.</p>\n<p><strong>Q7:</strong> <em>Can you think of any milestone such that if it were ever reached you would expect human\u2010level machine intelligence to be developed within five years thereafter?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Richard Carrier: </strong>There will not be \"a\" milestone like that, unless it is something wholly unexpected (like a massive breakthrough in circuit design that allows virtually infinite processing power on a desktop: which development would make P(AGI within five years) &gt; 33%). But wholly unexpected discoveries have a very low probability. Sticking only with what we already expect to occur, the five-year milestone for AGI will be AHI, artificial higher intelligence, e.g. a robot cat that behaved exactly like a real cat. Or a Watson who can actively learn on its own without being programmed with data (but still can only answer questions, and not plan or reason out problems). The CALO project is likely to develop an increasingly sophisticated Siri-like AI that won't be AGI but will gradually become more and more like AGI, so that there won't be any point where someone can say \"it will achieve AGI within 5 years.\" Rather it will achieve AGI gradually and unexpectedly, and people will even debate when or whether it had.<br /> <br /> Basically, I'd say once we have \"well-trained dog\" level AI, the probability of human-level AI becomes:<br /> <br /> P(&lt; 5 years) = 10%<br /> P(&lt; 10 years) = 25%<br /> P(&lt; 20 years) = 50%<br /> P(&lt; 40 years) = 90%</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dCTvFYNoLo6cQXvyK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 22, "extendedScore": null, "score": 8.143288546829713e-07, "legacy": true, "legacyId": "11441", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 16, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"_Click_here_to_see_a_list_of_all_interviews_\">[<a href=\"http://wiki.lesswrong.com/wiki/Interview_series_on_risks_from_AI\">Click here to see a list of all interviews</a>]</strong></p>\n<p>I am emailing experts in order to raise and estimate the academic awareness and perception of risks from AI.</p>\n<blockquote>\n<p><strong>Richard Carrier</strong> is a world-renowned author and speaker. As a professional historian, published philosopher, and prominent defender of the American freethought movement, Dr. Carrier has appeared across the country and on national television defending sound historical methods and the ethical worldview of secular naturalism. His books and articles have also received international attention. He holds a Ph.D. from Columbia University in ancient history, specializing in the intellectual history of Greece and Rome, particularly ancient philosophy, religion, and science, with emphasis on the origins of Christianity and the use and progress of science under the Roman empire. He is best known as the author of <strong><em><a class=\"spec\" href=\"http://astore.amazon.com/supportcarrier-20/detail/1420802933\">Sense and Goodness without God</a></em></strong>, <strong><em><a class=\"spec\" href=\"http://astore.amazon.com/supportcarrier-20/detail/0557044642\">Not the Impossible Faith</a></em></strong>, and <strong><em><a class=\"spec\" href=\"http://astore.amazon.com/supportcarrier-20/detail/1456588850\">Why I Am Not a Christian</a></em></strong>, and a major contributor to <strong><em><a class=\"spec\" href=\"http://astore.amazon.com/supportcarrier-20/detail/159102286X\">The Empty Tomb</a></em></strong>, <strong><em><a class=\"spec\" href=\"http://astore.amazon.com/supportcarrier-20/detail/1616141689\">The Christian Delusion</a></em></strong>, <strong><em><a class=\"spec\" href=\"http://astore.amazon.com/supportcarrier-20/detail/16161441300\">The End of Christianity</a></em></strong>, and <strong><em><a class=\"spec\" href=\"http://astore.amazon.com/supportcarrier-20/detail/1616141891\">Sources of the Jesus Tradition</a></em></strong>, as well as writer and editor-in-chief (now emeritus) for the <strong><a class=\"spec\" href=\"http://www.infidels.org/\">Secular Web</a></strong>, and for his copious work in history and philosophy online and in print. He is currently working on his next books, <strong><em>Proving History: Bayes's Theorem and the Quest for the Historical Jesus</em></strong>, <strong><em>On the Historicity of Jesus Christ</em></strong>, <strong><em>The Scientist in the Early Roman Empire</em></strong>, and <strong><em>Science Education in the Early Roman Empire</em></strong>. To learn more about Dr. Carrier and his work follow the links below.</p>\n</blockquote>\n<p><strong>Homepage:</strong> <a href=\"http://www.richardcarrier.info/about.html\">richardcarrier.info</a></p>\n<p><strong>Blog:</strong> <a href=\"http://freethoughtblogs.com/carrier/\">freethoughtblogs.com/carrier/</a> (old blog: <a href=\"http://richardcarrier.blogspot.com/\">richardcarrier.blogspot.com</a>)</p>\n<p><strong id=\"Selected_articles_\">Selected articles:</strong></p>\n<ul>\n<li><a href=\"http://freethoughtblogs.com/carrier/archives/80\">Bayes\u2019 Theorem: Lust for Glory!</a> (Blog post and <a href=\"http://youtu.be/HHIz-gR4xHo\">video talk</a>)</li>\n<li>\u201c<a href=\"http://www.richardcarrier.info/CarrierDec08.pdf\">Bayes\u2019 Theorem for Beginners: Formal Logic and Its Relevance to Historical Method</a>\u201d (Paper)</li>\n</ul>\n<h3 id=\"The_Interview_\"><strong>The Interview</strong>:</h3>\n<p style=\"padding-left: 30px;\"><strong>Richard Carrier:</strong> Note that I follow and support the work of The Singularity Institute on precisely this issue, which you are writing for (if you are a correspondent for Less Wrong). And I believe all AI developers should (e.g. <a href=\"http://en.wikipedia.org/wiki/CALO\">CALO</a>). So my answers won't be too surprising (below). But also keep in mind what I say (not just on \"singularity\" claims) at:<br> <br> <a href=\"http://richardcarrier.blogspot.com/2009/06/are-we-doomed.html\" target=\"_blank\">http://richardcarrier.blogspot.com/2009/06/are-we-doomed.html</a></p>\n<p><strong>Q1:</strong> <em>Assuming no global catastrophe halts progress, by what year would you assign a 10%/50%/90% chance of the development of roughly human-level machine intelligence?</em><br><em></em></p>\n<p style=\"padding-left: 30px;\"><strong>Richard Carrier: </strong>2020/2040/2080</p>\n<p><em>Explanatory remark to Q1:</em><br><br><em>P(human-level AI by (year) | no wars \u2227 no disasters \u2227 beneficially political and economic development) = 10%/50%/90%</em></p>\n<p><strong>Q2:</strong> <em>What probability do you assign to the possibility of human extinction</em><em> as a result of badly done AI?</em><br><em></em></p>\n<p style=\"padding-left: 30px;\"><strong>Richard Carrier: </strong>Here the relative probability is much higher that human extinction will result from benevolent AI, i.e. eventually Homo sapiens will be self-evidently obsolete and we will voluntarily transition to Homo cyberneticus. In other words, we will extinguish the Homo sapiens species ourselves, voluntarily. If you asked for a 10%/50%/90% deadline for this I would say 2500/3000/4000.<br> <br> However, perhaps you mean to ask regarding the extinction of all Homo, and their replacement with AI that did not originate as a human mind, i.e. the probability that some AI will kill us and just propagate itself.<br> <br> The answer to that is dependent on what you mean by \"badly done\" AI: (a) AI that has more power than we think we gave it, causing us problems, or (b) AI that has so much more power than we think we gave it that it can prevent our taking its power away.<br> <br> (a) is probably inevitable, or at any rate a high probability, and there will likely be deaths or other catastrophes, but like other tech failures (e.g. the Titanic, three mile island, hijacking jumbo jets and using them as guided missiles) we will prevail, and very quickly from a historical perspective (e.g. there won't be another 9/11 using airplanes as missiles; we only got jacked by that unforeseen failure once). We would do well to prevent as many problems as possible by being as smart as we can be about implementing AI, and not underestimating its ability to outsmart us, or to develop while we aren't looking (e.g. Siri could go sentient on its own, if no one is managing it closely to ensure that doesn't happen).<br> <br> (b) is very improbable because AI function is too dependent on human cooperation (e.g. power grid; physical servers that can be axed or bombed; an internet that can be shut down manually) and any move by AI to supplant that requirement would be too obvious and thus too easily stopped. In short, AI is infrastructure dependent, but it takes too much time and effort to build an infrastructure, and even more an infrastructure that is invulnerable to demolition. By the time AI has an independent infrastructure (e.g. its own robot population worldwide, its own power supplies, manufacturing plants, etc.) Homo sapiens will probably already be transitioning to Homo cyberneticus and there will be no effective difference between us and AI.<br> <br> However, given no deadline, it's likely there will be scenarios like: \"god\" AI's run sims in which digitized humans live, and any given god AI could decide to delete the sim and stop running it (and likewise all comparable AI shepherding scenarios). So then we'd be asking how likely is it that a god AI would ever do that, and more specifically, that all would (since there won't be just one sim run by one AI, but many, so one going rogue would not mean extinction of humanity).<br> <br> So setting aside AI that merely kills some people, and only focusing on total extinction of Homo sapiens, we have:<br> <br> P(voluntary human extinction by replacement | any AGI at all) = 90%+<br> <br> P(involuntary human extinction without replacement | badly done AGI type (a)) = &lt; 10^-20<br> <br> [and that's taking into account an infinite deadline, because the probability steeply declines with every year after first opportunity, e.g. AI that doesn't do it the first chance it gets is rapidly less likely to as time goes on, so the total probability has a limit even at infinite time, and I would put that limit somewhere as here assigned.]<br> <br> P(involuntary human extinction without replacement | badly done AGI type (b)) = .33 to .67<br> <br> However, P(badly done AGI type (b)) = &lt; 10^-20</p>\n<p><em>Explanatory remark to Q2:</em><br> <br><em>P(human extinction | badly done AI) = ?</em><br><br><em>(Where 'badly done' = AGI capable of self-modification that is <strong>not </strong>provably non-dangerous.)</em></p>\n<p><strong>Q3: </strong><em>What probability do you assign to the possibility of a human level AGI to self-modify its way up to massive superhuman intelligence within a matter of hours/days/&lt; 5 years? </em><br><em></em></p>\n<p style=\"padding-left: 30px;\"><strong>Richard Carrier: </strong>Depends on when it starts. For example, if we started a human-level AGI tomorrow, it's ability to revise itself would be hugely limited by our slow and expensive infrastructure (e.g. manufacturing the new circuits, building the mainframe extensions, supplying them with power, debugging the system). In that context, \"hours\" and \"days\" have P --&gt; 0, but 5 years has P = 33%+ if someone is funding the project, and likewise 10 years has P=67%+; and 25 years, P=90%+. However, suppose human-level AGI is first realized in fifty years when all these things can be done in a single room with relatively inexpensive automation and the power demands of any new system were not greater than are normally supplied to that room. Then P(days) = 90%+. And with massively more advanced tech, say such as we might have in 2500, then P(hours) = 90%+.<br> <br> However...</p>\n<p><em>Explanatory remark to Q3:</em><br><br><em>P(superhuman intelligence within hours | human-level AI running at human-level speed equipped with a 100 Gigabit Internet connection) = ?<br>P(superhuman intelligence within days | human-level AI running at human-level speed equipped with a 100 Gigabit Internet connection) = ?<br>P(superhuman intelligence within &lt; 5 years | human-level AI running at human-level speed equipped with a 100 Gigabit Internet connection) = ?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Richard Carrier: </strong>Perhaps you are confusing intelligence with knowledge. Internet connection can make no difference to the former (since an AGI will have no more control over the internet than human operators do). That can only expand a mind's knowledge. As to how quickly, it will depend more on the rate of processed seconds in the AGI itself, i.e. if it can simulate human thought only at the same pace as non-AI, then it will not be able to learn any faster than a regular person, no matter what kind of internet connection it has. But if the AGI can process ten seconds time in one second of non-AI time, then it can learn ten times as fast, up to the limit of data access (and that is where internet connection speed will matter). That is a calculation I can't do. A computer science expert would have to be consulted to calculate reasonable estimates of what connection speed would be needed to learn at ten times normal human pace, assuming the learner can learn that fast (which a ten:one time processor could); likewise a hundred times, etc. And all that would tell you is how quickly that mind can learn. But learning in and of itself doesn't make you smarter. That would require software or circuit redesign, which would require testing and debugging. Otherwise once you had all relevant knowledge available to any human software/circuit design team, you would simply be no smarter than them, and further learning would not help you (thus humans already have that knowledge level: that's why we work in teams to begin with), thus AI is not likely to much exceed us in that ability. The only edge it can exploit is speed of a serial design thought process, but even that runs up against the time and resource expense of testing and debugging anything it designed, and that is where physical infrastructure slows the rate of development, and massive continuing human funding is needed. Hence my probabilities above.</p>\n<p><strong>Q4: </strong><em>Is it important to figure out how to make AI provably friendly to us and our values (non-dangerous), before attempting to solve artificial general intelligence?</em><br><em></em></p>\n<p style=\"padding-left: 30px;\"><strong>Richard Carrier: </strong>Yes. At the very least it is important to take the risks very seriously, and incorporate it as a concern within every project flow. I believe there should always be someone expert in the matter assigned to any AGI design team, who is monitoring everything being done and assessing its risks and ensuring safeguards are in place before implementation at each step. It already concerns me that this might not be a component of the management of Siri, and Siri achieving AGI is a low probability (but not vanishingly low; I'd say it could be as high as 1% in 10 years unless Siri's processing space is being deliberately limited so it cannot achieve a certain level of complexity, or in other ways its cognitive abilities being actively limited).</p>\n<p><em>Explanatory remark to Q4:</em><br><br><em>How much money is currently required to mitigate possible risks from AI (to be instrumental in maximizing your personal long-term goals, e.g. surviving this century), less/no more/little more/much more/vastly more?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Richard Carrier: </strong>Required is not very much. A single expert monitoring Siri who has real power to implement safeguards would be sufficient, so with salary and benefits and collateral overhead, that's no more than $250,000/year, for a company that has billions in liquid capital. (Because safeguards are not expensive, e.g. capping Siri's processing space costs nothing in practical terms; likewise writing her software to limit what she can actually do no matter how sentient she became, e.g. imagine an army of human hackers hacked Siri at the source and could run Siri by a million direct terminals, what could they do? Answering that question will evoke obvious safeguards to put on Siri's physical access and software; the most obvious is making it impossible for Siri to rewrite her own core software.)<br> <br> But what actually is being spent I don't know. I suspect \"a little more\" needs to be spent than is, only because I get the impression AI developers aren't taking this seriously, and yet the cost of monitoring is not that high.<br> <br> And yet you may notice all this is separate from the question of making AGI \"provably friendly\" which is what you asked about (and even that is not the same as \"provably safe\" since friendly AGI poses risks as well, as the Singularity Institute has been pointing out).<br> <br> This is because all we need do now is limit AGI's power at its nascence. Then we can explore how to make AGI friendly, and then provably friendly, and then provably safe. In fact I expect AGI will even help us with that. Once AGI exists, the need to invest heavily in making it safe will be universally obvious. Whereas before AGI exists there is little we can do to ascertain how to make it safe, since we don't have a working model to test. Think of trying to make a ship safe, without ever getting to build and test any vessel, nor having knowledge of any other vessels, and without knowing anything about the laws of buoyancy. There wouldn't be a lot you could do.<br> <br> Nevertheless it would be worth some investment to explore how much we can now know, particularly as it can be cross-purposed with understanding human moral decision making better, and thus need not be sold as \"just AI morality\" research. How much more should we spend on this now? Much more than we are. But only because I see that money benefiting us directly, in understanding how to make ordinary people better, and detect bad people, and so on, which is of great value wholly apart from its application to AGI. Having it double as research on how to design moral thought processes unrestrained by human brain structure would then benefit any future AGI development.</p>\n<p><strong>Q5:</strong> <em>Do possible risks from AI outweigh other possible existential risks, e.g. risks associated with the possibility of advanced nanotechnology?<br><br></em><em>Explanatory remark to Q5:</em><br><br><em>What existential risk (human extinction type event) is currently most likely to have the greatest negative impact on your personal long-term goals, under the condition that nothing is done to mitigate the risk? </em></p>\n<p style=\"padding-left: 30px;\"><strong>Richard Carrier: </strong>All existential risks are of such vastly low probability it would be beyond human comprehension to rank them, and utterly pointless to anyway. And even if I were to rank them, extinction by comet, asteroid or cosmological gamma ray burst vastly outranks any manmade cause. Even extinction by supervolcano vastly outranks any manmade cause. So I don't concern myself with this (except to call for more investment in earth impactor detection, and the monitoring of supervolcano risks).<br> <br> We should be concerned not with existential risks, but ordinary risks, e.g. small scale nuclear or biological terrorism, which won't kill the human race, and might not even take civilization into the Dark Ages, but can cause thousands or millions to die and have other bad repercussions. Because ordinary risks are billions upon billions of times more likely than extinction events, and as it happens, mitigating ordinary risks entails mitigating existential risks anyway (e.g. limiting the ability to go nuclear prevents small scale nuclear attacks just as well as nuclear annihilation events, in fact it makes the latter billions of times less likely than it already is).<br> <br> Thus when it comes to AI, as an existential risk it just isn't one (P --&gt; 0), but as a panoply of ordinary risks, it is (P --&gt; 1). And it doesn't matter how it ranks, it should get full attention anyway, like all definite risks do. It thus doesn't need to be ranked against other risks, as if terrorism were such a great risk we should invest nothing in earthquake safety, or vice versa.</p>\n<p><strong>Q6: </strong><em>What is the current level of awareness of possible risks from AI, relative to the ideal level?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Richard Carrier: </strong>Very low. Even among AI developers it seems.</p>\n<p><strong>Q7:</strong> <em>Can you think of any milestone such that if it were ever reached you would expect human\u2010level machine intelligence to be developed within five years thereafter?</em></p>\n<p style=\"padding-left: 30px;\"><strong>Richard Carrier: </strong>There will not be \"a\" milestone like that, unless it is something wholly unexpected (like a massive breakthrough in circuit design that allows virtually infinite processing power on a desktop: which development would make P(AGI within five years) &gt; 33%). But wholly unexpected discoveries have a very low probability. Sticking only with what we already expect to occur, the five-year milestone for AGI will be AHI, artificial higher intelligence, e.g. a robot cat that behaved exactly like a real cat. Or a Watson who can actively learn on its own without being programmed with data (but still can only answer questions, and not plan or reason out problems). The CALO project is likely to develop an increasingly sophisticated Siri-like AI that won't be AGI but will gradually become more and more like AGI, so that there won't be any point where someone can say \"it will achieve AGI within 5 years.\" Rather it will achieve AGI gradually and unexpectedly, and people will even debate when or whether it had.<br> <br> Basically, I'd say once we have \"well-trained dog\" level AI, the probability of human-level AI becomes:<br> <br> P(&lt; 5 years) = 10%<br> P(&lt; 10 years) = 25%<br> P(&lt; 20 years) = 50%<br> P(&lt; 40 years) = 90%</p>", "sections": [{"title": "[Click here to see a list of all interviews]", "anchor": "_Click_here_to_see_a_list_of_all_interviews_", "level": 2}, {"title": "Selected articles:", "anchor": "Selected_articles_", "level": 2}, {"title": "The Interview:", "anchor": "The_Interview_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "22 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-13T10:57:33.916Z", "modifiedAt": null, "url": null, "title": "Open Problems Related to the Singularity (draft 1)", "slug": "open-problems-related-to-the-singularity-draft-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:04.891Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/p9DdaSFzJHW9At3JM/open-problems-related-to-the-singularity-draft-1", "pageUrlRelative": "/posts/p9DdaSFzJHW9At3JM/open-problems-related-to-the-singularity-draft-1", "linkUrl": "https://www.lesswrong.com/posts/p9DdaSFzJHW9At3JM/open-problems-related-to-the-singularity-draft-1", "postedAtFormatted": "Tuesday, December 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Open%20Problems%20Related%20to%20the%20Singularity%20(draft%201)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOpen%20Problems%20Related%20to%20the%20Singularity%20(draft%201)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp9DdaSFzJHW9At3JM%2Fopen-problems-related-to-the-singularity-draft-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Open%20Problems%20Related%20to%20the%20Singularity%20(draft%201)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp9DdaSFzJHW9At3JM%2Fopen-problems-related-to-the-singularity-draft-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fp9DdaSFzJHW9At3JM%2Fopen-problems-related-to-the-singularity-draft-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1332, "htmlBody": "<p>\"I've come to agree that <a href=\"http://facingthesingularity.com/\">navigating the Singularity</a>&nbsp;wisely is the most important thing humanity can do. I'm a researcher and I want to help. What do I work on?\"</p>\n<p>The Singularity Institute gets this question regularly, and we haven't published a clear answer to it anywhere. This is because it's an extremely difficult and complicated question. A large expenditure of limited resources is required to make a serious attempt at answering it. Nevertheless, it's an <em>important</em>&nbsp;question, so we'd like to work toward an answer.<a id=\"more\"></a></p>\n<p>A few preliminaries:</p>\n<ul>\n<li><strong>Defining each problem is part of the problem</strong>. As <a href=\"http://www.amazon.com/Adaptive-Control-Processes-Guided-Tour/dp/B000X9B8N6/\">Bellman (1961)</a> said, \"the very construction of a precise mathematical statement of a verbal problem is itself a problem of major difficulty.\"&nbsp;Many of the problems related to navigating the Singularity have not yet been stated with mathematical precision, and the need for a precise statement of the problem is <em>part</em> of these open problems. But there is reason for optimism. Many times, particular heroes have managed to formalize a previously fuzzy and mysterious concept: see Kolmogorov on complexity and simplicity (<a href=\"http://www.tandfonline.com.proxy.lib.siu.edu/doi/abs/10.1080/00207166808803030\">Kolmogorov 1965</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Grunwald-Vitanyi-Kolmogorov-complexity-and-information-theory.pdf\">Grunwald &amp; Vitanyi 2003</a>;&nbsp;<a href=\"http://www.amazon.com/Introduction-Kolmogorov-Complexity-Applications-Computer/dp/0387339981/\">Li &amp; Vit&aacute;nyi 2008</a>), Solomonoff on induction (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Solomonoff-A-formal-theory-of-inductive-inference-part-1.pdf\">Solomonoff 1964a</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Solomonoff-A-formal-theory-of-inductive-inference-part-2.pdf\">1964b</a>; <a href=\"http://www.hutter1.net/publ/uiphil.pdf\">Rathmanner &amp; Hutter 2011</a>), Von Neumann and Morgenstern on rationality (<a href=\"http://en.wikipedia.org/wiki/Theory_of_Games_and_Economic_Behavior\">Von Neumann &amp; Morgenstern 1947</a>; <a href=\"http://www.amazon.com/Foundations-Rational-Choice-Under-Risk/dp/0198774427/\">Anand 1995</a>), and Shannon on information (<a href=\"http://makseq.com/materials/lib/Articles-Books/General/InformationTheory/p3-shannon.pdf\">Shannon 1948</a>; <a href=\"http://www.amazon.com/Information-Measures-Description-Engineering-Communication/dp/354040855X/\">Arndt 2004</a>).</li>\n<li><strong>The nature of the problem space is unclear</strong>. Which problems will biological humans&nbsp;need to solve, and which problems can a successful FAI solve on its own (perhaps with the help of human uploads it creates to solve the remaining open problems)? Are Friendly AI (<a href=\"http://intelligence.org/upload/CFAI.html\">Yudkowsky 2001</a>) and CEV (<a href=\"http://intelligence.org/upload/CEV.html\">Yudkowsky 2004</a>) coherent ideas, given the <a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/\">confused</a> <a href=\"/r/discussion/lw/8oy/why_study_the_cognitive_science_of_concepts/\">nature</a> of human \"values\"? Should we aim instead for a \"maxipok\" solution (<a href=\"http://www.existential-risk.org/concept.html\">Bostrom 2011</a>) that maximizes the chance of an \"ok\" outcome, something like Oracle AI (<a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Armstrong et al. 2011</a>)? Which problems are we unable to state with precision because they are irreparably confused, and which problems are we unable to state due to a lack of insight?</li>\n<li><strong>Our research priorities are unclear</strong>. There are a limited number of capable researchers who will work on these problems. Which are the most important problems they should be working on, if they are capable of doing so? Should we focus on \"control problem\" theory (FAI, AI-boxing, oracle AI, etc.), or on strategic considerations (<a href=\"http://en.wikipedia.org/wiki/Differential_technological_development\">differential technological development</a>, <a href=\"/lw/76x/is_rationality_teachable/\">methods</a> for <a href=\"/lw/1e/raising_the_sanity_waterline/\">raising the sanity waterline</a>, methods for bringing more funding to existential risk reduction and growing the community of x-risk reducers, reducing the odds of <a href=\"http://intelligence.org/armscontrolintelligenceexplosions.pdf\">AI arms races</a>, etc.)? Is AI more urgent than other existential risks, especially synthetic biology?</li>\n<li><strong>Our intervention priorities are unclear</strong>. Is research the most urgent thing to be done, or should we focus on growing the community of x-risk reducers, raising the sanity waterline, bringing in more funding for x-risk reduction, etc.? Can we make better research progress in the next 10 years if we work to improve sanity and funding for 7 years and <em>then</em>&nbsp;have the resources to grab more and better researchers, or can we make better research progress by focusing on research now?</li>\n</ul>\n<p>Next, a division of labor into \"problem categories.\" There are many ways to categorize the open problems; some of them are probably more useful than the one I've chosen below.</p>\n<ul>\n<li><strong>Safe AI Architectures.</strong>&nbsp;This may include architectures for securely confined or \"boxed\" AIs (<a href=\"http://www.cs.umd.edu/~jkatz/TEACHING/comp_sec_F04/downloads/confinement.pdf\">Lampson 1973</a>), including Oracle AIs, and also AI architectures that could \"take\" a safe set of goals (resulting in Friendly AI).</li>\n<li><strong>Safe AI Goals</strong>. What could it mean to have a Friendly AI with \"good\" goals?</li>\n<li><strong>Strategy</strong>. A huge space of problems. How do we predict the future and make recommendations for differential technological development? Do we aim for Friendly AI or maxipok solutions or both? Do we focus on growing support now, or do we focus on research? How should we interact with the public and with governments?</li>\n</ul>\n<p>The list of open problems below is <em>very</em>&nbsp;preliminary.&nbsp;I'm sure there are many problems I've forgotten, and many problems I'm unaware of. Probably <em>all</em>&nbsp;of the problems are stated relatively poorly: this is only a \"first step\" document. Certainly, all listed problems are described at an extremely \"high\" level, very far away (so far) from mathematical precision, and can be broken down into several and often <em>dozens</em> of subproblems.</p>\n<h3>Safe AI Architectures</h3>\n<ul>\n<li>Is rationally-shaped (<a href=\"http://selfawaresystems.com/2011/10/07/rationally-shaped-artificial-intelligence/\">Omohundro 2011</a>) \"transparent\" AI the only safe AI architecture? Is it the only one that can take safe goals?</li>\n<li>How can we develop a reflective decision theory: one that doesn't go into infinite loops or stumble over <a href=\"http://yudkowsky.net/assets/44/LobsTheorem.pdf?1323322713\">Lob's Theorem</a>?</li>\n<li>How can we develop a timeless decision theory (<a href=\"http://intelligence.org/upload/TDT-v01o.pdf\">Yudkowsky 2010</a>) with the bugs worked out (e.g. blackmailing, <a href=\"/lw/15z/ingredients_of_timeless_decision_theory/1217\">5-and-10 problem</a>)</li>\n<li>How can we modify a transparent AI architecture like AIXI (<a href=\"http://www.amazon.com/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3642060528/\">Hutter 2004</a>) to have a utility function over the external world (<a href=\"http://www.danieldewey.net/learning-what-to-value.pdf\">Dewey 2011</a>)? Does this keep a superintelligence from wireheading or shutting itself off?</li>\n<li>How can an AIXI-like agent keep a stable utility function through ontological shifts (<a href=\"http://arxiv.org/pdf/1105.3821v1\">De Blanc 2011</a>)?</li>\n<li>How would an ideal agent with infinite computing power choose an ideal prior? (A guess: we'd need an anthropic, non-Cartesian, higher-order-logic version of Solomonoff induction.) How can this be process be approximated computably and tractably?</li>\n<li>What is the ideal theory of how to handle logical uncertainty?</li>\n<li>What is the ideal computable approximation of perfect Bayesianism?</li>\n<li>Do we need to solve anthropics, or is it perhaps a confused issue resulting from underspecified problems (<a href=\"http://arxiv.org/abs/1110.6437\">Armstrong 2011</a>)?</li>\n<li>Can we develop a safely confined (\"boxed\") AI? Can we develop Oracle AI?</li>\n<li>What convergent instrumental goals can we expect from superintelligent machines (<a href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\">Omohundro 2008</a>)?</li>\n</ul>\n<h3>Safe AI Goals</h3>\n<ul>\n<li>Can \"safe\" AI goals only be derived from contingent \"desires\" and \"goals,\" or might value \"fall out of\" game theory + decision theory, like in a more robust form than what <a href=\"http://www.amazon.com/Good-Real-Demystifying-Paradoxes-Bradford/dp/0262042339/\">Drescher (2006)</a> attempts?</li>\n<li>Are CEV and Friendly AI coherent ideas?</li>\n<li>How do we construe a utility function from what humans \"want\"? How should human values be extrapolated?</li>\n<li>What extrapolate the values of humans alone? What counts as a human? Do we need to scan the values of all humans? Do values converge if extrapolated? Under which extrapolation algorithms?</li>\n<li>How do we assign measure to beings in an infinite universe (<a href=\"http://www.anthropic-principle.com/preprints/knobe.pdf\">Knobe 2006</a>; <a href=\"http://www.nickbostrom.com/ethics/infinite.pdf\">Bostrom 2009</a>)? What can we make of other possible laws of physics (<a href=\"http://arxiv.org/pdf/0905.1283\">Tegmark 2005</a>)?</li>\n<li>Which kinds of minds/beings should we assign value to (<a href=\"http://www.nickbostrom.com/papers/experience.pdf\">Bostrom 2006</a>)?</li>\n<li>How should we deal with normative uncertainty (<a href=\"http://www.fil.lu.se/files/conference117.pdf\">Sepielli 2009</a>; <a href=\"http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html\">Bostrom 2009</a>)?</li>\n<li>Is it possible to program an AI to do what is \"morally right\" rather than give it an extrapolation of human goals?</li>\n</ul>\n<h3>Strategy</h3>\n<ul>\n<li>What methods can we use to predict technological development (<a href=\"http://tuvalu.santafe.edu/~bn/workingpapers/NagyFarmerTrancikBui.pdf\">Nagy 2010</a>)?</li>\n<li>Which kinds of differential technological development should we encourage, and how?</li>\n<li>Which open problems are safe to discuss, and which are potentially highly dangerous, like the man-made super-flu that \"<a href=\"http://rt.com/news/bird-flu-killer-strain-119/?utm_medium=referral&amp;utm_source=pulsenews\">could kill half of humanity</a>\"?</li>\n<li>What can we do to reduce the risk of an AI arms race?</li>\n<li>What can we do to raise the sanity waterline, and how much will this help?</li>\n<li>What can we do to attract more funding, support, and research to x-risk reduction and to specific sub-problems of successful Singularity navigation?</li>\n<li>Which interventions should we prioritize?</li>\n<li>How should x-risk reducers and AI risk reducers interact with governments and corporations?</li>\n<li>How can optimal philanthropists get the most x-risk reduction for their philanthropic buck?</li>\n<li>How does AI risk compare to other existential risks?</li>\n<li>How can we develop microeconomic models of WBEs and self-improving systems? Can this help us predict takeoff speed and the likelihood of monopolar (singleton) vs. multipolar outcomes?</li>\n</ul>\n<p><br /> My thanks for some notes written by Eliezer Yudkowsky, Carl Shulman, and Nick Bostrom, from which I've drawn.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Pa2SdZsLFmqhs42Do": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "p9DdaSFzJHW9At3JM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 42, "baseScore": 53, "extendedScore": null, "score": 0.0005863017106431277, "legacy": true, "legacyId": "11440", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 39, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>\"I've come to agree that <a href=\"http://facingthesingularity.com/\">navigating the Singularity</a>&nbsp;wisely is the most important thing humanity can do. I'm a researcher and I want to help. What do I work on?\"</p>\n<p>The Singularity Institute gets this question regularly, and we haven't published a clear answer to it anywhere. This is because it's an extremely difficult and complicated question. A large expenditure of limited resources is required to make a serious attempt at answering it. Nevertheless, it's an <em>important</em>&nbsp;question, so we'd like to work toward an answer.<a id=\"more\"></a></p>\n<p>A few preliminaries:</p>\n<ul>\n<li><strong>Defining each problem is part of the problem</strong>. As <a href=\"http://www.amazon.com/Adaptive-Control-Processes-Guided-Tour/dp/B000X9B8N6/\">Bellman (1961)</a> said, \"the very construction of a precise mathematical statement of a verbal problem is itself a problem of major difficulty.\"&nbsp;Many of the problems related to navigating the Singularity have not yet been stated with mathematical precision, and the need for a precise statement of the problem is <em>part</em> of these open problems. But there is reason for optimism. Many times, particular heroes have managed to formalize a previously fuzzy and mysterious concept: see Kolmogorov on complexity and simplicity (<a href=\"http://www.tandfonline.com.proxy.lib.siu.edu/doi/abs/10.1080/00207166808803030\">Kolmogorov 1965</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Grunwald-Vitanyi-Kolmogorov-complexity-and-information-theory.pdf\">Grunwald &amp; Vitanyi 2003</a>;&nbsp;<a href=\"http://www.amazon.com/Introduction-Kolmogorov-Complexity-Applications-Computer/dp/0387339981/\">Li &amp; Vit\u00e1nyi 2008</a>), Solomonoff on induction (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Solomonoff-A-formal-theory-of-inductive-inference-part-1.pdf\">Solomonoff 1964a</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Solomonoff-A-formal-theory-of-inductive-inference-part-2.pdf\">1964b</a>; <a href=\"http://www.hutter1.net/publ/uiphil.pdf\">Rathmanner &amp; Hutter 2011</a>), Von Neumann and Morgenstern on rationality (<a href=\"http://en.wikipedia.org/wiki/Theory_of_Games_and_Economic_Behavior\">Von Neumann &amp; Morgenstern 1947</a>; <a href=\"http://www.amazon.com/Foundations-Rational-Choice-Under-Risk/dp/0198774427/\">Anand 1995</a>), and Shannon on information (<a href=\"http://makseq.com/materials/lib/Articles-Books/General/InformationTheory/p3-shannon.pdf\">Shannon 1948</a>; <a href=\"http://www.amazon.com/Information-Measures-Description-Engineering-Communication/dp/354040855X/\">Arndt 2004</a>).</li>\n<li><strong>The nature of the problem space is unclear</strong>. Which problems will biological humans&nbsp;need to solve, and which problems can a successful FAI solve on its own (perhaps with the help of human uploads it creates to solve the remaining open problems)? Are Friendly AI (<a href=\"http://intelligence.org/upload/CFAI.html\">Yudkowsky 2001</a>) and CEV (<a href=\"http://intelligence.org/upload/CEV.html\">Yudkowsky 2004</a>) coherent ideas, given the <a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/\">confused</a> <a href=\"/r/discussion/lw/8oy/why_study_the_cognitive_science_of_concepts/\">nature</a> of human \"values\"? Should we aim instead for a \"maxipok\" solution (<a href=\"http://www.existential-risk.org/concept.html\">Bostrom 2011</a>) that maximizes the chance of an \"ok\" outcome, something like Oracle AI (<a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Armstrong et al. 2011</a>)? Which problems are we unable to state with precision because they are irreparably confused, and which problems are we unable to state due to a lack of insight?</li>\n<li><strong>Our research priorities are unclear</strong>. There are a limited number of capable researchers who will work on these problems. Which are the most important problems they should be working on, if they are capable of doing so? Should we focus on \"control problem\" theory (FAI, AI-boxing, oracle AI, etc.), or on strategic considerations (<a href=\"http://en.wikipedia.org/wiki/Differential_technological_development\">differential technological development</a>, <a href=\"/lw/76x/is_rationality_teachable/\">methods</a> for <a href=\"/lw/1e/raising_the_sanity_waterline/\">raising the sanity waterline</a>, methods for bringing more funding to existential risk reduction and growing the community of x-risk reducers, reducing the odds of <a href=\"http://intelligence.org/armscontrolintelligenceexplosions.pdf\">AI arms races</a>, etc.)? Is AI more urgent than other existential risks, especially synthetic biology?</li>\n<li><strong>Our intervention priorities are unclear</strong>. Is research the most urgent thing to be done, or should we focus on growing the community of x-risk reducers, raising the sanity waterline, bringing in more funding for x-risk reduction, etc.? Can we make better research progress in the next 10 years if we work to improve sanity and funding for 7 years and <em>then</em>&nbsp;have the resources to grab more and better researchers, or can we make better research progress by focusing on research now?</li>\n</ul>\n<p>Next, a division of labor into \"problem categories.\" There are many ways to categorize the open problems; some of them are probably more useful than the one I've chosen below.</p>\n<ul>\n<li><strong>Safe AI Architectures.</strong>&nbsp;This may include architectures for securely confined or \"boxed\" AIs (<a href=\"http://www.cs.umd.edu/~jkatz/TEACHING/comp_sec_F04/downloads/confinement.pdf\">Lampson 1973</a>), including Oracle AIs, and also AI architectures that could \"take\" a safe set of goals (resulting in Friendly AI).</li>\n<li><strong>Safe AI Goals</strong>. What could it mean to have a Friendly AI with \"good\" goals?</li>\n<li><strong>Strategy</strong>. A huge space of problems. How do we predict the future and make recommendations for differential technological development? Do we aim for Friendly AI or maxipok solutions or both? Do we focus on growing support now, or do we focus on research? How should we interact with the public and with governments?</li>\n</ul>\n<p>The list of open problems below is <em>very</em>&nbsp;preliminary.&nbsp;I'm sure there are many problems I've forgotten, and many problems I'm unaware of. Probably <em>all</em>&nbsp;of the problems are stated relatively poorly: this is only a \"first step\" document. Certainly, all listed problems are described at an extremely \"high\" level, very far away (so far) from mathematical precision, and can be broken down into several and often <em>dozens</em> of subproblems.</p>\n<h3 id=\"Safe_AI_Architectures\">Safe AI Architectures</h3>\n<ul>\n<li>Is rationally-shaped (<a href=\"http://selfawaresystems.com/2011/10/07/rationally-shaped-artificial-intelligence/\">Omohundro 2011</a>) \"transparent\" AI the only safe AI architecture? Is it the only one that can take safe goals?</li>\n<li>How can we develop a reflective decision theory: one that doesn't go into infinite loops or stumble over <a href=\"http://yudkowsky.net/assets/44/LobsTheorem.pdf?1323322713\">Lob's Theorem</a>?</li>\n<li>How can we develop a timeless decision theory (<a href=\"http://intelligence.org/upload/TDT-v01o.pdf\">Yudkowsky 2010</a>) with the bugs worked out (e.g. blackmailing, <a href=\"/lw/15z/ingredients_of_timeless_decision_theory/1217\">5-and-10 problem</a>)</li>\n<li>How can we modify a transparent AI architecture like AIXI (<a href=\"http://www.amazon.com/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3642060528/\">Hutter 2004</a>) to have a utility function over the external world (<a href=\"http://www.danieldewey.net/learning-what-to-value.pdf\">Dewey 2011</a>)? Does this keep a superintelligence from wireheading or shutting itself off?</li>\n<li>How can an AIXI-like agent keep a stable utility function through ontological shifts (<a href=\"http://arxiv.org/pdf/1105.3821v1\">De Blanc 2011</a>)?</li>\n<li>How would an ideal agent with infinite computing power choose an ideal prior? (A guess: we'd need an anthropic, non-Cartesian, higher-order-logic version of Solomonoff induction.) How can this be process be approximated computably and tractably?</li>\n<li>What is the ideal theory of how to handle logical uncertainty?</li>\n<li>What is the ideal computable approximation of perfect Bayesianism?</li>\n<li>Do we need to solve anthropics, or is it perhaps a confused issue resulting from underspecified problems (<a href=\"http://arxiv.org/abs/1110.6437\">Armstrong 2011</a>)?</li>\n<li>Can we develop a safely confined (\"boxed\") AI? Can we develop Oracle AI?</li>\n<li>What convergent instrumental goals can we expect from superintelligent machines (<a href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\">Omohundro 2008</a>)?</li>\n</ul>\n<h3 id=\"Safe_AI_Goals\">Safe AI Goals</h3>\n<ul>\n<li>Can \"safe\" AI goals only be derived from contingent \"desires\" and \"goals,\" or might value \"fall out of\" game theory + decision theory, like in a more robust form than what <a href=\"http://www.amazon.com/Good-Real-Demystifying-Paradoxes-Bradford/dp/0262042339/\">Drescher (2006)</a> attempts?</li>\n<li>Are CEV and Friendly AI coherent ideas?</li>\n<li>How do we construe a utility function from what humans \"want\"? How should human values be extrapolated?</li>\n<li>What extrapolate the values of humans alone? What counts as a human? Do we need to scan the values of all humans? Do values converge if extrapolated? Under which extrapolation algorithms?</li>\n<li>How do we assign measure to beings in an infinite universe (<a href=\"http://www.anthropic-principle.com/preprints/knobe.pdf\">Knobe 2006</a>; <a href=\"http://www.nickbostrom.com/ethics/infinite.pdf\">Bostrom 2009</a>)? What can we make of other possible laws of physics (<a href=\"http://arxiv.org/pdf/0905.1283\">Tegmark 2005</a>)?</li>\n<li>Which kinds of minds/beings should we assign value to (<a href=\"http://www.nickbostrom.com/papers/experience.pdf\">Bostrom 2006</a>)?</li>\n<li>How should we deal with normative uncertainty (<a href=\"http://www.fil.lu.se/files/conference117.pdf\">Sepielli 2009</a>; <a href=\"http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html\">Bostrom 2009</a>)?</li>\n<li>Is it possible to program an AI to do what is \"morally right\" rather than give it an extrapolation of human goals?</li>\n</ul>\n<h3 id=\"Strategy\">Strategy</h3>\n<ul>\n<li>What methods can we use to predict technological development (<a href=\"http://tuvalu.santafe.edu/~bn/workingpapers/NagyFarmerTrancikBui.pdf\">Nagy 2010</a>)?</li>\n<li>Which kinds of differential technological development should we encourage, and how?</li>\n<li>Which open problems are safe to discuss, and which are potentially highly dangerous, like the man-made super-flu that \"<a href=\"http://rt.com/news/bird-flu-killer-strain-119/?utm_medium=referral&amp;utm_source=pulsenews\">could kill half of humanity</a>\"?</li>\n<li>What can we do to reduce the risk of an AI arms race?</li>\n<li>What can we do to raise the sanity waterline, and how much will this help?</li>\n<li>What can we do to attract more funding, support, and research to x-risk reduction and to specific sub-problems of successful Singularity navigation?</li>\n<li>Which interventions should we prioritize?</li>\n<li>How should x-risk reducers and AI risk reducers interact with governments and corporations?</li>\n<li>How can optimal philanthropists get the most x-risk reduction for their philanthropic buck?</li>\n<li>How does AI risk compare to other existential risks?</li>\n<li>How can we develop microeconomic models of WBEs and self-improving systems? Can this help us predict takeoff speed and the likelihood of monopolar (singleton) vs. multipolar outcomes?</li>\n</ul>\n<p><br> My thanks for some notes written by Eliezer Yudkowsky, Carl Shulman, and Nick Bostrom, from which I've drawn.</p>", "sections": [{"title": "Safe AI Architectures", "anchor": "Safe_AI_Architectures", "level": 1}, {"title": "Safe AI Goals", "anchor": "Safe_AI_Goals", "level": 1}, {"title": "Strategy", "anchor": "Strategy", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "42 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 42, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hN2aRnu798yas5b2k", "aMfa3Lf9CN86qF64k", "H2zKAfiSJR6WJQ8pn", "XqmjdBKa4ZaXJtNmf"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-13T15:27:12.733Z", "modifiedAt": null, "url": null, "title": "Examples of the Mind Projection Fallacy?", "slug": "examples-of-the-mind-projection-fallacy", "viewCount": null, "lastCommentedAt": "2020-02-02T16:41:46.995Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fiddlemath", "createdAt": "2010-04-19T03:50:34.425Z", "isAdmin": false, "displayName": "fiddlemath"}, "userId": "5F5aTS6F8642KxHLK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PM9D3iLFZWvkT6mMX/examples-of-the-mind-projection-fallacy", "pageUrlRelative": "/posts/PM9D3iLFZWvkT6mMX/examples-of-the-mind-projection-fallacy", "linkUrl": "https://www.lesswrong.com/posts/PM9D3iLFZWvkT6mMX/examples-of-the-mind-projection-fallacy", "postedAtFormatted": "Tuesday, December 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Examples%20of%20the%20Mind%20Projection%20Fallacy%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AExamples%20of%20the%20Mind%20Projection%20Fallacy%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPM9D3iLFZWvkT6mMX%2Fexamples-of-the-mind-projection-fallacy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Examples%20of%20the%20Mind%20Projection%20Fallacy%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPM9D3iLFZWvkT6mMX%2Fexamples-of-the-mind-projection-fallacy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPM9D3iLFZWvkT6mMX%2Fexamples-of-the-mind-projection-fallacy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 209, "htmlBody": "<p>I suspect that achieving a clear mental picture of the sheer depth and breadth of the mind projection fallacy is a powerful mental tool. It's hard for me to state this in clearer terms, though, because I don't have a wide collection of good examples of the mind projection fallacy.</p>\n<p>In a discussion yesterday, we <em>all</em>&nbsp;had trouble finding actual example of the mind projection fallacy. Overall, we had essentially two examples:</p>\n<p>\n<ul>\n<li>Taste. People frequently confuse \"I like this\" and \"this is good.\" (This really subsumes the <a href=\"/lw/oi/mind_projection_fallacy/\">attractiveness example</a>.)</li>\n<li>Probability. This seems like a pretty good just-so-story for where frequentist probability comes from, as opposed to Bayesian probability.</li>\n</ul>\n</p>\n<p>Searching for \"mind projection fallacy\" on Less Wrong, I also see:</p>\n<p>\n<ul>\n<li>Thinking that purpose is an inherent property of something, instead of it having been placed there by someone for some reason. (<a href=\"/lw/te/three_fallacies_of_teleology/\">here</a>)</li>\n<li>Mulling or arguing over definitions to solve object-level problems. (actually, most the&nbsp;<a href=\"/lw/od/37_ways_that_words_can_be_wrong/\">ways words can be wrong</a>&nbsp;sequence)</li>\n</ul>\nImagine I'm trying to explain the mind projection fallacy to someone, and giving a handful of sharp, clear examples before explaining the general principle. What are some examples I could use? (I really want to explain it more sharply to <em>myself</em>, but also to members of my meetup.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"PJKgSRkXkCqXmCk3M": 2, "wMPYFGmhcFg4bSb4Z": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PM9D3iLFZWvkT6mMX", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 19, "extendedScore": null, "score": 8.144480351913223e-07, "legacy": true, "legacyId": "11443", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZTRiSNmeGQK8AkdN2", "2HxAkCG7NWTrrn5R3", "FaJaCgqBKphrDzDSj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-13T15:27:48.980Z", "modifiedAt": null, "url": null, "title": "What are you working on? December 2011", "slug": "what-are-you-working-on-december-2011", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:36.023Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7vraBisR7rQsShwdh/what-are-you-working-on-december-2011", "pageUrlRelative": "/posts/7vraBisR7rQsShwdh/what-are-you-working-on-december-2011", "linkUrl": "https://www.lesswrong.com/posts/7vraBisR7rQsShwdh/what-are-you-working-on-december-2011", "postedAtFormatted": "Tuesday, December 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20are%20you%20working%20on%3F%20December%202011&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20are%20you%20working%20on%3F%20December%202011%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7vraBisR7rQsShwdh%2Fwhat-are-you-working-on-december-2011%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20are%20you%20working%20on%3F%20December%202011%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7vraBisR7rQsShwdh%2Fwhat-are-you-working-on-december-2011", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7vraBisR7rQsShwdh%2Fwhat-are-you-working-on-december-2011", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 106, "htmlBody": "<p>\n<div style=\"background-image: initial; background-attachment: initial; background-origin: initial; background-clip: initial; padding: 0.5em; margin: 8px;\">\n<p>This is the sixth bimonthly 'What are you working On?' thread. Previous threads are&nbsp;<a href=\"/r/discussion/tag/waywo\">here</a>. So here's the question:</p>\n<p style=\"padding-left: 60px;\"><em>What are you working on?&nbsp;</em></p>\n<p>Here are some guidelines:</p>\n<ul>\n<li>Focus on projects that you have recently made progress on, not projects that you're thinking about doing but haven't started.</li>\n<li>Why this project and not others? Mention reasons why you're doing the project and/or why others should contribute to your project (if applicable).</li>\n<li>Talk about your goals for the project.</li>\n<li>Any kind of project is fair game: personal improvement, research project, art project, whatever.</li>\n<li>Link to your work if it's linkable.</li>\n</ul>\n</div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7vraBisR7rQsShwdh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 12, "extendedScore": null, "score": 8.14448255884654e-07, "legacy": true, "legacyId": "11444", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 117, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-13T16:28:25.803Z", "modifiedAt": null, "url": null, "title": "The Likelihood Ratio and the Problem of Evil", "slug": "the-likelihood-ratio-and-the-problem-of-evil", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:56.427Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "JQuinton", "createdAt": "2011-04-29T16:29:16.319Z", "isAdmin": false, "displayName": "JQuinton"}, "userId": "edHNo35soNo2w6ZwN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Wf7upNhAYSpmDp2Ku/the-likelihood-ratio-and-the-problem-of-evil", "pageUrlRelative": "/posts/Wf7upNhAYSpmDp2Ku/the-likelihood-ratio-and-the-problem-of-evil", "linkUrl": "https://www.lesswrong.com/posts/Wf7upNhAYSpmDp2Ku/the-likelihood-ratio-and-the-problem-of-evil", "postedAtFormatted": "Tuesday, December 13th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Likelihood%20Ratio%20and%20the%20Problem%20of%20Evil&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Likelihood%20Ratio%20and%20the%20Problem%20of%20Evil%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWf7upNhAYSpmDp2Ku%2Fthe-likelihood-ratio-and-the-problem-of-evil%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Likelihood%20Ratio%20and%20the%20Problem%20of%20Evil%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWf7upNhAYSpmDp2Ku%2Fthe-likelihood-ratio-and-the-problem-of-evil", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWf7upNhAYSpmDp2Ku%2Fthe-likelihood-ratio-and-the-problem-of-evil", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 671, "htmlBody": "<p>I'm posting this here because I want to see if my reasoning is incorrect.</p>\r\n<p>Generally, when people talk about the problem of evil, the underlying problem is&nbsp;actually one&nbsp;of indifference. Given that God exists,&nbsp;he doesn't seem to care some (most) of the time bad things happen, and seems to sometimes reward bad people with good fortune. This makes sense, of course, if there indeed was no god, but we have thousands of years of theodicy that argues that an all powerful, all knowing god exists despite the problem of indifference.</p>\r\n<p>So, I attempted to formulate the problem of&nbsp;indifference in terms of probability - the probability of an all powerful god creating the universe (H) verses the probability of naturalism (~H) - to see which one was more likely. E would be the state of the current universe which seems to have both \"good\" and \"evil\" in it. I had no idea how to&nbsp;determine the probability of&nbsp;P(E | H), but if the 2,500+ years of theodicy&nbsp;explaining the problem of indifference&nbsp;was in fact correct, then&nbsp;to be fair to theism I might grant that P(E | H)&nbsp;= .99. However, this seems to not be correct; I did know that P(E | H) + P(~E | H) = 1.00 so this would mean that the all powerful god of traditional theism wouldn't make sense if our universe were indeed ~E instead of E given that P(E | H) = .99.</p>\r\n<p>~E would be any other ratio of good::evil that we can imagine outside of the current state of affairs, or at least that's my reasoning. This means that if the universe were all good and zero evil, or all evil and zero good, granting that P(~E | H) = .01 doesn't seem like something traditional theism&nbsp;would accept. If we woke up tomorrow, and there was absolutely no evil in the world, and that was how the world always was, would traditional theism have no theodicies that explained why this world was evidence for their god(s)? That doesn't seem likely. Similarly, but less so, for a world that was overwhelmingly evil with very little good.</p>\r\n<p>So it seems that E and ~E can be broken up into these three scenarios. E<sub>1</sub> being our current world, E<sub>2</sub> being a world of all good an no evil, and E<sub>3</sub> being a world of all evil and no good. Then we have P(E<sub>1</sub> | H) + P(E<sub>2</sub> | H) + P(E<sub>3</sub> | H) = 1.00. This would also apply to naturalism, but it seems as though a world of all good/evil and no evil/good wouldn't make sense under naturalism which predicts a fundamentally uncaring universe; P(E | ~H) = .99 and P(~E | ~H) would make sense to be the remainder. If this is the case, then the likelihood ratio favors naturalism. P(E<sub>1</sub> | H) / P(E<sub>1</sub> | ~H) is less than 1 since traditional theism doesn't seem to be <a href=\"/lw/if/your_strength_as_a_rationalist/\" target=\"_blank\">restricting anticipation</a> on what type of world(s)&nbsp;it can explain while naturalism does.</p>\r\n<p>But then I thought that this applies to more situations beyond just the problem of evil/indifference. If an all powerful all knowing god can explain any and every sort of evidence we can imagine, and if there are 100 different types of evidence in a given class of evidence, then this god's explanatory power gets stretched across all 100 types of evidence, with any other hypothesis that restricts the type of evidence it can explain being favored over&nbsp;the god hypothesis. For something like the fine tuning argument for god, since there are infinite combinations of physical constants that an all powerful, all knowing god could create, this stretches the god hypothesis out among an infinite number of possible evidences, effectively favoring any alternative hypothesis by infinite decibles via the likelihood ratio. Meaning that something like the fine tuning argument is ironically an argument against H.</p>\r\n<p>And if this is the case, then an infinitely powerful, infinitely knowing god is infinitely less likely than any other hypothesis that restricts itself. Thoughts? Is my reasoning off somewhere? Is the the ultimate penalty for not restricting anticipation?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Wf7upNhAYSpmDp2Ku", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": -5, "extendedScore": null, "score": 8.144704013885038e-07, "legacy": true, "legacyId": "11445", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["5JDkW4MYXit2CquLs"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-14T01:46:50.640Z", "modifiedAt": null, "url": null, "title": "Meetup : West LA Meetup 12-14-2011", "slug": "meetup-west-la-meetup-12-14-2011", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:56.512Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "xgPZ27s4G27JhcA7n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vKhahR6j6hezZLe4C/meetup-west-la-meetup-12-14-2011", "pageUrlRelative": "/posts/vKhahR6j6hezZLe4C/meetup-west-la-meetup-12-14-2011", "linkUrl": "https://www.lesswrong.com/posts/vKhahR6j6hezZLe4C/meetup-west-la-meetup-12-14-2011", "postedAtFormatted": "Wednesday, December 14th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20West%20LA%20Meetup%2012-14-2011&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20West%20LA%20Meetup%2012-14-2011%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvKhahR6j6hezZLe4C%2Fmeetup-west-la-meetup-12-14-2011%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20West%20LA%20Meetup%2012-14-2011%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvKhahR6j6hezZLe4C%2Fmeetup-west-la-meetup-12-14-2011", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvKhahR6j6hezZLe4C%2Fmeetup-west-la-meetup-12-14-2011", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 159, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/5l'>West LA Meetup 12-14-2011</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 December 2011 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, December 14th.</p>\n\n<p><strong>Where:</strong> The <a href=\"http://www.westsidetavernla.com/\" rel=\"nofollow\">Westside Tavern</a> <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"http://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong>Recommended Reading:</strong></p>\n\n<ul>\n<li><a href=\"http://lesswrong.com/lw/og/wrong_questions/\">Wrong Questions</a></li>\n<li><a href=\"http://lesswrong.com/lw/oh/righting_a_wrong_question/\">Righting a Wrong Question</a></li>\n<li>Pick <em>any two</em> entries from <a href=\"http://lesswrong.com/lw/od/37_ways_that_words_can_be_wrong/\">37 Ways That Words Can Be Wrong</a>, and we will discuss the points in those posts!</li>\n</ul>\n\n<p>Whether you're a regular reader or totally new, here for the theoretical musings or the practical things, come by and say hello! The conversation is largely unstructured and casual, and the people are awesome. If we have a large group, we may also play a game!</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes&#39; Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/5l'>West LA Meetup 12-14-2011</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vKhahR6j6hezZLe4C", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.146744705297554e-07, "legacy": true, "legacyId": "11457", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup_12_14_2011\">Discussion article for the meetup : <a href=\"/meetups/5l\">West LA Meetup 12-14-2011</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 December 2011 07:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">10850 West Pico Blvd, Los Angeles, CA 90064</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p><strong>When:</strong> 7:00pm - 9:00pm Wednesday, December 14th.</p>\n\n<p><strong>Where:</strong> The <a href=\"http://www.westsidetavernla.com/\" rel=\"nofollow\">Westside Tavern</a> <em>in the upstairs Wine Bar</em> (all ages welcome), located inside the <a href=\"http://maps.google.com/maps?q=10850+West+Pico+Blvd,+Suite+312,+Los+Angeles,+CA+90064\" rel=\"nofollow\">Westside Pavillion</a> on the second floor, right by the movie theaters.</p>\n\n<p><strong>Parking</strong> is free for 3 hours.</p>\n\n<p><strong id=\"Recommended_Reading_\">Recommended Reading:</strong></p>\n\n<ul>\n<li><a href=\"http://lesswrong.com/lw/og/wrong_questions/\">Wrong Questions</a></li>\n<li><a href=\"http://lesswrong.com/lw/oh/righting_a_wrong_question/\">Righting a Wrong Question</a></li>\n<li>Pick <em>any two</em> entries from <a href=\"http://lesswrong.com/lw/od/37_ways_that_words_can_be_wrong/\">37 Ways That Words Can Be Wrong</a>, and we will discuss the points in those posts!</li>\n</ul>\n\n<p>Whether you're a regular reader or totally new, here for the theoretical musings or the practical things, come by and say hello! The conversation is largely unstructured and casual, and the people are awesome. If we have a large group, we may also play a game!</p>\n\n<p>I will bring a whiteboard with <a href=\"http://wiki.lesswrong.com/wiki/Bayes%27_theorem\">Bayes' Theorem</a> written on it.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___West_LA_Meetup_12_14_20111\">Discussion article for the meetup : <a href=\"/meetups/5l\">West LA Meetup 12-14-2011</a></h2>", "sections": [{"title": "Discussion article for the meetup : West LA Meetup 12-14-2011", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup_12_14_2011", "level": 1}, {"title": "Recommended Reading:", "anchor": "Recommended_Reading_", "level": 2}, {"title": "Discussion article for the meetup : West LA Meetup 12-14-2011", "anchor": "Discussion_article_for_the_meetup___West_LA_Meetup_12_14_20111", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["XzrqkhfwtiSDgKoAF", "rQEwySCcLtdKHkrHp", "FaJaCgqBKphrDzDSj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-14T06:05:40.047Z", "modifiedAt": null, "url": null, "title": "You Are Not So Smart - Book on Bias by david mcraney", "slug": "you-are-not-so-smart-book-on-bias-by-david-mcraney", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Goobahman", "createdAt": "2011-01-13T05:09:28.962Z", "isAdmin": false, "displayName": "Goobahman"}, "userId": "cidN68rGuy4wwnvFp", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nfximrcWGi4aGfdwy/you-are-not-so-smart-book-on-bias-by-david-mcraney", "pageUrlRelative": "/posts/nfximrcWGi4aGfdwy/you-are-not-so-smart-book-on-bias-by-david-mcraney", "linkUrl": "https://www.lesswrong.com/posts/nfximrcWGi4aGfdwy/you-are-not-so-smart-book-on-bias-by-david-mcraney", "postedAtFormatted": "Wednesday, December 14th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20You%20Are%20Not%20So%20Smart%20-%20Book%20on%20Bias%20by%20david%20mcraney&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AYou%20Are%20Not%20So%20Smart%20-%20Book%20on%20Bias%20by%20david%20mcraney%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnfximrcWGi4aGfdwy%2Fyou-are-not-so-smart-book-on-bias-by-david-mcraney%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=You%20Are%20Not%20So%20Smart%20-%20Book%20on%20Bias%20by%20david%20mcraney%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnfximrcWGi4aGfdwy%2Fyou-are-not-so-smart-book-on-bias-by-david-mcraney", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnfximrcWGi4aGfdwy%2Fyou-are-not-so-smart-book-on-bias-by-david-mcraney", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 90, "htmlBody": "<p><a href=\"http://youarenotsosmart.com/\">http://youarenotsosmart.com/</a><br /><br />Hi Everyone,<br /><br />I was just wondering if anyone's read the above book, and what they thought of it? (I have not)</p>\r\n<p>Not so much&nbsp;in terms of actual content, but in terms of&nbsp;popularizing Rationality and raising the sanity waterline.</p>\r\n<p>How useful a tool could something like this be? Should I buy 20 copies and hand them out on Christmas?<br /><br />In any case I plan to read it in the next week or two, and I'll let you know what I thought of it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nfximrcWGi4aGfdwy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "11460", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-14T12:49:04.390Z", "modifiedAt": null, "url": null, "title": "Tracking emotions with kinesthetic memory", "slug": "tracking-emotions-with-kinesthetic-memory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:56.841Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "eugman", "createdAt": "2009-09-28T01:40:39.582Z", "isAdmin": false, "displayName": "eugman"}, "userId": "rtJy8Y9zXRpjWmeMi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fQ7fXwFzwpdaNgNcL/tracking-emotions-with-kinesthetic-memory", "pageUrlRelative": "/posts/fQ7fXwFzwpdaNgNcL/tracking-emotions-with-kinesthetic-memory", "linkUrl": "https://www.lesswrong.com/posts/fQ7fXwFzwpdaNgNcL/tracking-emotions-with-kinesthetic-memory", "postedAtFormatted": "Wednesday, December 14th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Tracking%20emotions%20with%20kinesthetic%20memory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATracking%20emotions%20with%20kinesthetic%20memory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfQ7fXwFzwpdaNgNcL%2Ftracking-emotions-with-kinesthetic-memory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Tracking%20emotions%20with%20kinesthetic%20memory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfQ7fXwFzwpdaNgNcL%2Ftracking-emotions-with-kinesthetic-memory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfQ7fXwFzwpdaNgNcL%2Ftracking-emotions-with-kinesthetic-memory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 230, "htmlBody": "<p>I've been trying to manage my negative thoughts and I've stumbled on an interesting technique. In order to increase my mindfulness of my ruminations and my thoughts. I started making a check mark on a piece of paper when I had specific thought. I didn't feel like I had the emotional strength to battle every thought, but at least making a little checkmark was easy. Then, deciding I wanted to do this anywhere, I started instead making a quick checkmark gesture with my pointer finger.</p>\n<p>The results have been interesting. One effect is I can now more easily sense the heartbeat of my mood. Usually, I could tell how I was feeling at any one time, but I never noticed the flow of things. But with gestural movement, I can remember how often I've been doing it. I can also see if I'm doing it every few minutes or every few seconds.</p>\n<p>There's another result that's very interesting and offsetting. Sometimes, the movements are semiautomatic. This fits in with a modular theory of the brain, I think. So, I'll have a fleeting negative thought, reflexively make the gesture, and go \"Oh, I just had a negative thought, I didn't notice that. Maybe I should do something.\" It feels a little strange from the inside.</p>\n<p>So, just something that was interesting to me. I was wondering if anyone had any thoughts or interesting information.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwv9eHi7KGg5KA9oM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fQ7fXwFzwpdaNgNcL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 46, "extendedScore": null, "score": 0.0005384403465089948, "legacy": true, "legacyId": "11461", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-14T14:51:26.602Z", "modifiedAt": null, "url": null, "title": "How Many Worlds?", "slug": "how-many-worlds", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:54.834Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "smk", "createdAt": "2011-08-09T16:56:37.443Z", "isAdmin": false, "displayName": "smk"}, "userId": "cCbxNmvFe2C6G9yZ3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/3gMc33BfXB3HotLkm/how-many-worlds", "pageUrlRelative": "/posts/3gMc33BfXB3HotLkm/how-many-worlds", "linkUrl": "https://www.lesswrong.com/posts/3gMc33BfXB3HotLkm/how-many-worlds", "postedAtFormatted": "Wednesday, December 14th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20Many%20Worlds%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20Many%20Worlds%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3gMc33BfXB3HotLkm%2Fhow-many-worlds%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20Many%20Worlds%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3gMc33BfXB3HotLkm%2Fhow-many-worlds", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F3gMc33BfXB3HotLkm%2Fhow-many-worlds", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 52, "htmlBody": "<p>How many universes \"branch off\" from a \"quantum event\", and in how many of them is the <a href=\"http://en.wikipedia.org/wiki/Schrodinger%27s_cat\">cat</a> dead vs alive, and what about non-50/50 scenarios, and please answer so that a physics dummy can maybe kind of understand?</p>\n<p>(Is it just 1 with the live cat and 1 with the dead one?)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "3gMc33BfXB3HotLkm", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 2, "extendedScore": null, "score": 8.149611751568574e-07, "legacy": true, "legacyId": "11462", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 64, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-14T18:22:15.044Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] A Failed Just-So Story", "slug": "seq-rerun-a-failed-just-so-story", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:57.168Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7aHL8Z6NncT2N49LJ/seq-rerun-a-failed-just-so-story", "pageUrlRelative": "/posts/7aHL8Z6NncT2N49LJ/seq-rerun-a-failed-just-so-story", "linkUrl": "https://www.lesswrong.com/posts/7aHL8Z6NncT2N49LJ/seq-rerun-a-failed-just-so-story", "postedAtFormatted": "Wednesday, December 14th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20A%20Failed%20Just-So%20Story&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20A%20Failed%20Just-So%20Story%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7aHL8Z6NncT2N49LJ%2Fseq-rerun-a-failed-just-so-story%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20A%20Failed%20Just-So%20Story%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7aHL8Z6NncT2N49LJ%2Fseq-rerun-a-failed-just-so-story", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7aHL8Z6NncT2N49LJ%2Fseq-rerun-a-failed-just-so-story", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 162, "htmlBody": "<p>Today's post, <a href=\"/lw/mk/a_failed_justso_story/\">A Failed Just-So Story</a> was originally published on 05 January 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Part of the reason professional evolutionary biologists dislike just-so stories is that many of them are simply wrong.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8tn/seq_rerun_rational_vs_scientific_evpsych/\">Rational vs Scientific Ev-Psych</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7aHL8Z6NncT2N49LJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 8.150384609606836e-07, "legacy": true, "legacyId": "11463", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["kX6C2qdngKp4AdEAk", "t9864QmdouAqmyQtG", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-14T18:42:28.991Z", "modifiedAt": null, "url": null, "title": "Building case-studies of akrasia", "slug": "building-case-studies-of-akrasia", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:38.355Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mercurial", "createdAt": "2011-04-21T03:59:51.257Z", "isAdmin": false, "displayName": "Mercurial"}, "userId": "2dGsX6cZSR9PmQyBq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jZEsFXyhFyoTY6s3m/building-case-studies-of-akrasia", "pageUrlRelative": "/posts/jZEsFXyhFyoTY6s3m/building-case-studies-of-akrasia", "linkUrl": "https://www.lesswrong.com/posts/jZEsFXyhFyoTY6s3m/building-case-studies-of-akrasia", "postedAtFormatted": "Wednesday, December 14th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Building%20case-studies%20of%20akrasia&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABuilding%20case-studies%20of%20akrasia%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjZEsFXyhFyoTY6s3m%2Fbuilding-case-studies-of-akrasia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Building%20case-studies%20of%20akrasia%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjZEsFXyhFyoTY6s3m%2Fbuilding-case-studies-of-akrasia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjZEsFXyhFyoTY6s3m%2Fbuilding-case-studies-of-akrasia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 428, "htmlBody": "<p>The idea for this came to me when reading <a href=\"/r/lesswrong/lw/8s9/an_akrasia_case_study/\" target=\"_blank\">nyan_sandwich's \"An akrasia case study.\"</a> I outlined the idea itself in <a href=\"/r/lesswrong/lw/8s9/an_akrasia_case_study/5ffm\" target=\"_blank\">my comment in that thread.</a></p>\n<p>So here's the plan:</p>\n<ul>\n<li>In a comment reply to this, describe a <a href=\"/lw/ic/the_virtue_of_narrowness/\" target=\"_blank\">specific</a> instance of akrasia in your own life. Place an emphasis on the specificity. Focus on a specific task, either positive (i.e., that you judge to be good to do but self-sabotage from doing, like writing a paper) or negative (i.e., that you know you shouldn't do but do anyway, like buying more Frosted Flakes for breakfast and continuing to eat them). The more specific, the better.</li>\n<li>You can share multiple instances, but please create new comments for each.</li>\n<li>If you tried an anti-akrasia technique in this specific context, explain what you did and what effect it had. If you have some way in which you measured its effects objectively, please share that. If not, though, that's okay; we can still learn something from what various attempts to tackle different manifestations of akrasia <a href=\"/lw/no/how_an_algorithm_feels_from_inside/\" target=\"_blank\">feel like from the inside</a>. The goal here is <em>not</em>&nbsp;to <a href=\"/lw/ka/hold_off_on_proposing_solutions\" target=\"_blank\">propose solutions</a>; instead, it's to see what different things that feel like solutions seem to do to different kinds of akrasia. So even failed attempts are useful.</li>\n<li>If you tried multiple approaches or if your approach requires some explanation, you might consider describing it and its effects in a reply to your description of the instance of akrasia you applied it to.</li>\n</ul>\n<p>Let me emphasize one more time that <a href=\"/lw/ka/hold_off_on_proposing_solutions\" target=\"_blank\">we are not looking for solutions in this thread</a>. <em>Please</em>&nbsp;<a href=\"/lw/9v/beware_of_otheroptimizing/\" target=\"_blank\">don't give each other suggestions</a>! If you think you're on to something hot in terms of <a href=\"/lw/2c/a_sense_that_more_is_possible/1ff\" target=\"_blank\">the \"kicking\" aspect of the Art</a>, please <em>show</em>&nbsp;us with a description of how the technique worked <em>for you</em>&nbsp;on a <em>specific</em>&nbsp;instance - but share the instance <em>first</em>. The goal here is not to demonstrate that you have a clever anti-akrasia technique. The goal instead is to see what different instances of akrasia and attempts to tackle it <em>actually look like</em>.</p>\n<p>If at all possible, please share both successes <em>and</em> failures. This is especially helpful if we can see successes and failures <em>of the same technique</em>. This helps to balance out <a href=\"/lw/iw/positive_bias_look_into_the_dark/\" target=\"_blank\">positive bias</a>&nbsp;and gives us a better idea of the parameters within which different techniques work. Be especially wary if you have a favorite anti-akrasia technique because of <a href=\"/lw/1u/you_may_already_be_a_sinner\" target=\"_blank\">the subconscious desire to attempt to change reality by pretending your favorite technique is actually perfect</a>. If you <em>do</em>&nbsp;have a favorite technique, please actively seek out <a href=\"/lw/jy/avoiding_your_beliefs_real_weak_points/\" target=\"_blank\">its true weak points</a>.</p>\n<p>Let's crack this thing!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jZEsFXyhFyoTY6s3m", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 19, "extendedScore": null, "score": 8.150458623277795e-07, "legacy": true, "legacyId": "11464", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["xpLXck2nXbE4K3xdE", "yDfxTj9TKYsYiWH5o", "yA4gF5KrboK2m2Xu7", "uHYYA32CKgKT3FagE", "6NvbSwuSAooQxxf7f", "rmAbiEKQDpDnZzcRf", "Cq45AuedYnzekp3LX", "dHQkDNMhj692ayx78"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-14T19:58:17.516Z", "modifiedAt": null, "url": null, "title": "[LINK] SMBC discusses the Singularity.", "slug": "link-smbc-discusses-the-singularity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:22:08.480Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "radical_negative_one", "createdAt": "2009-12-08T16:05:42.376Z", "isAdmin": false, "displayName": "radical_negative_one"}, "userId": "qQqgj5ScvgQsJk7Ti", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/aQczgFpCMCBv4DmJY/link-smbc-discusses-the-singularity", "pageUrlRelative": "/posts/aQczgFpCMCBv4DmJY/link-smbc-discusses-the-singularity", "linkUrl": "https://www.lesswrong.com/posts/aQczgFpCMCBv4DmJY/link-smbc-discusses-the-singularity", "postedAtFormatted": "Wednesday, December 14th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20SMBC%20discusses%20the%20Singularity.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20SMBC%20discusses%20the%20Singularity.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaQczgFpCMCBv4DmJY%2Flink-smbc-discusses-the-singularity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20SMBC%20discusses%20the%20Singularity.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaQczgFpCMCBv4DmJY%2Flink-smbc-discusses-the-singularity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FaQczgFpCMCBv4DmJY%2Flink-smbc-discusses-the-singularity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 67, "htmlBody": "<div>\n<p>A Short History of Tool Use:</p>\n<p><a href=\"http://www.smbc-comics.com/index.php?db=comics&amp;id=2460#comic\">http://www.smbc-comics.com/index.php?db=comics&amp;id=2460#comic</a></p>\n<p>A human applies a wrench to a rock.<br />A human applies a chain of interlinked wrenches to a tower.<br />A human applies a chain of interlinked wrenches to its own head.<br />A chain of interlinked wrenches forms a loop.</p>\n<p>Hover your mouse over the red button at the lower-left to see the message, \"Reddit is somewhere between 2 and 3.\"</p>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "aQczgFpCMCBv4DmJY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": -4, "extendedScore": null, "score": -2e-06, "legacy": true, "legacyId": "11465", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-15T00:07:17.515Z", "modifiedAt": null, "url": null, "title": "Compressing Reality to Math", "slug": "compressing-reality-to-math", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:01.790Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2FXtpdzx6uoNRZXjS/compressing-reality-to-math", "pageUrlRelative": "/posts/2FXtpdzx6uoNRZXjS/compressing-reality-to-math", "linkUrl": "https://www.lesswrong.com/posts/2FXtpdzx6uoNRZXjS/compressing-reality-to-math", "postedAtFormatted": "Thursday, December 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Compressing%20Reality%20to%20Math&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACompressing%20Reality%20to%20Math%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2FXtpdzx6uoNRZXjS%2Fcompressing-reality-to-math%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Compressing%20Reality%20to%20Math%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2FXtpdzx6uoNRZXjS%2Fcompressing-reality-to-math", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2FXtpdzx6uoNRZXjS%2Fcompressing-reality-to-math", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2295, "htmlBody": "<p>This is part of a <a href=\"/lw/8xr/decision_analysis_sequence/\">sequence on decision analysis</a> and follows <a href=\"/lw/8m5/5_axioms_of_decision_making\">5 Axioms of Decision-Making</a>, which explains how to turn a well-formed problem into a solution. Here we discuss turning reality into a well-formed problem. There are three basic actions I'd like to introduce, and then work through some examples.<a id=\"more\"></a></p>\n<h1>Scope</h1>\n<p>The first thing you have to decide with a problem is, well, what the problem is. Suppose you're contemplating remodeling your kitchen, and the contractor you're looking at offers marble or granite countertops. While deciding whether you want marble or granite, you stop and wonder- is this really the contractor that you should be using? Actually, should you even be remodeling your kitchen? Maybe you should to move to a better city first. But if you're already thinking about moving, you might even want to emigrate to another country.</p>\n<p>At this point the contractor awkwardly coughs and asks whether you'd like marble or granite.</p>\n<p>Decisions take effort to solve, especially if you're trying to carefully avoid bias. It helps to partition the world and deal with local problems- you can figure out which countertops you want without first figuring out what country you want to live in. It's also important to keep in mind <a href=\"/lw/le/lost_purposes/\">lost purposes</a>- if you're going to move to a new city, remodeling your kitchen is probably a mistake, <a href=\"/lw/4e/cached_selves/\">even after you already called a contractor</a>. Be open to going up a level, but not paralyzed by the possibility, which is a careful balancing act. Spending time periodically going up levels and reevaluating your decisions and directions can help, as well as having a philosophy of life.</p>\n<h1>Model</h1>\n<p>Now that you've got a first draft of what your problem entails, how does that corner of the world work? What are the key decisions and the key uncertainties? A tool that can be of great help here is an <a href=\"http://en.wikipedia.org/wiki/Influence_diagram\">influence diagram</a>, which is a directed acyclic graph<sup>1</sup> which represents the uncertainties, decisions, and values inherent in a problem. While sketching out your model, do you become more or less comfortable with the scope of the decision? If you're less comfortable, move up (or down) a level and remodel.&nbsp;</p>\n<h1>Elicit</h1>\n<p>Now that you have an influence diagram, you need to populate it with numbers. What (conditional) probabilities do you assign to uncertainty nodes? What preferences do you assign to possible outcomes? Are there any other uncertainty nodes you could add to clarify your calculations? (For example, when making a decision based on a medical test, you may want to add a \"underlying reality\" node that influences the test results but that you can't see, to make it easier to elicit probabilities.)</p>\n<p>&nbsp;</p>\n<h2>Outing with a Friend<br /></h2>\n<p><img style=\"float: right;\" src=\"http://img.photobucket.com/albums/v347/Vaniver/ID1.jpg\" alt=\"\" /></p>\n<p>A friend calls me up: \"Let's do something this weekend.\" I agree, and ponder my options. We typically play <a href=\"http://en.wikipedia.org/wiki/Go_%28game%29\">Go</a>, and I can either host at my apartment or we could meet at a local park. If it rains, the park will not be a fun place to be; if it's sunny, though, the park is much nicer than my apartment. I check the weather forecast for Saturday and am not impressed: 50% chance of rain.<sup>2</sup> I check <a href=\"http://faculty.engr.utexas.edu/bickel/Papers/TWC_Calibration.pdf\">calibration data</a> and see that for a 3 day forecast, a 50% prediction is well calibrated, so I'll just use that number.<sup>3</sup></p>\n<p>If we play Go, we're locked in to whatever location we choose, because moving the board would be a giant hassle.<sup>4</sup> But we could just talk- it's been a while since we caught up. I don't think I would enjoy that as much as playing Go, but it would give us location flexibility- if it's nice out we'll go to the park, and if it starts raining we can just walk back to my apartment.</p>\n<p><img style=\"float: right;\" src=\"http://img.photobucket.com/albums/v347/Vaniver/ID2.jpg\" alt=\"\" width=\"200\" height=\"200\" /></p>\n<p>Note that I just added a decision to the problem, and so I update my diagram accordingly. With influence diagrams, you have a choice of how detailed to be- I could have made the activity decision point to two branches, one in which I see the weather then pick my location, and another where I pick my location and then see the weather. I chose a more streamlined version, in which I choose between playing Go or walking wherever the rain isn't (where the choice of location isn't part of my optimization problem, since I consider it obvious.</p>\n<p>Coming up with creative options like this is one of the major benefits of careful decision-making. When you evaluate every part of the decision and make explicit your dependencies, you can see holes in your model or places where you can make yourself better off by recasting the problem. Simply getting everything on paper in a visual format can do wonders to help clarify and solve problems.</p>\n<p>At this point, I'm happy with my model- I could come up with other options, but this is probably enough for a problem of this size.<sup>5</sup> I've got six outcomes- either (we walk, play go (inside, outside)) and it (rains, doesn't rain). I decide that I would most prefer playing go outside and it doesn't rain, and would least prefer playing go outside and it rains.<sup>6</sup> Ordering the rest is somewhat difficult, but I come up with the following matrix:</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img src=\"http://img.photobucket.com/albums/v347/Vaniver/Value_Matrix.jpg\" alt=\"\" /></p>\n<p>If we walk, I won't enjoy it quite as much as playing Go, but if we play Go in my apartment and it's sunny I'll regret that we didn't play outside. Note that all of the preferences probabilities are fairly high- that's because having a game interrupted by rain is much worse than all of the other outcomes. Calculating the preference value of each deal is easy, since the probability of rain is independent of my decision and .5: I decide that playing Go at Park is the worst option with an effective .5 chance of the best outcome, Go at Home is better with an effective .7 chance of the best outcome, and Walk is best with an effective .75 chance of the best outcome.</p>\n<p>Note that working through everything worked out better for us than doing scenario planning. If I knew it would rain, I would choose Go at Home; if I knew it wouldn't rain, I would choose Go at Park. Walk is dominated by both of those in the case of certainty, but its lower variance means it wins out when I'm very uncertain about whether it will rain or not.<sup>7</sup></p>\n<p>What's going up a level here? Evaluating whether or not I want to do something with this friend this weekend (and another level might be evaluating whether or not I want them as a friend, and so on). When evaluating the prospects, I might want to compare them to whatever I was planning before my friend called to make sure this is actually a better plan. It could be the chance of rain is so high it makes this plan worse than whatever alternatives I had before I knew it was likely to rain.</p>\n<h2>Managing a Store<br /></h2>\n<p>Suppose I manage a store that sells a variety of products. I decide that I want to maximize profits over the course of a year, plus some estimate of intangibles (like customer satisfaction). I have a number of year-long commitments (the lease on my space, employment contracts for full-time employees, etc.) already made, which I'll consider beyond the scope of the problem. I then have a number of decisions I have to make month-to-month<sup>8</sup> (how many seasonal employees to hire, what products to order, whether to change wages or prices, what hours the store should be open), and then decisions I have to make day-to-day (which employees to schedule, where to place items in the store, where to place employees in the store, how to spend my time).</p>\n<p>I look at the day-to-day decisions and decide that I'm happy modeling those as policies rather than individual decisions- I don't need to have mapped those out now, but I do need to put my January orders in next week. Which policies I adopt might be relevant to my decision, though, and so I still want to model them on that level.</p>\n<p>Well, what about my uncertainties? Employee morale seems like one that'll vary month-to-month or day-to-day, though I'm comfortable modeling it month-to-month, as that's when I would change the employee composition or wages. Customer satisfaction is an uncertainty that seems like it would be worth tracking, and so is customer demand- how the prices I set will influence sales. And I should model demand by item- or maybe just by category of items. Labor costs, inventory costs, and revenue are all nodes that I could stick in as uncertainty nodes (even though they might just deterministically calculate those based on their input nodes).</p>\n<p>You can imagine that the influence diagram I'm sketching is starting to get massive- and I'm just considering this year! I also need to think carefully about how I put the dependencies in this network- should I have customer satisfaction point to customer demand, or the other way around? For uncertainty nodes it doesn't matter much (as we know how to flip them), but may make elicitation easier or harder. For decision nodes, order is critical, as it represents the order the decisions have to be made in.</p>\n<p>But even though the problem is getting massive, I can still use this methodology to solve it. This'll make it easier for me to keep everything in mind- because I won't need to keep everything in <em>mind</em>. I can contemplate the dependencies and uncertainties of the system one piece at a time, record the results, and then integrate them together. Like with the previous example, it's easy to include the results of other people's calculations in your decision problem as a node, meaning that this can be extended to team decisions as well- maybe my marketer does all the elicitation for the demand nodes, and my assistant manager does all the elicitation for the employee nodes, and then I can combine them into one decision-making network.</p>\n<h2>Newcomb's Problem</h2>\n<p><a href=\"http://wiki.lesswrong.com/wiki/Newcomb%27s_problem\">Newcomb's Problem</a> is a thought experiment designed to highlight the difference between different decision theories that has <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">come up a lot</a> on Less Wrong. AnnaSalamon wrote <a href=\"/lw/17b/decision_theory_why_pearl_helps_reduce_could_and/\">an article</a> (that even includes influence diagrams that are much prettier than mine) analyzing Newcomb's Problem, in which she presents three ways to interpret the problem. I won't repeat her analysis, but will make several observations:</p>\n<ol>\n<li>Problem statements, and real life, are often ambiguous or uncertain. Navigating that ambiguity and uncertainty about what problem you're actually facing is a major component of making decisions. It's also a major place for bias to creep in: if you aren't careful about defining your problems, they will be defined in careless ways, which can impose real and large costs in worse solutions. </li>\n<li>It's easy to construct a thought experiment with contradictory premises, and not notice if you keep math and pictures out of it. Draw pictures, show the math. It makes normal problems easier, and helps you notice when a problem boils down to \"could an unstoppable force move an immovable object?\",<sup>9</sup> and <a href=\"/lw/of/dissolving_the_question/\">then you can move on</a>.</li>\n<li>If you're not quite sure which of several interpretations is true, you can model that explicitly. Put an uncertainty node at the top which points to the model where your behavior and Omega's decision are independent and the model where your behavior determines Omega's behavior. Elicit a probability, or calculate what the probability would need to be for you to make one decision and then compare that to how uncertain you are. </li>\n</ol> \n<hr />\n<p>1. This means that there are a bunch of nodes connected by arrows (directed edges), and that there are no cycles (arrows that point in a loop). As a consequence, there are some nodes with no arrows pointing to them and one node with no arrows leaving it (the value node). It's also worth mentioning that influence diagrams are a special case of <a href=\"http://en.wikipedia.org/wiki/Bayesian_network\">Bayesian Networks</a>.</p>\n<p>2. This is the actual prediction for me as of 1:30 PM on 12/14/2011. Link <a href=\"http://www.weather.com/weather/wxdetail/78701?dayNum=3\">here</a>, though that link should become worthless by tomorrow. Also note that I'm assuming that rain will only happen after we start the game of Go / decide where to play, or that the switching costs will be large.</p>\n<p>3. The actual percentage in the data they had collected by 2008 was ~53%, but as it was within the 'well calibrated' region I should use the number at face value.</p>\n<p>4. If it started raining while we were playing outside, we would probably just stop and do something else rather than playing through the rain, but neither of those are attractive prospects.</p>\n<p>5. I strongly recommend <a href=\"http://en.wikipedia.org/wiki/Satisfice\">satisficing</a>, in the AI sense of including the costs of optimizing in your optimization process, rather than maximization. Opportunity costs are real. For a problem with, say, millions of dollars on the line, you'll probably want to spend quite a bit of time trying to come up with other options.</p>\n<p>6. You may have noticed a trend that the best and worst outcome are often paired together. This is more likely in constructed problems, but is a feature of many difficult real-world problems.</p>\n<p>7. We can do something called sensitivity analysis to see how sensitive this result is; if it's very sensitive to a value we elicited, we might go back and see if we can narrow the uncertainty on that value. If it's not very sensitive, then we don't need to worry much about those uncertainties.</p>\n<p>8. This is an artificial constraint- really, you could change any of those policies any day, or even at any time during that day. It's often helpful to chunk continuous periods into a few discrete ones, but only to the degree that your bins carve reality at the joints.</p>\n<p>9. The unstoppable force being Omega's prediction ability, and the immovable object being causality only propagating forward in time. Two-boxers answer \"unmovable object,\" one-boxers answer \"unstoppable force.\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"KoXbd2HmbdRfqLngk": 2, "dPPATLhRmhdJtJM2t": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2FXtpdzx6uoNRZXjS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 33, "extendedScore": null, "score": 8.151646993201006e-07, "legacy": true, "legacyId": "11467", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>This is part of a <a href=\"/lw/8xr/decision_analysis_sequence/\">sequence on decision analysis</a> and follows <a href=\"/lw/8m5/5_axioms_of_decision_making\">5 Axioms of Decision-Making</a>, which explains how to turn a well-formed problem into a solution. Here we discuss turning reality into a well-formed problem. There are three basic actions I'd like to introduce, and then work through some examples.<a id=\"more\"></a></p>\n<h1 id=\"Scope\">Scope</h1>\n<p>The first thing you have to decide with a problem is, well, what the problem is. Suppose you're contemplating remodeling your kitchen, and the contractor you're looking at offers marble or granite countertops. While deciding whether you want marble or granite, you stop and wonder- is this really the contractor that you should be using? Actually, should you even be remodeling your kitchen? Maybe you should to move to a better city first. But if you're already thinking about moving, you might even want to emigrate to another country.</p>\n<p>At this point the contractor awkwardly coughs and asks whether you'd like marble or granite.</p>\n<p>Decisions take effort to solve, especially if you're trying to carefully avoid bias. It helps to partition the world and deal with local problems- you can figure out which countertops you want without first figuring out what country you want to live in. It's also important to keep in mind <a href=\"/lw/le/lost_purposes/\">lost purposes</a>- if you're going to move to a new city, remodeling your kitchen is probably a mistake, <a href=\"/lw/4e/cached_selves/\">even after you already called a contractor</a>. Be open to going up a level, but not paralyzed by the possibility, which is a careful balancing act. Spending time periodically going up levels and reevaluating your decisions and directions can help, as well as having a philosophy of life.</p>\n<h1 id=\"Model\">Model</h1>\n<p>Now that you've got a first draft of what your problem entails, how does that corner of the world work? What are the key decisions and the key uncertainties? A tool that can be of great help here is an <a href=\"http://en.wikipedia.org/wiki/Influence_diagram\">influence diagram</a>, which is a directed acyclic graph<sup>1</sup> which represents the uncertainties, decisions, and values inherent in a problem. While sketching out your model, do you become more or less comfortable with the scope of the decision? If you're less comfortable, move up (or down) a level and remodel.&nbsp;</p>\n<h1 id=\"Elicit\">Elicit</h1>\n<p>Now that you have an influence diagram, you need to populate it with numbers. What (conditional) probabilities do you assign to uncertainty nodes? What preferences do you assign to possible outcomes? Are there any other uncertainty nodes you could add to clarify your calculations? (For example, when making a decision based on a medical test, you may want to add a \"underlying reality\" node that influences the test results but that you can't see, to make it easier to elicit probabilities.)</p>\n<p>&nbsp;</p>\n<h2 id=\"Outing_with_a_Friend\">Outing with a Friend<br></h2>\n<p><img style=\"float: right;\" src=\"http://img.photobucket.com/albums/v347/Vaniver/ID1.jpg\" alt=\"\"></p>\n<p>A friend calls me up: \"Let's do something this weekend.\" I agree, and ponder my options. We typically play <a href=\"http://en.wikipedia.org/wiki/Go_%28game%29\">Go</a>, and I can either host at my apartment or we could meet at a local park. If it rains, the park will not be a fun place to be; if it's sunny, though, the park is much nicer than my apartment. I check the weather forecast for Saturday and am not impressed: 50% chance of rain.<sup>2</sup> I check <a href=\"http://faculty.engr.utexas.edu/bickel/Papers/TWC_Calibration.pdf\">calibration data</a> and see that for a 3 day forecast, a 50% prediction is well calibrated, so I'll just use that number.<sup>3</sup></p>\n<p>If we play Go, we're locked in to whatever location we choose, because moving the board would be a giant hassle.<sup>4</sup> But we could just talk- it's been a while since we caught up. I don't think I would enjoy that as much as playing Go, but it would give us location flexibility- if it's nice out we'll go to the park, and if it starts raining we can just walk back to my apartment.</p>\n<p><img style=\"float: right;\" src=\"http://img.photobucket.com/albums/v347/Vaniver/ID2.jpg\" alt=\"\" width=\"200\" height=\"200\"></p>\n<p>Note that I just added a decision to the problem, and so I update my diagram accordingly. With influence diagrams, you have a choice of how detailed to be- I could have made the activity decision point to two branches, one in which I see the weather then pick my location, and another where I pick my location and then see the weather. I chose a more streamlined version, in which I choose between playing Go or walking wherever the rain isn't (where the choice of location isn't part of my optimization problem, since I consider it obvious.</p>\n<p>Coming up with creative options like this is one of the major benefits of careful decision-making. When you evaluate every part of the decision and make explicit your dependencies, you can see holes in your model or places where you can make yourself better off by recasting the problem. Simply getting everything on paper in a visual format can do wonders to help clarify and solve problems.</p>\n<p>At this point, I'm happy with my model- I could come up with other options, but this is probably enough for a problem of this size.<sup>5</sup> I've got six outcomes- either (we walk, play go (inside, outside)) and it (rains, doesn't rain). I decide that I would most prefer playing go outside and it doesn't rain, and would least prefer playing go outside and it rains.<sup>6</sup> Ordering the rest is somewhat difficult, but I come up with the following matrix:</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img src=\"http://img.photobucket.com/albums/v347/Vaniver/Value_Matrix.jpg\" alt=\"\"></p>\n<p>If we walk, I won't enjoy it quite as much as playing Go, but if we play Go in my apartment and it's sunny I'll regret that we didn't play outside. Note that all of the preferences probabilities are fairly high- that's because having a game interrupted by rain is much worse than all of the other outcomes. Calculating the preference value of each deal is easy, since the probability of rain is independent of my decision and .5: I decide that playing Go at Park is the worst option with an effective .5 chance of the best outcome, Go at Home is better with an effective .7 chance of the best outcome, and Walk is best with an effective .75 chance of the best outcome.</p>\n<p>Note that working through everything worked out better for us than doing scenario planning. If I knew it would rain, I would choose Go at Home; if I knew it wouldn't rain, I would choose Go at Park. Walk is dominated by both of those in the case of certainty, but its lower variance means it wins out when I'm very uncertain about whether it will rain or not.<sup>7</sup></p>\n<p>What's going up a level here? Evaluating whether or not I want to do something with this friend this weekend (and another level might be evaluating whether or not I want them as a friend, and so on). When evaluating the prospects, I might want to compare them to whatever I was planning before my friend called to make sure this is actually a better plan. It could be the chance of rain is so high it makes this plan worse than whatever alternatives I had before I knew it was likely to rain.</p>\n<h2 id=\"Managing_a_Store\">Managing a Store<br></h2>\n<p>Suppose I manage a store that sells a variety of products. I decide that I want to maximize profits over the course of a year, plus some estimate of intangibles (like customer satisfaction). I have a number of year-long commitments (the lease on my space, employment contracts for full-time employees, etc.) already made, which I'll consider beyond the scope of the problem. I then have a number of decisions I have to make month-to-month<sup>8</sup> (how many seasonal employees to hire, what products to order, whether to change wages or prices, what hours the store should be open), and then decisions I have to make day-to-day (which employees to schedule, where to place items in the store, where to place employees in the store, how to spend my time).</p>\n<p>I look at the day-to-day decisions and decide that I'm happy modeling those as policies rather than individual decisions- I don't need to have mapped those out now, but I do need to put my January orders in next week. Which policies I adopt might be relevant to my decision, though, and so I still want to model them on that level.</p>\n<p>Well, what about my uncertainties? Employee morale seems like one that'll vary month-to-month or day-to-day, though I'm comfortable modeling it month-to-month, as that's when I would change the employee composition or wages. Customer satisfaction is an uncertainty that seems like it would be worth tracking, and so is customer demand- how the prices I set will influence sales. And I should model demand by item- or maybe just by category of items. Labor costs, inventory costs, and revenue are all nodes that I could stick in as uncertainty nodes (even though they might just deterministically calculate those based on their input nodes).</p>\n<p>You can imagine that the influence diagram I'm sketching is starting to get massive- and I'm just considering this year! I also need to think carefully about how I put the dependencies in this network- should I have customer satisfaction point to customer demand, or the other way around? For uncertainty nodes it doesn't matter much (as we know how to flip them), but may make elicitation easier or harder. For decision nodes, order is critical, as it represents the order the decisions have to be made in.</p>\n<p>But even though the problem is getting massive, I can still use this methodology to solve it. This'll make it easier for me to keep everything in mind- because I won't need to keep everything in <em>mind</em>. I can contemplate the dependencies and uncertainties of the system one piece at a time, record the results, and then integrate them together. Like with the previous example, it's easy to include the results of other people's calculations in your decision problem as a node, meaning that this can be extended to team decisions as well- maybe my marketer does all the elicitation for the demand nodes, and my assistant manager does all the elicitation for the employee nodes, and then I can combine them into one decision-making network.</p>\n<h2 id=\"Newcomb_s_Problem\">Newcomb's Problem</h2>\n<p><a href=\"http://wiki.lesswrong.com/wiki/Newcomb%27s_problem\">Newcomb's Problem</a> is a thought experiment designed to highlight the difference between different decision theories that has <a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">come up a lot</a> on Less Wrong. AnnaSalamon wrote <a href=\"/lw/17b/decision_theory_why_pearl_helps_reduce_could_and/\">an article</a> (that even includes influence diagrams that are much prettier than mine) analyzing Newcomb's Problem, in which she presents three ways to interpret the problem. I won't repeat her analysis, but will make several observations:</p>\n<ol>\n<li>Problem statements, and real life, are often ambiguous or uncertain. Navigating that ambiguity and uncertainty about what problem you're actually facing is a major component of making decisions. It's also a major place for bias to creep in: if you aren't careful about defining your problems, they will be defined in careless ways, which can impose real and large costs in worse solutions. </li>\n<li>It's easy to construct a thought experiment with contradictory premises, and not notice if you keep math and pictures out of it. Draw pictures, show the math. It makes normal problems easier, and helps you notice when a problem boils down to \"could an unstoppable force move an immovable object?\",<sup>9</sup> and <a href=\"/lw/of/dissolving_the_question/\">then you can move on</a>.</li>\n<li>If you're not quite sure which of several interpretations is true, you can model that explicitly. Put an uncertainty node at the top which points to the model where your behavior and Omega's decision are independent and the model where your behavior determines Omega's behavior. Elicit a probability, or calculate what the probability would need to be for you to make one decision and then compare that to how uncertain you are. </li>\n</ol> \n<hr>\n<p>1. This means that there are a bunch of nodes connected by arrows (directed edges), and that there are no cycles (arrows that point in a loop). As a consequence, there are some nodes with no arrows pointing to them and one node with no arrows leaving it (the value node). It's also worth mentioning that influence diagrams are a special case of <a href=\"http://en.wikipedia.org/wiki/Bayesian_network\">Bayesian Networks</a>.</p>\n<p>2. This is the actual prediction for me as of 1:30 PM on 12/14/2011. Link <a href=\"http://www.weather.com/weather/wxdetail/78701?dayNum=3\">here</a>, though that link should become worthless by tomorrow. Also note that I'm assuming that rain will only happen after we start the game of Go / decide where to play, or that the switching costs will be large.</p>\n<p>3. The actual percentage in the data they had collected by 2008 was ~53%, but as it was within the 'well calibrated' region I should use the number at face value.</p>\n<p>4. If it started raining while we were playing outside, we would probably just stop and do something else rather than playing through the rain, but neither of those are attractive prospects.</p>\n<p>5. I strongly recommend <a href=\"http://en.wikipedia.org/wiki/Satisfice\">satisficing</a>, in the AI sense of including the costs of optimizing in your optimization process, rather than maximization. Opportunity costs are real. For a problem with, say, millions of dollars on the line, you'll probably want to spend quite a bit of time trying to come up with other options.</p>\n<p>6. You may have noticed a trend that the best and worst outcome are often paired together. This is more likely in constructed problems, but is a feature of many difficult real-world problems.</p>\n<p>7. We can do something called sensitivity analysis to see how sensitive this result is; if it's very sensitive to a value we elicited, we might go back and see if we can narrow the uncertainty on that value. If it's not very sensitive, then we don't need to worry much about those uncertainties.</p>\n<p>8. This is an artificial constraint- really, you could change any of those policies any day, or even at any time during that day. It's often helpful to chunk continuous periods into a few discrete ones, but only to the degree that your bins carve reality at the joints.</p>\n<p>9. The unstoppable force being Omega's prediction ability, and the immovable object being causality only propagating forward in time. Two-boxers answer \"unmovable object,\" one-boxers answer \"unstoppable force.\"</p>", "sections": [{"title": "Scope", "anchor": "Scope", "level": 1}, {"title": "Model", "anchor": "Model", "level": 1}, {"title": "Elicit", "anchor": "Elicit", "level": 1}, {"title": "Outing with a Friend", "anchor": "Outing_with_a_Friend", "level": 2}, {"title": "Managing a Store", "anchor": "Managing_a_Store", "level": 2}, {"title": "Newcomb's Problem", "anchor": "Newcomb_s_Problem", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "7 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iWH8Tnh4dBkDpCPws", "zFQQEkx4c6bxdshr4", "sP2Hg6uPwpfp3jZJN", "BHYBdijDcAKQ6e45Z", "6ddcsdA2c2XpNpE5x", "miwf7qQTh2HXNnSuq", "Mc6QcrsbH5NRXbCRX"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-15T05:25:52.981Z", "modifiedAt": null, "url": null, "title": "A case study in fooling oneself", "slug": "a-case-study-in-fooling-oneself", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:06.983Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mitchell_Porter", "createdAt": "2009-05-28T02:36:19.394Z", "isAdmin": false, "displayName": "Mitchell_Porter"}, "userId": "fjERoRhgjipqw3z2b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/w7GYWtoRZsizsF8Xk/a-case-study-in-fooling-oneself", "pageUrlRelative": "/posts/w7GYWtoRZsizsF8Xk/a-case-study-in-fooling-oneself", "linkUrl": "https://www.lesswrong.com/posts/w7GYWtoRZsizsF8Xk/a-case-study-in-fooling-oneself", "postedAtFormatted": "Thursday, December 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20case%20study%20in%20fooling%20oneself&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20case%20study%20in%20fooling%20oneself%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw7GYWtoRZsizsF8Xk%2Fa-case-study-in-fooling-oneself%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20case%20study%20in%20fooling%20oneself%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw7GYWtoRZsizsF8Xk%2Fa-case-study-in-fooling-oneself", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw7GYWtoRZsizsF8Xk%2Fa-case-study-in-fooling-oneself", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2181, "htmlBody": "<p><em><strong>Note: This post assumes that the Oxford version of Many Worlds is wrong, and speculates as to why this isn't obvious. For a discussion of the hypothesis itself, see <a href=\"/r/discussion/lw/8vg/problems_of_the_deutschwallace_version_of_many/\">Problems of the Deutsch-Wallace version of Many Worlds</a>. </strong></em></p>\n<p>smk asks <a href=\"/r/discussion/lw/8ue/how_many_worlds/\">how many worlds</a> are produced in a quantum process where the outcomes have unequal probabilities; Emile says <a href=\"/r/discussion/lw/8ue/how_many_worlds/5g82\">there's no exact answer</a>, just like there's no exact answer for how many ink blots are in the messy picture; Tetronian says this analogy is <a href=\"/r/discussion/lw/8ue/how_many_worlds/5g89\">a great way to demonstrate</a> what a \"wrong question\" is; Emile has (at this writing) 9 upvotes, and Tetronian has 7. <br /><br />My thesis is that Emile has instead provided an example of how to dismiss a question and thereby fool oneself; Tetronian provides an example of treating an epistemically destructive technique of dismissal as epistemically virtuous and fruitful; and the upvotes show that this isn't just their problem. [<strong>edit</strong>: <a href=\"/r/discussion/lw/8uy/a_case_study_in_fooling_oneself/5ggy\">Emile</a> and <a href=\"/r/discussion/lw/8uy/a_case_study_in_fooling_oneself/5gfh\">Tetronian</a> respond.] <br /><br />I am as tired as anyone of the debate over Many Worlds. I don't expect the general climate of opinion on this site to change except as a result of new intellectual developments in the larger world of physics and philosophy of physics, which is where the question will be decided anyway. But the mission of Less Wrong is supposed to be the refinement of rationality, and so perhaps this \"case study\" is of interest, not just as another opportunity to argue over the interpretation of quantum mechanics, but as an opportunity to dissect a little bit of irrationality that is not only playing out here and now, but which evidently has a base of support. <br /><br />The question is not just, what's wrong with the argument, but also, how did it get that base of support? How was a situation created where one person says something irrational (or foolish, or however the problem is best understood), and a lot of other people nod in agreement and say, that's an excellent example of how to think? <br /><br />On this occasion, my quarrel is not with the Many Worlds interpretation as such; it is with the version of Many Worlds which says there's no actual number of worlds. Elsewhere in the thread, someone says there are uncountably many worlds, and someone else says there are two worlds. At least those are meaningful answers (although the advocate of \"two worlds\" as the answer, then goes on to say that one world is \"stronger\" than the other, which is meaningless). <br /><br />But the proposition that there is no definite number of worlds, is as foolish and self-contradictory as any of those other contortions from the history of thought that rationalists and advocates of common sense like to mock or boggle at. At times I have wondered how to place Less Wrong in the history of thought; well, this is one way to do it - it can have its own chapter in the history of intellectual folly; it can be known by its mistakes. <br /><br />Then again, this \"mistake\" is not original to Less Wrong. It appears to be one of the defining ideas of the Oxford-based approach to Many Worlds associated with David Deutsch and David Wallace; the other defining idea being the proposal to derive probabilities from rationality, rather than vice versa. (I refer to the attempt to derive the Born rule from arguments about how to behave rationally in the multiverse.) The Oxford version of MWI seems to be very popular among thoughtful non-physicist advocates of MWI - even though I would regard <em>both</em> its defining ideas as nonsense - and it may be that its ideas get a pass here, partly because of their social status. That is, an important faction of LW opinion believes that Many Worlds is the explanation of quantum mechanics, and the Oxford school of MWI has high status and high visibility within the world of MWI advocacy, and so its ideas will receive approbation without much examination or even much understanding, because of the social and psychological mechanisms which incline people to agree with, defend, and laud their favorite authorities, even if they don't really understand what these authorities are saying or why they are saying it. <br /><br />However, it is undoubtedly the case that many of the LW readers who believe there's no definite number of worlds, believe this because the idea genuinely makes sense to them. They aren't just stringing together words whose meaning isn't known, like a Taliban who recites the Quran without knowing a word of Arabic; they've actually thought about this themselves; they have gone through some subjective process as a result of which they have consciously adopted this opinion. So from the perspective of analyzing how it is that people come to hold absurd-sounding views, this should be good news. It means that we're dealing with a genuine failure to reason properly, as opposed to a simple matter of reciting slogans or affirming allegiance to a view on the basis of something other than thought. <br /><br />At a guess, the thought process involved is very simple. These people have thought about the wavefunctions that appear in quantum mechanics, at whatever level of technical detail they can muster; they have decided that the components or substructures of these wavefunctions which might be identified as \"worlds\" or \"branches\" are clearly approximate entities whose definition is somewhat arbitrary or subject to convention; and so they have concluded that there's no definite number of worlds in the wavefunction. And the failure in their thinking occurs when they don't take the next step and say, is this at all consistent with reality? That is, if a quantum world is something whose existence is fuzzy and which doesn't even have a definite multiplicity - that is, we can't even say if there's one, two, or many of them - if those are the properties of a quantum world, then is it possible for the real world to be one of those? It's the failure to ask that last question, and really think about it, which must be the oversight allowing the nonsense-doctrine of \"no definite number of worlds\" to gain a foothold in the minds of otherwise rational people. <br /><br />If this diagnosis is correct, then at some level it's a case of \"treating the map as the territory\" syndrome. A particular conception of the quantum-mechanical wavefunction is providing the \"map\" of reality, and the individual thinker is perhaps making correct statements about what's on their map, but they are failing to check the properties of the map against the properties of the territory. In this case, the property of reality that falsifies the map is, the fact that it definitely exists, or perhaps the corollary of that fact, that something which definitely exists definitely exists at least once, and therefore exists with a definite, objective multiplicity. <br /><br />Trying to go further in the diagnosis, I can identify a few cognitive tendencies which may be contributing. First is the phenomenon of bundled assumptions which have never been made distinct and questioned separately. I suppose that in a few people's heads, there's a rapid movement from \"science (or materialism) is correct\" to \"quantum mechanics is correct\" to \"Many Worlds is correct\" to \"the Oxford school of MWI is correct\". If you are used to encountering all of those ideas together, it may take a while to realize that they are not linked out of logical necessity, but just contingently, by the narrowness of your own experience. <br /><br />Second, it may seem that \"no definite number of worlds\" makes sense to an individual, because when they test their own worldview for semantic coherence, logical consistency, or empirical adequacy, it seems to pass. In the case of \"no-collapse\" or \"no-splitting\" versions of Many Worlds, it seems that it often passes the subjective making-sense test, because the individual is actually relying on ingredients borrowed from the Copenhagen interpretation. A semi-technical example would be the coefficients of a reduced density matrix. In the Copenhagen interpetation, they are probabilities. Because they have the mathematical attributes of probabilities (by this I just mean that they lie between 0 and 1), and because they can be obtained by strictly mathematical manipulations of the quantities composing the wavefunction, Many Worlds advocates tend to treat these quantities as inherently being probabilities, and use their \"existence\" as a way to obtain the Born probability rule from the ontology of \"wavefunction yes, wavefunction collapse no\". But just because something is a real number between 0 and 1, doesn't yet explain how it manages to be a probability. In particular, I would maintain that if you have a multiverse theory, in which all possibilities are actual, then a probability must refer to a <em>frequency</em>. The probability of an event in the multiverse is simply how often it occurs in the multiverse. And clearly, just having the number 0.5 associated with a particular multiverse branch is not yet the same thing as showing that the events in that branch occur half the time. <br /><br />I don't have a good name for this phenomenon, but we could call it \"borrowed support\", in which a belief system receives support from considerations which aren't legitimately its own to claim. (Ayn Rand apparently talked about a similar notion of \"borrowed concepts\".) <br /><br />Third, there is a possibility among people who have a capacity for highly abstract thought, to adopt an ideology, ontology, or \"theory of everything\" which is only expressed in those abstract terms, and to then treat that theory as the whole of reality, in a way that reifies the abstractions. This is a highly specific form of treating the map as the territory, peculiar to abstract thinkers. When someone says that reality is made of numbers, or made of computations, this is at work. In the case at hand, we're talking about a theory of physics, but the ontology of that theory is incompatible with the definiteness of one's own existence. My guess is that the main psychological factor at work here is intoxication with the feeling that one understands reality totally and in its essence. The universe has bowed to the imperial ego; one may not literally direct the stars in their courses, but one has known the essence of things. Combine that intoxication, with \"borrowed support\" and with the simple failure to think hard enough about where on the map the imperial ego itself might be located, and maybe you have a comprehensive explanation of how people manage to believe theories of reality which are flatly inconsistent with the most basic features of subjective experience. <br /><br />I should also say something about Emile's example of the ink blots. I find it rather superficial to just say \"there's no definite number of blots\". To say that the number of blots depends on definition is a lot closer to being true, but that undermines the argument, because that opens the possibility that there is a right definition of \"world\", and many wrong definitions, and that the true number of worlds is just the number of worlds according to the right definition. <br /><br />Emile's picture can be used for the opposite purpose. All we have to do is to scrutinize, more closely, what it actually is. It's a JPEG that is 314 pixels by 410 pixels in size. Each of those pixels will have an exact color coding. So clearly we can be entirely objective in the way we approach this question; all we have to do is be precise in our concepts, and engage with the genuine details of the object under discussion. Presumably the image is a scan of a physical object, but even in that case, we can be precise - it's made of atoms, they are particular atoms, we can make objective distinctions on the basis of contiguity and bonding between these atoms, and so the question will have an objective answer, if we bother to be sufficiently precise. The same goes for \"worlds\" or \"branches\" in a wavefunction. And the truly pernicious thing about this version of Many Worlds is that it prevents such inquiry. The ideology that tolerates vagueness about worlds serves to protect the proposed ontology from necessary scrutiny. <br /><br />The same may be said, on a broader scale, of the practice of \"dissolving a wrong question\". That is a gambit which should be used sparingly and cautiously, because it easily serves to instead justify the dismissal of a legitimate question. A community trained to dismiss questions may never even <em>notice</em> the gaping holes in its belief system, because the lines of inquiry which lead towards those holes are already dismissed as invalid, undefined, unnecessary. smk came to this topic fresh, and without a head cluttered with ideas about what questions are legitimate and what questions are illegitimate, and as a result managed to ask something which more knowledgeable people had already prematurely dismissed from their own minds.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "w7GYWtoRZsizsF8Xk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 60, "baseScore": -14, "extendedScore": null, "score": -7e-06, "legacy": true, "legacyId": "11482", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 79, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["6McLFoTupdCkxH9Nd", "3gMc33BfXB3HotLkm"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-15T16:25:15.443Z", "modifiedAt": null, "url": null, "title": "Checking Recent Upvotes?", "slug": "checking-recent-upvotes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:57.659Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yZiTTnfzf4RHR9SuA/checking-recent-upvotes", "pageUrlRelative": "/posts/yZiTTnfzf4RHR9SuA/checking-recent-upvotes", "linkUrl": "https://www.lesswrong.com/posts/yZiTTnfzf4RHR9SuA/checking-recent-upvotes", "postedAtFormatted": "Thursday, December 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Checking%20Recent%20Upvotes%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AChecking%20Recent%20Upvotes%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyZiTTnfzf4RHR9SuA%2Fchecking-recent-upvotes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Checking%20Recent%20Upvotes%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyZiTTnfzf4RHR9SuA%2Fchecking-recent-upvotes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyZiTTnfzf4RHR9SuA%2Fchecking-recent-upvotes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 41, "htmlBody": "<p>Periodically get Karma and its not from recent posts, and I'd kinda like to know what it was I said that people thought was cool. Is there a way to sort my stuff my \"most recently upvoted\" or something like that?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yZiTTnfzf4RHR9SuA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 14, "extendedScore": null, "score": 8.155226900830006e-07, "legacy": true, "legacyId": "11485", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 31, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-15T18:23:55.946Z", "modifiedAt": null, "url": null, "title": "How to label thoughts nonverbally", "slug": "how-to-label-thoughts-nonverbally", "viewCount": null, "lastCommentedAt": "2017-06-17T04:27:39.679Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "eugman", "createdAt": "2009-09-28T01:40:39.582Z", "isAdmin": false, "displayName": "eugman"}, "userId": "rtJy8Y9zXRpjWmeMi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YmPJdx6SYyTFYBPWL/how-to-label-thoughts-nonverbally", "pageUrlRelative": "/posts/YmPJdx6SYyTFYBPWL/how-to-label-thoughts-nonverbally", "linkUrl": "https://www.lesswrong.com/posts/YmPJdx6SYyTFYBPWL/how-to-label-thoughts-nonverbally", "postedAtFormatted": "Thursday, December 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20label%20thoughts%20nonverbally&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20label%20thoughts%20nonverbally%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYmPJdx6SYyTFYBPWL%2Fhow-to-label-thoughts-nonverbally%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20label%20thoughts%20nonverbally%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYmPJdx6SYyTFYBPWL%2Fhow-to-label-thoughts-nonverbally", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYmPJdx6SYyTFYBPWL%2Fhow-to-label-thoughts-nonverbally", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1404, "htmlBody": "<h2>Introduction<br /></h2>\n<p>Recently I've been attempting to put a damper on my <a href=\"http://en.wikipedia.org/wiki/Rumination_%28psychology%29\">ruminations</a>. Sometimes they can get out of control and be somewhat self-sustaining. These negative, repetitive thoughts can be harmful and make depression worse. Accordingly, I've been looking for techniques to help manage this.</p>\n<p>In a previous <a href=\"/lw/8ud/tracking_emotions_with_kinesthetic_memory/\">post</a>, I talked about how I stumbled onto a way to track emotions through kinesthetic memory. Specifically, I am using a checkmark gesture to mark negative thoughts, and at this point it has become semiautomatic. The results have been quite interesting.</p>\n<p>A little later, someone wanted to know about the process in detail. My first impulse was to shout \"<a href=\"http://en.wikipedia.org/wiki/Mu_%28negative%29\">MU!</a>\". It seems contradictory to try to use words to describe something that is inherently and <em>intentionally</em> a nonverbal process. However, I decided that is was worth a shot. Below, I cover the skills that you will need, how the process works, and my personal experiences with it.</p>\n<p><a id=\"more\"></a></p>\n<h2>Skill building</h2>\n<p>So, in order for this process to work, you need to develop three related but separate skills. First, you have to be able to tell when you are thinking/feeling. Second you have to be able to label these thoughts and emotions. Third is something I call the Pivot. It's changing the direction of your thoughts without mentally moving forward. Each of these are not natural skills, so don't be discouraged if you need a lot of practice.</p>\n<p>Basic meditation is perfect for noticing what you are thinking. Starting out, catching thoughts is the hardest thing to do. In my experience, there are a surprising number of things that you think without even being aware of it. What's worse is most of them are quite fleeting. Until you get very good at catching fleeting thoughts, this process probably won't work for you.</p>\n<p>Besides meditation, mindfulness practice helps too. The next time you are eating a meal, try to focus just on the food itself. Your thoughts are likely to wonder, or you may even end up checking Facebook on your phone. When you notice this, smile. You just caught yourself, congratulations. One last technique is to sit with a tablet and just write thoughts as they come to you. This is slow, but good starting practice.</p>\n<p>The second step is to start being able to label your thoughts and emotions. At first this may be difficult, and I would suggest simply trying to label them as positive, negative, or neutral. The three techniques I used personally were <a href=\"http://www.amazon.com/dp/1572244739/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">CBT</a>, straight-up practice, and monitoring my emotional <a href=\"/lw/20l/ureshiku_naritai/\">set point</a> for a few weeks. With the CBT, I would first experience an emotional trigger (maybe I forgot to send an email). Then I would write down the trigger and any related thoughts I was having. Then I would identify any <a href=\"http://en.wikipedia.org/wiki/Cognitive_distortion\">cognitive distortions</a> related to these thoughts.</p>\n<p>The final skill is difficult for me to describe by it's very nature. Whenever you are practicing meditation or mindfulness, your thoughts will naturally wander. At this point, you are supposed to <em>gently</em> guide yourself back to your original focus. It is this mental muscle that is being used in the Pivot. To use this muscle, you have to learn how to act without thought. Otherwise you'll always be thinking of <a href=\"http://en.wikipedia.org/wiki/White_Bear_Principle\">white polar bears</a>.</p>\n<p>The Pivot is a matter of completely changing your thoughts without thinking about it. It's a way of stopping where you are going, turning 90 degrees and continuing onward. I really don't know how to describe it any better than that. I can tell you that it was feverishly hard for me to develop as a skill. It's a lot like a Chinese finger trap. The more you struggle, the less it works. That's why the gesture works so well. It is a very gentle way of initiating this.</p>\n<h2>The process</h2>\n<p>So, now having the skill base, you can try making use of this technique. First you need a type of thought or feeling you want to monitor. It should be something easily recognizable. It doesn't matter if there's <a href=\"/lw/1za/the_spotlight/\">no word for it</a>. The idea here is to reduce the cognitive load as much as possible. If you are thinking to yourself \"Does this count as anger?\" you are doing it wrong. If you are thinking \"I feel so...'GRRR' \", then you are getting closer. Remember, the goal is to label without verbally thinking.</p>\n<p>Then you need a gesture that you can do anywhere, unobtrusively. A checkmark, a circle, or an X all work fine. Whenever you run into the type of thought you are trying to monitor, you want to make that gesture. If you are iffy on whether or not something counts, I'd say just throw it in. You'll get better at making distinctions as you practice. Additionally, we want to make this an automatic process.</p>\n<p>Finally, you'll want to do something whenever you notice this thought/feeling. If a positive feeling, it may be to try to extend it. You may want to bring it up to your conscious system for processing the thoughts intentionally. If it's a negative thought spiral, you may just want to pivot right out of it.</p>\n<h2>The results</h2>\n<p>The results have been promising so far. There are two tags that I've tried using. One was a circle for buoyant happiness. It's this sort of uplifting sense of happiness that feels like the rising crescendo in a song. I could extend the feeling slightly by continuing the gesture and and focusing on the feeling. I believe that if I had continued to build the association, I could summon the feeling at will. However, I didn't have the motivation to keep using it.</p>\n<p>The other gesture I used is a checkmark gesture. I don't use it to track emotions specifically, but more emotional triggers. These triggers feel like mental mosquito bites. They are really small things, sometimes hard to catch, sometimes really painful, and after a while they all add up. Usually these triggers have an automatic negative thought associated with them, but some are nonverbal.</p>\n<p>Now, a quick word of warning. If you are using this technique to deal with negative thoughts, be very careful. You don't want to create a general association and have it backfire. I once took an arbitrary word and tried to use it as a short circuit for a very specific type of thought I desperately wanted to avoid. It completely backfired and acted as a sort of key code for the ENTIRE subset.</p>\n<p>That's why I would suggest that you a) aim for broad targets, b) make categorization as easy as possible, and c) don't think about it. By aiming for reflexive and nonverbal, it's possible to avoid creating a specific association.</p>\n<p>So, if I had to guess, I'd say that there are two effects going on with this technique. First, I think the gesture acts as some sort of minimal amount of <a href=\"/lw/2yd/selfempathy_as_a_source_of_willpower/\">self-empathy</a> or <a href=\"/lw/5xx/overcoming_suffering_emotional_acceptance/\">emotional acceptance</a>. It's a way of acknowledging the part of my brain that's screaming \"You remember when something bad happened with something related to that?\". Once that's been done, it's easier for me to move on.</p>\n<p>The other effect is that it seems to function as some sort of intra-brain communication. Now, this makes a lot of sense if you believe in a <a href=\"/tag/whyeveryonehypocrite/\">modular</a> model of the brain. But from the inside it's weird as hell. There are now times where I'll make the gesture and then be surprised about it. At the same time, I feel responsible for the action and can generally recall intending to do it. The issue is that it's sort of done with a background process of my mind, just like walking. It feels like one part of my brain can't communicate very well with another part of my brain, so it reroutes the information through physical action.</p>\n<p>Despite any weirdness, I've found it to a useful habit so far. It has had a similar effect to using <a href=\"http://en.wikipedia.org/wiki/Getting_Things_Done\">GTD</a> to clear my brain of annoying thoughts by acknowledging their importance. Also, I can tell if I'm in a bad mood if my finger keeps going off every few seconds. Usually that means I need a nap or some coffee.</p>\n<p>Finally, it seems to act as an awesome Geiger counter for hidden ugh fields. There are some things that are too hard to even think about contemplating, and this offers a way of finding those things. As example, I might be on someone's Facebook page and my finger will go off and I'll think \"Oh, there must be something I'm trying to avoid.\"</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"AiNyf5iwbpc7mehiX": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YmPJdx6SYyTFYBPWL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 36, "baseScore": 46, "extendedScore": null, "score": 0.0005384403465089948, "legacy": true, "legacyId": "11484", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 35, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Introduction\">Introduction<br></h2>\n<p>Recently I've been attempting to put a damper on my <a href=\"http://en.wikipedia.org/wiki/Rumination_%28psychology%29\">ruminations</a>. Sometimes they can get out of control and be somewhat self-sustaining. These negative, repetitive thoughts can be harmful and make depression worse. Accordingly, I've been looking for techniques to help manage this.</p>\n<p>In a previous <a href=\"/lw/8ud/tracking_emotions_with_kinesthetic_memory/\">post</a>, I talked about how I stumbled onto a way to track emotions through kinesthetic memory. Specifically, I am using a checkmark gesture to mark negative thoughts, and at this point it has become semiautomatic. The results have been quite interesting.</p>\n<p>A little later, someone wanted to know about the process in detail. My first impulse was to shout \"<a href=\"http://en.wikipedia.org/wiki/Mu_%28negative%29\">MU!</a>\". It seems contradictory to try to use words to describe something that is inherently and <em>intentionally</em> a nonverbal process. However, I decided that is was worth a shot. Below, I cover the skills that you will need, how the process works, and my personal experiences with it.</p>\n<p><a id=\"more\"></a></p>\n<h2 id=\"Skill_building\">Skill building</h2>\n<p>So, in order for this process to work, you need to develop three related but separate skills. First, you have to be able to tell when you are thinking/feeling. Second you have to be able to label these thoughts and emotions. Third is something I call the Pivot. It's changing the direction of your thoughts without mentally moving forward. Each of these are not natural skills, so don't be discouraged if you need a lot of practice.</p>\n<p>Basic meditation is perfect for noticing what you are thinking. Starting out, catching thoughts is the hardest thing to do. In my experience, there are a surprising number of things that you think without even being aware of it. What's worse is most of them are quite fleeting. Until you get very good at catching fleeting thoughts, this process probably won't work for you.</p>\n<p>Besides meditation, mindfulness practice helps too. The next time you are eating a meal, try to focus just on the food itself. Your thoughts are likely to wonder, or you may even end up checking Facebook on your phone. When you notice this, smile. You just caught yourself, congratulations. One last technique is to sit with a tablet and just write thoughts as they come to you. This is slow, but good starting practice.</p>\n<p>The second step is to start being able to label your thoughts and emotions. At first this may be difficult, and I would suggest simply trying to label them as positive, negative, or neutral. The three techniques I used personally were <a href=\"http://www.amazon.com/dp/1572244739/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">CBT</a>, straight-up practice, and monitoring my emotional <a href=\"/lw/20l/ureshiku_naritai/\">set point</a> for a few weeks. With the CBT, I would first experience an emotional trigger (maybe I forgot to send an email). Then I would write down the trigger and any related thoughts I was having. Then I would identify any <a href=\"http://en.wikipedia.org/wiki/Cognitive_distortion\">cognitive distortions</a> related to these thoughts.</p>\n<p>The final skill is difficult for me to describe by it's very nature. Whenever you are practicing meditation or mindfulness, your thoughts will naturally wander. At this point, you are supposed to <em>gently</em> guide yourself back to your original focus. It is this mental muscle that is being used in the Pivot. To use this muscle, you have to learn how to act without thought. Otherwise you'll always be thinking of <a href=\"http://en.wikipedia.org/wiki/White_Bear_Principle\">white polar bears</a>.</p>\n<p>The Pivot is a matter of completely changing your thoughts without thinking about it. It's a way of stopping where you are going, turning 90 degrees and continuing onward. I really don't know how to describe it any better than that. I can tell you that it was feverishly hard for me to develop as a skill. It's a lot like a Chinese finger trap. The more you struggle, the less it works. That's why the gesture works so well. It is a very gentle way of initiating this.</p>\n<h2 id=\"The_process\">The process</h2>\n<p>So, now having the skill base, you can try making use of this technique. First you need a type of thought or feeling you want to monitor. It should be something easily recognizable. It doesn't matter if there's <a href=\"/lw/1za/the_spotlight/\">no word for it</a>. The idea here is to reduce the cognitive load as much as possible. If you are thinking to yourself \"Does this count as anger?\" you are doing it wrong. If you are thinking \"I feel so...'GRRR' \", then you are getting closer. Remember, the goal is to label without verbally thinking.</p>\n<p>Then you need a gesture that you can do anywhere, unobtrusively. A checkmark, a circle, or an X all work fine. Whenever you run into the type of thought you are trying to monitor, you want to make that gesture. If you are iffy on whether or not something counts, I'd say just throw it in. You'll get better at making distinctions as you practice. Additionally, we want to make this an automatic process.</p>\n<p>Finally, you'll want to do something whenever you notice this thought/feeling. If a positive feeling, it may be to try to extend it. You may want to bring it up to your conscious system for processing the thoughts intentionally. If it's a negative thought spiral, you may just want to pivot right out of it.</p>\n<h2 id=\"The_results\">The results</h2>\n<p>The results have been promising so far. There are two tags that I've tried using. One was a circle for buoyant happiness. It's this sort of uplifting sense of happiness that feels like the rising crescendo in a song. I could extend the feeling slightly by continuing the gesture and and focusing on the feeling. I believe that if I had continued to build the association, I could summon the feeling at will. However, I didn't have the motivation to keep using it.</p>\n<p>The other gesture I used is a checkmark gesture. I don't use it to track emotions specifically, but more emotional triggers. These triggers feel like mental mosquito bites. They are really small things, sometimes hard to catch, sometimes really painful, and after a while they all add up. Usually these triggers have an automatic negative thought associated with them, but some are nonverbal.</p>\n<p>Now, a quick word of warning. If you are using this technique to deal with negative thoughts, be very careful. You don't want to create a general association and have it backfire. I once took an arbitrary word and tried to use it as a short circuit for a very specific type of thought I desperately wanted to avoid. It completely backfired and acted as a sort of key code for the ENTIRE subset.</p>\n<p>That's why I would suggest that you a) aim for broad targets, b) make categorization as easy as possible, and c) don't think about it. By aiming for reflexive and nonverbal, it's possible to avoid creating a specific association.</p>\n<p>So, if I had to guess, I'd say that there are two effects going on with this technique. First, I think the gesture acts as some sort of minimal amount of <a href=\"/lw/2yd/selfempathy_as_a_source_of_willpower/\">self-empathy</a> or <a href=\"/lw/5xx/overcoming_suffering_emotional_acceptance/\">emotional acceptance</a>. It's a way of acknowledging the part of my brain that's screaming \"You remember when something bad happened with something related to that?\". Once that's been done, it's easier for me to move on.</p>\n<p>The other effect is that it seems to function as some sort of intra-brain communication. Now, this makes a lot of sense if you believe in a <a href=\"/tag/whyeveryonehypocrite/\">modular</a> model of the brain. But from the inside it's weird as hell. There are now times where I'll make the gesture and then be surprised about it. At the same time, I feel responsible for the action and can generally recall intending to do it. The issue is that it's sort of done with a background process of my mind, just like walking. It feels like one part of my brain can't communicate very well with another part of my brain, so it reroutes the information through physical action.</p>\n<p>Despite any weirdness, I've found it to a useful habit so far. It has had a similar effect to using <a href=\"http://en.wikipedia.org/wiki/Getting_Things_Done\">GTD</a> to clear my brain of annoying thoughts by acknowledging their importance. Also, I can tell if I'm in a bad mood if my finger keeps going off every few seconds. Usually that means I need a nap or some coffee.</p>\n<p>Finally, it seems to act as an awesome Geiger counter for hidden ugh fields. There are some things that are too hard to even think about contemplating, and this offers a way of finding those things. As example, I might be on someone's Facebook page and my finger will go off and I'll think \"Oh, there must be something I'm trying to avoid.\"</p>\n<p>&nbsp;</p>", "sections": [{"title": "Introduction", "anchor": "Introduction", "level": 1}, {"title": "Skill building", "anchor": "Skill_building", "level": 1}, {"title": "The process", "anchor": "The_process", "level": 1}, {"title": "The results", "anchor": "The_results", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "28 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["fQ7fXwFzwpdaNgNcL", "xnPFYBuaGhpq869mY", "Zstm38omrpeu7iWeS", "22HfpjsydDS2A6JhH", "tNnhxNYcXYdJYtQRh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-15T22:11:04.211Z", "modifiedAt": null, "url": null, "title": "Rationality Verification Opportunity?", "slug": "rationality-verification-opportunity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:07.606Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "beoShaffer", "createdAt": "2011-05-29T15:52:29.240Z", "isAdmin": false, "displayName": "beoShaffer"}, "userId": "589WwYp3jytZqATFL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BnHM3a3zkFfW5jAFg/rationality-verification-opportunity", "pageUrlRelative": "/posts/BnHM3a3zkFfW5jAFg/rationality-verification-opportunity", "linkUrl": "https://www.lesswrong.com/posts/BnHM3a3zkFfW5jAFg/rationality-verification-opportunity", "postedAtFormatted": "Thursday, December 15th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20Verification%20Opportunity%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20Verification%20Opportunity%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBnHM3a3zkFfW5jAFg%2Frationality-verification-opportunity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20Verification%20Opportunity%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBnHM3a3zkFfW5jAFg%2Frationality-verification-opportunity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBnHM3a3zkFfW5jAFg%2Frationality-verification-opportunity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 379, "htmlBody": "<p>One of the challenges of <a href=\"http://wiki.lesswrong.com/wiki/Problem_of_verifying_rationality\">rationality verification</a> is that most people who are willing to contribute personal data for it are already familiar with the&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">techniques</span>&nbsp;involved. &nbsp;This makes it difficult to tell if their performance on any form of rationality test is due to their training or their innate abilities. &nbsp;Does the start of a new sequence present a way around this for that sequence's content?</p>\n<p>I believe that it might, and will propose some ideas on how we can take advantage of these opportunities. &nbsp;But first I would suggest that you try to <a href=\"/lw/ka/hold_off_on_proposing_solutions\">think through the problem for yourself</a>&nbsp;(I know this is slightly different from what is talked about in that post, but I think the principle holds).</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Did you think through the general problem of rationality verification for new&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">sequences</span>&nbsp;before thinking of any solutions. Did you then think of your own solutions before getting your mind contaminated with mine? &nbsp;If yes, good. &nbsp;If no, not so good.</p>\n<p>&nbsp;</p>\n<p>If we had good measures of general rationality that could be retaken by the same person multiple times without losing&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">reliability&nbsp;</span>we could simply ask LWers to take them at various intervals and see if they improved after reading the new sequence. &nbsp;As that is not the case I suspect we would have to create specific measures for each sequence. &nbsp;It seems that most writers have a decent idea of what benefits they expect people to gain from their sequences' so perhaps they could try to come up with specific measures for the things that their sequences are supposed to improve. &nbsp;Then before running the sequence main sequence they could put out a call for people to complete these measures and send them in. &nbsp;They could then collect the data again from people who have read the completed sequence, preferably after they have had enough time to practice the material, but not long enough to have had to many other life changes. The necessity and viability of having additional experimental controls&nbsp;will vary between sequences. &nbsp;But I think we will&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">generally</span>&nbsp;be fine with a simple before and after picture.&nbsp;</p>\n<p>While there are some time and talent limitations I would be willing to help with creating the measures, collecting and&nbsp;<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px;\">interpreting&nbsp;</span>the data and any other necessary steps.</p>\n<p>I declare Crocker's rules on the content and style of this post. &nbsp;This includes the title.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BnHM3a3zkFfW5jAFg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 2, "extendedScore": null, "score": 8.156493438290299e-07, "legacy": true, "legacyId": "11481", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["uHYYA32CKgKT3FagE"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-16T00:11:56.284Z", "modifiedAt": null, "url": null, "title": "Do men have more partners than women?", "slug": "do-men-have-more-partners-than-women", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:59.687Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DuncanS", "createdAt": "2010-05-09T00:14:01.228Z", "isAdmin": false, "displayName": "DuncanS"}, "userId": "2drkoghBmtdeCwptH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WnYSE5pQhFGNpXkMp/do-men-have-more-partners-than-women", "pageUrlRelative": "/posts/WnYSE5pQhFGNpXkMp/do-men-have-more-partners-than-women", "linkUrl": "https://www.lesswrong.com/posts/WnYSE5pQhFGNpXkMp/do-men-have-more-partners-than-women", "postedAtFormatted": "Friday, December 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Do%20men%20have%20more%20partners%20than%20women%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADo%20men%20have%20more%20partners%20than%20women%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWnYSE5pQhFGNpXkMp%2Fdo-men-have-more-partners-than-women%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Do%20men%20have%20more%20partners%20than%20women%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWnYSE5pQhFGNpXkMp%2Fdo-men-have-more-partners-than-women", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWnYSE5pQhFGNpXkMp%2Fdo-men-have-more-partners-than-women", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 314, "htmlBody": "<p>Today I came across an article in the Telegraph that states that an average man has<a title=\"9 sexual partners in his lifetime, whilst a woman has only 4.\" href=\"http://www.telegraph.co.uk/relationships/sexual-health-and-advice/8958520/Average-man-has-9-sexual-partners-in-lifetime-women-have-4.html\" target=\"_blank\"> 9 sexual partners in his lifetime, whilst a woman has only 4. </a></p>\n<p>Let's assume for a moment that in fact all these men and women are heterosexuals. In that case, each partnership contains one man and one woman. So, in total, the number of partnerships that women enter into is exactly the same as the number of partnerships that men enter into. Given a few other pretty well-known facts - that men are roughly as numerous as women, and live roughly as long, we deduce that on average men enter into roughly as many sexual partnerships as women.</p>\n<p>There is, of course, a potential flaw in this - we know not all partnerships are in fact heterosexual. We know that heterosexual partnerships must have as many female participants as male, but we know no such thing about homosexual ones. Perhaps homosexual men have many, many more partnerships than heterosexual men, or women of any inclination? Whilst there is some evidence in favour of this concept, I don't really think it's going to be a big enough effect to skew the entire ratio this much. In order to have a ration of 9 to 4, most male partnerships would have to be homosexual ones.</p>\n<p>A better theory is that the data is not very good. Perhaps men tend to exaggerate the number of sexual partners they have, even in anonymous surveys. Perhaps women tend to emphasise their degree of virginity. Perhaps the distributions are different - perhaps for example, most women have fewer partners than most men, but some women have lots? And perhaps this sort of information is lost in the survey?</p>\n<p>Probably the only safe conclusion we can draw from these figures is that journalists and statistics don't mix.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WnYSE5pQhFGNpXkMp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 12, "extendedScore": null, "score": 8.156936198364003e-07, "legacy": true, "legacyId": "11486", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 29, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-16T03:03:12.521Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] But There's Still A Chance, Right?", "slug": "seq-rerun-but-there-s-still-a-chance-right", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:01.009Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sgohEEWWC65awhyiT/seq-rerun-but-there-s-still-a-chance-right", "pageUrlRelative": "/posts/sgohEEWWC65awhyiT/seq-rerun-but-there-s-still-a-chance-right", "linkUrl": "https://www.lesswrong.com/posts/sgohEEWWC65awhyiT/seq-rerun-but-there-s-still-a-chance-right", "postedAtFormatted": "Friday, December 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20But%20There's%20Still%20A%20Chance%2C%20Right%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20But%20There's%20Still%20A%20Chance%2C%20Right%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsgohEEWWC65awhyiT%2Fseq-rerun-but-there-s-still-a-chance-right%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20But%20There's%20Still%20A%20Chance%2C%20Right%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsgohEEWWC65awhyiT%2Fseq-rerun-but-there-s-still-a-chance-right", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsgohEEWWC65awhyiT%2Fseq-rerun-but-there-s-still-a-chance-right", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 220, "htmlBody": "<p>Today's post, <a href=\"/lw/ml/but_theres_still_a_chance_right/\">But There's Still A Chance, Right?</a> was originally published on 06 January 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Sometimes, you calculate the probability of a certain event and find that the number is so unbelievably small that your brain really can't keep track of how small it is, any more than you can spot an individual grain of sand on a beach from 100 meters off. But, because you're already thinking about that event enough to calculate the probability of it, it feels like it's still worth keeping track of. It's not.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/8uf/seq_rerun_a_failed_justso_story/\">A Failed Just-So Story</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sgohEEWWC65awhyiT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 12, "extendedScore": null, "score": 8.157563661781233e-07, "legacy": true, "legacyId": "11498", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["q7Me34xvSG3Wm97As", "7aHL8Z6NncT2N49LJ", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-16T05:51:55.864Z", "modifiedAt": null, "url": null, "title": "A discarded review of 'Godel, Escher Bach: an Eternal Golden Braid'", "slug": "a-discarded-review-of-godel-escher-bach-an-eternal-golden", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:05.501Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/X9mvw4qRyLt7Gqonx/a-discarded-review-of-godel-escher-bach-an-eternal-golden", "pageUrlRelative": "/posts/X9mvw4qRyLt7Gqonx/a-discarded-review-of-godel-escher-bach-an-eternal-golden", "linkUrl": "https://www.lesswrong.com/posts/X9mvw4qRyLt7Gqonx/a-discarded-review-of-godel-escher-bach-an-eternal-golden", "postedAtFormatted": "Friday, December 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20discarded%20review%20of%20'Godel%2C%20Escher%20Bach%3A%20an%20Eternal%20Golden%20Braid'&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20discarded%20review%20of%20'Godel%2C%20Escher%20Bach%3A%20an%20Eternal%20Golden%20Braid'%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX9mvw4qRyLt7Gqonx%2Fa-discarded-review-of-godel-escher-bach-an-eternal-golden%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20discarded%20review%20of%20'Godel%2C%20Escher%20Bach%3A%20an%20Eternal%20Golden%20Braid'%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX9mvw4qRyLt7Gqonx%2Fa-discarded-review-of-godel-escher-bach-an-eternal-golden", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FX9mvw4qRyLt7Gqonx%2Fa-discarded-review-of-godel-escher-bach-an-eternal-golden", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1050, "htmlBody": "<p>Recently I began to write a review of Hofstadter's <em><a href=\"http://www.amazon.com/G%C3%B6del-Escher-Bach-Eternal-Golden/dp/0465026567/\">Godel, Escher, Bach</a></em>, until I realized that the book defied summary more than all the other books I had previously said \"defied summary.\" Thus, I gave up on reviewing the book after not too long. I present my discarded review below just in case it motivates someone <em>else</em>&nbsp;to pick up this masterful tome and let it enrich their life.</p>\n<p><a id=\"more\"></a></p>\n<p>Of Hofstadter's&nbsp;<em>GEB</em>, Eliezer once <a href=\"http://yudkowsky.net/obsolete/bookshelf.html#lc_geb\">wrote</a>:</p>\n<blockquote>\n<p>This is simply the best and most beautiful book ever written by the human species...</p>\n<p>I'm not alone in this opinion, by the way. For one thing, <em>G&ouml;del, Escher, Bach</em> won a Pulitzer Prize. Or just pick a random scientist and ask ver what vis favorite book is, and 1 out of 5 will say: \"<em>G&ouml;del, Escher, Bach</em>\". No other book even comes close.</p>\n<p>It is saddening to contemplate that every day, 150,000 humans die without reading what is indisputably one of the greatest achievements of our species. Don't let it happen to you.</p>\n<p>Sure, if you're just an average person, you might not understand everything in this book - but when you're done reading, you won't be an average person any more.</p>\n</blockquote>\n<p>It's easy to see <em>GEB</em>'s effect on Eliezer's writing: the \"concrete, then abstract\" pattern, the koans, the puzzles, the conversational coverage of technical concepts in math and computer science... it's all here in spades in <em>GEB</em>.</p>\n<p>&nbsp;</p>\n<h4>What <em>GEB</em>&nbsp;Is</h4>\n<p>In the preface to the 20th anniversary edition, Hofstadter clarifies what <em>GEB</em>&nbsp;is and is not. It is not about how reality is \"a system of interconnected braids.\" It is not about how \"math, art, and music are really all the same thing at their core.\" Instead, says Hofstadter:</p>\n<blockquote>\n<p><em>GEB</em>&nbsp;is a very personal attempt to say how it is that animate beings can come out of inanimate matter... <em>GEB</em>&nbsp;approaches [this question] by slowly building up an analogy that likens inanimate molecules to meaningless symbols, and further likens selves... to certain special swirly, twisty, vortex-like, and <em>meaningful</em>&nbsp;patterns that arise only in particular types of systems of meaningless symbols. It is these strange, twisty patterns that the book spends so much time on, because they are little known, little appreciated, counterintuitive, and quite filled with mystery [that] I call... \"strange loops\"...</p>\n<p>...the Godelian strange loop that arises in formal systems in mathematics... is a loop that allows such systems to \"perceive itself,\" to talk about itself, to become \"self-aware,\" and in a sense it would not be going too far to say that by virtue of having such a loop, a formal system <em>acquires a self</em>.</p>\n<p>...the shift of focus from material components [of the human mind] to abstract patterns allows the [surprising] leap from inanimate to animate, from nonsemantic to semantic, from meaningless to meaningful, to take place. But how does that happen? After all, not <em>all</em>&nbsp;jumps from matter to pattern give rise to consciousness or soul or self... What kind of pattern is it, then, that is the telltale mark of a <em>self</em>? <em>GEB</em>'s answer is: a strange loop.</p>\n<p>The irony is that the first strange loop ever found... was found in a system <em>tailor-made to keep loopiness out</em>... Bertrand Russell and Alfred North Whitehead's famous treatise <em>Principia Mathematica</em>...</p>\n<p>...For the French, the enemy was Germany; for Russell, it was self-reference. Russell believed that for a mathematical system to be able to talk about itself in any way whatsoever was the kiss of death, for self-reference would... necessarily open the door to self-contradiction...</p>\n<p>Kurt Godel realized that... self-reference not only had lurked from Day One in <em>Principia Mathematica</em>, but in fact plagued poor <em>PM</em>&nbsp;in a totally unremovable manner. Moreover, as Godel made brutally clear, this thorough riddling of the system by self-reference was not due to some weakness in <em>PM</em>, but quite to the contrary, it was due to its <em>strength</em>. Any similar system would have exactly the same \"defect.\"</p>\n<p>[Godel had discovered that] any formal system designed to spew forth truths about \"mere\" numbers would also wind up spewing forth truths... about its own properties, and would thereby become \"self-aware,\" in a manner of speaking.</p>\n<p>[But] strange loops are an abstract structure that crop up in various media and in varying degrees of richness.</p>\n</blockquote>\n<p>&nbsp;</p>\n<h4>A Musico-Logical Offering</h4>\n<p>Hofstadter opens with the story of J.S. Bach's <em>Musical Offering</em>&nbsp;for King Frederick, which contains a particular canon that sneakily shifts from one key to another before its apparent conclusion, and when this modulation is repeated 6 times, the piece ends up at the original key but one octave higher. This is our first example of a \"Strange Loop\":</p>\n<blockquote>\n<p>The \"Strange Loop\" phenomenon occurs whenever, by moving upwards (or downwards) through the levels of some heirarchical system, we unexpectedly find ourselves right back where we started. (Here, the system is that of musical keys.)</p>\n</blockquote>\n<p>Other examples occur in the drawings of M.C. Escher, for example <a href=\"http://2.bp.blogspot.com/-sCwJVYudavM/TY8PU3IoMbI/AAAAAAAAEY8/u8ClzTFu79A/s1600/11+Waterfall.jpg\">this famous one</a>.</p>\n<p>The liar's paradox (e.g. \"This statement is false\") is a one-step Strange Loop. Related to this is a Strange Loop found in the proof for Godel's Incompleteness Theorem, which states, roughly:</p>\n<blockquote>\n<p>All consistent axiomatic formulations of number theory include undecidable propositions.</p>\n</blockquote>\n<p>Before Godel, Russell and Whitehead tried to banish Strange Loops from set theory in <em>Principia Mathematica</em>. But Godel's theorem showed</p>\n<blockquote>\n<p>...not only that there were irreparable \"holes\" in the axiomatic system proposed by Russell and Whitehead, but more generally, that no axiomatic system whatsoever could produce all number-theoretical truths, unless it were an inconsistent system!</p>\n</blockquote>\n<p>The goal of the book is to explain these Strange Loops in more detail, and how they may explain how animate beings arise from inanimate matter.</p>\n<p>&nbsp;</p>\n<h4>Meaning and Form in Mathematics</h4>\n<p>After a tutorial on formal systems, Hofstadter argues that</p>\n<blockquote>\n<p>...symbols of a formal system, though initially without meaning, cannot avoid taking on \"meaning\" of sorts, at least if an isomorphism is found.</p>\n</blockquote>\n<p>The vast majority of interpretations for a formal system are meaningless, but if an isomorphism can be found between the formal system and some piece of reality, that isomorphism provides the symbols their \"meaning.\"</p>\n<p>But you may discover multiple isomorphisms, and thus the symbols of a formal system may have multiple meanings. It makes no sense to ask, \"But which one is <em>the</em>&nbsp;meaning of the string?\":</p>\n<blockquote>\n<p>An interpretation will be meaningful to the extent that it accurately reflects some isomorphism to the real world.</p>\n</blockquote>\n<p>...</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"AJDHQ4mFnsNbBzPhT": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "X9mvw4qRyLt7Gqonx", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 12, "extendedScore": null, "score": 2.5e-05, "legacy": true, "legacyId": "11203", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Recently I began to write a review of Hofstadter's <em><a href=\"http://www.amazon.com/G%C3%B6del-Escher-Bach-Eternal-Golden/dp/0465026567/\">Godel, Escher, Bach</a></em>, until I realized that the book defied summary more than all the other books I had previously said \"defied summary.\" Thus, I gave up on reviewing the book after not too long. I present my discarded review below just in case it motivates someone <em>else</em>&nbsp;to pick up this masterful tome and let it enrich their life.</p>\n<p><a id=\"more\"></a></p>\n<p>Of Hofstadter's&nbsp;<em>GEB</em>, Eliezer once <a href=\"http://yudkowsky.net/obsolete/bookshelf.html#lc_geb\">wrote</a>:</p>\n<blockquote>\n<p>This is simply the best and most beautiful book ever written by the human species...</p>\n<p>I'm not alone in this opinion, by the way. For one thing, <em>G\u00f6del, Escher, Bach</em> won a Pulitzer Prize. Or just pick a random scientist and ask ver what vis favorite book is, and 1 out of 5 will say: \"<em>G\u00f6del, Escher, Bach</em>\". No other book even comes close.</p>\n<p>It is saddening to contemplate that every day, 150,000 humans die without reading what is indisputably one of the greatest achievements of our species. Don't let it happen to you.</p>\n<p>Sure, if you're just an average person, you might not understand everything in this book - but when you're done reading, you won't be an average person any more.</p>\n</blockquote>\n<p>It's easy to see <em>GEB</em>'s effect on Eliezer's writing: the \"concrete, then abstract\" pattern, the koans, the puzzles, the conversational coverage of technical concepts in math and computer science... it's all here in spades in <em>GEB</em>.</p>\n<p>&nbsp;</p>\n<h4 id=\"What_GEB_Is\">What <em>GEB</em>&nbsp;Is</h4>\n<p>In the preface to the 20th anniversary edition, Hofstadter clarifies what <em>GEB</em>&nbsp;is and is not. It is not about how reality is \"a system of interconnected braids.\" It is not about how \"math, art, and music are really all the same thing at their core.\" Instead, says Hofstadter:</p>\n<blockquote>\n<p><em>GEB</em>&nbsp;is a very personal attempt to say how it is that animate beings can come out of inanimate matter... <em>GEB</em>&nbsp;approaches [this question] by slowly building up an analogy that likens inanimate molecules to meaningless symbols, and further likens selves... to certain special swirly, twisty, vortex-like, and <em>meaningful</em>&nbsp;patterns that arise only in particular types of systems of meaningless symbols. It is these strange, twisty patterns that the book spends so much time on, because they are little known, little appreciated, counterintuitive, and quite filled with mystery [that] I call... \"strange loops\"...</p>\n<p>...the Godelian strange loop that arises in formal systems in mathematics... is a loop that allows such systems to \"perceive itself,\" to talk about itself, to become \"self-aware,\" and in a sense it would not be going too far to say that by virtue of having such a loop, a formal system <em>acquires a self</em>.</p>\n<p>...the shift of focus from material components [of the human mind] to abstract patterns allows the [surprising] leap from inanimate to animate, from nonsemantic to semantic, from meaningless to meaningful, to take place. But how does that happen? After all, not <em>all</em>&nbsp;jumps from matter to pattern give rise to consciousness or soul or self... What kind of pattern is it, then, that is the telltale mark of a <em>self</em>? <em>GEB</em>'s answer is: a strange loop.</p>\n<p>The irony is that the first strange loop ever found... was found in a system <em>tailor-made to keep loopiness out</em>... Bertrand Russell and Alfred North Whitehead's famous treatise <em>Principia Mathematica</em>...</p>\n<p>...For the French, the enemy was Germany; for Russell, it was self-reference. Russell believed that for a mathematical system to be able to talk about itself in any way whatsoever was the kiss of death, for self-reference would... necessarily open the door to self-contradiction...</p>\n<p>Kurt Godel realized that... self-reference not only had lurked from Day One in <em>Principia Mathematica</em>, but in fact plagued poor <em>PM</em>&nbsp;in a totally unremovable manner. Moreover, as Godel made brutally clear, this thorough riddling of the system by self-reference was not due to some weakness in <em>PM</em>, but quite to the contrary, it was due to its <em>strength</em>. Any similar system would have exactly the same \"defect.\"</p>\n<p>[Godel had discovered that] any formal system designed to spew forth truths about \"mere\" numbers would also wind up spewing forth truths... about its own properties, and would thereby become \"self-aware,\" in a manner of speaking.</p>\n<p>[But] strange loops are an abstract structure that crop up in various media and in varying degrees of richness.</p>\n</blockquote>\n<p>&nbsp;</p>\n<h4 id=\"A_Musico_Logical_Offering\">A Musico-Logical Offering</h4>\n<p>Hofstadter opens with the story of J.S. Bach's <em>Musical Offering</em>&nbsp;for King Frederick, which contains a particular canon that sneakily shifts from one key to another before its apparent conclusion, and when this modulation is repeated 6 times, the piece ends up at the original key but one octave higher. This is our first example of a \"Strange Loop\":</p>\n<blockquote>\n<p>The \"Strange Loop\" phenomenon occurs whenever, by moving upwards (or downwards) through the levels of some heirarchical system, we unexpectedly find ourselves right back where we started. (Here, the system is that of musical keys.)</p>\n</blockquote>\n<p>Other examples occur in the drawings of M.C. Escher, for example <a href=\"http://2.bp.blogspot.com/-sCwJVYudavM/TY8PU3IoMbI/AAAAAAAAEY8/u8ClzTFu79A/s1600/11+Waterfall.jpg\">this famous one</a>.</p>\n<p>The liar's paradox (e.g. \"This statement is false\") is a one-step Strange Loop. Related to this is a Strange Loop found in the proof for Godel's Incompleteness Theorem, which states, roughly:</p>\n<blockquote>\n<p>All consistent axiomatic formulations of number theory include undecidable propositions.</p>\n</blockquote>\n<p>Before Godel, Russell and Whitehead tried to banish Strange Loops from set theory in <em>Principia Mathematica</em>. But Godel's theorem showed</p>\n<blockquote>\n<p>...not only that there were irreparable \"holes\" in the axiomatic system proposed by Russell and Whitehead, but more generally, that no axiomatic system whatsoever could produce all number-theoretical truths, unless it were an inconsistent system!</p>\n</blockquote>\n<p>The goal of the book is to explain these Strange Loops in more detail, and how they may explain how animate beings arise from inanimate matter.</p>\n<p>&nbsp;</p>\n<h4 id=\"Meaning_and_Form_in_Mathematics\">Meaning and Form in Mathematics</h4>\n<p>After a tutorial on formal systems, Hofstadter argues that</p>\n<blockquote>\n<p>...symbols of a formal system, though initially without meaning, cannot avoid taking on \"meaning\" of sorts, at least if an isomorphism is found.</p>\n</blockquote>\n<p>The vast majority of interpretations for a formal system are meaningless, but if an isomorphism can be found between the formal system and some piece of reality, that isomorphism provides the symbols their \"meaning.\"</p>\n<p>But you may discover multiple isomorphisms, and thus the symbols of a formal system may have multiple meanings. It makes no sense to ask, \"But which one is <em>the</em>&nbsp;meaning of the string?\":</p>\n<blockquote>\n<p>An interpretation will be meaningful to the extent that it accurately reflects some isomorphism to the real world.</p>\n</blockquote>\n<p>...</p>", "sections": [{"title": "What GEB\u00a0Is", "anchor": "What_GEB_Is", "level": 1}, {"title": "A Musico-Logical Offering", "anchor": "A_Musico_Logical_Offering", "level": 1}, {"title": "Meaning and Form in Mathematics", "anchor": "Meaning_and_Form_in_Mathematics", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "30 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 30, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-16T06:55:55.479Z", "modifiedAt": null, "url": null, "title": "Problems of the Deutsch-Wallace version of Many Worlds", "slug": "problems-of-the-deutsch-wallace-version-of-many-worlds", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:04.626Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mitchell_Porter", "createdAt": "2009-05-28T02:36:19.394Z", "isAdmin": false, "displayName": "Mitchell_Porter"}, "userId": "fjERoRhgjipqw3z2b", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6McLFoTupdCkxH9Nd/problems-of-the-deutsch-wallace-version-of-many-worlds", "pageUrlRelative": "/posts/6McLFoTupdCkxH9Nd/problems-of-the-deutsch-wallace-version-of-many-worlds", "linkUrl": "https://www.lesswrong.com/posts/6McLFoTupdCkxH9Nd/problems-of-the-deutsch-wallace-version-of-many-worlds", "postedAtFormatted": "Friday, December 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Problems%20of%20the%20Deutsch-Wallace%20version%20of%20Many%20Worlds&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AProblems%20of%20the%20Deutsch-Wallace%20version%20of%20Many%20Worlds%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6McLFoTupdCkxH9Nd%2Fproblems-of-the-deutsch-wallace-version-of-many-worlds%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Problems%20of%20the%20Deutsch-Wallace%20version%20of%20Many%20Worlds%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6McLFoTupdCkxH9Nd%2Fproblems-of-the-deutsch-wallace-version-of-many-worlds", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6McLFoTupdCkxH9Nd%2Fproblems-of-the-deutsch-wallace-version-of-many-worlds", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 341, "htmlBody": "<p>The subject has already been raised in <a href=\"/r/discussion/lw/8uy/a_case_study_in_fooling_oneself/\">this thread</a>, but in a clumsy fashion. So here is a fresh new thread, where we can discuss, calmly and objectively, the pros and cons of the \"Oxford\" version of the Many Worlds interpretation of quantum mechanics.</p>\n<p>This version of MWI is distinguished by two propositions. First, there is no definite number of \"worlds\" or \"branches\". They have a fuzzy, vague, approximate, definition-dependent existence. Second, the probability law of quantum mechanics (the Born rule) is to be obtained, not by counting the frequencies of events in the multiverse, but by an analysis of rational behavior in the multiverse. Normally, a prescription for rational behavior is obtained by maximizing expected utility, a quantity which is calculated by averaging \"probability x utility\" for each possible outcome of an action. In the Oxford school's \"decision-theoretic\" derivation of the Born rule, we somehow <em>start</em> with a ranking of actions that is deemed rational, then we \"divide out\" by the utilities, and obtain probabilities that were implicit in the original ranking.</p>\n<p>I reject the two propositions. \"Worlds\" or \"branches\" can't be vague if they are to correspond to observed reality, because vagueness results from an object being dependent on observer definition, and the local portion of reality does not owe its existence to how we define anything; and the upside-down decision-theoretic derivation, if it ever works, must implicitly smuggle in the premises of probability theory in order to obtain its original rationality ranking.</p>\n<p>Some references:</p>\n<p><a href=\"http://users.ox.ac.uk/~mert0130/papers-ev.shtml#FAPP\">\"Decoherence and Ontology: or, How I Learned to Stop Worrying and Love FAPP\"</a> by David Wallace. In this paper, Wallace says, for example, that the question \"how many branches are there?\" \"does not... make sense\", that the question \"how many branches are there in which it is sunny?\" is \"a question which has no answer\", \"it is a non-question to ask how many [worlds]\", etc.</p>\n<p><a href=\"http://lanl.arxiv.org/abs/quant-ph/9907024\">\"Quantum Probability from Decision Theory?\"</a> by Barnum et al. This is a rebuttal of the original argument (due to David Deutsch) that the Born rule can be justified by an analysis of multiverse rationality.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6McLFoTupdCkxH9Nd", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 5, "extendedScore": null, "score": 8.158416376329026e-07, "legacy": true, "legacyId": "11500", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 93, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["w7GYWtoRZsizsF8Xk"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-16T09:22:39.436Z", "modifiedAt": null, "url": null, "title": "Allen & Wallach on Friendly AI", "slug": "allen-and-wallach-on-friendly-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:58.158Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vvKAgLH5vpicF6Yrj/allen-and-wallach-on-friendly-ai", "pageUrlRelative": "/posts/vvKAgLH5vpicF6Yrj/allen-and-wallach-on-friendly-ai", "linkUrl": "https://www.lesswrong.com/posts/vvKAgLH5vpicF6Yrj/allen-and-wallach-on-friendly-ai", "postedAtFormatted": "Friday, December 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Allen%20%26%20Wallach%20on%20Friendly%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAllen%20%26%20Wallach%20on%20Friendly%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvvKAgLH5vpicF6Yrj%2Fallen-and-wallach-on-friendly-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Allen%20%26%20Wallach%20on%20Friendly%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvvKAgLH5vpicF6Yrj%2Fallen-and-wallach-on-friendly-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvvKAgLH5vpicF6Yrj%2Fallen-and-wallach-on-friendly-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 418, "htmlBody": "<p><a href=\"http://www.indiana.edu/~hpscdept/people/allen.shtml\">Colin Allen</a> and <a href=\"http://intelligence.org/summit2007/speakers/wallach/\">Wendell Wallach</a>, who wrote <em><a href=\"http://www.amazon.com/Moral-Machines-Teaching-Robots-Right/dp/0199737975/\">Moral Machines</a></em>&nbsp;(<em>MM</em>)&nbsp;for OUP in 2009, address the problem of Friendly AI in their recent chapter for <em><a href=\"http://www.amazon.com/Robot-Ethics-Implications-Intelligent-Autonomous/dp/0262016664/\">Robot Ethics</a></em>&nbsp;(MIT Press). Their chapter is a precis of <em>MM</em>&nbsp;and a response to objections, one of which is:</p>\n<blockquote>\n<p>The work of researchers focused on ensuring that a technological singularity will be friendly to humans (friendly AI) was not given its due in <em>MM</em>.</p>\n</blockquote>\n<p>Their brief response to this objection is:</p>\n<blockquote>\n<p>The project of building AMAs [artificial moral agents] is bracketed by the more conservative expectations of computer scientists, engaged with the basic challenges and thresholds yet to be crossed, and the more radical expectations of those who believe that human-like and superhuman systems will be built in the near future. There are a wide variety of theories and opinions about how sophisticated computers and robotic systems will become in the next twenty to fifty years. Two separate groups focused on ensuring the safety of (ro)bots have emerged around these differing expectations: the machine ethics community and the singularitarians (friendly AI), exemplified by the Singularity Institute for Artificial Intelligence (SIAI). Those affiliated with SIAI are specifically concerned with the existential dangers to humanity posed by AI systems that are smarter than humans. <em>MM</em> has been criticized for failing to give fuller attention to the projects of those dedicated to a singularity in which AI systems friendly to humans prevail.</p>\n<p>SIAI has been expressly committed to the development of general mathematical models that can, for example, yield probabilistic predictions about future possibilities in the development&nbsp;of AI. One of Eliezer Yudkowsky&rsquo;s projects is motivationally stable goal systems for advanced forms of AI. If satisfactory predictive models or strategies for stable goal architectures can be developed, their value for AMAs is apparent. But will they be developed, and what other technological thresholds must be crossed, before such strategies could be implemented in AI? In a similar vein, no one questions the tremendous value machine learning would have for facilitating the acquisition by AI systems of many skills, including moral decision making. But until sophisticated machine learning strategies are developed, discussing their application is speculative. That said, since the publication of <em>MM</em>, there has been an increase in projects that could lead to further collaboration between these two communities, a prospect we encourage.</p>\n</blockquote>\n<p>Meh. Not much to this. I suppose <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">The Singularity and Machine Ethics</a> is another plank in bridging the two communities.</p>\n<p>The most interesting chapter in the book is, imo, Anthony Beavers' \"Moral Machines and the Threat of Ethical Nihilism.\"</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vvKAgLH5vpicF6Yrj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 10, "extendedScore": null, "score": 8.158954112752927e-07, "legacy": true, "legacyId": "11501", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-16T13:09:54.128Z", "modifiedAt": null, "url": null, "title": "Question about timeless physics", "slug": "question-about-timeless-physics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:19.581Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PS53eCQriX7qF5aqM/question-about-timeless-physics", "pageUrlRelative": "/posts/PS53eCQriX7qF5aqM/question-about-timeless-physics", "linkUrl": "https://www.lesswrong.com/posts/PS53eCQriX7qF5aqM/question-about-timeless-physics", "postedAtFormatted": "Friday, December 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Question%20about%20timeless%20physics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQuestion%20about%20timeless%20physics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPS53eCQriX7qF5aqM%2Fquestion-about-timeless-physics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Question%20about%20timeless%20physics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPS53eCQriX7qF5aqM%2Fquestion-about-timeless-physics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPS53eCQriX7qF5aqM%2Fquestion-about-timeless-physics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 306, "htmlBody": "<p><strong>Related to:</strong> <a href=\"/lw/qp/timeless_physics/\">lesswrong.com/lw/qp/timeless_physics/</a></p>\n<p>Why do I find myself at this point in <del>time</del>, configuration space, rather than another point? In other words, why do I have certain expectations rather than others?</p>\n<p>I don't expect the U.S. presidential elections to have happened but to happen next, where \"to happen\" and \"to have happened\" internally marks the sequential order of steps indexed by consecutive timestamps. But why do I find myself to have that particular expectation rather than any other, what is it that does privilege this point?</p>\n<blockquote>\n<p>So you seem to remember Time proceeding along a <em>single line</em>.&nbsp; You remember that the particle first went left, and then went right.&nbsp; You ask, \"Which way will the particle go <em>this </em>time?\"</p>\n</blockquote>\n<p>My question is why I find myself to remember that the particle went left and then right rather than left but not <em>yet</em> right?</p>\n<blockquote>\n<p>But both branches, both future versions of you, just <em>exist.</em>&nbsp; There <em>is no fact of the matter</em> as to \"which branch you go down\".&nbsp; Different versions of you experience both branches.</p>\n</blockquote>\n<p>Yes, but why does my version experience this point of my branch and not any other point of my branch?</p>\n<p>I understand that if this universe was a giant simulation and that if it was to halt and then resume, after some indexical measure of causal steps used by those outside of it, then I wouldn't notice it. Therefore if you remove the notion of an outside world there ceases to be any measure of how many causal steps it took until I continued my relational measure of progression.</p>\n<p>But that's not my question. Assume for a moment that my consciousness experience is not a causal continuum but a discrete sequence of causal steps from 1, 2, 3, ... to N where N marks this point. Why do I find myself at N rather than 10 or N+1?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PS53eCQriX7qF5aqM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 6, "extendedScore": null, "score": 8.159787028931986e-07, "legacy": true, "legacyId": "11504", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["rrW7yf42vQYDf8AcH"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-16T15:08:23.362Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Atlanta, Austin, Melbourne, Pittsburgh", "slug": "weekly-lw-meetups-atlanta-austin-melbourne-pittsburgh", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZAuXxpas8bEoeRCyG/weekly-lw-meetups-atlanta-austin-melbourne-pittsburgh", "pageUrlRelative": "/posts/ZAuXxpas8bEoeRCyG/weekly-lw-meetups-atlanta-austin-melbourne-pittsburgh", "linkUrl": "https://www.lesswrong.com/posts/ZAuXxpas8bEoeRCyG/weekly-lw-meetups-atlanta-austin-melbourne-pittsburgh", "postedAtFormatted": "Friday, December 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Atlanta%2C%20Austin%2C%20Melbourne%2C%20Pittsburgh&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Atlanta%2C%20Austin%2C%20Melbourne%2C%20Pittsburgh%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZAuXxpas8bEoeRCyG%2Fweekly-lw-meetups-atlanta-austin-melbourne-pittsburgh%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Atlanta%2C%20Austin%2C%20Melbourne%2C%20Pittsburgh%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZAuXxpas8bEoeRCyG%2Fweekly-lw-meetups-atlanta-austin-melbourne-pittsburgh", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZAuXxpas8bEoeRCyG%2Fweekly-lw-meetups-atlanta-austin-melbourne-pittsburgh", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 354, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/54\">Next Atlanta Less Wrong Meetup:&nbsp;<span class=\"date\">10 December 2011 06:00PM</span></a></li>\n<li><a href=\"/meetups/5g\">Pittsburgh Holiday Meetup:&nbsp;<span class=\"date\">16 December 2011 08:00PM</span></a></li>\n<li><a href=\"/meetups/5h\">Is there anybody from Indianapolis here?:&nbsp;<span class=\"date\">08 January 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/5f\">First Brussels meetup:&nbsp;<span class=\"date\">18 February 2012 11:00AM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/5i\">Austin, TX:&nbsp;<span class=\"date\">10 December 2011 01:30PM</span></a></li>\n<li><a href=\"/meetups/5i\"></a><a href=\"/meetups/5e\">Melbourne social meetup:&nbsp;<span class=\"date\">16 December 2011 07:00PM</span></a></li>\n</ul>\n<p>The Mountain View meetup is now every other week, rather than weekly.</p>\n<p>Cities with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>,</strong><strong> <a href=\"/r/discussion/lw/5pd/southern_california_meetup_may_21_weekly_irvine\">Irvine</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison, WI</a></strong>,<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin, CA</a> </strong>(uses the Bay Area List)<strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#San_Francisco\">San Francisco</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>, and <strong><a href=\"/r/discussion/lw/6at/west_la_biweekly_meetups\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong><strong>.</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZAuXxpas8bEoeRCyG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 4, "extendedScore": null, "score": 8.160221377569828e-07, "legacy": true, "legacyId": "11365", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pAHo9zSFXygp5A5dL", "tHFu6kvy2HMvQBEhW", "d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-16T16:21:45.453Z", "modifiedAt": null, "url": null, "title": "The rationalist's checklist", "slug": "the-rationalist-s-checklist", "viewCount": null, "lastCommentedAt": "2017-06-17T04:13:05.098Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Technoguyrob", "createdAt": "2010-12-23T01:08:03.374Z", "isAdmin": false, "displayName": "Technoguyrob"}, "userId": "WxPBN5cdcazmWbcr8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tLR9YZHiNoDE2Czjh/the-rationalist-s-checklist", "pageUrlRelative": "/posts/tLR9YZHiNoDE2Czjh/the-rationalist-s-checklist", "linkUrl": "https://www.lesswrong.com/posts/tLR9YZHiNoDE2Czjh/the-rationalist-s-checklist", "postedAtFormatted": "Friday, December 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20rationalist's%20checklist&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20rationalist's%20checklist%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtLR9YZHiNoDE2Czjh%2Fthe-rationalist-s-checklist%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20rationalist's%20checklist%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtLR9YZHiNoDE2Czjh%2Fthe-rationalist-s-checklist", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtLR9YZHiNoDE2Czjh%2Fthe-rationalist-s-checklist", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 444, "htmlBody": "<p>Doctor Peter Pronovost has managed to single-handedly reduce the infection rates in ICU facilities nationwide from numbers like fourteen percent or twenty percent to <em>zer</em>o. His solution is idiotically simple: a checklist. In a process as complex as ICU treatment, doctors perform chained simple steps very many times, and it can be easy to forget a step. These things add up. Read <a href=\"http://www.newyorker.com/reporting/2007/12/10/071210fa_fact_gawande?currentPage=all \">the article</a> before continuing.</p>\n<p>In their phenomenal book, <a href=\"http://www.amazon.com/Power-Full-Engagement-Managing-Performance/dp/0743226747\">The Power of Full Engagement</a>,&nbsp;<span>Jim Loehr and Tony Schwartz discuss a pattern they have discovered among all top performers, ranging from sports to music and business. Beyond a certain level, all top performers had established positive rituals for relaxation and deliberate practice. These positive rituals were daily ingrained habits and allowed them to surpass the merely excellent performers.</span></p>\n<p><span>It is difficult to make use of most Less Wrong posts in terms of changing one's behavior. Even if you integrate a lesson fully, you will still miss steps on occasion. I propose we suggest checklists for various recurring activities that will offload these responsibilities from conscious thought to its much more reliable brother, ingrained habit. Fortunately, we <a href=\"/r/discussion/lw/8sa/why_i_write_about_the_basics/\">should not need many checklists</a>.</span></p>\n<p><span>An example of a checklist is a morning routine:</span></p>\n<ul>\n<li><span>Short exercise (40 pushups, 50 situps)</span></li>\n<li><span>Shower</span></li>\n<li>Daily cosmetics (brush teeth,&nbsp;shave,&nbsp;skin moisturizer, hair forming cream, male scented lotion, deodorant)</li>\n<li>Make breakfast, which can be depending on what day it is mod 3: Black mango tea as well as</li>\n<li>&nbsp; &nbsp;Omelette with cheese and tomatoes</li>\n<li>&nbsp; &nbsp;Cereal with a side of oatmeal</li>\n<li>&nbsp; &nbsp;A piece of fruit and yoghurt</li>\n<li>Brief non-work and non-study related reading, for example, a novel</li>\n</ul>\n<p><span>Another example of a checklist is during a conversation with a non-rationalist on the&nbsp;<a href=\"/lw/5kz/the_5second_level/\">5-second level</a>: If you feel strong affect (anger or annoyance) at someone's point in a debate,</span></p>\n<ul>\n<li><span>Say \"Let me think about this for a second.\"</span></li>\n</ul>\n<ul>\n<li><span>Are you two on the same &nbsp;<a href=\"/lw/kg/expecting_short_inferential_distances\">inferential level</a>? If not, backtrack by trying to bridge it first.</span></li>\n</ul>\n<ul>\n<li><span>Have either of you committed a &nbsp;<a href=\"http://en.wikipedia.org/wiki/List_of_cognitive_biases\">cognitive bias</a>? (This may not be so helpful, given this is a potential fifty-item checklist!) If so, inform them about them about it in a friendly manner. (\"Ah, I see! You have to be careful here because...\" or \"Oh, oops, I had forgotten here about...\")</span></li>\n</ul>\n<p><span>Think of this as a re-run of the 5-second level post, with an outreach towards more than just specific rationality skills. The checklists that are not required to be performed extemporaneously (i.e., not in conversation) should be like a physical checklist, that one can write down and should go through every time.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"2oWPnnnzMbiAxWfbs": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tLR9YZHiNoDE2Czjh", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 38, "baseScore": 44, "extendedScore": null, "score": 7.9e-05, "legacy": true, "legacyId": "11506", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 34, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SA2ZRgXJBDoGjGrjb", "JcpzFpPBSmzuksmWM", "HLqWn5LASfhhArZ7w"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-16T17:21:27.081Z", "modifiedAt": null, "url": null, "title": "[link] Admitting errors (in meteorology)", "slug": "link-admitting-errors-in-meteorology", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:57.597Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "fortyeridania", "createdAt": "2010-07-21T15:35:12.558Z", "isAdmin": false, "displayName": "fortyeridania"}, "userId": "roBPqtzsvG6dC3YFT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dWCcLWKE9h22QxHwu/link-admitting-errors-in-meteorology", "pageUrlRelative": "/posts/dWCcLWKE9h22QxHwu/link-admitting-errors-in-meteorology", "linkUrl": "https://www.lesswrong.com/posts/dWCcLWKE9h22QxHwu/link-admitting-errors-in-meteorology", "postedAtFormatted": "Friday, December 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20Admitting%20errors%20(in%20meteorology)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20Admitting%20errors%20(in%20meteorology)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdWCcLWKE9h22QxHwu%2Flink-admitting-errors-in-meteorology%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20Admitting%20errors%20(in%20meteorology)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdWCcLWKE9h22QxHwu%2Flink-admitting-errors-in-meteorology", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdWCcLWKE9h22QxHwu%2Flink-admitting-errors-in-meteorology", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 32, "htmlBody": "<p>From <a href=\"http://cafehayek.com/2011/12/man-bites-dog.html\">Cafe Hayek</a>&nbsp;(<a href=\"http://www.ottawacitizen.com/entertainment/Hurricane+predictors+admit+they+predict+hurricanes/5847032/story.html\">original</a>): Two meteorologists have announced that they will stop using certain forecast methods, even though they've used them for 20 years.</p>\r\n<p>There's a correction at the end of the article, too!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dWCcLWKE9h22QxHwu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 16, "extendedScore": null, "score": 3e-05, "legacy": true, "legacyId": "11507", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-16T21:06:28.888Z", "modifiedAt": null, "url": null, "title": "I started a blog: Concept Space Cartography", "slug": "i-started-a-blog-concept-space-cartography", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:57.634Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/t4iw9PmwybD2FoWcB/i-started-a-blog-concept-space-cartography", "pageUrlRelative": "/posts/t4iw9PmwybD2FoWcB/i-started-a-blog-concept-space-cartography", "linkUrl": "https://www.lesswrong.com/posts/t4iw9PmwybD2FoWcB/i-started-a-blog-concept-space-cartography", "postedAtFormatted": "Friday, December 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I%20started%20a%20blog%3A%20Concept%20Space%20Cartography&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI%20started%20a%20blog%3A%20Concept%20Space%20Cartography%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft4iw9PmwybD2FoWcB%2Fi-started-a-blog-concept-space-cartography%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I%20started%20a%20blog%3A%20Concept%20Space%20Cartography%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft4iw9PmwybD2FoWcB%2Fi-started-a-blog-concept-space-cartography", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ft4iw9PmwybD2FoWcB%2Fi-started-a-blog-concept-space-cartography", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 95, "htmlBody": "<p>For quite a while, I've felt that my mind contains important insights which are getting held up because I don't write enough. So to encourage myself to write more, I've started a blog: <a href=\"http://conceptspacecartography.com\">Concept Space Cartography</a>. My most recent post there, <a href=\"http://conceptspacecartography.com/two-definitions-for-critical-thinking/\">Two Definitions for Critical Thinking</a>, is likely to be of interest to Less Wrong.</p>\n<p>Also, every page on my blog has an embedded copy of <a href=\"http://textcelerator.com\">Textcelerator</a>, a tool I created&nbsp;for speed-reading. (It's normally a browser plugin, which makes it appear on <em>every</em> web page. But you can use it on my blog without the plugin.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "t4iw9PmwybD2FoWcB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 9, "extendedScore": null, "score": 8.161534315483576e-07, "legacy": true, "legacyId": "11508", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-16T21:36:56.640Z", "modifiedAt": null, "url": null, "title": "No one knows what Peano arithmetic doesn't know", "slug": "no-one-knows-what-peano-arithmetic-doesn-t-know", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:01.075Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PWP5j38tihSHkLsMc/no-one-knows-what-peano-arithmetic-doesn-t-know", "pageUrlRelative": "/posts/PWP5j38tihSHkLsMc/no-one-knows-what-peano-arithmetic-doesn-t-know", "linkUrl": "https://www.lesswrong.com/posts/PWP5j38tihSHkLsMc/no-one-knows-what-peano-arithmetic-doesn-t-know", "postedAtFormatted": "Friday, December 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20No%20one%20knows%20what%20Peano%20arithmetic%20doesn't%20know&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANo%20one%20knows%20what%20Peano%20arithmetic%20doesn't%20know%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPWP5j38tihSHkLsMc%2Fno-one-knows-what-peano-arithmetic-doesn-t-know%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=No%20one%20knows%20what%20Peano%20arithmetic%20doesn't%20know%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPWP5j38tihSHkLsMc%2Fno-one-knows-what-peano-arithmetic-doesn-t-know", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPWP5j38tihSHkLsMc%2Fno-one-knows-what-peano-arithmetic-doesn-t-know", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 338, "htmlBody": "<p><strong>WARNING:</strong> this post requires some knowledge of mathematical logic and computability theory.</p>\n<p>I was just talking with Wei Dai and something came up that seems at once obvious and counterintuitive. Though if the argument is correct, I guess it will be old news to about 50% of the people who read my posts :-)</p>\n<p>Imagine you have an oracle that can determine if an arbitrary statement is provable in Peano arithmetic. Then you can try using it as a halting oracle: for an arbitrary Turing machine T, ask \"can PA prove that there's an integer N such that T makes N steps and then halts?\". If the oracle says yes, you know that the statement is true for standard integers because they're one of the models of PA, therefore N is a standard integer, therefore T halts. And if the oracle says no, you know that there's no such standard integer N because otherwise the oracle would've found a long and boring proof involving the encoding of N as SSS...S0, therefore T doesn't halt. So your oracle can indeed serve as a halting oracle.</p>\n<p>On the other hand, if you had a halting oracle to begin with, you could use it as a provability oracle for PA: \"if a program successively enumerates all proofs in PA, will it ever find a proof for such-and-such statement?\"</p>\n<p>So having a provability oracle for PA or any other consistent formal system that proves some valid arithmetic truths (like ZFC) is equivalent to having a halting oracle, and thus leads to a provability oracle for any other formal system.&nbsp;In other words, if you knew all about the logical implications of PA, then you would also know all about the logical implications of ZFC and all other formal systems. Hee hee.</p>\n<p><strong>ETA:</strong> this line leads to a nontrivial question. Is there a formal system (not talking about the standard integers, I guess) whose provability oracle is strictly weaker than the halting oracle, but still uncomputable?</p>\n<p><strong>ETA 2:</strong> the question seems to be resolved, see Zetetic's comment and my reply.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GY5kPPpCoyt9fnTMn": 1, "6nS8oYmSMuFMaiowF": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PWP5j38tihSHkLsMc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": 28, "extendedScore": null, "score": 5.9e-05, "legacy": true, "legacyId": "11509", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 28, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-16T22:05:16.117Z", "modifiedAt": null, "url": null, "title": "Christopher Hitchens 1949-2011", "slug": "christopher-hitchens-1949-2011", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:02.386Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Costanza", "createdAt": "2010-09-14T16:22:53.235Z", "isAdmin": false, "displayName": "Costanza"}, "userId": "cXudnoTp54SYgqfgF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7JP2jgswgv2k2cPFz/christopher-hitchens-1949-2011", "pageUrlRelative": "/posts/7JP2jgswgv2k2cPFz/christopher-hitchens-1949-2011", "linkUrl": "https://www.lesswrong.com/posts/7JP2jgswgv2k2cPFz/christopher-hitchens-1949-2011", "postedAtFormatted": "Friday, December 16th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Christopher%20Hitchens%201949-2011&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AChristopher%20Hitchens%201949-2011%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7JP2jgswgv2k2cPFz%2Fchristopher-hitchens-1949-2011%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Christopher%20Hitchens%201949-2011%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7JP2jgswgv2k2cPFz%2Fchristopher-hitchens-1949-2011", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7JP2jgswgv2k2cPFz%2Fchristopher-hitchens-1949-2011", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 58, "htmlBody": "<p>Having been diagnosed with cancer last year, writer <a href=\"http://www.vanityfair.com/culture/christopher-hitchens\">Christopher Hitchens</a> has died. He was known as as an outspoken atheist, which is not, in itself, identical to being a committed rationalist in any systematic way. Even so, he seemed to have the virtue of moral courage, the willingness to speak the truth as he saw it, without fear.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7JP2jgswgv2k2cPFz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 12, "extendedScore": null, "score": 8.161749892478092e-07, "legacy": true, "legacyId": "11510", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-17T05:42:39.630Z", "modifiedAt": null, "url": null, "title": "[Transcript] Tyler Cowen on Stories", "slug": "transcript-tyler-cowen-on-stories", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:54.321Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Grognor", "createdAt": "2011-01-31T02:54:34.463Z", "isAdmin": false, "displayName": "Grognor"}, "userId": "LoykQRMTxJFxwwdPy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4kphivjxngJmEdWsN/transcript-tyler-cowen-on-stories", "pageUrlRelative": "/posts/4kphivjxngJmEdWsN/transcript-tyler-cowen-on-stories", "linkUrl": "https://www.lesswrong.com/posts/4kphivjxngJmEdWsN/transcript-tyler-cowen-on-stories", "postedAtFormatted": "Saturday, December 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BTranscript%5D%20Tyler%20Cowen%20on%20Stories&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BTranscript%5D%20Tyler%20Cowen%20on%20Stories%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4kphivjxngJmEdWsN%2Ftranscript-tyler-cowen-on-stories%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BTranscript%5D%20Tyler%20Cowen%20on%20Stories%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4kphivjxngJmEdWsN%2Ftranscript-tyler-cowen-on-stories", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4kphivjxngJmEdWsN%2Ftranscript-tyler-cowen-on-stories", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2800, "htmlBody": "<p>I was shocked, absolutely <em>shocked</em>, to find that <a href=\"http://en.wikipedia.org/wiki/Tyler_Cowen\">Tyler Cowen</a>'s excellent TEDxMidAtlantic <a href=\"http://www.youtube.com/watch?v=RoEEDKwzNBw\">talk</a> on stories had not yet been transcribed. It generated a lot of discussion in the <a href=\"/r/lesswrong/lw/1em/the_danger_of_stories/\">thread about it where it was first introduced</a>, so I went ahead and transcribed it. I added hyperlinks to background information where I thought it was due. Here you go:</p>\n<p><a id=\"more\"></a>Host: In normal times, a blog written by an economist might not get that much attention, but our next presenter's blog, called <a href=\"http://marginalrevolution.com/\">Marginal Revolution</a>, is quite popular, and he writes a column for the New York Times called the Economic Scene, here to explain the world to us in terms of the Great Recession and beyond, is Tyler Cowen.</p>\n<blockquote>\n<p>Cowen: I was told to come here and tell you all stories, but what I'd like to do is instead tell you why I'm suspicious of stories, why stories make me nervous. In fact, the more inspired a story makes me feel, very often the more nervous I get. So the best stories are often the trickiest ones. The good and bad things about stories is they're a kind of filter. They take a lot of information, and they leave some of it out, and they keep some of it in. But the thing about this filter, it always leaves the same things in. You're always left with the same few stories. There's the old saying, just about every story can be summed up as, \"A stranger came to town.\" There's a <a href=\"http://www.amazon.com/Seven-Basic-Plots-Tell-Stories/dp/0826480373\">book</a> by <a href=\"http://en.wikipedia.org/wiki/Christopher_Booker\">Christopher Booker</a>, he claims there are really just seven types of stories. There's monster, rags to riches, quest, voyage and return, comedy, tragedy, rebirth. You don't have to agree with that list exactly, but the point is this: if you think in terms of stories, you're telling yourself the same things over and over again.</p>\n<p>There was a study done, we asked some people to describe their lives. And when asked to describe their lives, what's interesting is how few people said, \"mess\". It's probably the best answer; I don't mean that in a bad way. \"Mess\" can be liberating, \"mess\" can be empowering, \"mess\" can be a way of drawing upon multiple strengths. But what people wanted to say was, \"My life is a journey.\" 51% wanted to turn his or her life into a story. 11% said, \"My life is a battle.\" Again, that's a kind of story. 8% said, \"My life is a novel,\" 5% \"My life is a play.\" I don't think anyone said, \"My life is a reality TV show.\" Again, we're imposing order on the mess we observe, and it's taking the same patterns, and when something is in the form of a story, often we remember it when we shouldn't. So how many of you know the story about George Washington and the cherry tree. It's not obvious that's exactly what happened. The story of Paul Revere, it's not obvious that that's exactly the way it happened. So again, we should be suspicious of stories. We're biologically programmed to respond to them. They contain a lot of information. They have social power. They connect us to other people. So they're like a kind of candy that we're fed when we consume political information, when we read novels. When we read nonfiction books, we're really being fed stories. Nonfiction is, in a sense, the new fiction. The book may happen to say true things, but everything's taking the same form of these stories.</p>\n<p>So what are the problems of relying too heavily on stories? You view your life like \"this\" instead of the mess that it is or it ought to be. But more specifically, I think of a few major problems when we think too much in terms of narrative. First, narratives tend to be too simple. The point of a narrative is to strip it way, not just into 18 minutes, but most narratives you could present in a sentence or two. So when you strip away detail, you tend to tell stories in terms of good vs. evil, whether it's a story about your own life or a story about politics. Now, some things actually are good vs. evil. We all know this, right? But I think, as a general rule, we're too inclined to tell the good vs. evil story. As a simple rule of thumb, just imagine every time you're telling a good vs. evil story, you're basically lowering your IQ by ten points or more. If you just adopt that as a kind of inner mental habit, it's, in my view, one way to get a lot smarter pretty quickly. You don't have to read any books. Just imagine yourself pressing a button every time you tell the good vs. evil story, and by pressing that button you're lowering your IQ by ten points or more.</p>\n<p>Another set of stories that are popular - if you know Oliver Stone movies or Michael Moore movies. You can't make a movie and say, \"It was all a big accident.\" No, it has to be a conspiracy, people plotting together, because a story is about intention. A story is not about spontaneous order or complex human institutions which are the product of human action but not of human design. No, a story is about evil people plotting together. So you hear stories about plots, or even stories about good people plotting things together, just like when you're watching movies. This, again, is reason to be suspicious. As a good rule of thumb, \"When I hear a story, when should I be especially suspicious?\" If you hear a story and you think, \"Wow, that would make a great movie!\" That's when the \"uh-oh\" reaction should pop in a bit more, and you should start thinking more in terms of how the whole thing is maybe a bit of a mess. Another common story or storyline - the claim that we \"have to get tough\". You hear this in so many contexts. \"We have to get tough with the banks.\" \"We had to get tough with the labor unions.\" \"We need to get tough with some other country, some foreign dictator, someone we're negotiating with.\" Now, again, the point is not against getting tough. Sometimes we should get tough. That we got tough with the Nazis was a good thing. But this is a story we fall back upon all too readily. When we don't really know why something happened, we blame someone, and we say, \"We need to get tough with them!\" as if it had never occurred to your predecessor this idea of getting tough. I view it usually as a kind of mental laziness. It's a simple story you tell. \"We need to get tough, we needed to get tough, we will have to get tough.\" Usually, that's a kind of warning signal.</p>\n<p>Another kind of problem with stories is, you can only fit so many stories into your mind at once or in the course of a day, or even in the course of a lifetime. So your stories are serving too many purposes. For instance, just to get out of bed in the morning, you tell yourself the story that your job is really important, what you're doing is really important, and maybe it is, but I tell myself that story even when it's not. And you know what? That story works. It gets me out of bed. It's a kind of self-deception, but the problem comes when I need to change that story. The whole point of the story is that I grab onto it and I hold it, and it gets me out of bed. So when I'm really doing something that is a waste of time, in my mess of a life, I'm too tied into my story that got me out of bed, and ideally I ought to have some kind of complex story map in my mind, with combinatorials and a matrix of computation, and the like. But that's not how stories work. Stories, to work, have to be simple, easily grasped, easily told to others, easily remembered. So stories will serve dual and conflicting purposes, and very often they will lead us astray. I used to think I was within the camp of economists, I was one of the good guys, and I was allied with other good guys, and we were fighting the ideas of the bad guys. I used to think that! And probably, I was wrong! Maybe sometimes, I'm one of the good guys, but on some issues, I finally realized, \"Hey, I wasn't one of the good guys.\" I'm not sure I was the bad guys in the sense of having evil intent, but it was very hard for me to get away with that story.</p>\n<p>One interesting thing about cognitive biases - they're the subject of so many books these days. There's the <a href=\"http://www.amazon.com/Nudge-Improving-Decisions-Health-Happiness/dp/0300122233\">Nudge</a> book, the <a href=\"http://www.amazon.com/Sway-Irresistible-Pull-Irrational-Behavior/dp/0385530609/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1324099925&amp;sr=1-1\">Sway</a> book, the <a href=\"http://www.amazon.com/Blink-Power-Thinking-Without/dp/0316172324\">Blink</a> book, like the <a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/OneWordTitle\">one-title</a> book, all about the ways in which we screw up. And there are so many ways, but what I find interesting is that none of these books identify what, to me, is the single, central, most important way we screw up, and that is, we tell ourselves too many stories, or we are too easily seduced by stories. And why don't these books tell us that? It's because the books themselves are all about stories. The more of these books you read, you're learning about some of your biases, but you're making some of your other biases essentially worse. So the books themselves are part of your cognitive bias. Often, people buy them as a kind of talisman, like \"I bought this book. I won't be <a href=\"http://www.amazon.com/Predictably-Irrational-Hidden-Forces-Decisions/dp/006135323X\">Predictably Irrational</a>.\" It's like people want to hear the worst, so psychologically, they can prepare for it or defend against it. It's why there's such a market for pessimism. But to think that buying the book gets you somewhere, that's maybe the bigger fallacy. It's just like the evidence that shows the most dangerous people are those that have been taught some financial literacy. They're the ones who go out and make the worst mistakes. It's the people that realize, \"I don't know anything at all,\" that end up doing pretty well.</p>\n<p>A third problem with stories is that outsiders manipulate us using stories, and we all like to think advertising only works on the other guy, but that's not how it is. Advertising works on all of us, so if you're too attached to stories, what will happen is people selling products come along, and they will bundle their product with a story. You're like, \"Hey, a free story,\" and you end up buying the product, because the product and the story go together. And if you think about how capitalism works, there's a bias here. Let's consider two kinds of stories about cars. Story A is, \"Buy this car, and you will have beautiful, romantic partners and a fascinating life.\" There are a lot of people who have a financial incentive to promote that story. But say the alternative story is, \"You don't actually need a car as nice as your income would indicate. What you usually do is look at what your peers do and copy them. That's a good heuristic for a lot of problems, but when it comes to cars, just buy a Toyota.\" Maybe Toyota has an incentive there, but even Toyota's making money off the luxury cars, and less money off the cheaper cars. So if you think which set of stories you end up hearing, you end up hearing the glamor stories, the seductive stories, and again I'm telling you, don't trust them. They're people using your love of stories to manipulate you. Pull back and say, \"What are the messages, and what are the stories that no one has an incentive to tell?\" and start telling yourself those, and see if any of your decisions change. That's one simple way - you can never get out of the pattern of thinking in terms of stories, but you can improve the extent to which you think in stories and make some better decisions.</p>\n<p>So if I'm thinking about this talk, I'm wondering, of course, what is it you take away from this talk? What story do you take away from Tyler Cowen? One story you might take away is the story of the quest. \"Tyler came here, and he told us not to think so much in terms of stories.\" That would be a story you could tell about this talk. It would fit a pretty well-known pattern. You might remember it. You could tell it to other people. \"This weird guy came, and he said not to think in terms of stories. Let me tell you what happened today!\" and you tell your story. Another possibility is you might tell a story of rebirth. &nbsp;You might say, \"I used to think too much in terms of stories, but then I heard Tyler Cowen, and now I think less in terms of stories!\" That too, is a narrative you will remember, you can tell to other people, and it may stick. You also could tell a story of deep tragedy. \"This guy Tyler Cowen came and he told us not to think in terms of stories, but all he could do was tell us stories about how other people think too much in terms of stories.\" So, today, which one is it? Quest, rebirth, tragedy? or maybe some combination of the three? I'm really not sure, and I'm not here to tell you to burn your DVD player and throw out your Tolstoy. To think in terms of stories is fundamentally human. There's a <a href=\"http://en.wikipedia.org/wiki/Gabriel_Garc%C3%ADa_M%C3%A1rquez\">Gabriel Garc&iacute;a M&aacute;rquez</a> memoir, <a href=\"http://www.amazon.com/Living-Tell-Gabriel-Garcia-Marquez/dp/1400041341\">Living to Tell the Tale</a>, that we use stories to make sense of what we've done, to give meaning to our lives, to establish connections with other people. None of this will go away, should go away, or can go away. But as an economist, I'm thinking about life on the margin. The extra decision: should we think more in terms of stories, or less in terms of stories? When we hear stories, should we be more suspicious? and what kind of stories should we be suspicious of? Again, I'm telling you it's the stories that you like the most, that you find the most rewarding, the most inspiring. The stories that don't focus on opportunity cost, or the complex, unintended consequences of human action, because that very often does not make for a good story. So often a story is of triumph, of struggle; there are opposing forces, which are either evil or ignorant; there is a person on a quest, someone making a voyage, and a stranger coming to town. And those are your categories, but don't let them make you too happy.</p>\n<p>So as an alternative, at the margin (again, no burning of Tolstoy), just be a little more messy. If I actually had to live those journeys and quests and battles, that would be so oppressive to me! It's like, my goodness, can't I just have my life in its messy, ordinary - I hesitate to use the word - glory? It's fun for me - do I really have to follow some kind of narrative? Can't I just live? So be more with comfortable with messy. Be more comfortable with agnostic, and I mean this about the things that make you feel good. It's so easy to pick a few areas you're agnostic in, and then feel good about like, \"I'm agnostic about religion, or politics.\" It's a kind of portfolio move you make to be more dogmatic elsewhere, right? Sometimes, the most intellectually trustworthy people are the ones who pick one area, and they're totally dogmatic in that. So pig-headedly unreasonable you think, \"How can they possibly believe that!?\" But it soaks up their stubbornness, and then on other things, they can be pretty open-minded. So don't fall into the trap of thinking because you're agnostic on somethings, that you're being fundamentally reasonable about your self-deception and your stories and your open-mindedness.</p>\n<p>This idea of hovering, of epistemological hovering, and messiness, and incompleteness, and not everything ties up into a neat bow, and you're really not on a journey here. You're here for some messy reason or reasons, and maybe you don't know what it is, and maybe I don't know what it is, but anyway I'm happy to be invited, and thank you all for listening.</p>\n<p>[audience applause]</p>\n</blockquote>\n<p><span style=\"font-size: xx-small;\">By the time I got to the line, \"throw out your Tolstoy,\" I had written \"story\" so many times that I accidentally wrote, \"throw out your Tolstory\".</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"pszEEb3ctztv3rozd": 4, "BhfefamXXee6c2CH8": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4kphivjxngJmEdWsN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 70, "baseScore": 93, "extendedScore": null, "score": 0.00021146733614774127, "legacy": true, "legacyId": "11521", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 73, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["GS5Ef9FePbn6eP2CR"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-17T06:11:18.254Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Fallacy of Gray", "slug": "seq-rerun-the-fallacy-of-gray", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:02.484Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5rniaj2XDW4yYgyHa/seq-rerun-the-fallacy-of-gray", "pageUrlRelative": "/posts/5rniaj2XDW4yYgyHa/seq-rerun-the-fallacy-of-gray", "linkUrl": "https://www.lesswrong.com/posts/5rniaj2XDW4yYgyHa/seq-rerun-the-fallacy-of-gray", "postedAtFormatted": "Saturday, December 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Fallacy%20of%20Gray&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Fallacy%20of%20Gray%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5rniaj2XDW4yYgyHa%2Fseq-rerun-the-fallacy-of-gray%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Fallacy%20of%20Gray%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5rniaj2XDW4yYgyHa%2Fseq-rerun-the-fallacy-of-gray", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5rniaj2XDW4yYgyHa%2Fseq-rerun-the-fallacy-of-gray", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 183, "htmlBody": "<p>Today's post, <a href=\"/lw/mm/the_fallacy_of_gray/\">The Fallacy of Gray</a> was originally published on 07 January 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Nothing is perfectly black or white. Everything is gray. However, this does not mean that everything is the same shade of gray. It may be impossible to completely eliminate bias, but it is still worth reducing bias.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8ve/seq_rerun_but_theres_still_a_chance_right/\">But There's Still A Chance, Right?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5rniaj2XDW4yYgyHa", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.163532609030532e-07, "legacy": true, "legacyId": "11523", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dLJv2CoRCgeC2mPgj", "sgohEEWWC65awhyiT", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-17T11:50:01.021Z", "modifiedAt": null, "url": null, "title": "Anesthesia and Consciousness [link]", "slug": "anesthesia-and-consciousness-link", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Pc4EBuXuJKsjK6YKT/anesthesia-and-consciousness-link", "pageUrlRelative": "/posts/Pc4EBuXuJKsjK6YKT/anesthesia-and-consciousness-link", "linkUrl": "https://www.lesswrong.com/posts/Pc4EBuXuJKsjK6YKT/anesthesia-and-consciousness-link", "postedAtFormatted": "Saturday, December 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anesthesia%20and%20Consciousness%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnesthesia%20and%20Consciousness%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPc4EBuXuJKsjK6YKT%2Fanesthesia-and-consciousness-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anesthesia%20and%20Consciousness%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPc4EBuXuJKsjK6YKT%2Fanesthesia-and-consciousness-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPc4EBuXuJKsjK6YKT%2Fanesthesia-and-consciousness-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 136, "htmlBody": "<p><a href=\"http://www.newscientist.com/article/mg21228402.300-banishing-consciousness-the-mystery-of-anaesthesia.html\">link</a></p>\n<blockquote>Engel found that at the deepest levels of anaesthesia, the primary sensory cortex was the only region to respond to the electric shock. \"Long-distance communication seems to be blocked, so the brain cannot build the global workspace,\" says Engel, who presented the work at last year's Society for Neuroscience meeting in San Diego. \"It's like the message is reaching the mailbox, but no one is picking it up.\"</blockquote>\n<blockquote><br /></blockquote>\n<blockquote>What could be causing the blockage? Engel has unpublished EEG data suggesting that propofol interferes with communication between the primary sensory cortex and other brain regions by causing abnormally strong synchrony between them. \"It's not just shutting things down. The communication has changed,\" he says. \"If too many neurons fire in a strongly synchronised rhythm, there is no room for exchange of specific messages.\"</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Pc4EBuXuJKsjK6YKT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 9, "extendedScore": null, "score": 8.164775375135995e-07, "legacy": true, "legacyId": "11525", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-17T15:02:28.193Z", "modifiedAt": null, "url": null, "title": "MINE: Free tool for detecting novel associations in large data sets", "slug": "mine-free-tool-for-detecting-novel-associations-in-large", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:02.919Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "curiousepic", "createdAt": "2010-04-15T14:35:25.116Z", "isAdmin": false, "displayName": "curiousepic"}, "userId": "wxLCJJwvPiQbkXjTe", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BHH5LeAXdKyd2G9kA/mine-free-tool-for-detecting-novel-associations-in-large", "pageUrlRelative": "/posts/BHH5LeAXdKyd2G9kA/mine-free-tool-for-detecting-novel-associations-in-large", "linkUrl": "https://www.lesswrong.com/posts/BHH5LeAXdKyd2G9kA/mine-free-tool-for-detecting-novel-associations-in-large", "postedAtFormatted": "Saturday, December 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20MINE%3A%20Free%20tool%20for%20detecting%20novel%20associations%20in%20large%20data%20sets&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMINE%3A%20Free%20tool%20for%20detecting%20novel%20associations%20in%20large%20data%20sets%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBHH5LeAXdKyd2G9kA%2Fmine-free-tool-for-detecting-novel-associations-in-large%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=MINE%3A%20Free%20tool%20for%20detecting%20novel%20associations%20in%20large%20data%20sets%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBHH5LeAXdKyd2G9kA%2Fmine-free-tool-for-detecting-novel-associations-in-large", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBHH5LeAXdKyd2G9kA%2Fmine-free-tool-for-detecting-novel-associations-in-large", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 77, "htmlBody": "<p>I was waiting for someone more&nbsp;knowledgeable&nbsp;to post something about this, but it's been a couple days and thought I'd bring it to LW's attention.</p>\n<blockquote>\n<p>The maximal information coefficient (MIC) is a measure of two-variable dependence developed with the guidelines of generality and equitability in mind. The published paper describing MIC shows that it comes very close to achieving both goals simultaneously, and that it significantly outperforms competing methods in this regard.</p>\n</blockquote>\n<p><a href=\"http://www.broadinstitute.org/news-and-publications/mine-detecting-novel-associations-large-data-sets\">Video summary</a></p>\n<p><a href=\"http://www.exploredata.net/\">Main site with access to tool</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BHH5LeAXdKyd2G9kA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 7, "extendedScore": null, "score": 8.165481650090942e-07, "legacy": true, "legacyId": "11527", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-17T22:24:40.075Z", "modifiedAt": null, "url": null, "title": "[LINK] High school students coerced into non-optimal philanthropy via psychological warfare", "slug": "link-high-school-students-coerced-into-non-optimal", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:04.082Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dallas", "createdAt": "2011-08-22T11:21:12.081Z", "isAdmin": false, "displayName": "Dallas"}, "userId": "JtDFxbaMfnDsuEvCj", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/RMMacPWZzE9kLLReJ/link-high-school-students-coerced-into-non-optimal", "pageUrlRelative": "/posts/RMMacPWZzE9kLLReJ/link-high-school-students-coerced-into-non-optimal", "linkUrl": "https://www.lesswrong.com/posts/RMMacPWZzE9kLLReJ/link-high-school-students-coerced-into-non-optimal", "postedAtFormatted": "Saturday, December 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20High%20school%20students%20coerced%20into%20non-optimal%20philanthropy%20via%20psychological%20warfare&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20High%20school%20students%20coerced%20into%20non-optimal%20philanthropy%20via%20psychological%20warfare%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRMMacPWZzE9kLLReJ%2Flink-high-school-students-coerced-into-non-optimal%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20High%20school%20students%20coerced%20into%20non-optimal%20philanthropy%20via%20psychological%20warfare%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRMMacPWZzE9kLLReJ%2Flink-high-school-students-coerced-into-non-optimal", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FRMMacPWZzE9kLLReJ%2Flink-high-school-students-coerced-into-non-optimal", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 175, "htmlBody": "<p>From the article:</p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 13px; line-height: 18px;\"> </span></p>\n<p class=\"body.text\" style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1.5em; margin-left: 0px; color: #3d3c3c; padding: 0px;\">For someone who isn&rsquo;t a fan of teen idol Justin Bieber, being forced to listen to one of his songs over and over again could be considered cruel and unusual punishment.</p>\n<p class=\"body.text\" style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1.5em; margin-left: 0px; color: #3d3c3c; padding: 0px;\">At Evanston Township High School this week, they called it a fund-raiser.</p>\n<p class=\"body.text\" style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1.5em; margin-left: 0px; color: #3d3c3c; padding: 0px;\">To motivate their fellow students to donate money for a struggling cafe/arts center popular with ETHS kids, seniors Charlotte Runzel and Jesse Chatz persuaded administrators to let them blast Bieber&rsquo;s hit &ldquo;Baby&rdquo; over the school&rsquo;s loudspeaker system at the end of each class period &mdash; and not stop playing the song until Runzel and Chatz had met their goal.</p>\n<p class=\"body.text\" style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1.5em; margin-left: 0px; color: #3d3c3c; padding: 0px;\">...</p>\n<span style=\"font-family: Arial, Helvetica, sans-serif; font-size: 13px; line-height: 18px;\">\n<p class=\"body.text\" style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1.5em; margin-left: 0px; color: #3d3c3c; padding: 0px;\">Perhaps not surprisingly, Runzel and Chatz, who were given one week to meet their goal of $1,000 for Boocoo cafe on Church Street, were able to raise the money in just three days.</p>\n<p class=\"body.text\" style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1.5em; margin-left: 0px; color: #3d3c3c; padding: 0px;\">&ldquo;It made me smile to look at what we can do and look at the money we are raising,&rdquo; said Chatz.</p>\n</span>\n<p>&nbsp;</p>\n</blockquote>\n<p><a href=\"http://www.suntimes.com/9449070-418/baby-baby-baby-no-pay-up-or-be-forced-to-listen-to-justin-bieber.html\">Link.</a>&nbsp;(Chicago Sun-Times)</p>\n<p>I wonder if this atrocity is going to go unpunished?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "RMMacPWZzE9kLLReJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 10, "extendedScore": null, "score": 3e-05, "legacy": true, "legacyId": "11529", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-17T22:38:45.029Z", "modifiedAt": null, "url": null, "title": "[Link] Belief in religion considered harmful?", "slug": "link-belief-in-religion-considered-harmful", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:39.659Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "ziAGPmXhLcpYj8Zjv", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fMHq4djhTRBFedQyq/link-belief-in-religion-considered-harmful", "pageUrlRelative": "/posts/fMHq4djhTRBFedQyq/link-belief-in-religion-considered-harmful", "linkUrl": "https://www.lesswrong.com/posts/fMHq4djhTRBFedQyq/link-belief-in-religion-considered-harmful", "postedAtFormatted": "Saturday, December 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Belief%20in%20religion%20considered%20harmful%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Belief%20in%20religion%20considered%20harmful%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfMHq4djhTRBFedQyq%2Flink-belief-in-religion-considered-harmful%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Belief%20in%20religion%20considered%20harmful%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfMHq4djhTRBFedQyq%2Flink-belief-in-religion-considered-harmful", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfMHq4djhTRBFedQyq%2Flink-belief-in-religion-considered-harmful", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1979, "htmlBody": "<p>I've recently run across <a href=\"http://unqualified-reservations.blogspot.com/2007/04/why-do-atheists-believe-in-religion.html\">this</a> 2007 post on the blog Unqualified Reservations (archive best read <a href=\"http://moldbuggery.blogspot.com/\">here</a>). It is written by Mencious Moldbug, who is probably familiar to some Overcoming Bias and Lesswrong readers. He is a erudite, controversial and most of all contrarian social critic and writer. In 2010 he <a href=\"http://vimeo.com/9262193\">debated</a> Robin Hanson on the subject of <a href=\"http://en.wikipedia.org/wiki/Futarchy\">Futarchy</a>.</p>\n<blockquote>\n<h2>Why do atheists believe in religion? <br /></h2>\n<p>Not everyone these days believes in God. But pretty much everyone believes in religion.<br /><br /><strong>By \"believing in religion,\" I mean recognizing a significant categorical distinction between \"religious\" phenomena, and those that are \"nonreligious\" or \"secular.\"</strong><br /><br />For example, the concepts of \"freedom of religion\" and \"separation of church and state\" are dependent on the concept of \"religion.\" If \"religion\" is a noninformative, unimportant, or confusing category, these concepts must also be noninformative, unimportant, or confusing.<br /><br />Since most atheists, agnostics, etc, consider the First Amendment pretty important, we can assume they \"believe in religion.\"<br /><br />My question is: <strong>why?</strong> <strong>Is this a useful belief? Does it help us understand the world? Or does it confuse or misinform us?</strong> Once again, our team of crack philosophers is on the case.<br /><br />Let's rule out the possibility that \"religion\" is noninformative. We can define \"religion\" as the attribution of existence to anthropomorphic paranormal entities. This definition has its fuzzy corner cases, notably some kinds of Buddhism, but it's short and it'll do for the moment.<br /><br />We are left with the question: is \"religion\" an important or clarifying category? Or is it unimportant and confusing?<br /><br />If you believe in God, obviously you have to believe in religion. Religion is an important category because your religion is true, and all other religions are false. (As Sam Harris puts it, \"everyone's an atheist with respect to Zeus.\")<br /><br />For atheists of the all-around variety - including me - the question remains. Why do we believe in \"religion?\"<br /><br /><strong>One obvious answer is that we have to share the planet with a lot of religious people.</strong> If you are an atheist, there is no getting around it: <strong> religion, as per Dawkins, is a delusion.</strong> <strong> Deluded people do crazy things and are often dangerous. </strong>We need to have a category for these people, just as we have a category for \"large, man-eating carnivores.\"<strong> Certainly, religious violence has killed a lot more people lately than lions, tigers, or bears.</strong><br /><br />This argument sounds convincing, but it hides a fallacy.<br /><br />The fallacy is that the <em>distinction</em> between \"religion\" and <em>other classes of delusion</em> must be clarifying or important. If there is a case for this proposition, we haven't met it yet.<br /><br /><strong>Peoples' actions matter. And peoples' beliefs matter, because they motivate actions.</strong><br /><br />But actions <em>in the real world</em> must be motivated by beliefs <em>about the real world</em>. Delusions about the paranormal world are only relevant - at least to us atheists - in the special case that they motivate delusions about the real world.<br /><br /><strong>So, as atheists, why should we care about the former? Why not forget about the details of metaphysical doctrine, which pertain to an ethereal plane that doesn't even exist, and concentrate our attention on beliefs about reality?</strong><br /><br />If you believe that nine Jewish virgins need to be thrown into Mt. Fuji, you are, in my opinion, deluded. Whether you believe this because you are receiving secret messages from Amaterasu Omikami, or because it's just payback for the dirty deeds of the Elders of Zion, affects neither me nor the virgins.<br /><br />If you believe \"partial-birth abortion\" is wrong because it's \"against God's law,\" or if you think it's just \"unethical,\" your vote will be the same.<br /><br />If you are tolerant and respectful of others because you think Allah wants you to be tolerant and respectful of others, how can I possibly have a problem with this? If you stab people in the street because you've misinterpreted Nietzsche and decided that morality is not for you, is that less of a problem?<br /><br />Lots of people have delusions about the real world. People believe all kinds of crazy things for all kinds of crazy reasons. Some even believe sensible things for crazy reasons. <strong>Why should we establish a special category for delusions that are motivated by anthropomorphic paranormal forces?</strong></p>\n<p>A reasonable answer is: why not?<br /><br /><strong>Certainly, religion is an important force in the world today.</strong> <strong>Certainly at least some forms of religion - \"fundamentalist,\" one might say - are actively dangerous.</strong> No one is actually stabbing people in the street because of Nietzsche. The same cannot be said for Allah.<br /><br />How can it possibly confuse or distract us to recognize and protect ourselves against this important class of delusion?</p>\n<p>To see the answer, we need to break Godwin's Law.</p>\n<p>Which I think may indeed be appropriate.&nbsp; <br /><br />Suppose Hitler had declared that, rather than being just some guy from Linz, he was Thor's prophet on earth. (<a href=\"http://en.wikipedia.org/Thule_Society\">Some people</a> would have been positively delighted by this.) Suppose that everything the Nazis did was done in the name of Thor. Suppose, in other words, that Nazism was in the category \"religion.\"</p>\n<p><strong>This is by no means a new idea.</strong></p>\n</blockquote>\n<p>Violating <a href=\"http://en.wikipedia.org/wiki/Godwin%27s_law\">Godwin's law</a> to breach the fence between religion and ideology to see what cognitive dissonances we can dredge up is old hat for us LWers (<a href=\"/lw/fm/a_parable_on_obsolete_ideologies/\">A Parable On Obsolete Ideologies</a> 2009 by Yvain).</p>\n<blockquote>\n<p>Many writers, including <a href=\"http://en.wikipedia.org/wiki/Political_religion\">Eric Voegelin</a>, <a href=\"http://en.wikipedia.org/wiki/Eric_Hoffer\">Eric Hoffer</a>, <a href=\"http://en.wikipedia.org/wiki/Victor_Klemperer\">Victor Klemperer</a>, <a href=\"http://en.wikipedia.org/wiki/Michael_Burleigh\">Michael Burleigh</a>, etc, etc, have described the similarities between Nazism and religions. But Nazism does not fit our definition of religion above - no paranormal entities. This is the definition most people use, so most people don't think of Nazism as a religion.<br /><br />The Allies invaded Nazi Germany and completely suppressed Nazism. To this day in Germany it is illegal to teach National Socialism. I think most Americans, and most Germans, would agree that this is a good thing.<br /><br />But if we make this one trivial change,<strong> turning Nazism into Thorism and making it a \"religion,\"</strong> which as we've seen need not change the magnitude or details of Nazi crimes at all, <strong>the acts of the Allies are a blatant act of religious intolerance.</strong><br /><br /><strong>Aren't we supposed to respect other faiths?</strong> Shouldn't we at least have restricted our unfriendly attentions to \"fundamentalist Nazism,\" and promoted a more \"moderate\" version of the creed? Suppose we gave the Taliban the same treatment? What, exactly, is the difference between Eisenhower's policy and <a href=\"http://www.nationalreview.com/coulter/coulter.shtml\">Ann Coulter's</a>?<br /><br />It gets worse. Another one of Voegelin's \"political religions,\" which by our definition are not religions at all (no anthropomorphic paranormal entities) is Marxism. Let's tweak Marxism slightly and assert that the writings of Marx were divinely inspired, leaving everything else in the history of Communism unchanged.<br /><br />Marxism, unlike Nazism, is still very popular in the world today. A substantial fraction of the professors in Western universities are either Marxists, or strongly influenced by Marxist thought. Nor are these beliefs passive - many fields that are actively taught and quite popular, such as <a href=\"http://en.wikipedia.org/wiki/Postcolonialism\">postcolonial studies</a>, seem largely or entirely Marxist in content.<br /><br />This is certainly not true of Nazism. It is also not true of Christianity or any other \"religion\" proper. Many professors are Christians, true, and some are even fundamentalists. But the US educational system is quite sensitive to the possibility that it might be indoctrinating youth with Christian fundamentalism. \"Creation science,\" for example, is not taught in any mainstream university and seems unlikely to achieve that status.<br /><br />If Marxism was a religion, Marxist economics would come pretty close to being the exact equivalent of \"intelligent design.\" But, again, Marxism as religion and Marxism as non-religion involve exactly the same set of delusions about the real world. (Of course, to a Marxist, they are not delusions.)<br /><br />Should non-Marxist atheists, such as myself, be as concerned about separating Marxism from state-supported education as we are with Christianity? If Marxism is a religion, or if the difference between Marxism as it is in the real world and the version in which Marx was a prophet is insignificant, our \"wall of separation\" is a torn-up chainlink fence.<br /><br />But there was a period in which Americans tried to eradicate Marxism the way they fight against \"intelligent design\" today. It was called <a href=\"http://en.wikipedia.org/wiki/Mccarthyism\">McCarthyism</a>. And believers in civil liberties were on exactly the opposite side of the barricades.<br /><br />As non-Marxist atheists, do we want McCarthy 2.0? Should loyalty oaths be hip this year? Should we schedule new hearings?<br /><br /><strong>This is why the concept of \"religion\" is harmful. If trivial changes to hypothetical history convert reasonable policies into monstrous injustices, or vice versa, your perception of reality cannot be correct. </strong> You have been infected by a toxic <a href=\"http://en.wikipedia.org/wiki/Meme\">meme</a>.<strong><br /><br />If memes are analogous to parasitic organisms, believing in \"religion\" is like taking a narrow-spectrum antibiotic on an irregular schedule. </strong> The <a href=\"http://en.wikipedia.org/wiki/God_delusion\">Dawkins treatment</a> - our latest version of what used to be called <a href=\"http://en.wikipedia.org/wiki/Anticlericalism\">anticlericalism</a> - wipes out a colony of susceptible bacteria which have spent a long time learning to coexist reasonably, if imperfectly, with the host. And clears the field for an entirely different phylum of bugs which are unaffected by antireligious therapy. Whose growth, in fact, it may even stimulate.<br /><br /><strong>In the last two centuries, \"political religions\" have caused far, far more morbidity than \"religious religions.\"</strong> But here we are with Dawkins, Harris, and Dennett - still popping the penicillin. Hm. Kind of makes you think, doesn't it?</p>\n</blockquote>\n<p>I hope you can now see reason I've picked a partially misleading title, since I think Moldbug makes a pretty convincing argument that belief in \"religion\" may be considered harmful even for atheists, let alone those of us who aspire to refine rationality.</p>\n<p>In such a model questions like <em>\"is the <a href=\"http://en.wikipedia.org/wiki/Church_of_Scientology\">Church of Scientology</a> a religion?\"</em> dissolve rapidly. Whether something should be tax exempt because it is \"really\" a \"religion\" or \"a church\" is a legal question of importance only to activists trying to challenge law and lawyers, that shouldn't change our ethical intuitions or cause us to try to imagine a sea or play up rather minor geographical features, to <a href=\"http://en.wikipedia.org/wiki/Boundaries_between_continents#Europe_and_Asia\">separate the continents</a> of Religion and Ideology in our maps of reality.</p>\n<p>Every single proposed mechanism for the retention and spread of religion from convenient curiosity stoppers, indoctrination of youth, to tribal identity markers hold for ideology just as strongly as for religion. Even seemingly very specific memetic adaptations like <a href=\"http://en.wikipedia.org/wiki/God_of_the_gaps\">\"God of the gaps\"</a>, seem to arise in various non-theistic ideologies. Maybe similar adaptations arise because <em>it is the</em> <em>same niche</em>?</p>\n<p>Thinking about the implications of such a hypothesis, atheism for one additional god is a rather easy step of rationality to take. Very few people believe in the great Juju or Zeus. Adding YHWH to the list isn't that much of a stretch, for those fortunate enough to be educated and living in most of the West.</p>\n<p>But how hard is it for someone to question, in a unbiased fashion, such gods and <a href=\"http://www.youtube.com/watch?v=ipe5EjcchvY\">holy words</a> such as say <a href=\"/lw/jb/applause_lights/\">Democracy</a>?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NSMKfa8emSbGNXRKD": 1, "FkzScn5byCs9PxGsA": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fMHq4djhTRBFedQyq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 33, "extendedScore": null, "score": 6.1e-05, "legacy": true, "legacyId": "11528", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>I've recently run across <a href=\"http://unqualified-reservations.blogspot.com/2007/04/why-do-atheists-believe-in-religion.html\">this</a> 2007 post on the blog Unqualified Reservations (archive best read <a href=\"http://moldbuggery.blogspot.com/\">here</a>). It is written by Mencious Moldbug, who is probably familiar to some Overcoming Bias and Lesswrong readers. He is a erudite, controversial and most of all contrarian social critic and writer. In 2010 he <a href=\"http://vimeo.com/9262193\">debated</a> Robin Hanson on the subject of <a href=\"http://en.wikipedia.org/wiki/Futarchy\">Futarchy</a>.</p>\n<blockquote>\n<h2 id=\"Why_do_atheists_believe_in_religion__\">Why do atheists believe in religion? <br></h2>\n<p>Not everyone these days believes in God. But pretty much everyone believes in religion.<br><br><strong>By \"believing in religion,\" I mean recognizing a significant categorical distinction between \"religious\" phenomena, and those that are \"nonreligious\" or \"secular.\"</strong><br><br>For example, the concepts of \"freedom of religion\" and \"separation of church and state\" are dependent on the concept of \"religion.\" If \"religion\" is a noninformative, unimportant, or confusing category, these concepts must also be noninformative, unimportant, or confusing.<br><br>Since most atheists, agnostics, etc, consider the First Amendment pretty important, we can assume they \"believe in religion.\"<br><br>My question is: <strong>why?</strong> <strong>Is this a useful belief? Does it help us understand the world? Or does it confuse or misinform us?</strong> Once again, our team of crack philosophers is on the case.<br><br>Let's rule out the possibility that \"religion\" is noninformative. We can define \"religion\" as the attribution of existence to anthropomorphic paranormal entities. This definition has its fuzzy corner cases, notably some kinds of Buddhism, but it's short and it'll do for the moment.<br><br>We are left with the question: is \"religion\" an important or clarifying category? Or is it unimportant and confusing?<br><br>If you believe in God, obviously you have to believe in religion. Religion is an important category because your religion is true, and all other religions are false. (As Sam Harris puts it, \"everyone's an atheist with respect to Zeus.\")<br><br>For atheists of the all-around variety - including me - the question remains. Why do we believe in \"religion?\"<br><br><strong>One obvious answer is that we have to share the planet with a lot of religious people.</strong> If you are an atheist, there is no getting around it: <strong> religion, as per Dawkins, is a delusion.</strong> <strong> Deluded people do crazy things and are often dangerous. </strong>We need to have a category for these people, just as we have a category for \"large, man-eating carnivores.\"<strong> Certainly, religious violence has killed a lot more people lately than lions, tigers, or bears.</strong><br><br>This argument sounds convincing, but it hides a fallacy.<br><br>The fallacy is that the <em>distinction</em> between \"religion\" and <em>other classes of delusion</em> must be clarifying or important. If there is a case for this proposition, we haven't met it yet.<br><br><strong>Peoples' actions matter. And peoples' beliefs matter, because they motivate actions.</strong><br><br>But actions <em>in the real world</em> must be motivated by beliefs <em>about the real world</em>. Delusions about the paranormal world are only relevant - at least to us atheists - in the special case that they motivate delusions about the real world.<br><br><strong>So, as atheists, why should we care about the former? Why not forget about the details of metaphysical doctrine, which pertain to an ethereal plane that doesn't even exist, and concentrate our attention on beliefs about reality?</strong><br><br>If you believe that nine Jewish virgins need to be thrown into Mt. Fuji, you are, in my opinion, deluded. Whether you believe this because you are receiving secret messages from Amaterasu Omikami, or because it's just payback for the dirty deeds of the Elders of Zion, affects neither me nor the virgins.<br><br>If you believe \"partial-birth abortion\" is wrong because it's \"against God's law,\" or if you think it's just \"unethical,\" your vote will be the same.<br><br>If you are tolerant and respectful of others because you think Allah wants you to be tolerant and respectful of others, how can I possibly have a problem with this? If you stab people in the street because you've misinterpreted Nietzsche and decided that morality is not for you, is that less of a problem?<br><br>Lots of people have delusions about the real world. People believe all kinds of crazy things for all kinds of crazy reasons. Some even believe sensible things for crazy reasons. <strong>Why should we establish a special category for delusions that are motivated by anthropomorphic paranormal forces?</strong></p>\n<p>A reasonable answer is: why not?<br><br><strong>Certainly, religion is an important force in the world today.</strong> <strong>Certainly at least some forms of religion - \"fundamentalist,\" one might say - are actively dangerous.</strong> No one is actually stabbing people in the street because of Nietzsche. The same cannot be said for Allah.<br><br>How can it possibly confuse or distract us to recognize and protect ourselves against this important class of delusion?</p>\n<p>To see the answer, we need to break Godwin's Law.</p>\n<p>Which I think may indeed be appropriate.&nbsp; <br><br>Suppose Hitler had declared that, rather than being just some guy from Linz, he was Thor's prophet on earth. (<a href=\"http://en.wikipedia.org/Thule_Society\">Some people</a> would have been positively delighted by this.) Suppose that everything the Nazis did was done in the name of Thor. Suppose, in other words, that Nazism was in the category \"religion.\"</p>\n<p><strong id=\"This_is_by_no_means_a_new_idea_\">This is by no means a new idea.</strong></p>\n</blockquote>\n<p>Violating <a href=\"http://en.wikipedia.org/wiki/Godwin%27s_law\">Godwin's law</a> to breach the fence between religion and ideology to see what cognitive dissonances we can dredge up is old hat for us LWers (<a href=\"/lw/fm/a_parable_on_obsolete_ideologies/\">A Parable On Obsolete Ideologies</a> 2009 by Yvain).</p>\n<blockquote>\n<p>Many writers, including <a href=\"http://en.wikipedia.org/wiki/Political_religion\">Eric Voegelin</a>, <a href=\"http://en.wikipedia.org/wiki/Eric_Hoffer\">Eric Hoffer</a>, <a href=\"http://en.wikipedia.org/wiki/Victor_Klemperer\">Victor Klemperer</a>, <a href=\"http://en.wikipedia.org/wiki/Michael_Burleigh\">Michael Burleigh</a>, etc, etc, have described the similarities between Nazism and religions. But Nazism does not fit our definition of religion above - no paranormal entities. This is the definition most people use, so most people don't think of Nazism as a religion.<br><br>The Allies invaded Nazi Germany and completely suppressed Nazism. To this day in Germany it is illegal to teach National Socialism. I think most Americans, and most Germans, would agree that this is a good thing.<br><br>But if we make this one trivial change,<strong> turning Nazism into Thorism and making it a \"religion,\"</strong> which as we've seen need not change the magnitude or details of Nazi crimes at all, <strong>the acts of the Allies are a blatant act of religious intolerance.</strong><br><br><strong>Aren't we supposed to respect other faiths?</strong> Shouldn't we at least have restricted our unfriendly attentions to \"fundamentalist Nazism,\" and promoted a more \"moderate\" version of the creed? Suppose we gave the Taliban the same treatment? What, exactly, is the difference between Eisenhower's policy and <a href=\"http://www.nationalreview.com/coulter/coulter.shtml\">Ann Coulter's</a>?<br><br>It gets worse. Another one of Voegelin's \"political religions,\" which by our definition are not religions at all (no anthropomorphic paranormal entities) is Marxism. Let's tweak Marxism slightly and assert that the writings of Marx were divinely inspired, leaving everything else in the history of Communism unchanged.<br><br>Marxism, unlike Nazism, is still very popular in the world today. A substantial fraction of the professors in Western universities are either Marxists, or strongly influenced by Marxist thought. Nor are these beliefs passive - many fields that are actively taught and quite popular, such as <a href=\"http://en.wikipedia.org/wiki/Postcolonialism\">postcolonial studies</a>, seem largely or entirely Marxist in content.<br><br>This is certainly not true of Nazism. It is also not true of Christianity or any other \"religion\" proper. Many professors are Christians, true, and some are even fundamentalists. But the US educational system is quite sensitive to the possibility that it might be indoctrinating youth with Christian fundamentalism. \"Creation science,\" for example, is not taught in any mainstream university and seems unlikely to achieve that status.<br><br>If Marxism was a religion, Marxist economics would come pretty close to being the exact equivalent of \"intelligent design.\" But, again, Marxism as religion and Marxism as non-religion involve exactly the same set of delusions about the real world. (Of course, to a Marxist, they are not delusions.)<br><br>Should non-Marxist atheists, such as myself, be as concerned about separating Marxism from state-supported education as we are with Christianity? If Marxism is a religion, or if the difference between Marxism as it is in the real world and the version in which Marx was a prophet is insignificant, our \"wall of separation\" is a torn-up chainlink fence.<br><br>But there was a period in which Americans tried to eradicate Marxism the way they fight against \"intelligent design\" today. It was called <a href=\"http://en.wikipedia.org/wiki/Mccarthyism\">McCarthyism</a>. And believers in civil liberties were on exactly the opposite side of the barricades.<br><br>As non-Marxist atheists, do we want McCarthy 2.0? Should loyalty oaths be hip this year? Should we schedule new hearings?<br><br><strong>This is why the concept of \"religion\" is harmful. If trivial changes to hypothetical history convert reasonable policies into monstrous injustices, or vice versa, your perception of reality cannot be correct. </strong> You have been infected by a toxic <a href=\"http://en.wikipedia.org/wiki/Meme\">meme</a>.<strong><br><br>If memes are analogous to parasitic organisms, believing in \"religion\" is like taking a narrow-spectrum antibiotic on an irregular schedule. </strong> The <a href=\"http://en.wikipedia.org/wiki/God_delusion\">Dawkins treatment</a> - our latest version of what used to be called <a href=\"http://en.wikipedia.org/wiki/Anticlericalism\">anticlericalism</a> - wipes out a colony of susceptible bacteria which have spent a long time learning to coexist reasonably, if imperfectly, with the host. And clears the field for an entirely different phylum of bugs which are unaffected by antireligious therapy. Whose growth, in fact, it may even stimulate.<br><br><strong>In the last two centuries, \"political religions\" have caused far, far more morbidity than \"religious religions.\"</strong> But here we are with Dawkins, Harris, and Dennett - still popping the penicillin. Hm. Kind of makes you think, doesn't it?</p>\n</blockquote>\n<p>I hope you can now see reason I've picked a partially misleading title, since I think Moldbug makes a pretty convincing argument that belief in \"religion\" may be considered harmful even for atheists, let alone those of us who aspire to refine rationality.</p>\n<p>In such a model questions like <em>\"is the <a href=\"http://en.wikipedia.org/wiki/Church_of_Scientology\">Church of Scientology</a> a religion?\"</em> dissolve rapidly. Whether something should be tax exempt because it is \"really\" a \"religion\" or \"a church\" is a legal question of importance only to activists trying to challenge law and lawyers, that shouldn't change our ethical intuitions or cause us to try to imagine a sea or play up rather minor geographical features, to <a href=\"http://en.wikipedia.org/wiki/Boundaries_between_continents#Europe_and_Asia\">separate the continents</a> of Religion and Ideology in our maps of reality.</p>\n<p>Every single proposed mechanism for the retention and spread of religion from convenient curiosity stoppers, indoctrination of youth, to tribal identity markers hold for ideology just as strongly as for religion. Even seemingly very specific memetic adaptations like <a href=\"http://en.wikipedia.org/wiki/God_of_the_gaps\">\"God of the gaps\"</a>, seem to arise in various non-theistic ideologies. Maybe similar adaptations arise because <em>it is the</em> <em>same niche</em>?</p>\n<p>Thinking about the implications of such a hypothesis, atheism for one additional god is a rather easy step of rationality to take. Very few people believe in the great Juju or Zeus. Adding YHWH to the list isn't that much of a stretch, for those fortunate enough to be educated and living in most of the West.</p>\n<p>But how hard is it for someone to question, in a unbiased fashion, such gods and <a href=\"http://www.youtube.com/watch?v=ipe5EjcchvY\">holy words</a> such as say <a href=\"/lw/jb/applause_lights/\">Democracy</a>?</p>", "sections": [{"title": "Why do atheists believe in religion? ", "anchor": "Why_do_atheists_believe_in_religion__", "level": 1}, {"title": "This is by no means a new idea.", "anchor": "This_is_by_no_means_a_new_idea_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "54 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 54, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Ltey8BS83qSkd9M3u", "dLbkrPu5STNCBLRjr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-17T22:58:46.209Z", "modifiedAt": null, "url": null, "title": "Selfish reasons for FAI", "slug": "selfish-reasons-for-fai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:58.592Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "snarles", "createdAt": "2009-06-01T03:48:38.132Z", "isAdmin": false, "displayName": "snarles"}, "userId": "YsmFaM5MdsDW8GNop", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Jk6RAS35aCXeD7bbL/selfish-reasons-for-fai", "pageUrlRelative": "/posts/Jk6RAS35aCXeD7bbL/selfish-reasons-for-fai", "linkUrl": "https://www.lesswrong.com/posts/Jk6RAS35aCXeD7bbL/selfish-reasons-for-fai", "postedAtFormatted": "Saturday, December 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Selfish%20reasons%20for%20FAI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASelfish%20reasons%20for%20FAI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJk6RAS35aCXeD7bbL%2Fselfish-reasons-for-fai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Selfish%20reasons%20for%20FAI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJk6RAS35aCXeD7bbL%2Fselfish-reasons-for-fai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJk6RAS35aCXeD7bbL%2Fselfish-reasons-for-fai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 140, "htmlBody": "<p>Let's take for granted that pursuing FAI is the best strategy for researchers interested in the future of all humanity. &nbsp;However, let's also assume that controlling unfriendly AI is not completely impossible. &nbsp;I would like to see arguments on why FAI may or may not be the best strategy for AGI researchers who are solely interested in selfish values: i.e., personal status, curiosity, well-being of their loved ones, etc.</p>\n<p>I believe such discussion is important because i) all researchers are to some extent selfish and ii) it may be unwise to ignore researchers who fail to commit to perfect altruism. &nbsp;I, myself, do not know how selfish I would be if I were to become an AGI researcher in the future.</p>\n<p>&nbsp;</p>\n<p>EDIT: Moved some of the original post content to a comment, since I suspect it was distracting from my main point.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Jk6RAS35aCXeD7bbL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 0, "extendedScore": null, "score": 8.167230070767129e-07, "legacy": true, "legacyId": "11530", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-17T23:32:20.026Z", "modifiedAt": null, "url": null, "title": "Philosophy and Machine Learning Panel on Ethics", "slug": "philosophy-and-machine-learning-panel-on-ethics", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsteinhardt", "createdAt": "2010-08-05T03:07:27.568Z", "isAdmin": false, "displayName": "jsteinhardt"}, "userId": "EF8W65G6RaXxZjLBX", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ChXPWGWDZfJGCXLR7/philosophy-and-machine-learning-panel-on-ethics", "pageUrlRelative": "/posts/ChXPWGWDZfJGCXLR7/philosophy-and-machine-learning-panel-on-ethics", "linkUrl": "https://www.lesswrong.com/posts/ChXPWGWDZfJGCXLR7/philosophy-and-machine-learning-panel-on-ethics", "postedAtFormatted": "Saturday, December 17th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Philosophy%20and%20Machine%20Learning%20Panel%20on%20Ethics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APhilosophy%20and%20Machine%20Learning%20Panel%20on%20Ethics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FChXPWGWDZfJGCXLR7%2Fphilosophy-and-machine-learning-panel-on-ethics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Philosophy%20and%20Machine%20Learning%20Panel%20on%20Ethics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FChXPWGWDZfJGCXLR7%2Fphilosophy-and-machine-learning-panel-on-ethics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FChXPWGWDZfJGCXLR7%2Fphilosophy-and-machine-learning-panel-on-ethics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 125, "htmlBody": "<p>Just got back from <a href=\"http://nips.cc/\">NIPS 2011</a> (Neural Information Processing Systems, one of about 4 major machine learning conferences). One of the workshops there was on Philosophy and Machine Learning. An interesting statistic was that, when asked what topics we should discuss at the same workshop next year, 3 out of 10 of the discussion panelists answered \"ethics\". I didn't keep as close of track of the other responses, but I think they were roughly split between causality and universal learning, with a few also wanting to talk more about induction (there was some overlap there with universal learning).</p>\n<p>The full program can be found <a href=\"http://www.dsi.unive.it/PhiMaLe2011/Program.html\">here</a>, although it is also linked to from the main NIPS website.</p>\n<p>P.S. Did anyone else from LW end up going to NIPS?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ChXPWGWDZfJGCXLR7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 14, "extendedScore": null, "score": 8.167353302111321e-07, "legacy": true, "legacyId": "11531", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-18T03:05:33.679Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Absolute Authority", "slug": "seq-rerun-absolute-authority", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:59.982Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/exFfb4rFGMMnQ9jBy/seq-rerun-absolute-authority", "pageUrlRelative": "/posts/exFfb4rFGMMnQ9jBy/seq-rerun-absolute-authority", "linkUrl": "https://www.lesswrong.com/posts/exFfb4rFGMMnQ9jBy/seq-rerun-absolute-authority", "postedAtFormatted": "Sunday, December 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Absolute%20Authority&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Absolute%20Authority%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FexFfb4rFGMMnQ9jBy%2Fseq-rerun-absolute-authority%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Absolute%20Authority%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FexFfb4rFGMMnQ9jBy%2Fseq-rerun-absolute-authority", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FexFfb4rFGMMnQ9jBy%2Fseq-rerun-absolute-authority", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 202, "htmlBody": "<p>Today's post, <a href=\"/lw/mn/absolute_authority/\">Absolute Authority</a> was originally published on 08 January 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Those without the understanding of the Quantitative way will often map the process of arriving at beliefs onto the social domains of Authority. They think that if Science is not infinitely certain, or if it has ever admitted a mistake, then it is no longer a trustworthy source, and can be ignored. This cultural gap is rather difficult to cross.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8w3/seq_rerun_the_fallacy_of_gray/\">The Fallacy of Gray</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "exFfb4rFGMMnQ9jBy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 8.168136261257462e-07, "legacy": true, "legacyId": "11533", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PmQkensvTGg7nGtJE", "5rniaj2XDW4yYgyHa", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-18T09:54:03.221Z", "modifiedAt": null, "url": null, "title": "List of machine learning researchers", "slug": "list-of-machine-learning-researchers", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Technoguyrob", "createdAt": "2010-12-23T01:08:03.374Z", "isAdmin": false, "displayName": "Technoguyrob"}, "userId": "WxPBN5cdcazmWbcr8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tmMCtroDGPnFncEvj/list-of-machine-learning-researchers", "pageUrlRelative": "/posts/tmMCtroDGPnFncEvj/list-of-machine-learning-researchers", "linkUrl": "https://www.lesswrong.com/posts/tmMCtroDGPnFncEvj/list-of-machine-learning-researchers", "postedAtFormatted": "Sunday, December 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20List%20of%20machine%20learning%20researchers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AList%20of%20machine%20learning%20researchers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtmMCtroDGPnFncEvj%2Flist-of-machine-learning-researchers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=List%20of%20machine%20learning%20researchers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtmMCtroDGPnFncEvj%2Flist-of-machine-learning-researchers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FtmMCtroDGPnFncEvj%2Flist-of-machine-learning-researchers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 354, "htmlBody": "<p>This is not a long post, but I wanted to draw attention to <a href=\"http://en.wikipedia.org/wiki/Category:Machine_learning_researchers\">Wikipedia's list of notable machine learning researchers</a>, because it has done so much for me. For anyone interested in problems related to AI (to get a feel for what AI is like), I recommend reading at least the Wikipedia page of each of those people, followed by playing around on their professional home page (just put their name into Google). You will soon be overwhelmed, but that is to be expected unless you have a PhD in statistics or machine learning. (And believe me, it is much less overwhelming than delving into my world of <a href=\"http://en.wikipedia.org/wiki/List_of_mathematicians\">pure math</a>, for example!)</p>\n<p>Nevertheless, you will run into many insightful things, such as Steve Omohundro's (who is an advisor to SIAI) <a href=\"http://selfawaresystems.com/\">Self-Aware Systems</a>, and Ghahramani's <a href=\"http://mlg.eng.cam.ac.uk/zoubin/topics-list.html\">list of topics for a qualifying exam on machine learning</a>. I also recommend skimming the bibliographies of the research articles for those researchers who post PDFs on their webpages as to get a familiarity for what journals and foundational papers/books are important. (And coming back here to post that information!)</p>\n<p><em>Know these names</em>. When Eliezer uses the words \"the programmers of Friendly AI,\" there is a significantly higher probability than the average person that these names have something to do with that (future graduate students, or otherwise intellectual descendants). If you are yourself a researcher in machine learning, ethics, or statistics, it would be helpful if you could post any names not listed in the above Wikipedia category. Their homepages have utility for us.</p>\n<p>Incidentally, I recommend working through <em>The Elements of Statistical Learning</em>&nbsp;by Hastie, Tibshirani, and Friedman, where a lot of these ideas are gathered into one (certainly not comprehensive) treatise.</p>\n<p>As a related question, is there an equivalent to the unbelievably helpful MathSciNet for other domains? (Where there are reviews of papers available as well as hyperlinks to the database entry for citations as to determine what important papers and books in the field are, etc.)</p>\n<p>One final remark: Under Crocker's Rules, please give me suggestions how to rephrase the above sentences to have less parentheses. I am much too fond of them.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tmMCtroDGPnFncEvj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 0, "extendedScore": null, "score": 1e-06, "legacy": true, "legacyId": "11534", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-18T13:17:50.729Z", "modifiedAt": null, "url": null, "title": "Donating to wikipedia", "slug": "donating-to-wikipedia", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:00.568Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "snarles", "createdAt": "2009-06-01T03:48:38.132Z", "isAdmin": false, "displayName": "snarles"}, "userId": "YsmFaM5MdsDW8GNop", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CP9hAxnQchY559ng3/donating-to-wikipedia", "pageUrlRelative": "/posts/CP9hAxnQchY559ng3/donating-to-wikipedia", "linkUrl": "https://www.lesswrong.com/posts/CP9hAxnQchY559ng3/donating-to-wikipedia", "postedAtFormatted": "Sunday, December 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Donating%20to%20wikipedia&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADonating%20to%20wikipedia%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCP9hAxnQchY559ng3%2Fdonating-to-wikipedia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Donating%20to%20wikipedia%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCP9hAxnQchY559ng3%2Fdonating-to-wikipedia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCP9hAxnQchY559ng3%2Fdonating-to-wikipedia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 71, "htmlBody": "<p>I donated to Wikipedia before. &nbsp;However, this year, I'm not donating, and my rationalization is this: there need to be more Wikipedia clones on the internet, to best prevent the possibility that a single organization (Wikimedia) abuses its control of the biggest worldwide information source. &nbsp;Thus, instead of donating to Wikipedia, I would like to subsidize potential Wikipedia clones. &nbsp;Whether or not it has ads is of little concern to me.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CP9hAxnQchY559ng3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": -14, "extendedScore": null, "score": 8.170381812106507e-07, "legacy": true, "legacyId": "11535", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-18T13:29:04.575Z", "modifiedAt": null, "url": null, "title": "Request: Induction, Proof, Experimental mathematics", "slug": "request-induction-proof-experimental-mathematics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:01:59.372Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "snarles", "createdAt": "2009-06-01T03:48:38.132Z", "isAdmin": false, "displayName": "snarles"}, "userId": "YsmFaM5MdsDW8GNop", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6rdwDw5bhaQotfJ8W/request-induction-proof-experimental-mathematics", "pageUrlRelative": "/posts/6rdwDw5bhaQotfJ8W/request-induction-proof-experimental-mathematics", "linkUrl": "https://www.lesswrong.com/posts/6rdwDw5bhaQotfJ8W/request-induction-proof-experimental-mathematics", "postedAtFormatted": "Sunday, December 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Request%3A%20Induction%2C%20Proof%2C%20Experimental%20mathematics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARequest%3A%20Induction%2C%20Proof%2C%20Experimental%20mathematics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6rdwDw5bhaQotfJ8W%2Frequest-induction-proof-experimental-mathematics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Request%3A%20Induction%2C%20Proof%2C%20Experimental%20mathematics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6rdwDw5bhaQotfJ8W%2Frequest-induction-proof-experimental-mathematics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6rdwDw5bhaQotfJ8W%2Frequest-induction-proof-experimental-mathematics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 59, "htmlBody": "<p>Suppose we made an algorithm capable of forming empirical conjectures for mathematics. &nbsp;How might such an algorithm discover the principle of mathematical proof?</p>\n<p>&nbsp;</p>\n<p>I would like to see an article on the relevant philosophy and mathematical logic background for this problem. &nbsp;Since I currently lack the inclination to research and write up such an article, I instead made this post.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6rdwDw5bhaQotfJ8W", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": -7, "extendedScore": null, "score": 8.170426534399024e-07, "legacy": true, "legacyId": "11536", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-18T14:18:56.858Z", "modifiedAt": null, "url": null, "title": "A model of UDT with a halting oracle", "slug": "a-model-of-udt-with-a-halting-oracle", "viewCount": null, "lastCommentedAt": "2017-06-17T04:19:36.090Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Bj244uWzDBXvE2N2S/a-model-of-udt-with-a-halting-oracle", "pageUrlRelative": "/posts/Bj244uWzDBXvE2N2S/a-model-of-udt-with-a-halting-oracle", "linkUrl": "https://www.lesswrong.com/posts/Bj244uWzDBXvE2N2S/a-model-of-udt-with-a-halting-oracle", "postedAtFormatted": "Sunday, December 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20model%20of%20UDT%20with%20a%20halting%20oracle&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20model%20of%20UDT%20with%20a%20halting%20oracle%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBj244uWzDBXvE2N2S%2Fa-model-of-udt-with-a-halting-oracle%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20model%20of%20UDT%20with%20a%20halting%20oracle%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBj244uWzDBXvE2N2S%2Fa-model-of-udt-with-a-halting-oracle", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBj244uWzDBXvE2N2S%2Fa-model-of-udt-with-a-halting-oracle", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 682, "htmlBody": "<p>This post requires some knowledge of mathematical logic and computability theory. The basic idea is due to Vladimir Nesov and me.</p>\n<p>Let the universe be a computer program U that can make calls to a halting oracle. Let the agent be a subprogram A within U that can also make calls to the oracle. The source code of both A and U is available to A.</p>\n<div>Here's an example U that runs <a href=\"http://wiki.lesswrong.com/wiki/Newcomb's_problem\">Newcomb's problem</a> and returns the resulting utility value:</div>\n<p style=\"font-family:monospace;font-size: 110%;\">&nbsp;&nbsp;def U(): <br />&nbsp;&nbsp;&nbsp;&nbsp;# Fill boxes, according to predicted action. <br />&nbsp;&nbsp;&nbsp;&nbsp;box1 = 1000 <br />&nbsp;&nbsp;&nbsp;&nbsp;box2 = 1000000 if (A() == 1) else 0<br />&nbsp; &nbsp; # Compute reward, based on actual action. <br />&nbsp; &nbsp; return box2 if (A() == 1) else (box1 + box2)</p>\n<p>A complete definition of U should also include the definition of A, so let's define it. We will use the halting oracle only as a&nbsp;<a href=\"/lw/8vp/no_one_knows_what_peano_arithmetic_doesnt_know/\">provability oracle</a>&nbsp;for some formal system S, e.g. Peano arithmetic. Here's the algorithm of A:</p>\n<ol>\n<li>Play chicken with the universe: if S proves that A()&ne;a for some action a, then return a.</li>\n<li>For every possible action a, find some utility value u such that S proves that A()=a &rArr; U()=u. If such a proof cannot be found for some a, break down and cry because the universe is unfair.</li>\n<li>Return the action that corresponds to the highest utility found on step 2.</li>\n</ol>\n<p>Now we want to prove that the agent one-boxes, i.e. A()=1 and U()=1000000. That will follow from two lemmas.</p>\n<p>Lemma 1: S proves that A()=1&nbsp;&rArr; U()=1000000 and A()=2&nbsp;&rArr; U()=1000. Proof:&nbsp;you can derive that from just the source code of U, without looking at A at all.</p>\n<p>Lemma 2: S doesn't prove any other utility values for A()=1 or A()=2.&nbsp;Proof: assume, for example, that&nbsp;S proves that A()=1&nbsp;&rArr; U()=42. But S also proves that&nbsp;A()=1&nbsp;&rArr; U()=1000000, therefore S proves that A()&ne;1. According to the first step of the algorithm, A will play chicken with the universe and return 1, making S&nbsp;<span style=\"text-decoration: line-through;\">inconsistent</span>&nbsp;<a href=\"http://en.wikipedia.org/wiki/Soundness\">unsound</a>&nbsp;(thx Misha). So if S is sound, that can't happen.</p>\n<p>We see that the agent defined above will do the right thing in Newcomb's problem. And the proof transfers easily to many other toy problems, like the symmetric Prisoner's Dilemma.</p>\n<h3>But why? What's the point of this result?</h3>\n<p>There's a big problem about formalizing UDT. If the agent chooses a certain action in a deterministic universe, then it's a true fact about the universe that choosing a different action would have caused Santa to appear. Moreover, if the universe is computable, then such silly logical counterfactuals are not just true but provable in any reasonable formal system. When we can't compare actual decisions with counterfactual ones, it's hard to define what it means for a decision to be \"optimal\".</p>\n<p>For example,&nbsp;<a href=\"/lw/2l2/what_a_reduction_of_could_could_look_like/\">one previous formalization</a>&nbsp;searched for formal proofs up to a specified length limit. Problem is, that limit is a magic constant in the code that can't be derived from the universe program alone. And if you try searching for proofs without a length limit, you might encounter a proof of a \"silly\" counterfactual which will make you stop early before finding the \"serious\" one. Then your decision based on that silly counterfactual can make it true by making its antecedent false... But the bigger problem is that we can't say exactly what makes a \"silly\" counterfactual different from a \"serious\" one.</p>\n<p>In contrast, the new model with oracles has a nice notion of optimality, relative to the agent's formal system. The agent will always return whatever action is proved by the formal system to be optimal, if such an action exists. This notion of optimality matches our intuitions even though the universe is still perfectly deterministic and the agent is still embedded in it, because the oracle ensures that determinism is just out of the formal system's reach.</p>\n<p><strong>P.S.</strong>&nbsp;I became a SingInst&nbsp;<a href=\"http://intelligence.org/aboutus/researchassociates\">research associate</a>&nbsp;on Dec 1. They did not swear me to secrecy, and I hope this post shows that I'm still a fan of working in the open. I might just try to be a little more careful because I wouldn't want to discredit SingInst by making stupid math mistakes in public :-)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"dPPATLhRmhdJtJM2t": 1, "fihKHQuS5WZBJgkRm": 1, "5f5c37ee1b5cdee568cfb26d": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Bj244uWzDBXvE2N2S", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 46, "baseScore": 67, "extendedScore": null, "score": 0.000143, "legacy": true, "legacyId": "11532", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 67, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 102, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PWP5j38tihSHkLsMc", "dC3rxrMkYKLfgTYEa"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-18T18:57:58.570Z", "modifiedAt": null, "url": null, "title": "Rational wart removal", "slug": "rational-wart-removal", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:04.148Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TwistingFingers", "createdAt": "2011-09-16T02:07:04.963Z", "isAdmin": false, "displayName": "TwistingFingers"}, "userId": "tWYLp5wq8caRLNiSJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/d5CzBFdM7XKCtucWW/rational-wart-removal", "pageUrlRelative": "/posts/d5CzBFdM7XKCtucWW/rational-wart-removal", "linkUrl": "https://www.lesswrong.com/posts/d5CzBFdM7XKCtucWW/rational-wart-removal", "postedAtFormatted": "Sunday, December 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rational%20wart%20removal&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARational%20wart%20removal%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd5CzBFdM7XKCtucWW%2Frational-wart-removal%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rational%20wart%20removal%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd5CzBFdM7XKCtucWW%2Frational-wart-removal", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fd5CzBFdM7XKCtucWW%2Frational-wart-removal", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 10, "htmlBody": "<p>What's the best way to get rid of a wart?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "d5CzBFdM7XKCtucWW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 47, "baseScore": -34, "extendedScore": null, "score": 8.171633338968853e-07, "legacy": true, "legacyId": "11537", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-18T21:32:26.850Z", "modifiedAt": "2020-03-04T16:02:33.324Z", "url": null, "title": "Inverse p-zombies: the other direction in the Hard Problem of Consciousness", "slug": "inverse-p-zombies-the-other-direction-in-the-hard-problem-of", "viewCount": null, "lastCommentedAt": "2020-02-17T22:40:44.632Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wzj6WkudtrXQFqL8e/inverse-p-zombies-the-other-direction-in-the-hard-problem-of", "pageUrlRelative": "/posts/wzj6WkudtrXQFqL8e/inverse-p-zombies-the-other-direction-in-the-hard-problem-of", "linkUrl": "https://www.lesswrong.com/posts/wzj6WkudtrXQFqL8e/inverse-p-zombies-the-other-direction-in-the-hard-problem-of", "postedAtFormatted": "Sunday, December 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Inverse%20p-zombies%3A%20the%20other%20direction%20in%20the%20Hard%20Problem%20of%20Consciousness&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AInverse%20p-zombies%3A%20the%20other%20direction%20in%20the%20Hard%20Problem%20of%20Consciousness%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwzj6WkudtrXQFqL8e%2Finverse-p-zombies-the-other-direction-in-the-hard-problem-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Inverse%20p-zombies%3A%20the%20other%20direction%20in%20the%20Hard%20Problem%20of%20Consciousness%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwzj6WkudtrXQFqL8e%2Finverse-p-zombies-the-other-direction-in-the-hard-problem-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fwzj6WkudtrXQFqL8e%2Finverse-p-zombies-the-other-direction-in-the-hard-problem-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2848, "htmlBody": "<blockquote>\n<p>402. \"Nothing is so certain as that I possess consciousness.\" In that case, why shouldn't I let the matter rest? This certainty is like a mighty force whose point of application does not move, and so no work is accomplished by it.</p>\n<p>403. Remember: most people say one feels nothing under anaesthetic. But some say: It <em>could</em> be that one feels, and simply forgets it completely.</p>\n<p>--Wittgenstein, <em>Zettel</em> (1929-1948)</p>\n</blockquote>\n<p>I offer for LW's consideration the interesting 2008 paper <a href=\"https://www.gwern.net/docs/psychology/2008-mashour.pdf\">\"Inverse zombies, anesthesia awareness, and the hard problem of unconsciousness\"</a> (Mashour &amp; LaRock; <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/18635380\">NCBI</a>); the abstract:</p>\n<blockquote>\n<p>Philosophical (p-) zombies are constructs that possess all of the behavioral features and responses of a sentient human being, yet are not conscious. P-zombies are intimately linked to the hard problem of consciousness and have been invoked as arguments against physicalist approaches. But what if we were to invert the characteristics of p-zombies? Such an inverse (i-) zombie would possess all of the behavioral features and responses of an insensate being, yet would nonetheless be conscious. While p-zombies are logically possible but naturally improbable, an approximation of i-zombies actually exists: individuals experiencing what is referred to as \"anesthesia awareness.\" Patients under general anesthesia may be intubated (preventing speech), paralyzed (preventing movement), and narcotized (minimizing response to nociceptive stimuli). Thus, they appear--and typically are--unconscious. In 1-2 cases/1000, however, patients may be aware of intraoperative events, sometimes without any objective indices. Furthermore, a much higher percentage of patients (22% in a recent study) may have the subjective experience of dreaming during general anesthesia. P-zombies confront us with the hard problem of consciousness--how do we explain the presence of qualia? I-zombies present a more practical problem--how do we detect the presence of qualia? The current investigation compares p-zombies to i-zombies and explores the \"hard problem\" of unconsciousness with a focus on anesthesia awareness.</p>\n</blockquote>\n<h1><a></a></h1>\n<h1>3. Inverse zombies<br /></h1>\n<blockquote>\n<p>What would an inverse (i-) zombie look like? Since the p-zombie is a creature that behaves and responds as if it were conscious when in fact it is unconscious, we posit an i-zombie to be a creature that appears to be unconscious when in fact it is conscious. Any query of a p-zombie elicits a response to indicate consciousness; thus, any query of an i-zombie should thus elicit a response (or lack thereof) to indicate the absence of consciousness. Characteristics of the unconscious appearance of an i-zombie could be unresponsiveness to verbal commands, absence of spontaneous or evoked vocalization or speech, absence of spontaneous or evoked movement, and unresponsiveness to noxious stimulus. Like the p-zombie, the concept of the i-zombie entails no logical contradiction and hence can be considered both conceivable and possible. Unlike the p-zombie, however, i-zombies are naturally probable. We argue that a subset of patients experiencing awareness during general anesthesia, or \u2018\u2018<a href=\"https://en.wikipedia.org/wiki/Anesthesia_awareness\">anesthesia awareness</a>,\u201d may fall into the category of i-zombie.<br />Having looked at some differences, we might also consider some similarities between p-zombies and i-zombies. It would seem that whatever solution we find for the problem of detecting consciousness in the case of i-zombies would be equally applicable to p-zombies in some important sense. What sense do we have in mind? In i-zombie cases, some type of consciousness detector could be used to confirm or disconfirm the hypothesis that anesthetized (or possibly even comatose) patients are conscious. In p-zombie cases, we could also use some type of consciousness detector to confirm or reject the same hypothesis with respect to infants, humans, animals, or aliens, which behave and function as if they are conscious. A consciousness detector of some sort would have to be able to distinguish between the presence and absence of consciousness in any possible creature and would therefore apply in detecting both p-zombies and i-zombies. Below we explore potential solutions to this consciousness detection problem (see Section 5).</p>\n</blockquote>\n<h1>4. Anesthesia awareness and anesthetic depth</h1>\n<blockquote>\n<p>Although the terms 'awareness' and 'explicit recall'are distinct and dissociable cognitive processes, in the clinical practice of anesthesiology 'anesthesia awareness' denotes both awareness and subsequent explicit recall of intraoperative events. Anesthesia awareness is a problem receiving increased attention by clinicians, patients, and the general public. A multi-center American study estimated incidence of awareness with explicit recall of approximately 0.13% (Sebel et al., 2004), a rate consistent with large European studies demonstrating awareness in 1-2/1000 cases (Sandin, Enlund, Samuelsson, &amp; Lennmarken, 2000). A proportion of patients experiencing awareness may subsequently develop serious psychological sequelae, including post-traumatic stress disorder (Osterman, Hopper, Heran, Keane, &amp; van der Kolk, 2001).<br />There are a number of subjective states that are associated with general anesthesia. In a recent study, dreaming has been reported in 22% of patients undergoing elective surgery (Leslie, Skrzypek, Paech, Kurowski, &amp; Whybrow, 2007). Awareness itself can vary from the transient perception of conversations in the operating room to the sensation of being awake, paralyzed, and in pain (Sebel et al., 2004). The condition of anesthesia awareness is truly a clinical \u2018\u2018problem of consciousness.\u201d This can also occur in patients with neurologic injury leading to vegetative states or locked-in syndromes (Laureys, Perrin, &amp; Bredart, 2007).<br />...These shortcomings led to the development of EEG techniques to assess anesthetic depth and detect consciousness. In the 1930s, it was demonstrated that the EEG was sensitive to the effects of anesthetics (Gibbs, Gibbs, &amp; Lennox, 1937). There is not, however, a unique electrical signature that is common to all agents. Furthermore, the apparatus is bulky, labor intensive, and requires a dedicated observer in the operating room. Due to these limitations, processed EEG modules that often rely on Fourier transformation have been developed. Such 'awareness monitors' include the Bispectral Index, Narcotrend, Patient State Index, A-line, and others (Mashour, 2006). In general, these modules collect raw EEG and/or electromyographic data, subject them to Fourier transform, and then analyze parameters that are thought to best represent a state of hypnosis. The output is often a dimensionless number, usually on a scale of 100 (wide awake) to 0 (isoelectric EEG). One such monitor has been shown to reduce the incidence of awareness in a high-risk population (Myles, Leslie, McNeil, Forbes, &amp; Chan, 2004), although the results of this study have recently come into question (Avidan et al., 2008).<br />...Such EEG-based monitors, although promising, also have limitations (Dahaba, 2005). Many of these modules are insensitive to well-known anesthetics such as nitrous oxide, ketamine, and xenon. These agents may be pharmacologically similar in their effect on the N-methyl-D-aspartate glutamate receptor. Conversely, EEG monitors can be sensitive to agents that do not suppress consciousness, such as B-adrenergic blockers or neuromuscular blockers. There are other ways by which such 'awareness monitors' can be confounded, such as individuals who have a congenitally low-voltage EEG, as well as patients who are hypothermic or hypoglycemic. Finally, such monitors are subject to artifact from other electrical equipment in the operating room.<br />The current limitations of assessing anesthetic depth entail that we have no completely reliable way to ensure the absence of consciousness in a patient undergoing anesthesia and thus there is a class of individuals who may appear completely unconscious and yet who are nonetheless conscious. Furthermore, despite advances in demonstrating intentionality in patients with persistent vegetative states (Owen et al., 2006), neuroimaging techniques are not practical or even possible for real-time intraoperative monitoring. In short, for all practical purposes, i-zombies are not simply possible or probable\u2014they are known to exist.</p>\n</blockquote>\n<h1>5. Philosophical implications of i-zombies</h1>\n<blockquote>\n<p>Standard philosophical criticisms of behaviorism are built around conceptual considerations alone and sometimes appeal to intuitions that behaviorists would find question-begging. By contrast, the existence of an i-zombie implies a compelling, empirically based counterexample to behaviorism. An i-zombie is not only real, but has feelings without the possibility of behaviorally responding to stimuli. Therefore, feeling is not simply responding to stimuli.<br />...A plausible alternative to behaviorism is functionalism. Functionalism arose on the philosophical scene in response to the shortcomings of behaviorism and type-type identity theory. Functionalism holds that mental states are interdefined in terms of causal relations: the defining characteristic of any mental state P is the set of causal relations that P has with respect to inputs, internal mental processes, and behavioral outputs (Fodor, 2000; see also Churchland, 1996). Instead of characterizing the mind simply in behavorial terms, functionalists argue for the causal efficacy of mental states. For example, my belief that a tidal wave is about to form is caused in me by my perception of wave patterns characteristic of tidal waves; and in relation to my desire to preserve my life, the fear of a potential tidal wave will cause me to seek shelter. In contrast to type-type identity theory, functionalists do not hold that mental states can be identified exclusively with a single type of matter (e.g., the neural stuff that composes our brains), but instead maintain that mental states can be realized in any suitably organized system.<br />An explanatory advantage of functionalism is that it affirms the mental as the source of behavior causation by insisting that mind is defined in terms of function, or by what it does\u2014an interdefined web of causal relations between inputs, inner processes and outputs. An explanatory weakness, however, is that by defining mind in terms of causal relations, functionalism is logically compatible with the absence of experience itself (Armstrong, 2000, p. 142; see also Chalmers, 1996; Churchland, 1996; LaRock, 2007)...In order to motivate functionalism within this practical context, we need to answer a basic question, such as: Where is consciousness caused in the brain?<br />Answering the 'where' question of consciousness in functionalist terms returns us to our discussion of anesthetic depth. In order to localize the neurophysiologic endpoints of anesthesia such as loss of consciousness, we should not use structural space but rather functional or phase space. Phase space, fractal geometry, and strange attractors are now being employed to characterize states of consciousness and anesthesia. In the late 1980s, Watt &amp; Hameroff, (1988) demonstrated that phase space analysis of EEG reveals distinct attractors and dimensions for the waking state, anesthesia, and burst suppression. More recent work from van den Broek, van Rijn, van Egmond, Coenen, &amp; Booij, (2006) confirms fractal dimensionality as a measure of anesthetic depth. <br />...Taken together, one answer to the 'where' question of consciousness and anesthesia is 'phase space'. This form of explanation is consistent with functionalism as it does not attach itself to a specific neural process or location, but rather considers the overall dynamic or 'functional' properties of the system. Furthermore, because it can be applied to EEG analysis of the anesthetized patient, it also holds promise in the detection of i-zombies in the clinical realm.</p>\n</blockquote>\n<h1>6. The hard problem of unconsciousness</h1>\n<blockquote>\n<p>It should be clear immediately that the hard problem of unconsciousness is fundamentally practical or clinical. The fact that there is no uniformly reliable method to identify or predict intraoperative awareness leaves us with a situation in which consciousness is truly a problem. Assuming 30,000,000 general anesthetics delivered every year in the U.S. alone, with an incidence of anesthesia awareness of approximately 0.15%, we are left with 45,000 patients each year who have not had the adequate suppression of qualia. If we include patients who dream during general anesthesia, the number of potential i-zombies increases dramatically.<br />This problem is not limited to the operating room: it is becoming clear that patients who carry a clinical diagnosis of persistent vegetative state are capable of 'responding' (as assessed by functional imaging) in a way that indicates both comprehension and conscious intentionality (Owen et al., 2006). This hard problem of unconsciousness\u2014detecting the presence of qualia\u2014is again relevant. Decisions of continued life support, as in the highly publicized case of Terry Schiavo, are often made on the assumption of an absence of qualia. <br />...The foregoing examples highlight the ethical dimension of the hard problem of unconsciousness. The demonstrated natural possibility of i-zombies has implications for our treatment of individuals presumed unconscious. How should clinicians behave in the operating room given the demonstrated incidence of 1\u20132 individuals/1000 that may still experience qualia during a surgery? Should we comport ourselves acknowledging that the patient has the capacity for suffering? Should we at least ensure that if qualia cannot be extinguished that suffering is minimized with adequate analgesia? Should we restrict our speech to that which is respectful to all patients, conscious or \u2018\u2018unconscious\u201d? These ethical implications seem to readily fall out of the possibility of i-zombies.<br />...A more controversial ethical question relates to life support for patients with a diagnosis of persistent vegetative state. Given recent data suggesting that these patients may somehow covertly experience undetected qualia, what are the implications? Do we need to further consider the possibility that patients with even more dire diagnoses such as coma or brain death could potentially be i-zombies? The ethical exploration of this question is beyond the scope of this essay and would have important implications for end-of-life decision processes in critical care medicine, as well as organ donation.<br />...Furthermore, there can still be brain activation during general anesthesia. For example, primary and feed-forward visual processing persists during general anesthesia, while higher order processing is interrupted (Imas, Ropella, Ward, et al., 2005a, 2005b). A study of auditory processing under propofol anesthesia has reached a similar conclusion (Plourde, Belin, Chartrand, et al., 2006). These findings further emphasize the need to assess which brain states are associated with qualia. Mere activation or arousal of the brain does not necessitate consciousness and may still be a feature of an unconscious being. Indeed, this question touches not simply on the detection but on the very definition of i-zombies.</p>\n</blockquote>\n<p>The awareness rate is chilling. One wonders whether surgery rates would be significantly affected in everyone was aware of this; it's like that utilitarian puzzler 'how much would I have to pay you to torture you with amnesia afterwards?' but in real life.</p>\n<h1>Further reading<br /></h1>\n<ul>\n<li><a href=\"/lw/69/the_zombie_preacher_of_somerset/\">\"The Zombie Preacher of Somerset\"</a></li>\n<li><a href=\"http://www.ncbi.nlm.nih.gov/pubmed/21986366\">\"Consciousness lost and found: Subjective experiences in an unresponsive state\"</a></li>\n</ul>\n<h1>References</h1>\n<ul>\n<li>Armstrong, D. (2000). The nature of mind. In B. Cooney (Ed.). The place of mind (pp. 136\u2013144). Belmont, CA: Wadsworth.</li>\n<li>Avidan, M. S., Zhang, L., Burnside, B. A., Finkel, K. J., Searleman, A. C., Selvidge, J. A., et al (2008). Anesthesia awareness and the bispectral index. The New England Journal of Medicine, 358(11), 1097\u20131108</li>\n<li>Chalmers, D. (1996). The conscious mind in search of a fundamental theory. Oxford: Oxford University Press.</li>\n<li>Churchland, P. (1996). Matter and consciousness. Cambridge: MIT Press.</li>\n<li>Dahaba, A. A. (2005). Different conditions that could result in the bispectral index indicating an incorrect hypnotic state. Anesthesia and Analgesia, 101(3), 765\u2013773</li>\n<li>Fodor, J.J.A. (2000). The Mind-Body Problem. In: J.C. II (Ed.), Problems in mind: Readings in contemporary philosophy of mind (pp. 118-129): Mountain View: Mayfield Publishing</li>\n<li>Gibbs, F. A., Gibbs, L. E., &amp; Lennox, W. G. (1937). Effect on the electroencephalogram of certain drugs which influence nervous activity. Archives of Internal Medicine, 60, 154\u2013166</li>\n<li>Imas, O. A., Ropella, K. M., Ward, B. D., et al (2005a). Volatile anesthetics enhance flash-induced gamma oscillations in rat visual cortex. Anesthesiology,<br />102(5), 937\u2013947.</li>\n<li>Imas, O. A., Ropella, K. M., Ward, B. D., et al (2005b). Volatile anesthetics disrupt frontal-posterior recurrent information transfer at gamma frequencies in rat. Neuroscience Letters, 387(3), 145\u2013150</li>\n<li>LaRock, E. (2007). Disambiguation, binding, and the unity of visual consciousness. Theory and Psychology, 17, 747\u2013777.</li>\n<li>Laureys, S., Perrin, F., &amp; Bredart, S. (2007). Self-consciousness in non-communicative patients. Consciousness and Cognition, 16(3), 722\u2013741. discussion 742\u2013725</li>\n<li>Leslie, K., Skrzypek, H., Paech, M. J., Kurowski, I., &amp; Whybrow, T. (2007). Dreaming during anesthesia and anesthetic depth in elective surgery patients: A prospective cohort study. Anesthesiology, 106(1), 33\u201342.</li>\n<li>Mashour, G. A. (2006). Monitoring consciousness: EEG-based measures of anesthetic depth. Seminars in Anesthesia, Perioperative Medicine and Pain, 25, 205\u2013210</li>\n<li>Myles, P. S., Leslie, K., McNeil, J., Forbes, A., &amp; Chan, M. T. (2004). Bispectral index monitoring to prevent awareness during anaesthesia: The B-aware randomised controlled trial. Lancet, 363(9423), 1757\u20131763</li>\n<li>Osterman, J. E., Hopper, J., Heran, W. J., Keane, T. M., &amp; van der Kolk, B. A. (2001). Awareness under anesthesia and the development of posttraumatic stress disorder. General Hospital Psychiatry, 23(4), 198\u2013204</li>\n<li>Owen, A. M., Coleman, M. R., Boly, M., Davis, M. H., Laureys, S., &amp; Pickard, J. D. (2006). Detecting awareness in the vegetative state. Science, 313(5792), 1402.</li>\n<li>Plourde, G., Belin, P., Chartrand, D., et al (2006). Cortical processing of complex auditory stimuli during alterations of consciousness with the general anesthetic propofol. Anesthesiology, 104(3), 448\u2013457</li>\n<li>Sandin, R. H., Enlund, G., Samuelsson, P., &amp; Lennmarken, C. (2000). Awareness during anaesthesia: A prospective case study. Lancet, 355(9205), 707\u2013711.</li>\n<li>Sebel, P. S., Bowdle, T. A., Ghoneim, M. M., Rampil, I. J., Padilla, R. E., Gan, T. J., et al (2004). The incidence of awareness during anesthesia: A multicenter United States study. Anesthesia and Analgesia, 99(3), 833\u2013839</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"KqrZ5sDEyHm6JaaKp": 1, "uLqT8mmFiA8NeytTi": 1, "XSryTypw5Hszpa4TS": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wzj6WkudtrXQFqL8e", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 28, "extendedScore": null, "score": 5.9e-05, "legacy": true, "legacyId": "11538", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<blockquote>\n<p>402. \"Nothing is so certain as that I possess consciousness.\" In that case, why shouldn't I let the matter rest? This certainty is like a mighty force whose point of application does not move, and so no work is accomplished by it.</p>\n<p>403. Remember: most people say one feels nothing under anaesthetic. But some say: It <em>could</em> be that one feels, and simply forgets it completely.</p>\n<p>--Wittgenstein, <em>Zettel</em> (1929-1948)</p>\n</blockquote>\n<p>I offer for LW's consideration the interesting 2008 paper <a href=\"https://www.gwern.net/docs/psychology/2008-mashour.pdf\">\"Inverse zombies, anesthesia awareness, and the hard problem of unconsciousness\"</a> (Mashour &amp; LaRock; <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/18635380\">NCBI</a>); the abstract:</p>\n<blockquote>\n<p>Philosophical (p-) zombies are constructs that possess all of the behavioral features and responses of a sentient human being, yet are not conscious. P-zombies are intimately linked to the hard problem of consciousness and have been invoked as arguments against physicalist approaches. But what if we were to invert the characteristics of p-zombies? Such an inverse (i-) zombie would possess all of the behavioral features and responses of an insensate being, yet would nonetheless be conscious. While p-zombies are logically possible but naturally improbable, an approximation of i-zombies actually exists: individuals experiencing what is referred to as \"anesthesia awareness.\" Patients under general anesthesia may be intubated (preventing speech), paralyzed (preventing movement), and narcotized (minimizing response to nociceptive stimuli). Thus, they appear--and typically are--unconscious. In 1-2 cases/1000, however, patients may be aware of intraoperative events, sometimes without any objective indices. Furthermore, a much higher percentage of patients (22% in a recent study) may have the subjective experience of dreaming during general anesthesia. P-zombies confront us with the hard problem of consciousness--how do we explain the presence of qualia? I-zombies present a more practical problem--how do we detect the presence of qualia? The current investigation compares p-zombies to i-zombies and explores the \"hard problem\" of unconsciousness with a focus on anesthesia awareness.</p>\n</blockquote>\n<h1><a></a></h1>\n<h1 id=\"3__Inverse_zombies\">3. Inverse zombies<br></h1>\n<blockquote>\n<p>What would an inverse (i-) zombie look like? Since the p-zombie is a creature that behaves and responds as if it were conscious when in fact it is unconscious, we posit an i-zombie to be a creature that appears to be unconscious when in fact it is conscious. Any query of a p-zombie elicits a response to indicate consciousness; thus, any query of an i-zombie should thus elicit a response (or lack thereof) to indicate the absence of consciousness. Characteristics of the unconscious appearance of an i-zombie could be unresponsiveness to verbal commands, absence of spontaneous or evoked vocalization or speech, absence of spontaneous or evoked movement, and unresponsiveness to noxious stimulus. Like the p-zombie, the concept of the i-zombie entails no logical contradiction and hence can be considered both conceivable and possible. Unlike the p-zombie, however, i-zombies are naturally probable. We argue that a subset of patients experiencing awareness during general anesthesia, or \u2018\u2018<a href=\"https://en.wikipedia.org/wiki/Anesthesia_awareness\">anesthesia awareness</a>,\u201d may fall into the category of i-zombie.<br>Having looked at some differences, we might also consider some similarities between p-zombies and i-zombies. It would seem that whatever solution we find for the problem of detecting consciousness in the case of i-zombies would be equally applicable to p-zombies in some important sense. What sense do we have in mind? In i-zombie cases, some type of consciousness detector could be used to confirm or disconfirm the hypothesis that anesthetized (or possibly even comatose) patients are conscious. In p-zombie cases, we could also use some type of consciousness detector to confirm or reject the same hypothesis with respect to infants, humans, animals, or aliens, which behave and function as if they are conscious. A consciousness detector of some sort would have to be able to distinguish between the presence and absence of consciousness in any possible creature and would therefore apply in detecting both p-zombies and i-zombies. Below we explore potential solutions to this consciousness detection problem (see Section 5).</p>\n</blockquote>\n<h1 id=\"4__Anesthesia_awareness_and_anesthetic_depth\">4. Anesthesia awareness and anesthetic depth</h1>\n<blockquote>\n<p>Although the terms 'awareness' and 'explicit recall'are distinct and dissociable cognitive processes, in the clinical practice of anesthesiology 'anesthesia awareness' denotes both awareness and subsequent explicit recall of intraoperative events. Anesthesia awareness is a problem receiving increased attention by clinicians, patients, and the general public. A multi-center American study estimated incidence of awareness with explicit recall of approximately 0.13% (Sebel et al., 2004), a rate consistent with large European studies demonstrating awareness in 1-2/1000 cases (Sandin, Enlund, Samuelsson, &amp; Lennmarken, 2000). A proportion of patients experiencing awareness may subsequently develop serious psychological sequelae, including post-traumatic stress disorder (Osterman, Hopper, Heran, Keane, &amp; van der Kolk, 2001).<br>There are a number of subjective states that are associated with general anesthesia. In a recent study, dreaming has been reported in 22% of patients undergoing elective surgery (Leslie, Skrzypek, Paech, Kurowski, &amp; Whybrow, 2007). Awareness itself can vary from the transient perception of conversations in the operating room to the sensation of being awake, paralyzed, and in pain (Sebel et al., 2004). The condition of anesthesia awareness is truly a clinical \u2018\u2018problem of consciousness.\u201d This can also occur in patients with neurologic injury leading to vegetative states or locked-in syndromes (Laureys, Perrin, &amp; Bredart, 2007).<br>...These shortcomings led to the development of EEG techniques to assess anesthetic depth and detect consciousness. In the 1930s, it was demonstrated that the EEG was sensitive to the effects of anesthetics (Gibbs, Gibbs, &amp; Lennox, 1937). There is not, however, a unique electrical signature that is common to all agents. Furthermore, the apparatus is bulky, labor intensive, and requires a dedicated observer in the operating room. Due to these limitations, processed EEG modules that often rely on Fourier transformation have been developed. Such 'awareness monitors' include the Bispectral Index, Narcotrend, Patient State Index, A-line, and others (Mashour, 2006). In general, these modules collect raw EEG and/or electromyographic data, subject them to Fourier transform, and then analyze parameters that are thought to best represent a state of hypnosis. The output is often a dimensionless number, usually on a scale of 100 (wide awake) to 0 (isoelectric EEG). One such monitor has been shown to reduce the incidence of awareness in a high-risk population (Myles, Leslie, McNeil, Forbes, &amp; Chan, 2004), although the results of this study have recently come into question (Avidan et al., 2008).<br>...Such EEG-based monitors, although promising, also have limitations (Dahaba, 2005). Many of these modules are insensitive to well-known anesthetics such as nitrous oxide, ketamine, and xenon. These agents may be pharmacologically similar in their effect on the N-methyl-D-aspartate glutamate receptor. Conversely, EEG monitors can be sensitive to agents that do not suppress consciousness, such as B-adrenergic blockers or neuromuscular blockers. There are other ways by which such 'awareness monitors' can be confounded, such as individuals who have a congenitally low-voltage EEG, as well as patients who are hypothermic or hypoglycemic. Finally, such monitors are subject to artifact from other electrical equipment in the operating room.<br>The current limitations of assessing anesthetic depth entail that we have no completely reliable way to ensure the absence of consciousness in a patient undergoing anesthesia and thus there is a class of individuals who may appear completely unconscious and yet who are nonetheless conscious. Furthermore, despite advances in demonstrating intentionality in patients with persistent vegetative states (Owen et al., 2006), neuroimaging techniques are not practical or even possible for real-time intraoperative monitoring. In short, for all practical purposes, i-zombies are not simply possible or probable\u2014they are known to exist.</p>\n</blockquote>\n<h1 id=\"5__Philosophical_implications_of_i_zombies\">5. Philosophical implications of i-zombies</h1>\n<blockquote>\n<p>Standard philosophical criticisms of behaviorism are built around conceptual considerations alone and sometimes appeal to intuitions that behaviorists would find question-begging. By contrast, the existence of an i-zombie implies a compelling, empirically based counterexample to behaviorism. An i-zombie is not only real, but has feelings without the possibility of behaviorally responding to stimuli. Therefore, feeling is not simply responding to stimuli.<br>...A plausible alternative to behaviorism is functionalism. Functionalism arose on the philosophical scene in response to the shortcomings of behaviorism and type-type identity theory. Functionalism holds that mental states are interdefined in terms of causal relations: the defining characteristic of any mental state P is the set of causal relations that P has with respect to inputs, internal mental processes, and behavioral outputs (Fodor, 2000; see also Churchland, 1996). Instead of characterizing the mind simply in behavorial terms, functionalists argue for the causal efficacy of mental states. For example, my belief that a tidal wave is about to form is caused in me by my perception of wave patterns characteristic of tidal waves; and in relation to my desire to preserve my life, the fear of a potential tidal wave will cause me to seek shelter. In contrast to type-type identity theory, functionalists do not hold that mental states can be identified exclusively with a single type of matter (e.g., the neural stuff that composes our brains), but instead maintain that mental states can be realized in any suitably organized system.<br>An explanatory advantage of functionalism is that it affirms the mental as the source of behavior causation by insisting that mind is defined in terms of function, or by what it does\u2014an interdefined web of causal relations between inputs, inner processes and outputs. An explanatory weakness, however, is that by defining mind in terms of causal relations, functionalism is logically compatible with the absence of experience itself (Armstrong, 2000, p. 142; see also Chalmers, 1996; Churchland, 1996; LaRock, 2007)...In order to motivate functionalism within this practical context, we need to answer a basic question, such as: Where is consciousness caused in the brain?<br>Answering the 'where' question of consciousness in functionalist terms returns us to our discussion of anesthetic depth. In order to localize the neurophysiologic endpoints of anesthesia such as loss of consciousness, we should not use structural space but rather functional or phase space. Phase space, fractal geometry, and strange attractors are now being employed to characterize states of consciousness and anesthesia. In the late 1980s, Watt &amp; Hameroff, (1988) demonstrated that phase space analysis of EEG reveals distinct attractors and dimensions for the waking state, anesthesia, and burst suppression. More recent work from van den Broek, van Rijn, van Egmond, Coenen, &amp; Booij, (2006) confirms fractal dimensionality as a measure of anesthetic depth. <br>...Taken together, one answer to the 'where' question of consciousness and anesthesia is 'phase space'. This form of explanation is consistent with functionalism as it does not attach itself to a specific neural process or location, but rather considers the overall dynamic or 'functional' properties of the system. Furthermore, because it can be applied to EEG analysis of the anesthetized patient, it also holds promise in the detection of i-zombies in the clinical realm.</p>\n</blockquote>\n<h1 id=\"6__The_hard_problem_of_unconsciousness\">6. The hard problem of unconsciousness</h1>\n<blockquote>\n<p>It should be clear immediately that the hard problem of unconsciousness is fundamentally practical or clinical. The fact that there is no uniformly reliable method to identify or predict intraoperative awareness leaves us with a situation in which consciousness is truly a problem. Assuming 30,000,000 general anesthetics delivered every year in the U.S. alone, with an incidence of anesthesia awareness of approximately 0.15%, we are left with 45,000 patients each year who have not had the adequate suppression of qualia. If we include patients who dream during general anesthesia, the number of potential i-zombies increases dramatically.<br>This problem is not limited to the operating room: it is becoming clear that patients who carry a clinical diagnosis of persistent vegetative state are capable of 'responding' (as assessed by functional imaging) in a way that indicates both comprehension and conscious intentionality (Owen et al., 2006). This hard problem of unconsciousness\u2014detecting the presence of qualia\u2014is again relevant. Decisions of continued life support, as in the highly publicized case of Terry Schiavo, are often made on the assumption of an absence of qualia. <br>...The foregoing examples highlight the ethical dimension of the hard problem of unconsciousness. The demonstrated natural possibility of i-zombies has implications for our treatment of individuals presumed unconscious. How should clinicians behave in the operating room given the demonstrated incidence of 1\u20132 individuals/1000 that may still experience qualia during a surgery? Should we comport ourselves acknowledging that the patient has the capacity for suffering? Should we at least ensure that if qualia cannot be extinguished that suffering is minimized with adequate analgesia? Should we restrict our speech to that which is respectful to all patients, conscious or \u2018\u2018unconscious\u201d? These ethical implications seem to readily fall out of the possibility of i-zombies.<br>...A more controversial ethical question relates to life support for patients with a diagnosis of persistent vegetative state. Given recent data suggesting that these patients may somehow covertly experience undetected qualia, what are the implications? Do we need to further consider the possibility that patients with even more dire diagnoses such as coma or brain death could potentially be i-zombies? The ethical exploration of this question is beyond the scope of this essay and would have important implications for end-of-life decision processes in critical care medicine, as well as organ donation.<br>...Furthermore, there can still be brain activation during general anesthesia. For example, primary and feed-forward visual processing persists during general anesthesia, while higher order processing is interrupted (Imas, Ropella, Ward, et al., 2005a, 2005b). A study of auditory processing under propofol anesthesia has reached a similar conclusion (Plourde, Belin, Chartrand, et al., 2006). These findings further emphasize the need to assess which brain states are associated with qualia. Mere activation or arousal of the brain does not necessitate consciousness and may still be a feature of an unconscious being. Indeed, this question touches not simply on the detection but on the very definition of i-zombies.</p>\n</blockquote>\n<p>The awareness rate is chilling. One wonders whether surgery rates would be significantly affected in everyone was aware of this; it's like that utilitarian puzzler 'how much would I have to pay you to torture you with amnesia afterwards?' but in real life.</p>\n<h1 id=\"Further_reading\">Further reading<br></h1>\n<ul>\n<li><a href=\"/lw/69/the_zombie_preacher_of_somerset/\">\"The Zombie Preacher of Somerset\"</a></li>\n<li><a href=\"http://www.ncbi.nlm.nih.gov/pubmed/21986366\">\"Consciousness lost and found: Subjective experiences in an unresponsive state\"</a></li>\n</ul>\n<h1 id=\"References\">References</h1>\n<ul>\n<li>Armstrong, D. (2000). The nature of mind. In B. Cooney (Ed.). The place of mind (pp. 136\u2013144). Belmont, CA: Wadsworth.</li>\n<li>Avidan, M. S., Zhang, L., Burnside, B. A., Finkel, K. J., Searleman, A. C., Selvidge, J. A., et al (2008). Anesthesia awareness and the bispectral index. The New England Journal of Medicine, 358(11), 1097\u20131108</li>\n<li>Chalmers, D. (1996). The conscious mind in search of a fundamental theory. Oxford: Oxford University Press.</li>\n<li>Churchland, P. (1996). Matter and consciousness. Cambridge: MIT Press.</li>\n<li>Dahaba, A. A. (2005). Different conditions that could result in the bispectral index indicating an incorrect hypnotic state. Anesthesia and Analgesia, 101(3), 765\u2013773</li>\n<li>Fodor, J.J.A. (2000). The Mind-Body Problem. In: J.C. II (Ed.), Problems in mind: Readings in contemporary philosophy of mind (pp. 118-129): Mountain View: Mayfield Publishing</li>\n<li>Gibbs, F. A., Gibbs, L. E., &amp; Lennox, W. G. (1937). Effect on the electroencephalogram of certain drugs which influence nervous activity. Archives of Internal Medicine, 60, 154\u2013166</li>\n<li>Imas, O. A., Ropella, K. M., Ward, B. D., et al (2005a). Volatile anesthetics enhance flash-induced gamma oscillations in rat visual cortex. Anesthesiology,<br>102(5), 937\u2013947.</li>\n<li>Imas, O. A., Ropella, K. M., Ward, B. D., et al (2005b). Volatile anesthetics disrupt frontal-posterior recurrent information transfer at gamma frequencies in rat. Neuroscience Letters, 387(3), 145\u2013150</li>\n<li>LaRock, E. (2007). Disambiguation, binding, and the unity of visual consciousness. Theory and Psychology, 17, 747\u2013777.</li>\n<li>Laureys, S., Perrin, F., &amp; Bredart, S. (2007). Self-consciousness in non-communicative patients. Consciousness and Cognition, 16(3), 722\u2013741. discussion 742\u2013725</li>\n<li>Leslie, K., Skrzypek, H., Paech, M. J., Kurowski, I., &amp; Whybrow, T. (2007). Dreaming during anesthesia and anesthetic depth in elective surgery patients: A prospective cohort study. Anesthesiology, 106(1), 33\u201342.</li>\n<li>Mashour, G. A. (2006). Monitoring consciousness: EEG-based measures of anesthetic depth. Seminars in Anesthesia, Perioperative Medicine and Pain, 25, 205\u2013210</li>\n<li>Myles, P. S., Leslie, K., McNeil, J., Forbes, A., &amp; Chan, M. T. (2004). Bispectral index monitoring to prevent awareness during anaesthesia: The B-aware randomised controlled trial. Lancet, 363(9423), 1757\u20131763</li>\n<li>Osterman, J. E., Hopper, J., Heran, W. J., Keane, T. M., &amp; van der Kolk, B. A. (2001). Awareness under anesthesia and the development of posttraumatic stress disorder. General Hospital Psychiatry, 23(4), 198\u2013204</li>\n<li>Owen, A. M., Coleman, M. R., Boly, M., Davis, M. H., Laureys, S., &amp; Pickard, J. D. (2006). Detecting awareness in the vegetative state. Science, 313(5792), 1402.</li>\n<li>Plourde, G., Belin, P., Chartrand, D., et al (2006). Cortical processing of complex auditory stimuli during alterations of consciousness with the general anesthetic propofol. Anesthesiology, 104(3), 448\u2013457</li>\n<li>Sandin, R. H., Enlund, G., Samuelsson, P., &amp; Lennmarken, C. (2000). Awareness during anaesthesia: A prospective case study. Lancet, 355(9205), 707\u2013711.</li>\n<li>Sebel, P. S., Bowdle, T. A., Ghoneim, M. M., Rampil, I. J., Padilla, R. E., Gan, T. J., et al (2004). The incidence of awareness during anesthesia: A multicenter United States study. Anesthesia and Analgesia, 99(3), 833\u2013839</li>\n</ul>", "sections": [{"title": "3. Inverse zombies", "anchor": "3__Inverse_zombies", "level": 1}, {"title": "4. Anesthesia awareness and anesthetic depth", "anchor": "4__Anesthesia_awareness_and_anesthetic_depth", "level": 1}, {"title": "5. Philosophical implications of i-zombies", "anchor": "5__Philosophical_implications_of_i_zombies", "level": 1}, {"title": "6. The hard problem of unconsciousness", "anchor": "6__The_hard_problem_of_unconsciousness", "level": 1}, {"title": "Further reading", "anchor": "Further_reading", "level": 1}, {"title": "References", "anchor": "References", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "32 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["Fy2b55mLtghd4fQpx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-18T23:47:31.817Z", "modifiedAt": null, "url": null, "title": "The Generalized Anti-Pascal Principle: Utility Convergence of Infinitesimal Probabilities", "slug": "the-generalized-anti-pascal-principle-utility-convergence-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:00.825Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jacob_cannell", "createdAt": "2010-08-24T03:58:15.241Z", "isAdmin": false, "displayName": "jacob_cannell"}, "userId": "N2R9wMRJd7SBSjpiT", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vFCBFS2KBXrgJTr8K/the-generalized-anti-pascal-principle-utility-convergence-of", "pageUrlRelative": "/posts/vFCBFS2KBXrgJTr8K/the-generalized-anti-pascal-principle-utility-convergence-of", "linkUrl": "https://www.lesswrong.com/posts/vFCBFS2KBXrgJTr8K/the-generalized-anti-pascal-principle-utility-convergence-of", "postedAtFormatted": "Sunday, December 18th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Generalized%20Anti-Pascal%20Principle%3A%20Utility%20Convergence%20of%20Infinitesimal%20Probabilities&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Generalized%20Anti-Pascal%20Principle%3A%20Utility%20Convergence%20of%20Infinitesimal%20Probabilities%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvFCBFS2KBXrgJTr8K%2Fthe-generalized-anti-pascal-principle-utility-convergence-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Generalized%20Anti-Pascal%20Principle%3A%20Utility%20Convergence%20of%20Infinitesimal%20Probabilities%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvFCBFS2KBXrgJTr8K%2Fthe-generalized-anti-pascal-principle-utility-convergence-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FvFCBFS2KBXrgJTr8K%2Fthe-generalized-anti-pascal-principle-utility-convergence-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1608, "htmlBody": "<p>\n<p>Edit: &nbsp;Added clarification of the limit in response to gwern's comment.</p>\n<div>Recently I've found several instances of this Pascal Mugging/Wager problem come up, and I'm somewhat surprised that 1.) a number of people apparently still consider it a real issue, and 2.) the rest don't rely on a standard goto defense.</div>\n</p>\n<p>For recent examples, see <a href=\"/lw/8j7/criticisms_of_intelligence_explosion/5auu\">this post</a> by MileyCyrus, or <a href=\"/lw/8ts/open_problems_related_to_the_singularity_draft_1/5g69\">this post</a> from XiXiDu (where I reply with unbounded utility functions, which is not the general solution).</p>\n<p>I encountered this issue again while reading through a <a href=\"http://johncarlosbaez.wordpress.com/2011/04/24/what-to-do/#comment-55159\">fascinating discussion thread</a> on John Baez's blog from earlier this year where Greg Egan jumped in with a \"Yudkowsky/Bostrom\" criticism:</p>\n<blockquote>\n<p><span style=\"color: #333333; font-family: 'Lucida Grande', Verdana, Arial, sans-serif; font-size: 11px; line-height: 16px; background-color: #f8f8f8; \">The Yudkowsky/Bostrom strategy is to contrive probabilities for immensely unlikely scenarios, and adjust the figures until the expectation value for the benefits of working on &mdash; or donating to &mdash; their particular pet projects exceed the benefits of doing anything else. Combined with the appeal to vanity of &ldquo;saving the universe&rdquo;, some people apparently find this irresistible, but frankly, their attempt to prescribe what rational altruists should be doing with their time and money is just laughable, and it&rsquo;s a shame you&rsquo;ve given it so much air time.</span></p>\n</blockquote>\n<p>In short, Egan is indirectly accusing SIAI and FHI of <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">Pascal Mugging</a>(among else): something serious indeed. &nbsp;Egan in particular presents the following (presumably Yudkowsky) quote as evidence:</p>\n<blockquote>\n<p><span style=\"font-family: 'Lucida Grande', Verdana, Arial, sans-serif; font-size: 11px; line-height: 16px; \">Anyway: In terms of expected utility maximization, even&nbsp;</span><em style=\"font-family: 'Lucida Grande', Verdana, Arial, sans-serif; font-size: 11px; line-height: 16px; text-align: justify; \">large&nbsp;</em><span style=\"font-family: 'Lucida Grande', Verdana, Arial, sans-serif; font-size: 11px; line-height: 16px; text-align: justify; \">probabilities of jumping the interval between a universe-history in which 95% of existing biological species survive Earth&rsquo;s 21st century, versus a universe-history where 80% of species survive, are just about impossible to trade off against&nbsp;</span><em style=\"font-family: 'Lucida Grande', Verdana, Arial, sans-serif; font-size: 11px; line-height: 16px; text-align: justify; \">tiny&nbsp;</em><span style=\"font-family: 'Lucida Grande', Verdana, Arial, sans-serif; font-size: 11px; line-height: 16px; text-align: justify; \">probabilities of jumping the interval between interesting universe-histories, versus boring ones where intelligent life goes extinct, or the wrong sort of AI self-improves.</span></p>\n</blockquote>\n<p>Yudkowsky responds with his <a href=\"/lw/z0/the_pascals_wager_fallacy_fallacy/\">Pascal's Wager Fallacy Fallacy</a>, and points out that in fact he agrees there is no case for investing in defense against highly improbable existential risks: &nbsp;</p>\n<blockquote>\n<p><span style=\"color: #333333; font-family: 'Lucida Grande', Verdana, Arial, sans-serif; font-size: 11px; line-height: 16px; background-color: #f8f8f8; \">And I don&rsquo;t think the odds of us being wiped out by badly done AI&nbsp;</span><em style=\"color: #333333; font-family: 'Lucida Grande', Verdana, Arial, sans-serif; font-size: 11px; line-height: 16px; text-align: justify; background-color: #f8f8f8; \">are</em><span style=\"color: #333333; font-family: 'Lucida Grande', Verdana, Arial, sans-serif; font-size: 11px; line-height: 16px; text-align: justify; background-color: #f8f8f8; \">&nbsp;small. I think they&rsquo;re easily larger than 10%. And if you can carry a qualitative argument that the probability is under, say, 1%, then that means AI is probably the wrong use of marginal resources &ndash; not because global warming is more important, of course, but because</span><em style=\"color: #333333; font-family: 'Lucida Grande', Verdana, Arial, sans-serif; font-size: 11px; line-height: 16px; text-align: justify; background-color: #f8f8f8; \">other</em><span style=\"color: #333333; font-family: 'Lucida Grande', Verdana, Arial, sans-serif; font-size: 11px; line-height: 16px; text-align: justify; background-color: #f8f8f8; \">&nbsp;ignored existential risks like nanotech would be more important. I am not trying to play burden-of-proof tennis. If the chances are under 1%, that&rsquo;s low enough, we&rsquo;ll drop the AI business from consideration until everything more realistic has been handled.</span></p>\n</blockquote>\n<p>The rest of the thread makes for an entertaining read, but the takeaway I'd like to focus on is the original source of Egan's criticism: the apparent domination of immensely unlikely scenarios of immensely high utility.</p>\n<p>It occurred to me that the expected value of any action - properly summed over subsets of integrated futures - necessarily converges to zero as the probability of those considered subsets goes to zero. &nbsp;Critically this convergence occurs for *all* utility functions, as it is not dependent on any particular utility assignments. &nbsp;Alas LW is vast enough that there may be little new left under the sun: In researching this idea, I encountered an earlier form of it in a post by SilasBart <a href=\"/lw/29x/to_signal_effectively_use_a_nonhuman_nonstoppable/21xj?c=1\">here</a>, as well as some earlier attempts by <a href=\"/lw/2sz/pascals_mugging_as_an_epistemic_problem/2r2j?c=1\">RichardKennaway</a>, <a href=\"/lw/39d/a_thought_on_pascals_mugging/\">Komponisto</a>, and <a href=\"/lw/2sz/pascals_mugging_as_an_epistemic_problem/2r4k?c=1\">jimrandomh</a>.</p>\n<p>Now that we've covered the background, I'll jump to the principle:</p>\n<p>The <strong>Infinitesimal Probability Utility Convergence Principle (IPUP)</strong>: &nbsp;For any action A, utility function U, and a subset of possible post-action futures F, EU(F) -&gt; 0 as p(F) -&gt; 0.&nbsp;</p>\n<p>In Pascal's Mugging scenarios we are considering possible scenarios (futures) that have some low probability. &nbsp;It is important to remember that rational agents compute expected reward over all possible futures, not just the one scenario we may be focusing on.</p>\n<p>The principle can be formalized in the theoretical context of perfect omniscience-approaching agents running on computers approaching infinite power.</p>\n<p>The <a href=\"http://www.hutter1.net/official/index.htm\">AIXI formalization</a> provides a simple mathematical model of such agents. &nbsp;It's single line equation has a concise English summary:</p>\n<blockquote>\n<p><span style=\"font-family: 'Times New Roman'; font-size: medium; \">&nbsp;If the environment is modeled by a deterministic&nbsp;</span><span style=\"font-size: medium; color: #a52a2a;\">program&nbsp;<em>q</em></span><span style=\"font-family: 'Times New Roman'; font-size: medium; \">, then the future&nbsp;</span><span style=\"font-size: medium; color: red;\">perceptions</span><span style=\"font-family: 'Times New Roman'; font-size: medium; \">&nbsp;</span><em style=\"font-family: 'Times New Roman'; font-size: medium; \"><span style=\"color: red;\">...</span><span style=\"color: fuchsia;\">o<sub>k</sub></span><span style=\"color: purple;\">r<sub>k</sub></span>...<span style=\"color: fuchsia;\">o<sub>m</sub></span><span style=\"color: purple;\">r<sub>m</sub></span>&nbsp;= U(<span style=\"color: #a52a2a;\">q</span>,<span style=\"color: green;\">a<sub>1</sub>..a<sub>m</sub></span>)</em><span style=\"font-family: 'Times New Roman'; font-size: medium; \">&nbsp;can be computed, where&nbsp;</span><em style=\"font-family: 'Times New Roman'; font-size: medium; \"><span style=\"color: blue;\">U</span></em><span style=\"font-family: 'Times New Roman'; font-size: medium; \">&nbsp;is a&nbsp;</span><span style=\"font-size: medium; color: blue;\">universal (monotone Turing) machine</span><span style=\"font-family: 'Times New Roman'; font-size: medium; \">&nbsp;executing&nbsp;</span><em style=\"font-family: 'Times New Roman'; font-size: medium; \"><span style=\"color: #a52a2a;\">q</span></em><span style=\"font-family: 'Times New Roman'; font-size: medium; \">&nbsp;given&nbsp;</span><em style=\"font-family: 'Times New Roman'; font-size: medium; \"><span style=\"color: green;\">a<sub>1</sub>..a<sub>m</sub></span></em><span style=\"font-family: 'Times New Roman'; font-size: medium; \">. Since&nbsp;</span><em style=\"font-family: 'Times New Roman'; font-size: medium; \"><span style=\"color: #a52a2a;\">q</span></em><span style=\"font-family: 'Times New Roman'; font-size: medium; \">&nbsp;is unknown,&nbsp;</span><span style=\"font-size: medium; color: blue;\">AIXI</span><span style=\"font-family: 'Times New Roman'; font-size: medium; \">&nbsp;has to maximize its&nbsp;</span><span style=\"font-size: medium; color: red;\">expected</span><span style=\"font-family: 'Times New Roman'; font-size: medium; \">&nbsp;reward, i.e. average&nbsp;</span><em style=\"font-family: 'Times New Roman'; font-size: medium; \"><span style=\"color: purple;\">r<sub>k</sub>+...+r<sub>m</sub></span></em><span style=\"font-family: 'Times New Roman'; font-size: medium; \">&nbsp;over all possible future&nbsp;</span><span style=\"font-size: medium; color: #a52a2a;\">perceptions&nbsp;</span><span style=\"font-family: 'Times New Roman'; font-size: medium; \">created by all possible environments&nbsp;</span><em style=\"font-family: 'Times New Roman'; font-size: medium; \"><span style=\"color: #a52a2a;\">q</span></em><span style=\"font-family: 'Times New Roman'; font-size: medium; \">&nbsp;that are consistent with past perceptions. The simpler an environment, the higher is its a-priori contribution&nbsp;</span><span style=\"font-size: medium; color: blue;\">2<sup>-<em>l(<span style=\"color: #a52a2a;\">q</span>)</em></sup></span><span style=\"font-family: 'Times New Roman'; font-size: medium; \">, where simplicity is measured by the&nbsp;</span><span style=\"font-size: medium; color: blue;\">length&nbsp;<em>l</em></span><span style=\"font-family: 'Times New Roman'; font-size: medium; \">&nbsp;of program&nbsp;</span><em style=\"font-family: 'Times New Roman'; font-size: medium; \"><span style=\"color: #a52a2a;\">q</span></em><span style=\"font-family: 'Times New Roman'; font-size: medium; \">.&nbsp;</span><span style=\"font-size: medium; color: blue;\">AIXI</span><span style=\"font-family: 'Times New Roman'; font-size: medium; \">&nbsp;effectively learns by eliminating Turing machines&nbsp;</span><span style=\"font-size: medium; color: #a52a2a;\">q</span><span style=\"font-family: 'Times New Roman'; font-size: medium; \">&nbsp;once they become inconsistent with the progressing history. Since&nbsp;</span><span style=\"font-size: medium; color: #a52a2a;\">noisy environments</span><span style=\"font-family: 'Times New Roman'; font-size: medium; \">&nbsp;are just mixtures of deterministic environments, they are automatically included.</span>&nbsp;</p>\n</blockquote>\n<div>\n<p>AIXI is just a mathematical equation. &nbsp;We must be very careful in mapping it to abstract scenarios lest we lose much in translation. &nbsp;It is best viewed as a family of agent-models, the reward observations it seeks to maximize could be anything.</p>\n<p>When one ponders: \"What would AIXI/Omega do?\" &nbsp;There are a couple of key points to keep in mind:</p>\n<ol>\n<li>AIXI like models (probably) simulate the entire complete infinitely branching multiverse from the beginning of time to infinity (as particular simulation programs). &nbsp;This is often lost in translation.</li>\n<li>AIXI like models compute 1 (the infinite totality of existence), not once, but for each of an infinite number of programs (corresponding to what we would call universal physics: theories of everything) in parallel. &nbsp;Thus AIXI computes (in parallel) the entire Tegmark multiverse: every possible universe that could exist in principle.</li>\n<li>AIXI 'learns' by eliminating sub-universes (and theories) that do not perfectly agree with it's observation history to date. &nbsp;Of course this is only ever a finite reduction, it never collapses the multiverse from an infinite set into a finite set.</li>\n<li>AIXI finally picks an action A that maximizes expected reward. &nbsp;It computes this measure by summing over, for each observation-valid universe (computed by a particular theory-program 1) in the multiverse ensemble (2), the total accumulated reward in the sub-universes branching off from that action, weighted by a scoring term for each valid universe that decreases with the negative exponent of the theory's program length.&nbsp;</li>\n</ol>\n<p>&nbsp;</p>\n<p>In other words the perfectly rational agent considers everything that could possibly happen as a consequence of it's action in <em>every possible universe it could be in</em>, weighted by an exponential penalty against high-complexity universes.</p>\n<p>Here is a sketch of how the limit convergence (IPUP above) can be derived: &nbsp;When considering a possible action A, such as giving $5 to a Pascal Mugger, an optimal agent considers all possible dependent futures for <em>all possible physics-universes</em>. &nbsp;As we advance into scenarios of infinitesimal probability, we are advancing up the complexity ladder into increasingly <em>chaotic</em> universes which feature completely random rewards which approach positive/negative infinity. &nbsp;As we advance into this regime of infinitesimal probability, causality itself breaks down completely and expected reward of any action goes to zero.</p>\n<p>\n<p>The convergence principle can be derived from the program length prior 2^-l(q). &nbsp;An agent which has accumulated P perception bits so far can fully explain those perceptions by completely random programs of length P, thus 2^-l(P) forms a probability limit at which the agent's perceptions start becoming irrelevant, and chaotic non-causal physics dominate. &nbsp;Chaos should dominate expected reward for actions where p(A) &lt;&lt; &nbsp;2^-l(P).</p>\n<div>In the the vast vast majority of futures, the agent gives $5 to the Mugger and nothing much of extreme importance happens (the agent is lying or crazy, etc.)</div>\n</p>\n<p>Thinking as a limited human, we impose abstractions and collapse all extremely similar (to us) futures. &nbsp;All the tiny random quantum-dependent variations of a particular future correspond to \"giving the Mugger $5\" we collapse into a single <em>set</em> of futures which we assign a probability to based on counting the subinstances in that set as a fraction of the whole. &nbsp;</p>\n<p>AIXI does not do this: it actually computes each individual future path.</p>\n<p>But as we can't hope to think that way, we have to think in terms of probability categorizations. &nbsp;Fine. &nbsp;Imagine collapsing any futures that are sufficiently indistinguishable such that humans would consider them identical: described by the same natural language. &nbsp;We then get subsets of futures which we assign probabilities as relative size measures. &nbsp;</p>\n<p>Now consider ranking all of those future-sets in decreasing probability order. &nbsp;Most of the early list is dominated by Mugger is (joking/lying/crazy/etc). &nbsp;Farther down the list you get into scenarios where we do live in a multi-level Simulation (AIXI only ever considers itself in some simulation), but the Mugger is still (joking/lying/crazy/etc). &nbsp;</p>\n<p>By the time you get down the list to scenarios described where the Mugger says \"Or else I will&nbsp;use my magic powers from outside the Matrix to run a Turing machine that simulates and kills 3^^^^3 people\" and what the Mugger says actually happens, we are almost certainly down in infinitesimal probability land.</p>\n<p>Infinitesimal probability land is a wierd place. &nbsp;It is a regime where the physics that we commonly accept is wrong - which is to say simply that the exponential complexity penalty no longer rules out ultra-complex universes. &nbsp;It is dominated by chaos: universes of every possible fancy, where nothing is as what it seems, where everything you possibly thought is completely wrong, where there is no causality, etc. etc. &nbsp;</p>\n<p>At the complete limit of improbability, we just get universes where our entire observation history is completely random - generated by programs more complex than our observations. &nbsp;You give the mugger $5 and the universe simply dissolves in white noise and nothing happens (or god appears and gives you infinite heaven, or infinite hell, or the speed of light goes to zero, or a black hole forms near your nose, or the Mugger turns into jellybeans, etc. etc., an infinite number of stories, over which the net reward summation <em>necessarily </em>collapses to zero.)</p>\n<p>Remember AIXI doesn't consider the mugger's words as 'evidence', they are simply observations. &nbsp;In the more complex universes they are completely devoid of meaning, as causality itself collapses.</p>\n<br />\n<p>&nbsp;</p>\n<br />\n<p>&nbsp;</p>\n</div>\n<div><br /></div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vFCBFS2KBXrgJTr8K", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": -6, "extendedScore": null, "score": 8.172699340369693e-07, "legacy": true, "legacyId": "11539", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["a5JAiTdytou3Jg749", "TQSb4wd6v5C3p6HX2", "vEZ3Ajyp3LBtBwyuG"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-19T05:25:26.835Z", "modifiedAt": null, "url": null, "title": "The Controls are Lying: A Note on the Memetic Hazards of Video Games [Link]", "slug": "the-controls-are-lying-a-note-on-the-memetic-hazards-of", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:01.879Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nic_Smith", "createdAt": "2009-10-23T03:32:46.312Z", "isAdmin": false, "displayName": "Nic_Smith"}, "userId": "XP9GcTgRGLBCnf9ih", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6fBasgNon5EHkEFHP/the-controls-are-lying-a-note-on-the-memetic-hazards-of", "pageUrlRelative": "/posts/6fBasgNon5EHkEFHP/the-controls-are-lying-a-note-on-the-memetic-hazards-of", "linkUrl": "https://www.lesswrong.com/posts/6fBasgNon5EHkEFHP/the-controls-are-lying-a-note-on-the-memetic-hazards-of", "postedAtFormatted": "Monday, December 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Controls%20are%20Lying%3A%20A%20Note%20on%20the%20Memetic%20Hazards%20of%20Video%20Games%20%5BLink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Controls%20are%20Lying%3A%20A%20Note%20on%20the%20Memetic%20Hazards%20of%20Video%20Games%20%5BLink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6fBasgNon5EHkEFHP%2Fthe-controls-are-lying-a-note-on-the-memetic-hazards-of%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Controls%20are%20Lying%3A%20A%20Note%20on%20the%20Memetic%20Hazards%20of%20Video%20Games%20%5BLink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6fBasgNon5EHkEFHP%2Fthe-controls-are-lying-a-note-on-the-memetic-hazards-of", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6fBasgNon5EHkEFHP%2Fthe-controls-are-lying-a-note-on-the-memetic-hazards-of", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 320, "htmlBody": "<p>Chris Pruett writes <a href=\"http://robotinvader.com/blog/?p=164\"> on the <em>Robot Invader</em></a> blog:</p>\n<blockquote>\n<p>Good player handling code is often smoke and mirrors; the player  presses buttons and sees a reasonable result, but in between those two  operations a whole lot of code is working to ensure that the result is  the <em>best</em> of many potential results. &nbsp;For example, my friend <a href=\"http://greggman.com/\">Greggman</a> discovered that <a href=\"http://games.greggman.com/game/programming_m_c__kids/\">Mario 3's jumping rules change depending on whether or not a level has slopes in it.</a> Halo's targeting reticle famously slows as it passes over an enemy to  make it easier to target with an analog stick without using an auto-aim  system. When Spider-Man swings, he certainly does not orient about the  spot where his web connects to a building (at least, he didn't in the <a href=\"http://www.youtube.com/watch?v=orbWj7zh9lE\">swinging system I wrote</a>).</p>\n<p>Good player handling code doesn't just translate the player's inputs into action, it tries to discern the player's <em>intent</em>.  Once the intended action has been identified, if the rules of the game  allow it, good player handling code makes the action happen&ndash;<em>even if it means breaking the rules of the simulation a little. </em>The  goal of good handling code isn't to maintain a \"correct\" simulation,  it's to provide a fun game. It sucks to miss a jump by three  centimeters. It sucks to take the full force of a hit from a blow that  visually missed. It sucks to swing into a brick wall at 80 miles per hour instead of continuing down the street. To the extent that the code can understand the player's intent, it should act on that intent rather than on the raw input. <em>Do what I mean, not what I say.</em></p>\n</blockquote>\n<p>I suppose this explains why I am better at arcade bowling games than I am at actual bowling. More seriously, while I had some vague awareness of this, I am slightly surprised at the breadth (<em>Mario 3!?</em>) and depth to which this \"control re-interpretation\" takes place.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"RLQumypPQGPYg9t6G": 1, "HFou6RHqFagkyrKkW": 1, "ZsWDPoXcchbGneaMX": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6fBasgNon5EHkEFHP", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 28, "baseScore": 31, "extendedScore": null, "score": 5.4e-05, "legacy": true, "legacyId": "11550", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 21, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-19T05:30:11.497Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Infinite Certainty", "slug": "seq-rerun-infinite-certainty", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:04.313Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/i66WnBhZmkXRBCxCk/seq-rerun-infinite-certainty", "pageUrlRelative": "/posts/i66WnBhZmkXRBCxCk/seq-rerun-infinite-certainty", "linkUrl": "https://www.lesswrong.com/posts/i66WnBhZmkXRBCxCk/seq-rerun-infinite-certainty", "postedAtFormatted": "Monday, December 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Infinite%20Certainty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Infinite%20Certainty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi66WnBhZmkXRBCxCk%2Fseq-rerun-infinite-certainty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Infinite%20Certainty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi66WnBhZmkXRBCxCk%2Fseq-rerun-infinite-certainty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fi66WnBhZmkXRBCxCk%2Fseq-rerun-infinite-certainty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 216, "htmlBody": "<p>Today's post, <a href=\"/lw/mo/infinite_certainty/\">Infinite Certainty</a> was originally published on 09 January 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>If you say you are 99.9999% confident of a proposition, you're saying that you could make one million equally likely statements and be wrong, on average, once. Probability 1 indicates a state of infinite certainty. Furthermore, once you assign a probability 1 to a proposition, Bayes' theorem says that it can never be changed, in response to any evidence. Probability 1 is a lot harder to get to with a human brain than you would think.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/8wd/seq_rerun_absolute_authority/\">Absolute Authority</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "i66WnBhZmkXRBCxCk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 8, "extendedScore": null, "score": 8.173959093319911e-07, "legacy": true, "legacyId": "11551", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ooypcn7qFzsMcy53R", "exFfb4rFGMMnQ9jBy", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-19T09:51:15.496Z", "modifiedAt": null, "url": null, "title": "Q&A with Michael Littman on risks from AI", "slug": "q-and-a-with-michael-littman-on-risks-from-ai", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:32.155Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/j5ComXKhingWjqSgA/q-and-a-with-michael-littman-on-risks-from-ai", "pageUrlRelative": "/posts/j5ComXKhingWjqSgA/q-and-a-with-michael-littman-on-risks-from-ai", "linkUrl": "https://www.lesswrong.com/posts/j5ComXKhingWjqSgA/q-and-a-with-michael-littman-on-risks-from-ai", "postedAtFormatted": "Monday, December 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Q%26A%20with%20Michael%20Littman%20on%20risks%20from%20AI&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AQ%26A%20with%20Michael%20Littman%20on%20risks%20from%20AI%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj5ComXKhingWjqSgA%2Fq-and-a-with-michael-littman-on-risks-from-ai%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Q%26A%20with%20Michael%20Littman%20on%20risks%20from%20AI%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj5ComXKhingWjqSgA%2Fq-and-a-with-michael-littman-on-risks-from-ai", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fj5ComXKhingWjqSgA%2Fq-and-a-with-michael-littman-on-risks-from-ai", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 892, "htmlBody": "<p><strong>[<a href=\"http://wiki.lesswrong.com/wiki/Interview_series_on_risks_from_AI\">Click here to see a list of all interviews</a>]</strong></p>\n<blockquote>\n<p><a href=\"http://en.wikipedia.org/wiki/Michael_L._Littman\"><strong>Michael L. Littman</strong></a> is a computer scientist. He works mainly in reinforcement learning, but has done work in machine learning, game theory, <span class=\"mw-redirect\">computer networking</span>, Partially observable Markov decision process solving, computer solving of analogy problems and other areas. He is currently a professor of computer science and department chair at Rutgers University.</p>\n</blockquote>\n<p><strong>Homepage:</strong> <a href=\"http://www.cs.rutgers.edu/~mlittman/\">cs.rutgers.edu/~mlittman/</a></p>\n<p><strong>Google Scholar:</strong> <a href=\"http://scholar.google.com/scholar?q=Michael+Littman\">scholar.google.com/scholar?q=Michael+Littman</a></p>\n<h3><strong>The Interview</strong>:</h3>\n<p style=\"padding-left: 30px;\"><strong>Michael Littman:</strong> A little background on me.&nbsp; I've been an academic in AI for not-quite 25 years.&nbsp; I work mainly on reinforcement learning, which I think is a key technology for human-level AI---understanding the algorithms behind motivated behavior.&nbsp; I've also worked a bit on topics in statistical natural language processing (like the first human-level crossword solving program).&nbsp; I carried out a similar sort of survey when I taught AI at Princeton in 2001 and got some interesting answers from my colleagues.&nbsp; I think the survey says more about the mental state of researchers than it does about the reality of the predictions.&nbsp; <br /><br />In my case, my answers are colored by the fact that my group sometimes uses robots to demonstrate the learning algorithms we develop.&nbsp; We do that because we find that non-technical people find it easier to understand and appreciate the idea of a learning robot than pages of equations and graphs.&nbsp; But, after every demo, we get the same question: \"Is this the first step toward Skynet?\"&nbsp; It's a \"have you stopped beating your wife\" type of question, and I find that it stops all useful and interesting discussion about the research.<br /><br />Anyhow, here goes:</p>\n<p><strong>Q1:</strong> Assuming no global catastrophe halts progress, by what year would you assign a 10%/50%/90% chance of the development of roughly human-level machine intelligence?</p>\n<p style=\"padding-left: 30px;\"><strong>Michael Littman: </strong></p>\n<p style=\"padding-left: 30px;\">10%: 2050 (I also think P=NP in that year.)<br />50%: 2062<br />90%: 2112</p>\n<p><strong>Q2:</strong> What probability do you assign to the possibility of human extinction as a result of badly done AI?</p>\n<p style=\"padding-left: 30px;\"><strong>Michael Littman: </strong>epsilon, assuming you mean: P(human extinction caused by badly done AI | badly done AI) <br /><br />I think complete human extinction is unlikely, but, if society as we know it collapses, it'll be because people are being stupid (not because machines are being smart).</p>\n<p><strong>Q3:</strong> What probability do you assign to the possibility of a human level AGI to self-modify its way up to massive superhuman intelligence within a matter of hours/days/&lt; 5 years?</p>\n<p style=\"padding-left: 30px;\"><strong>Michael Littman: </strong>epsilon (essentially zero).&nbsp; I'm not sure exactly what constitutes intelligence, but I don't think it's something that can be turbocharged by introspection, even superhuman introspection.&nbsp; It involves experimenting with the world and seeing what works and what doesn't.&nbsp; The world, as they say, is its best model.&nbsp; Anything short of the real world is an approximation that is excellent for proposing possible solutions but not sufficient to evaluate them.</p>\n<p><strong>Q3-sub:</strong> P(superhuman intelligence within days | human-level AI running at human-level speed equipped with a 100 Gigabit Internet connection) = ?</p>\n<p style=\"padding-left: 30px;\"><strong>Michael Littman: </strong>Ditto.</p>\n<p><strong>Q3-sub: </strong>P(superhuman intelligence within &lt; 5 years | human-level AI running at human-level speed equipped with a 100 Gigabit Internet connection) = ?</p>\n<p style=\"padding-left: 30px;\"><strong>Michael Littman: </strong>1%. At least 5 years is enough for some experimentation.</p>\n<p><strong>Q4:</strong> Is it important to figure out how to make AI provably friendly to us and our values (non-dangerous), before attempting to solve artificial general intelligence?</p>\n<p style=\"padding-left: 30px;\"><strong>Michael Littman: </strong>No, I don't think it's possible.&nbsp; I mean, seriously, humans aren't even provably friendly to us and we have thousands of years of practice negotiating with them.</p>\n<p><strong>Q5:</strong> Do possible risks from AI outweigh other possible existential risks, e.g. risks associated with the possibility of advanced nanotechnology?</p>\n<p style=\"padding-left: 30px;\"><strong>Michael Littman: </strong>In terms of science risks (outside of human fundamentalism which is the only non-negligible risk I am aware of), I'm most afraid of high energy physics experiments, then biological agents, then, much lower, information technology related work like AI.</p>\n<p><strong>Q6:</strong> What is the current level of awareness of possible risks from AI, relative to the ideal level?</p>\n<p style=\"padding-left: 30px;\"><strong>Michael Littman: </strong>I think people are currently hypersensitive.&nbsp; As I said, every time I do a demo of any AI ideas, no matter how innocuous, I am asked whether it is the first step toward Skynet.&nbsp; It's ridiculous.&nbsp; Given the current state of AI, these questions come from a simple lack of knowledge about what the systems are doing and what they are capable of.&nbsp; What society lacks is not a lack of awareness of risks but a lack of technical understanding to *evaluate* risks.&nbsp; It shouldn't just be the scientists assuring people everything is ok.&nbsp; People should have enough background to ask intelligent questions about the dangers and promise of new ideas.</p>\n<p><strong>Q7:</strong> Can you think of any milestone such that if it were ever reached you would expect human\u2010level machine intelligence to be developed within five years thereafter?</p>\n<p style=\"padding-left: 30px;\"><strong>Michael Littman: </strong>Slightly subhuman intelligence?&nbsp; What we think of as human intelligence is layer upon layer of interacting subsystems.&nbsp; Most of these subsystems are complex and hard to get right.&nbsp; If we get them right, they will show very little improvement in the overall system, but will take us a step closer.&nbsp; The last 5 years before human intelligence is demonstrated by a machine will be pretty boring, akin to the 5 years between the ages of 12 to 17 in a human's development.&nbsp; Yes, there are milestones, but they will seem minor compared to first few years of rapid improvement.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "j5ComXKhingWjqSgA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 23, "extendedScore": null, "score": 8.174919104985807e-07, "legacy": true, "legacyId": "11555", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"_Click_here_to_see_a_list_of_all_interviews_\">[<a href=\"http://wiki.lesswrong.com/wiki/Interview_series_on_risks_from_AI\">Click here to see a list of all interviews</a>]</strong></p>\n<blockquote>\n<p><a href=\"http://en.wikipedia.org/wiki/Michael_L._Littman\"><strong>Michael L. Littman</strong></a> is a computer scientist. He works mainly in reinforcement learning, but has done work in machine learning, game theory, <span class=\"mw-redirect\">computer networking</span>, Partially observable Markov decision process solving, computer solving of analogy problems and other areas. He is currently a professor of computer science and department chair at Rutgers University.</p>\n</blockquote>\n<p><strong>Homepage:</strong> <a href=\"http://www.cs.rutgers.edu/~mlittman/\">cs.rutgers.edu/~mlittman/</a></p>\n<p><strong>Google Scholar:</strong> <a href=\"http://scholar.google.com/scholar?q=Michael+Littman\">scholar.google.com/scholar?q=Michael+Littman</a></p>\n<h3 id=\"The_Interview_\"><strong>The Interview</strong>:</h3>\n<p style=\"padding-left: 30px;\"><strong>Michael Littman:</strong> A little background on me.&nbsp; I've been an academic in AI for not-quite 25 years.&nbsp; I work mainly on reinforcement learning, which I think is a key technology for human-level AI---understanding the algorithms behind motivated behavior.&nbsp; I've also worked a bit on topics in statistical natural language processing (like the first human-level crossword solving program).&nbsp; I carried out a similar sort of survey when I taught AI at Princeton in 2001 and got some interesting answers from my colleagues.&nbsp; I think the survey says more about the mental state of researchers than it does about the reality of the predictions.&nbsp; <br><br>In my case, my answers are colored by the fact that my group sometimes uses robots to demonstrate the learning algorithms we develop.&nbsp; We do that because we find that non-technical people find it easier to understand and appreciate the idea of a learning robot than pages of equations and graphs.&nbsp; But, after every demo, we get the same question: \"Is this the first step toward Skynet?\"&nbsp; It's a \"have you stopped beating your wife\" type of question, and I find that it stops all useful and interesting discussion about the research.<br><br>Anyhow, here goes:</p>\n<p><strong>Q1:</strong> Assuming no global catastrophe halts progress, by what year would you assign a 10%/50%/90% chance of the development of roughly human-level machine intelligence?</p>\n<p style=\"padding-left: 30px;\"><strong id=\"Michael_Littman__\">Michael Littman: </strong></p>\n<p style=\"padding-left: 30px;\">10%: 2050 (I also think P=NP in that year.)<br>50%: 2062<br>90%: 2112</p>\n<p><strong>Q2:</strong> What probability do you assign to the possibility of human extinction as a result of badly done AI?</p>\n<p style=\"padding-left: 30px;\"><strong>Michael Littman: </strong>epsilon, assuming you mean: P(human extinction caused by badly done AI | badly done AI) <br><br>I think complete human extinction is unlikely, but, if society as we know it collapses, it'll be because people are being stupid (not because machines are being smart).</p>\n<p><strong>Q3:</strong> What probability do you assign to the possibility of a human level AGI to self-modify its way up to massive superhuman intelligence within a matter of hours/days/&lt; 5 years?</p>\n<p style=\"padding-left: 30px;\"><strong>Michael Littman: </strong>epsilon (essentially zero).&nbsp; I'm not sure exactly what constitutes intelligence, but I don't think it's something that can be turbocharged by introspection, even superhuman introspection.&nbsp; It involves experimenting with the world and seeing what works and what doesn't.&nbsp; The world, as they say, is its best model.&nbsp; Anything short of the real world is an approximation that is excellent for proposing possible solutions but not sufficient to evaluate them.</p>\n<p><strong>Q3-sub:</strong> P(superhuman intelligence within days | human-level AI running at human-level speed equipped with a 100 Gigabit Internet connection) = ?</p>\n<p style=\"padding-left: 30px;\"><strong>Michael Littman: </strong>Ditto.</p>\n<p><strong>Q3-sub: </strong>P(superhuman intelligence within &lt; 5 years | human-level AI running at human-level speed equipped with a 100 Gigabit Internet connection) = ?</p>\n<p style=\"padding-left: 30px;\"><strong>Michael Littman: </strong>1%. At least 5 years is enough for some experimentation.</p>\n<p><strong>Q4:</strong> Is it important to figure out how to make AI provably friendly to us and our values (non-dangerous), before attempting to solve artificial general intelligence?</p>\n<p style=\"padding-left: 30px;\"><strong>Michael Littman: </strong>No, I don't think it's possible.&nbsp; I mean, seriously, humans aren't even provably friendly to us and we have thousands of years of practice negotiating with them.</p>\n<p><strong>Q5:</strong> Do possible risks from AI outweigh other possible existential risks, e.g. risks associated with the possibility of advanced nanotechnology?</p>\n<p style=\"padding-left: 30px;\"><strong>Michael Littman: </strong>In terms of science risks (outside of human fundamentalism which is the only non-negligible risk I am aware of), I'm most afraid of high energy physics experiments, then biological agents, then, much lower, information technology related work like AI.</p>\n<p><strong>Q6:</strong> What is the current level of awareness of possible risks from AI, relative to the ideal level?</p>\n<p style=\"padding-left: 30px;\"><strong>Michael Littman: </strong>I think people are currently hypersensitive.&nbsp; As I said, every time I do a demo of any AI ideas, no matter how innocuous, I am asked whether it is the first step toward Skynet.&nbsp; It's ridiculous.&nbsp; Given the current state of AI, these questions come from a simple lack of knowledge about what the systems are doing and what they are capable of.&nbsp; What society lacks is not a lack of awareness of risks but a lack of technical understanding to *evaluate* risks.&nbsp; It shouldn't just be the scientists assuring people everything is ok.&nbsp; People should have enough background to ask intelligent questions about the dangers and promise of new ideas.</p>\n<p><strong>Q7:</strong> Can you think of any milestone such that if it were ever reached you would expect human\u2010level machine intelligence to be developed within five years thereafter?</p>\n<p style=\"padding-left: 30px;\"><strong>Michael Littman: </strong>Slightly subhuman intelligence?&nbsp; What we think of as human intelligence is layer upon layer of interacting subsystems.&nbsp; Most of these subsystems are complex and hard to get right.&nbsp; If we get them right, they will show very little improvement in the overall system, but will take us a step closer.&nbsp; The last 5 years before human intelligence is demonstrated by a machine will be pretty boring, akin to the 5 years between the ages of 12 to 17 in a human's development.&nbsp; Yes, there are milestones, but they will seem minor compared to first few years of rapid improvement.</p>", "sections": [{"title": "[Click here to see a list of all interviews]", "anchor": "_Click_here_to_see_a_list_of_all_interviews_", "level": 2}, {"title": "The Interview:", "anchor": "The_Interview_", "level": 1}, {"title": "Michael Littman: ", "anchor": "Michael_Littman__", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "88 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 88, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-19T18:48:08.015Z", "modifiedAt": null, "url": null, "title": "\"Trials and Errors: Why Science Is Failing Us\"", "slug": "trials-and-errors-why-science-is-failing-us", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:03.038Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YB8c2LmFBcSx57chF/trials-and-errors-why-science-is-failing-us", "pageUrlRelative": "/posts/YB8c2LmFBcSx57chF/trials-and-errors-why-science-is-failing-us", "linkUrl": "https://www.lesswrong.com/posts/YB8c2LmFBcSx57chF/trials-and-errors-why-science-is-failing-us", "postedAtFormatted": "Monday, December 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Trials%20and%20Errors%3A%20Why%20Science%20Is%20Failing%20Us%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Trials%20and%20Errors%3A%20Why%20Science%20Is%20Failing%20Us%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYB8c2LmFBcSx57chF%2Ftrials-and-errors-why-science-is-failing-us%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Trials%20and%20Errors%3A%20Why%20Science%20Is%20Failing%20Us%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYB8c2LmFBcSx57chF%2Ftrials-and-errors-why-science-is-failing-us", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYB8c2LmFBcSx57chF%2Ftrials-and-errors-why-science-is-failing-us", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 65, "htmlBody": "<p>Jonah Lehrer has up another of his contrarian science articles: <a href=\"http://www.wired.com/magazine/2011/12/ff_causation/all/1#post-50632\">\"Trials and Errors: Why Science Is Failing Us\"</a>.</p>\n<p>Main topics: the failure of drugs in clinical trials, diminishing returns to pharmaceutical research, doctors over-treating, and Humean causality-correlation distinction, with some Ioannidis mixed through-out.</p>\n<p>See also \"<a href=\"/lw/72f/why_epidemiology_will_not_correct_itself/\">Why epidemiology will not correct itself\"</a></p>\n<p>\n<hr />\nIn completely unrelated news, Nick Bostrom is <a href=\"http://ieet.org/index.php/IEET/more/bigchanges2011\">stepping down</a> from IEET's Chairman of the Board.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YB8c2LmFBcSx57chF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 9, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "11557", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["n7E2FC63MZGnvZAdr"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-19T19:31:41.651Z", "modifiedAt": null, "url": null, "title": "SingInst bloomberg coverage [link]", "slug": "singinst-bloomberg-coverage-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:22.174Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MMh4kfoN8KhNW5g7v/singinst-bloomberg-coverage-link", "pageUrlRelative": "/posts/MMh4kfoN8KhNW5g7v/singinst-bloomberg-coverage-link", "linkUrl": "https://www.lesswrong.com/posts/MMh4kfoN8KhNW5g7v/singinst-bloomberg-coverage-link", "postedAtFormatted": "Monday, December 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20SingInst%20bloomberg%20coverage%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASingInst%20bloomberg%20coverage%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMMh4kfoN8KhNW5g7v%2Fsinginst-bloomberg-coverage-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=SingInst%20bloomberg%20coverage%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMMh4kfoN8KhNW5g7v%2Fsinginst-bloomberg-coverage-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMMh4kfoN8KhNW5g7v%2Fsinginst-bloomberg-coverage-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>http://www.sfgate.com/cgi-bin/article.cgi?f=/c/a/2011/12/19/BUFU1MD58T.DTL&amp;type=printable</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MMh4kfoN8KhNW5g7v", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 8, "extendedScore": null, "score": 8.177054238414908e-07, "legacy": true, "legacyId": "11558", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-19T22:19:20.868Z", "modifiedAt": null, "url": null, "title": "Presents for impoving rationality or reducing superstition?", "slug": "presents-for-impoving-rationality-or-reducing-superstition", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:02.778Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "mtaran", "createdAt": "2009-03-01T03:59:15.397Z", "isAdmin": false, "displayName": "mtaran"}, "userId": "En3rJXsxKr3AFNjLq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MuMjYTdkWb9agSaLW/presents-for-impoving-rationality-or-reducing-superstition", "pageUrlRelative": "/posts/MuMjYTdkWb9agSaLW/presents-for-impoving-rationality-or-reducing-superstition", "linkUrl": "https://www.lesswrong.com/posts/MuMjYTdkWb9agSaLW/presents-for-impoving-rationality-or-reducing-superstition", "postedAtFormatted": "Monday, December 19th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Presents%20for%20impoving%20rationality%20or%20reducing%20superstition%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APresents%20for%20impoving%20rationality%20or%20reducing%20superstition%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMuMjYTdkWb9agSaLW%2Fpresents-for-impoving-rationality-or-reducing-superstition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Presents%20for%20impoving%20rationality%20or%20reducing%20superstition%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMuMjYTdkWb9agSaLW%2Fpresents-for-impoving-rationality-or-reducing-superstition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMuMjYTdkWb9agSaLW%2Fpresents-for-impoving-rationality-or-reducing-superstition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 130, "htmlBody": "<p>It's traditional in many parts of the world to buy (or make) presents for at least the closest members one's family around this time of the year. I would like to know if anyone here has ideas for presents to give to people from college age to middle age who are <em>not</em> rationalists, but not completely closed off to the idea.</p>\n<p>So far I've considered mostly things that would help rid them of various superstitions, particularly astrology and the 2012 apocalypse myth. For this purpose I've looked at books and videos on</p>\n<ul>\n<li>the history of astrology</li>\n<li>the actual causes of various natural disasters</li>\n<li>modern cosmology</li>\n</ul>\n<p>Has anyone else on LW faced this sort of situation before? Or does anyone here have general advice on this topic?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MuMjYTdkWb9agSaLW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 8, "extendedScore": null, "score": 8.177671135986528e-07, "legacy": true, "legacyId": "11559", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 35, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-20T07:39:13.731Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] 0 And 1 Are Not Probabilities", "slug": "seq-rerun-0-and-1-are-not-probabilities", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:07.770Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/T3CcPhBqnFv7XjxM7/seq-rerun-0-and-1-are-not-probabilities", "pageUrlRelative": "/posts/T3CcPhBqnFv7XjxM7/seq-rerun-0-and-1-are-not-probabilities", "linkUrl": "https://www.lesswrong.com/posts/T3CcPhBqnFv7XjxM7/seq-rerun-0-and-1-are-not-probabilities", "postedAtFormatted": "Tuesday, December 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%200%20And%201%20Are%20Not%20Probabilities&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%200%20And%201%20Are%20Not%20Probabilities%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT3CcPhBqnFv7XjxM7%2Fseq-rerun-0-and-1-are-not-probabilities%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%200%20And%201%20Are%20Not%20Probabilities%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT3CcPhBqnFv7XjxM7%2Fseq-rerun-0-and-1-are-not-probabilities", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FT3CcPhBqnFv7XjxM7%2Fseq-rerun-0-and-1-are-not-probabilities", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 189, "htmlBody": "<p>Today's post, <a href=\"/lw/mp/0_and_1_are_not_probabilities/\">0 And 1 Are Not Probabilities</a> was originally published on 10 January 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>In the ordinary way of writing probabilities, 0 and 1 both seem like entirely reachable quantities. But when you transform probabilities into odds ratios, or log-odds, you realize that in order to get a proposition to probability 1 would require an infinite amount of evidence.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8wv/seq_rerun_infinite_certainty/\">Infinite Certainty</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "T3CcPhBqnFv7XjxM7", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 9, "extendedScore": null, "score": 8.179731866117631e-07, "legacy": true, "legacyId": "11575", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["QGkYCwyC7wTDyt3yT", "i66WnBhZmkXRBCxCk", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-20T07:40:09.245Z", "modifiedAt": null, "url": null, "title": "Visual Map of US LW-ers", "slug": "visual-map-of-us-lw-ers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:02.867Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/XJ4EQwhKstNnb3dje/visual-map-of-us-lw-ers", "pageUrlRelative": "/posts/XJ4EQwhKstNnb3dje/visual-map-of-us-lw-ers", "linkUrl": "https://www.lesswrong.com/posts/XJ4EQwhKstNnb3dje/visual-map-of-us-lw-ers", "postedAtFormatted": "Tuesday, December 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Visual%20Map%20of%20US%20LW-ers&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AVisual%20Map%20of%20US%20LW-ers%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXJ4EQwhKstNnb3dje%2Fvisual-map-of-us-lw-ers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Visual%20Map%20of%20US%20LW-ers%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXJ4EQwhKstNnb3dje%2Fvisual-map-of-us-lw-ers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FXJ4EQwhKstNnb3dje%2Fvisual-map-of-us-lw-ers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 145, "htmlBody": "<p>Earlier this month, Metus did a post asking for LW-ers locations. I thought it would even more useful to have this information in visual format, so I created a Google map. You can access it on the website below. Unfortunately, I am terrible at this post-writing interface, so I can't get the image of the map to load onto here. You'll have to click the link to view it.</p>\n<p><strong>Original Post</strong>:&nbsp;<a href=\"/r/discussion/lw/8sm/where_do_you_live_meetup_planners_want_to_know/\">http://lesswrong.com/r/discussion/lw/8sm/where_do_you_live_meetup_planners_want_to_know/</a></p>\n<p>(If you haven't filled out the poll yet, please do it! If more people submit their location info, I'll add them to the map.)</p>\n<ul>\n<li><strong><a href=\"http://maps.google.com/maps/ms?ie=UTF&amp;msa=0&amp;msid=210190591453187349305.0004b4806a4115a36be54\">Link to the Map</a></strong></li>\n</ul>\n<p>These are only for US locations that had a 5-digit zip-code, or a city name.&nbsp;I may or may not map the other countries. If someone else wants to volunteer, send me a message, and I'll add you as a collaborator to the map, so you can edit.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "XJ4EQwhKstNnb3dje", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 21, "extendedScore": null, "score": 8.179733509531786e-07, "legacy": true, "legacyId": "11574", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HXJiAALt7DSnL46pp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-20T14:07:44.396Z", "modifiedAt": null, "url": null, "title": "Is anyone else worried about SOPA? Trying to do anything about it?", "slug": "is-anyone-else-worried-about-sopa-trying-to-do-anything", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:52.610Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "false_vacuum", "createdAt": "2010-09-05T19:05:34.581Z", "isAdmin": false, "displayName": "false_vacuum"}, "userId": "ANT4GY6qFZCi8PDC7", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2YDNK4uwkc5ALZ8f4/is-anyone-else-worried-about-sopa-trying-to-do-anything", "pageUrlRelative": "/posts/2YDNK4uwkc5ALZ8f4/is-anyone-else-worried-about-sopa-trying-to-do-anything", "linkUrl": "https://www.lesswrong.com/posts/2YDNK4uwkc5ALZ8f4/is-anyone-else-worried-about-sopa-trying-to-do-anything", "postedAtFormatted": "Tuesday, December 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20anyone%20else%20worried%20about%20SOPA%3F%20Trying%20to%20do%20anything%20about%20it%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20anyone%20else%20worried%20about%20SOPA%3F%20Trying%20to%20do%20anything%20about%20it%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2YDNK4uwkc5ALZ8f4%2Fis-anyone-else-worried-about-sopa-trying-to-do-anything%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20anyone%20else%20worried%20about%20SOPA%3F%20Trying%20to%20do%20anything%20about%20it%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2YDNK4uwkc5ALZ8f4%2Fis-anyone-else-worried-about-sopa-trying-to-do-anything", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2YDNK4uwkc5ALZ8f4%2Fis-anyone-else-worried-about-sopa-trying-to-do-anything", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 238, "htmlBody": "<p class=\"node-title\">Referring, of course, to the proposed U.S. legislation which could cause severe damage to the Internet&mdash;at least, that's what a lot of people are saying.&nbsp; See, e.g., this <a href=\"https://www.eff.org/deeplinks/2011/12/internet-inventors-warn-against-sopa-and-pipa\">Open Letter From Internet Engineers to the U.S. Congress</a> (the first signatory listed is Vint Cerf).&nbsp; On Wikipedia, people including Jimbo Wales are <a href=\"https://en.wikipedia.org/wiki/User_talk:Jimbo_Wales#SOPA_update\">discussing</a> <a href=\"https://en.wikipedia.org/wiki/Wikipedia:SOPA_initiative\">strategies</a> as extreme as <em>blanking the entire site</em> (except for an explanatory message) to get people's attention, and thereby perhaps incite them to action, such as calling their Congressional representative.</p>\n<p class=\"node-title\">I just happened to find out about all this a few hours ago, being someone who tries to avoid distractions like most kinds of news, so possibly others here with similar habits will appreciate having it called to ther attention.&nbsp; Or possibly they won't.&nbsp; But to those of you who possess relevant kinds of expertise:&nbsp;</p>\n<ul>\n<li>Is it possible to project likely consequences of this legislation's being passed?</li>\n<li>What are those consequences?</li>\n<li>Assuming they are on net undesirable, what can be done that would be most likely to prevent its passage?</li>\n</ul>\n<p>(I think this subject can be discussed without political advocacy, in which I am mostly not at all interested anyway.&nbsp; It just looks like a practical problem to me.)</p>\n<p><strong>Edited to Add:</strong> I forgot to include a fourth bullet point:</p>\n<ul>\n<li>Again assuming they are undesirable, what can be done to ameliorate|circumvent them?</li>\n</ul>\n<p>It seems to have been assumed by many commenters, nevertheless.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2YDNK4uwkc5ALZ8f4", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 32, "baseScore": 19, "extendedScore": null, "score": 8.181162383843149e-07, "legacy": true, "legacyId": "11577", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 89, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-20T18:05:10.213Z", "modifiedAt": null, "url": null, "title": "[LINK] Non-Constructive Proof of Confirmation Bias", "slug": "link-non-constructive-proof-of-confirmation-bias", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Nymogenous", "createdAt": "2011-12-16T05:45:41.526Z", "isAdmin": false, "displayName": "Nymogenous"}, "userId": "w66nd9ontGBKpPZFq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qcznY6dYWbGxjWtGw/link-non-constructive-proof-of-confirmation-bias", "pageUrlRelative": "/posts/qcznY6dYWbGxjWtGw/link-non-constructive-proof-of-confirmation-bias", "linkUrl": "https://www.lesswrong.com/posts/qcznY6dYWbGxjWtGw/link-non-constructive-proof-of-confirmation-bias", "postedAtFormatted": "Tuesday, December 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Non-Constructive%20Proof%20of%20Confirmation%20Bias&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Non-Constructive%20Proof%20of%20Confirmation%20Bias%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqcznY6dYWbGxjWtGw%2Flink-non-constructive-proof-of-confirmation-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Non-Constructive%20Proof%20of%20Confirmation%20Bias%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqcznY6dYWbGxjWtGw%2Flink-non-constructive-proof-of-confirmation-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqcznY6dYWbGxjWtGw%2Flink-non-constructive-proof-of-confirmation-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 27, "htmlBody": "<p>Stumbled across <a href=\"http://www.smbc-comics.com/index.php?db=comics&amp;id=2444#comic\">this</a> today, thought the LW crowd would find it amusing. Would make a great non-mathematical example of a nonconstructive proof if you ever need one.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qcznY6dYWbGxjWtGw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "11578", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-20T19:18:58.259Z", "modifiedAt": null, "url": null, "title": "What is your rationality blind spot?", "slug": "what-is-your-rationality-blind-spot", "viewCount": null, "lastCommentedAt": "2017-06-17T04:06:07.248Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "shminux", "createdAt": "2011-03-15T18:17:44.196Z", "isAdmin": false, "displayName": "shminux"}, "userId": "CpPz4596hmk9Pk8Jh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/BiAN9MsXaqDf9ojZF/what-is-your-rationality-blind-spot", "pageUrlRelative": "/posts/BiAN9MsXaqDf9ojZF/what-is-your-rationality-blind-spot", "linkUrl": "https://www.lesswrong.com/posts/BiAN9MsXaqDf9ojZF/what-is-your-rationality-blind-spot", "postedAtFormatted": "Tuesday, December 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20is%20your%20rationality%20blind%20spot%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20is%20your%20rationality%20blind%20spot%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBiAN9MsXaqDf9ojZF%2Fwhat-is-your-rationality-blind-spot%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20is%20your%20rationality%20blind%20spot%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBiAN9MsXaqDf9ojZF%2Fwhat-is-your-rationality-blind-spot", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FBiAN9MsXaqDf9ojZF%2Fwhat-is-your-rationality-blind-spot", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 195, "htmlBody": "<p>It has been noticed since the time immemorial that cognitive biases have a nasty tendency of being invisible to self (note the proverbial log in one's eye). Uncovering their own blind spot is probably the hardest task for an aspired rationalist. EY and others have devoted a number of posts to this issue (e.g. the&nbsp;<span class=\"toctext\" style=\"color: #6a8a6b; font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: left; background-color: #f9f9f9;\"><a style=\"color: #6a8a6b; font-family: Arial, Helvetica, sans-serif; font-size: 12px; line-height: 18px; text-align: left; background-color: #f9f9f9;\" href=\"http://wiki.lesswrong.com/wiki/Sequences#How_To_Actually_Change_Your_Mind\">How To Actually Change Your Mind</a></span>&nbsp;sequence), and I am wondering if it is bearing fruit for the LW participants.&nbsp;</p>\n<p>To this end, I suggest that people post what they think their current rationality blind spot they are struggling with is (not the usual sweet success stories of \"overcoming bias\"), and let others comment on whether they agree or not, given their impressions of the person here and possibly in real life. My guess is that most of us would miss the mark widely (it's called a blind spot for a reason). Needless to say, if you post, you should expect to get crockered. Also needless to say, if you disagree with a person pointing out your bias, odds are that you are the one who is wrong.</p>\n<p>(Who, me, go first? Oh, I have no biases, at least none that I can see.)</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "BiAN9MsXaqDf9ojZF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 18, "extendedScore": null, "score": 4.3e-05, "legacy": true, "legacyId": "11579", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 81, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-20T20:37:58.229Z", "modifiedAt": "2020-12-02T23:56:13.022Z", "url": null, "title": "Ritual Report: NYC Less Wrong Solstice Celebration", "slug": "ritual-report-nyc-less-wrong-solstice-celebration", "viewCount": null, "lastCommentedAt": "2020-12-19T16:06:40.870Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Raemon", "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jES7mcPvKpfmzMTgC/ritual-report-nyc-less-wrong-solstice-celebration", "pageUrlRelative": "/posts/jES7mcPvKpfmzMTgC/ritual-report-nyc-less-wrong-solstice-celebration", "linkUrl": "https://www.lesswrong.com/posts/jES7mcPvKpfmzMTgC/ritual-report-nyc-less-wrong-solstice-celebration", "postedAtFormatted": "Tuesday, December 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ritual%20Report%3A%20NYC%20Less%20Wrong%20Solstice%20Celebration&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARitual%20Report%3A%20NYC%20Less%20Wrong%20Solstice%20Celebration%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjES7mcPvKpfmzMTgC%2Fritual-report-nyc-less-wrong-solstice-celebration%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ritual%20Report%3A%20NYC%20Less%20Wrong%20Solstice%20Celebration%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjES7mcPvKpfmzMTgC%2Fritual-report-nyc-less-wrong-solstice-celebration", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjES7mcPvKpfmzMTgC%2Fritual-report-nyc-less-wrong-solstice-celebration", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3243, "htmlBody": "<p><i>Note: Secular Solstice has evolved a bit since this original post (most noteably, it no longer has a major Lovecraft theme.&nbsp;</i></p><p>Last Friday, the NYC Less Wrong community held their first Winter Solstice Celebration. Approximately twenty of us gathered for dinner and a night of ritual. We sang songs, told stories, and recited litanies. The night celebrated ancient astronomers, and the work that humanity has done for the past 5000 years. It paid tribute to the harshness of the universe, respecting it as worthy opponent. We explored Lovecraftian mythology, which intersects with our beliefs in interesting ways.</p><p>And finally, we looked to the future, vowing to give a gift to tomorrow.</p><p>This is the first of 2-3 posts on this subject. In this one, I'm telling a story about what we did and why I wanted to. In the followup(s), I\u2019ll explain the design principles that went into planning such an event, and what we learned from our first execution of it. I\u2019ll also be posting a PDF of a ritual book, similar to the one we read from but with a few changes based on initial, obvious observations.</p><p>Why exactly did we do this? Doesn\u2019t this smack of organized religion? Who the hell is Lovecraft and why do we care?</p><p>Depending on your background, this may require the bridging of some inferential distance, as well as emotional distance. Bear with me.</p><p>&nbsp;</p><p>(If at the end, you DO still think this was a dangerous idea, or one you don't want popularized on Less Wrong, I want you to let me know. We're probably just going to disagree, but I want a sense of what the costs are of emphasizing this type of thing here)</p><h1>Winter Solstice</h1><p><i>To begin, a Just So Story, true enough for our purposes:</i></p><p>The Winter Solstice is the longest night of the year. It ushers in a time of cold and darkness.</p><p>For young civilizations, it was a time when if you HADN\u2019T spent the year preparing adequately for the future, then before spring returned, you would run out of food and die. If you hadn\u2019t striven to use your tribe\u2019s collective wisdom, to work hard beyond what was necessary for immediate gratification... if you hadn\u2019t harnessed the physical and mental tools that humans have but that few other animals do... then the universe, unflinchingly neutral, would destroy you without a second thought. And even if you did do these things, it might kill you anyway. Because fairness isn\u2019t built into the equations of the cosmos.</p><p>But it wasn't just the threat of death that inspired the first winter holidays. It was that sense of unfairness, coupled with the desperate hope that world couldn\u2019t <i>really</i> be that unfair. It wouldn\u2019t have occurred to the first squirrels that stored food for winter, but it gradually dawned upon ancient hominids, as their capacity for abstract reasoning developed, alongside their desire to throw parties.</p><p>Our tendency is to anthropomorphize. Today, we angrily yell at our cars and computers when they fail us. Rationally we know they are unthinking hulks of metal, but we still ascribe malevolence when the real culprit is a broken, unsentient machine.</p><p>There are plausible reasons for humans to have evolved this trait. One of the most complicated tasks a human has to do is predict the actions of other humans. We need to be able to make allies, to identify deceptive enemies, to please lovers. I\u2019m not an evolutionary psychologist and I should be careful when telling this sort of Just-So story, but I can easily imagine selection pressures that resulted in a powerful ability to draw conclusions about sentient creatures similar to ourselves.</p><p>And then, there was <i>NOT </i>a whole lot of pressure to <i>NOT </i>use this tool to predict, say, the weather. Many natural forces are just too complex for humans to be good at predicting. The rain would come, or it wouldn\u2019t, regardless of whether we ascribed it to gods or \u201cemergent complexity.\u201d So we told stories about gods, with human motivations, and we honestly believed them because there was nothing better.</p><p>And then, we had the solstice.</p><p>The world was dark and cold. The sun was retreating, leaving us only with the pale moon and stars that lay unimaginably far away. There was the enroaching threat of death, and just as powerfully, there was the threat that sentient cosmic forces that held supreme power over our world were <i>turning their backs on us.</i> And the best we could hope for was to throw a celebration in their honor and pray that they wouldn\u2019t be angry forever, that the sun would return and the world would be reborn.</p><p>And regardless, take a moment to be glad for having worked hard the previous year, so that we had meat stored up and wine that had finished fermenting.</p><p>But as ages passed, people noticed something interesting: there was a <i>pattern</i> to the gods getting angry. Weather may be complex and nigh-unpredictable. But the movements of the heavens... they follow rules simple enough for human minds to understand, if only you take the time to look.</p><p>We had a question. \u201cWhen will the sun retreat, and when will it return?\u201d</p><p>When you really care about knowing the answer, you can\u2019t make something up. When you need to plan your harvest and prepare for winter so that your family doesn\u2019t starve, you can\u2019t just say \u201cOh, God will stop getting angry in a few months.\u201d</p><p>If you want real knowledge, that you can apply to make your world better...</p><p>Then you need to do science. Astronomy was born.</p><p>I want to give you some perspective on how much we cared about this. Stonehenge is an ancient archaeological wonder. To the best of our knowledge, it began as a burial site around 3000 BCE. Over the next thousand years, it was gradually built, in major phases of activity every few hundred years. Between 2600 and 2400 BCE, there was a surge of construction. Huge stones were carted over huge distances, to create a monument that\u2019s lasted five thousand years.</p><p>30 Sarsen stones. Each of them was at least 25 tons. They were carried 25 miles.<br>80 bluestones. Four tons each. Carried over 150 miles.<br>In this era, the height of locomotive technology was \u201cthrow it on a pile of logs and roll it.\u201d</p><p>We don\u2019t know exactly how they did all this. We don\u2019t know all the reasons why. But we know at least one: The megaliths at Stonehenge are arranged, very specifically, to predict the Solstices. To the moment of dawn.</p><p>30 stones, each 25 tons, carried over 25 miles. 80 stones, each four tons, each carried<i> over 150 miles.</i></p><p>200 years of that.</p><p>That\u2019s how much we cared about the answer to that question.</p><h2>A Modern Journey</h2><p>To modern society, Winter Solstice isn\u2019t very scary. We have oil to heat our homes, we have mechanical plows that clear our streets when the snow falls and other mechanical plows that work our fields all year round to supply us with food, carted from thousands of miles away, across land and sea. Many people today claim to enjoy Winter, although Richard Adams may accurately say that they really enjoy their protection from it.</p><p>Modern winter holidays are about enjoying that protection, not assuaging fear.</p><p>But there is a power in that, all the same. My family\u2019s Christmas Eve celebration is one of my favorite parts of the year. The extended family gathers. We have a big feast. Then 20+ people huddle up and sing songs and tell stories for hours. I don\u2019t believe in the literal messages of these rituals, but they have a power to them that I rarely see outside of religious-inspired works of art. They feel timeless and magical even though most Christmas carols have only existed for 50 years or so. The repetition of them each year grants them ritual strength. And the closeness I feel with my family grants them warmth.</p><p>Together, all these things are precious.</p><p>I didn\u2019t realize how precious, though, until the year I invited a friend of mine to the Christmas Eve party. Her first reaction amused me: \u201cWait, you guys literally sit around a fire and sing Christmas carols? Like, in movies?\u201d Her second reaction, as the night ended, was even more amusing: \u201cOh my god, I had no idea Christmas could be so <i>awesome!\u201d</i> But I knew what she meant, and it was accompanied with the realization that NOT everybody got to have experiences like this.</p><p>And that made Christmas Eve all the more special. It also made me realize how ridiculous it is that I only get to have that experience once a year.</p><p>That desire nagged at me a few years, and it was accompanied by another nagging dissatisfaction: That I didn\u2019t really believe in the words of the songs. They had power, generated by the <i>magnitude </i>of the songwriter\u2019s belief, and given lyric form by carefully honed skill. But they weren\u2019t true, and the falsehood itched at the back of my mind. Not because of the songs themselves, but because there weren't other songs, equally beautiful and with the same cultural weight, that were about things that I truly believed in.<br><br>Flash forward five years. I\u2019ve since discovered the sequences at Less Wrong. They outline studies in human behavior, how lots of our thinking is flawed if we want to achieve particular goals, how it can be hard to even know what our goals ARE, and why these are incredibly important questions to answer. Not just so we can succeed at life, but because if you\u2019re developing machine intelligence, and you haven\u2019t studied these questions (and solved problems that are, as I write this, unsolved), you could really, really, wreck the world. Wreck it worse than cold, uncompromising Nature ever could, worse and more unrecoverably than Hollywood has portrayed in explosive blockbuster films.</p><p>But if these questions are answered, and certain technological problems are solved, we can do incredible, important, beautiful things. In the past year I\u2019ve read powerful works of science, prose, and poetry that have resonated with all my strongest values. They\u2019ve changed how I approach my life and how I look at the future.</p><p>For the past year I\u2019ve attended the local Less Wrong meetup. I\u2019ve made new friends. I\u2019ve gotten involved with a community that encourages everyone to figure out what their goals are and try to achieve them, using the best tools they can find. We\u2019re going through similar life experiences. And for the past year, I\u2019ve been seeking out songs and stories that are fun, powerful and that we all truly believe in.</p><p>Ritual has been important in my life. I recognize that there is a risk whenever you begin elevating ideas and seeking them out <i>because</i> they are powerful and moving. I <i>don\u2019t </i>want to start a self-propogating organization designed to accrue followers blindly reciting the faith. But those of us who have studied these ideas and take them seriously - I want us to be able to find each other, to create friendship and family, and to celebrate together.</p><p>However, these powerful beliefs we share come with a cost:</p><p>I now believe a lot of really weird stuff that\u2019s hard to explain to the average person without sounding crazy. To certain people, they sound genuinely horrifying. I believe that living forever is a perfectly reasonable goal. I think that in the not too distant future, people will be able to radically alter their minds and bodies. In the not much more distant future, there\u2019s a good chance people will be able to live as uploaded computer programs. More frightening: I believe that people will eventually WANT to do this.</p><p>To be clear: I\u2019m <i>currently </i>lukewarm about a lot of this - my beliefs are complex, and like most humans I have a poor understanding of what I really value. But I can imagine the future me, plugging into the Matrix like it was no big deal.</p><p>All of this pales compared to the possibility of AI. The rest of humanity goes about their daily lives, planning for a future that involves slightly smaller iPhones and bigger televisions, vaguely annoyed that it\u2019s 2012 and we don\u2019t have flying cars yet. Blissfully unaware that with barely any warning, an AGI might be created and then bootstrap itself to godhood.</p><p>Blissfully unaware of how big mindspace is, and how little human morality would matter to a ghost of perfect emptiness, and how hard it is to create a mind from scratch that would care about us the way we care about ourselves.</p><p>But perhaps most blissful of all, they look upon the horrors that nature has inflicted us, and they give them nice sounding names like \u201cGod\u2019s mysterious ways\u201d, or \u201cThe Natural Order of Things.\u201d</p><p>&nbsp;</p><h2>Alien Gods, and Other Horrors</h2><p>&nbsp;</p><p>Now, who the hell is Lovecraft and why should we care?</p><p>H.P. Lovecraft was a science fiction/horror writer from the 1920s. He wrote about alien gods, about humans changing their bodies and minds, about the pursuit of immortality. But what makes him particularly relevant is one dominant underlying theme - that the universe is absolutely, unforgivingly neutral. That human life and morality has no inherent value. That mind-space is huge, and that possibility space is even huger, and that 99% of the things in possibility space are utterly terrifying to modern human values. <i>\u201cAll my tales,\u201d</i> Lovecraft said, <i>\u201care based on the fundamental premise that common human laws and interests and emotions have no validity or significance in the vast cosmos-at-large.\u201d</i></p><p>Lovecraft identified as an atheist, a materialist and even a rationalist, and his protagonists often identify as such. He was also, as far as I can tell, a pessimist who hated people in general. I\u2019m not sure what his beliefs about morality in the real world were. But he fascinates me because his writings suggest a dark mirror image of our ideals. Professor Quirrell to our Harry Potter, as a certain fanfiction would have it.</p><p>This is how Call of Cthulhu begins:</p><p><i>\u201cThe most merciful thing in the world, I think, is the inability of the human mind to correlate all its contents. We live on a placid island of ignorance in the midst of black seas of infinity, and it was not meant that we should voyage far. The sciences, each straining in its own direction, have hitherto harmed us little; but some day the piecing together of dissociated knowledge will open up such terrifying vistas of reality, and of our frightful position therein, that we shall either go mad from the revelation or flee from the light into the peace and safety of a new dark age.\u201d</i></p><p><br>We, of the Less Wrong community, have gotten a glimpse of an expanse of possibility-space outside the scope of most people\u2019s imagination. I know some people who are genuinely incapable of processing it. I know others who would, unless they took an initially painful plunge into the deep after us, look upon us with confusion and despair.</p><p>We ask hard questions about humanity, and about the universe, and a lot of the answers are dark. The Milgram experiment has been repeated many times, and consistently, we find that over half of humanity is willing to electrocute another person to death on the authority of a man in a lab coat. Across the world, people are born into situations \u2014 some natural, some human-made \u2014 where they can\u2019t provide for themselves, and it is often beyond their power to change that situation.</p><p>Every day, approximately 150,000 people die, their minds forever gone.</p><p>These are the facts. Some people stare into the Abyss and the Abyss stares back and they crawl away from the truth into the safety of ignorance.</p><p>These are facts, but there is more than one way to feel about them. We can look at the darkness of the world and wallow in despair. We can make up reasons why the darkness isn\u2019t so bad. Or we can look at the light, the things that, by our standards, are beautiful and good. And we can say:</p><p>\u201cThis is what is possible. This is the kind of future we can have.\u201d</p><p>And we can look at the darkness and say: \u201cThis is not acceptable. We will not rest until it is gone.\u201d However long it takes, however hard. Our gift and curse is that we look at something as awful as Death and see no natural order of things, only a problem to be solved, that we can\u2019t in good conscience resign ourselves to accepting.</p><p>We can do all this without Lovecraft or other made up stories. There are plenty of truths that are powerful and beautiful enough to craft a night of ritual. But an important part of Solstice Festivals IS the fun, the joviality. It can be difficult to slip directly into the kind of profound state that I want to achieve. In my family\u2019s Christmas Eve, we begin the night with songs about Santa and Frosty - boistrous, fun songs that suggest a time of magic, friendship and generosity, even if they don\u2019t actually have to do with a virgin born savior. As we progress through the hymnal, the songs grow more somber, and they turn to the ideas that Christmas is supposed to actually be about - the birth of Christ, peace on earth, God\u2019s forgiveness of the world. We end with a solemn Silent Night.</p><p>In this Solstice Eve celebration, Cthulhu, Azathoth and the Necronomicon play a part akin to Santa Claus - fun, ridiculous things that don\u2019t directly parallel AI or Existential Risk or Evolution or Immortality, but which nonetheless pay tribute to the core ideas that make those things important to us.</p><p>The night begins with many sources of light - from candles and oil lamps to gas lanterns to florescent bulbs to lasers and lava lamps. We begin with fun songs like \u201cIt\u2019s Beginning to Look A Lot Like Fish Men.\u201d As the night progresses, we turn the lights off, one by one, and the songs grow darker. We occasionally read relevant snippets of Lovecraft, then abridged versions of Eliezer\u2019s Sequences. We read the Litany of Tarski, over and over, each time facing a darker possibility that we must prepare ourselves for.</p><p>The Gift We Give to Tomorrow will be read with one candle remaining, extinguished immediately afterward.</p><p>Solstice Celebrations haven\u2019t been truly scary for a long time, and I think that\u2019s a mistake. We are alive today, enjoying the comfort of a warm apartment with food on the table, because millions of people have spent their lives preparing for the future. Using the best wisdom their tribe was able to give them. Finding new wisdom of their own. Working hard. Sometimes courageously speaking out, when the tribe feared a new idea. Dragging eight-thousand-pound rocks across 150 miles of land so that they could figure out when winter was coming, and prepare, so that they and their children could survive.</p><p>We honor those people, those first astronomers, and all the laborers and scientists and revolutionaries who have come since, for creating the world we have today.</p><p>And then we look to our future. Tiny stars in the distant sky, unimaginably far away, surrounded by black seas of infinity.</p><p>We will stare into that Abyss, and the Abyss <i>will</i> stare back at us. But we will go crazy-meta and challenge the Abyss to a staring contest and win the hell at it, because we\u2019re aspiring rationalists and good rationalists win.</p><p>And then, jubilantly, sing of a tomorrow that is brighter than today, a tomorrow where we are worthy of those stars, and have the power to reach them.</p><p>&nbsp;</p><p>&nbsp;</p><hr><p>&nbsp;</p><p>&nbsp;</p><p>This begins the Ritual mini-sequence. The next article is <a href=\"/lw/93l/the_value_and_danger_of_ritual/\">The Value (and Danger) of Ritual</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"vtozKm5BZ8gf6zd45": 4, "izp6eeJJEg9v5zcur": 2, "hXTqT62YDTTiqJfxG": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jES7mcPvKpfmzMTgC", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 120, "baseScore": 139, "extendedScore": null, "score": 0.000287, "legacy": true, "legacyId": "11561", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": "3bbvzoRA8n6ZgbiyK", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "the-value-and-danger-of-ritual", "canonicalPrevPostSlug": "on-rationalist-solstice-and-epistemic-caution", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 140, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><i>Note: Secular Solstice has evolved a bit since this original post (most noteably, it no longer has a major Lovecraft theme.&nbsp;</i></p><p>Last Friday, the NYC Less Wrong community held their first Winter Solstice Celebration. Approximately twenty of us gathered for dinner and a night of ritual. We sang songs, told stories, and recited litanies. The night celebrated ancient astronomers, and the work that humanity has done for the past 5000 years. It paid tribute to the harshness of the universe, respecting it as worthy opponent. We explored Lovecraftian mythology, which intersects with our beliefs in interesting ways.</p><p>And finally, we looked to the future, vowing to give a gift to tomorrow.</p><p>This is the first of 2-3 posts on this subject. In this one, I'm telling a story about what we did and why I wanted to. In the followup(s), I\u2019ll explain the design principles that went into planning such an event, and what we learned from our first execution of it. I\u2019ll also be posting a PDF of a ritual book, similar to the one we read from but with a few changes based on initial, obvious observations.</p><p>Why exactly did we do this? Doesn\u2019t this smack of organized religion? Who the hell is Lovecraft and why do we care?</p><p>Depending on your background, this may require the bridging of some inferential distance, as well as emotional distance. Bear with me.</p><p>&nbsp;</p><p>(If at the end, you DO still think this was a dangerous idea, or one you don't want popularized on Less Wrong, I want you to let me know. We're probably just going to disagree, but I want a sense of what the costs are of emphasizing this type of thing here)</p><h1 id=\"Winter_Solstice\">Winter Solstice</h1><p><i>To begin, a Just So Story, true enough for our purposes:</i></p><p>The Winter Solstice is the longest night of the year. It ushers in a time of cold and darkness.</p><p>For young civilizations, it was a time when if you HADN\u2019T spent the year preparing adequately for the future, then before spring returned, you would run out of food and die. If you hadn\u2019t striven to use your tribe\u2019s collective wisdom, to work hard beyond what was necessary for immediate gratification... if you hadn\u2019t harnessed the physical and mental tools that humans have but that few other animals do... then the universe, unflinchingly neutral, would destroy you without a second thought. And even if you did do these things, it might kill you anyway. Because fairness isn\u2019t built into the equations of the cosmos.</p><p>But it wasn't just the threat of death that inspired the first winter holidays. It was that sense of unfairness, coupled with the desperate hope that world couldn\u2019t <i>really</i> be that unfair. It wouldn\u2019t have occurred to the first squirrels that stored food for winter, but it gradually dawned upon ancient hominids, as their capacity for abstract reasoning developed, alongside their desire to throw parties.</p><p>Our tendency is to anthropomorphize. Today, we angrily yell at our cars and computers when they fail us. Rationally we know they are unthinking hulks of metal, but we still ascribe malevolence when the real culprit is a broken, unsentient machine.</p><p>There are plausible reasons for humans to have evolved this trait. One of the most complicated tasks a human has to do is predict the actions of other humans. We need to be able to make allies, to identify deceptive enemies, to please lovers. I\u2019m not an evolutionary psychologist and I should be careful when telling this sort of Just-So story, but I can easily imagine selection pressures that resulted in a powerful ability to draw conclusions about sentient creatures similar to ourselves.</p><p>And then, there was <i>NOT </i>a whole lot of pressure to <i>NOT </i>use this tool to predict, say, the weather. Many natural forces are just too complex for humans to be good at predicting. The rain would come, or it wouldn\u2019t, regardless of whether we ascribed it to gods or \u201cemergent complexity.\u201d So we told stories about gods, with human motivations, and we honestly believed them because there was nothing better.</p><p>And then, we had the solstice.</p><p>The world was dark and cold. The sun was retreating, leaving us only with the pale moon and stars that lay unimaginably far away. There was the enroaching threat of death, and just as powerfully, there was the threat that sentient cosmic forces that held supreme power over our world were <i>turning their backs on us.</i> And the best we could hope for was to throw a celebration in their honor and pray that they wouldn\u2019t be angry forever, that the sun would return and the world would be reborn.</p><p>And regardless, take a moment to be glad for having worked hard the previous year, so that we had meat stored up and wine that had finished fermenting.</p><p>But as ages passed, people noticed something interesting: there was a <i>pattern</i> to the gods getting angry. Weather may be complex and nigh-unpredictable. But the movements of the heavens... they follow rules simple enough for human minds to understand, if only you take the time to look.</p><p>We had a question. \u201cWhen will the sun retreat, and when will it return?\u201d</p><p>When you really care about knowing the answer, you can\u2019t make something up. When you need to plan your harvest and prepare for winter so that your family doesn\u2019t starve, you can\u2019t just say \u201cOh, God will stop getting angry in a few months.\u201d</p><p>If you want real knowledge, that you can apply to make your world better...</p><p>Then you need to do science. Astronomy was born.</p><p>I want to give you some perspective on how much we cared about this. Stonehenge is an ancient archaeological wonder. To the best of our knowledge, it began as a burial site around 3000 BCE. Over the next thousand years, it was gradually built, in major phases of activity every few hundred years. Between 2600 and 2400 BCE, there was a surge of construction. Huge stones were carted over huge distances, to create a monument that\u2019s lasted five thousand years.</p><p>30 Sarsen stones. Each of them was at least 25 tons. They were carried 25 miles.<br>80 bluestones. Four tons each. Carried over 150 miles.<br>In this era, the height of locomotive technology was \u201cthrow it on a pile of logs and roll it.\u201d</p><p>We don\u2019t know exactly how they did all this. We don\u2019t know all the reasons why. But we know at least one: The megaliths at Stonehenge are arranged, very specifically, to predict the Solstices. To the moment of dawn.</p><p>30 stones, each 25 tons, carried over 25 miles. 80 stones, each four tons, each carried<i> over 150 miles.</i></p><p>200 years of that.</p><p>That\u2019s how much we cared about the answer to that question.</p><h2 id=\"A_Modern_Journey\">A Modern Journey</h2><p>To modern society, Winter Solstice isn\u2019t very scary. We have oil to heat our homes, we have mechanical plows that clear our streets when the snow falls and other mechanical plows that work our fields all year round to supply us with food, carted from thousands of miles away, across land and sea. Many people today claim to enjoy Winter, although Richard Adams may accurately say that they really enjoy their protection from it.</p><p>Modern winter holidays are about enjoying that protection, not assuaging fear.</p><p>But there is a power in that, all the same. My family\u2019s Christmas Eve celebration is one of my favorite parts of the year. The extended family gathers. We have a big feast. Then 20+ people huddle up and sing songs and tell stories for hours. I don\u2019t believe in the literal messages of these rituals, but they have a power to them that I rarely see outside of religious-inspired works of art. They feel timeless and magical even though most Christmas carols have only existed for 50 years or so. The repetition of them each year grants them ritual strength. And the closeness I feel with my family grants them warmth.</p><p>Together, all these things are precious.</p><p>I didn\u2019t realize how precious, though, until the year I invited a friend of mine to the Christmas Eve party. Her first reaction amused me: \u201cWait, you guys literally sit around a fire and sing Christmas carols? Like, in movies?\u201d Her second reaction, as the night ended, was even more amusing: \u201cOh my god, I had no idea Christmas could be so <i>awesome!\u201d</i> But I knew what she meant, and it was accompanied with the realization that NOT everybody got to have experiences like this.</p><p>And that made Christmas Eve all the more special. It also made me realize how ridiculous it is that I only get to have that experience once a year.</p><p>That desire nagged at me a few years, and it was accompanied by another nagging dissatisfaction: That I didn\u2019t really believe in the words of the songs. They had power, generated by the <i>magnitude </i>of the songwriter\u2019s belief, and given lyric form by carefully honed skill. But they weren\u2019t true, and the falsehood itched at the back of my mind. Not because of the songs themselves, but because there weren't other songs, equally beautiful and with the same cultural weight, that were about things that I truly believed in.<br><br>Flash forward five years. I\u2019ve since discovered the sequences at Less Wrong. They outline studies in human behavior, how lots of our thinking is flawed if we want to achieve particular goals, how it can be hard to even know what our goals ARE, and why these are incredibly important questions to answer. Not just so we can succeed at life, but because if you\u2019re developing machine intelligence, and you haven\u2019t studied these questions (and solved problems that are, as I write this, unsolved), you could really, really, wreck the world. Wreck it worse than cold, uncompromising Nature ever could, worse and more unrecoverably than Hollywood has portrayed in explosive blockbuster films.</p><p>But if these questions are answered, and certain technological problems are solved, we can do incredible, important, beautiful things. In the past year I\u2019ve read powerful works of science, prose, and poetry that have resonated with all my strongest values. They\u2019ve changed how I approach my life and how I look at the future.</p><p>For the past year I\u2019ve attended the local Less Wrong meetup. I\u2019ve made new friends. I\u2019ve gotten involved with a community that encourages everyone to figure out what their goals are and try to achieve them, using the best tools they can find. We\u2019re going through similar life experiences. And for the past year, I\u2019ve been seeking out songs and stories that are fun, powerful and that we all truly believe in.</p><p>Ritual has been important in my life. I recognize that there is a risk whenever you begin elevating ideas and seeking them out <i>because</i> they are powerful and moving. I <i>don\u2019t </i>want to start a self-propogating organization designed to accrue followers blindly reciting the faith. But those of us who have studied these ideas and take them seriously - I want us to be able to find each other, to create friendship and family, and to celebrate together.</p><p>However, these powerful beliefs we share come with a cost:</p><p>I now believe a lot of really weird stuff that\u2019s hard to explain to the average person without sounding crazy. To certain people, they sound genuinely horrifying. I believe that living forever is a perfectly reasonable goal. I think that in the not too distant future, people will be able to radically alter their minds and bodies. In the not much more distant future, there\u2019s a good chance people will be able to live as uploaded computer programs. More frightening: I believe that people will eventually WANT to do this.</p><p>To be clear: I\u2019m <i>currently </i>lukewarm about a lot of this - my beliefs are complex, and like most humans I have a poor understanding of what I really value. But I can imagine the future me, plugging into the Matrix like it was no big deal.</p><p>All of this pales compared to the possibility of AI. The rest of humanity goes about their daily lives, planning for a future that involves slightly smaller iPhones and bigger televisions, vaguely annoyed that it\u2019s 2012 and we don\u2019t have flying cars yet. Blissfully unaware that with barely any warning, an AGI might be created and then bootstrap itself to godhood.</p><p>Blissfully unaware of how big mindspace is, and how little human morality would matter to a ghost of perfect emptiness, and how hard it is to create a mind from scratch that would care about us the way we care about ourselves.</p><p>But perhaps most blissful of all, they look upon the horrors that nature has inflicted us, and they give them nice sounding names like \u201cGod\u2019s mysterious ways\u201d, or \u201cThe Natural Order of Things.\u201d</p><p>&nbsp;</p><h2 id=\"Alien_Gods__and_Other_Horrors\">Alien Gods, and Other Horrors</h2><p>&nbsp;</p><p>Now, who the hell is Lovecraft and why should we care?</p><p>H.P. Lovecraft was a science fiction/horror writer from the 1920s. He wrote about alien gods, about humans changing their bodies and minds, about the pursuit of immortality. But what makes him particularly relevant is one dominant underlying theme - that the universe is absolutely, unforgivingly neutral. That human life and morality has no inherent value. That mind-space is huge, and that possibility space is even huger, and that 99% of the things in possibility space are utterly terrifying to modern human values. <i>\u201cAll my tales,\u201d</i> Lovecraft said, <i>\u201care based on the fundamental premise that common human laws and interests and emotions have no validity or significance in the vast cosmos-at-large.\u201d</i></p><p>Lovecraft identified as an atheist, a materialist and even a rationalist, and his protagonists often identify as such. He was also, as far as I can tell, a pessimist who hated people in general. I\u2019m not sure what his beliefs about morality in the real world were. But he fascinates me because his writings suggest a dark mirror image of our ideals. Professor Quirrell to our Harry Potter, as a certain fanfiction would have it.</p><p>This is how Call of Cthulhu begins:</p><p><i>\u201cThe most merciful thing in the world, I think, is the inability of the human mind to correlate all its contents. We live on a placid island of ignorance in the midst of black seas of infinity, and it was not meant that we should voyage far. The sciences, each straining in its own direction, have hitherto harmed us little; but some day the piecing together of dissociated knowledge will open up such terrifying vistas of reality, and of our frightful position therein, that we shall either go mad from the revelation or flee from the light into the peace and safety of a new dark age.\u201d</i></p><p><br>We, of the Less Wrong community, have gotten a glimpse of an expanse of possibility-space outside the scope of most people\u2019s imagination. I know some people who are genuinely incapable of processing it. I know others who would, unless they took an initially painful plunge into the deep after us, look upon us with confusion and despair.</p><p>We ask hard questions about humanity, and about the universe, and a lot of the answers are dark. The Milgram experiment has been repeated many times, and consistently, we find that over half of humanity is willing to electrocute another person to death on the authority of a man in a lab coat. Across the world, people are born into situations \u2014 some natural, some human-made \u2014 where they can\u2019t provide for themselves, and it is often beyond their power to change that situation.</p><p>Every day, approximately 150,000 people die, their minds forever gone.</p><p>These are the facts. Some people stare into the Abyss and the Abyss stares back and they crawl away from the truth into the safety of ignorance.</p><p>These are facts, but there is more than one way to feel about them. We can look at the darkness of the world and wallow in despair. We can make up reasons why the darkness isn\u2019t so bad. Or we can look at the light, the things that, by our standards, are beautiful and good. And we can say:</p><p>\u201cThis is what is possible. This is the kind of future we can have.\u201d</p><p>And we can look at the darkness and say: \u201cThis is not acceptable. We will not rest until it is gone.\u201d However long it takes, however hard. Our gift and curse is that we look at something as awful as Death and see no natural order of things, only a problem to be solved, that we can\u2019t in good conscience resign ourselves to accepting.</p><p>We can do all this without Lovecraft or other made up stories. There are plenty of truths that are powerful and beautiful enough to craft a night of ritual. But an important part of Solstice Festivals IS the fun, the joviality. It can be difficult to slip directly into the kind of profound state that I want to achieve. In my family\u2019s Christmas Eve, we begin the night with songs about Santa and Frosty - boistrous, fun songs that suggest a time of magic, friendship and generosity, even if they don\u2019t actually have to do with a virgin born savior. As we progress through the hymnal, the songs grow more somber, and they turn to the ideas that Christmas is supposed to actually be about - the birth of Christ, peace on earth, God\u2019s forgiveness of the world. We end with a solemn Silent Night.</p><p>In this Solstice Eve celebration, Cthulhu, Azathoth and the Necronomicon play a part akin to Santa Claus - fun, ridiculous things that don\u2019t directly parallel AI or Existential Risk or Evolution or Immortality, but which nonetheless pay tribute to the core ideas that make those things important to us.</p><p>The night begins with many sources of light - from candles and oil lamps to gas lanterns to florescent bulbs to lasers and lava lamps. We begin with fun songs like \u201cIt\u2019s Beginning to Look A Lot Like Fish Men.\u201d As the night progresses, we turn the lights off, one by one, and the songs grow darker. We occasionally read relevant snippets of Lovecraft, then abridged versions of Eliezer\u2019s Sequences. We read the Litany of Tarski, over and over, each time facing a darker possibility that we must prepare ourselves for.</p><p>The Gift We Give to Tomorrow will be read with one candle remaining, extinguished immediately afterward.</p><p>Solstice Celebrations haven\u2019t been truly scary for a long time, and I think that\u2019s a mistake. We are alive today, enjoying the comfort of a warm apartment with food on the table, because millions of people have spent their lives preparing for the future. Using the best wisdom their tribe was able to give them. Finding new wisdom of their own. Working hard. Sometimes courageously speaking out, when the tribe feared a new idea. Dragging eight-thousand-pound rocks across 150 miles of land so that they could figure out when winter was coming, and prepare, so that they and their children could survive.</p><p>We honor those people, those first astronomers, and all the laborers and scientists and revolutionaries who have come since, for creating the world we have today.</p><p>And then we look to our future. Tiny stars in the distant sky, unimaginably far away, surrounded by black seas of infinity.</p><p>We will stare into that Abyss, and the Abyss <i>will</i> stare back at us. But we will go crazy-meta and challenge the Abyss to a staring contest and win the hell at it, because we\u2019re aspiring rationalists and good rationalists win.</p><p>And then, jubilantly, sing of a tomorrow that is brighter than today, a tomorrow where we are worthy of those stars, and have the power to reach them.</p><p>&nbsp;</p><p>&nbsp;</p><hr><p>&nbsp;</p><p>&nbsp;</p><p>This begins the Ritual mini-sequence. The next article is <a href=\"/lw/93l/the_value_and_danger_of_ritual/\">The Value (and Danger) of Ritual</a>.</p>", "sections": [{"title": "Winter Solstice", "anchor": "Winter_Solstice", "level": 1}, {"title": "A Modern Journey", "anchor": "A_Modern_Journey", "level": 2}, {"title": "Alien Gods, and Other Horrors", "anchor": "Alien_Gods__and_Other_Horrors", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "180 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 180, "af": false, "version": "1.1.0", "pingbacks": {"Posts": ["rijoxTpkSPXcTXRbN"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 41, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-20T21:17:37.794Z", "modifiedAt": null, "url": null, "title": "[link] Innocentive challenge: $8000 for examples promoting altruistic behavior", "slug": "link-innocentive-challenge-usd8000-for-examples-promoting", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:04.426Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "dvasya", "createdAt": "2011-03-08T00:30:12.369Z", "isAdmin": false, "displayName": "dvasya"}, "userId": "2484AHxytrNyQXajh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nxegRzt8Aey3uHuuK/link-innocentive-challenge-usd8000-for-examples-promoting", "pageUrlRelative": "/posts/nxegRzt8Aey3uHuuK/link-innocentive-challenge-usd8000-for-examples-promoting", "linkUrl": "https://www.lesswrong.com/posts/nxegRzt8Aey3uHuuK/link-innocentive-challenge-usd8000-for-examples-promoting", "postedAtFormatted": "Tuesday, December 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20Innocentive%20challenge%3A%20%248000%20for%20examples%20promoting%20altruistic%20behavior&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20Innocentive%20challenge%3A%20%248000%20for%20examples%20promoting%20altruistic%20behavior%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnxegRzt8Aey3uHuuK%2Flink-innocentive-challenge-usd8000-for-examples-promoting%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20Innocentive%20challenge%3A%20%248000%20for%20examples%20promoting%20altruistic%20behavior%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnxegRzt8Aey3uHuuK%2Flink-innocentive-challenge-usd8000-for-examples-promoting", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnxegRzt8Aey3uHuuK%2Flink-innocentive-challenge-usd8000-for-examples-promoting", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 122, "htmlBody": "<p>A challenge recently posted on Innocentive seemed to me like something that may interest many LWers: <a href=\"https://www.innocentive.com/ar/challenge/9932631?refP=mahSSW1ad0Y%3D&amp;refC=AqF%2FJdMSa5M%3D\" target=\"_blank\">\"Models Motivating and Supporting Altruism Within Communities\"</a>, with a grand prize of $8000. To quote from the challenge:</p>\n<blockquote>\n<p>We are interested in looking at novel concepts from nature, business, or other areas that may elucidate the dynamics that help promote and maintain altruistic behaviors.</p>\n</blockquote>\n<p>Further details are available on <a href=\"https://www.innocentive.com/ar/challenge/9932631?refP=mahSSW1ad0Y%3D&amp;refC=AqF%2FJdMSa5M%3D\" target=\"_blank\">innocentive.com</a>.&nbsp;I think that it would be a nice opportunity for our LW decision theory experts.</p>\n<p><sub>[For anybody who decides to participate: the links I provided contain a referral string so that, in case you win a prize, I can match your donation to the SIAI with the same fraction of my referral award ;) Please use them to register.]</sub></p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nxegRzt8Aey3uHuuK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 7, "extendedScore": null, "score": 8.182745776330957e-07, "legacy": true, "legacyId": "11580", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 7, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-20T21:54:01.573Z", "modifiedAt": null, "url": null, "title": "Talking to Children: A Pre-Holiday Guide", "slug": "talking-to-children-a-pre-holiday-guide", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:54.922Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/6YjPdEvqGm2QiRqF5/talking-to-children-a-pre-holiday-guide", "pageUrlRelative": "/posts/6YjPdEvqGm2QiRqF5/talking-to-children-a-pre-holiday-guide", "linkUrl": "https://www.lesswrong.com/posts/6YjPdEvqGm2QiRqF5/talking-to-children-a-pre-holiday-guide", "postedAtFormatted": "Tuesday, December 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Talking%20to%20Children%3A%20A%20Pre-Holiday%20Guide&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ATalking%20to%20Children%3A%20A%20Pre-Holiday%20Guide%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6YjPdEvqGm2QiRqF5%2Ftalking-to-children-a-pre-holiday-guide%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Talking%20to%20Children%3A%20A%20Pre-Holiday%20Guide%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6YjPdEvqGm2QiRqF5%2Ftalking-to-children-a-pre-holiday-guide", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F6YjPdEvqGm2QiRqF5%2Ftalking-to-children-a-pre-holiday-guide", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 568, "htmlBody": "<p><strong>Note</strong>: This is based on anecdotal evidence, personal experience (I have worked with children for many years. It is my full-time job.) and \"general knowledge\" rather than scientific studies, though I welcome any relevant links on either side of the issue.</p>\n<p>&nbsp;</p>\n<hr />\n<p>&nbsp;</p>\n<p>The holidays are upon us, and I would guess that even though most of us are atheists, that we will still be spending time with our extended families sometime in the next week. These extended families are likely to include nieces and nephews, or other children, that you will have to interact with (probably whether you like it or not...)</p>\n<p>Many LW-ers might not spend a lot of time with children in their day-to-day lives, and therefore I would like to make a quick comment on how to interact with them in a way that is conducive to their development.&nbsp;After all, if we want to live in a rationalist world tomorrow, one of the best ways to get there is by raising children who can become rationalist adults.&nbsp;<br /><br /><strong>PLEASE READ THIS LINK if there are any little girls you will be seeing this holiday season</strong>:</p>\n<p>How To Talk to Little Girls: <a title=\"How To Talk To Little Girls\" href=\"http://www.huffingtonpost.com/lisa-bloom/how-to-talk-to-little-gir_b_882510.html?ref=fb&amp;src=sp&amp;comm_ref=false\">http://www.huffingtonpost.com/lisa-bloom/how-to-talk-to-little-gir_b_882510.html?ref=fb&amp;src=sp&amp;comm_ref=false</a><br /><br /><br />I know it's hard, but DON'T tell little girls that they look cute, and DON'T comment on their adorable little outfits, or their pony-tailed hair. The world is already screaming at them that the primary thing other people notice and care about for them is their looks. Ask them about their opinions, or their hobbies. Point them toward growing into a well-rounded adult with a mind of her own.<br /><br />This does not just apply to little girls and their looks, but can be extrapolated to SO many other circumstances. For example, when children (of either gender) are succeeding in something, whether it is school-work, or a drawing, DON'T comment on how smart or skilled they are. Instead, say something like: \"Wow, that was a really difficult math problem you just solved. You must have <em>studied really hard</em> to understand it!\" Have your comments focus on complementing their hard work, and their determination.<br /><br />By commenting on children's innate abilities, you are setting them up to believe that if they are good at something, it is <em>solely</em> based on talent. Conversely, by commenting on the amount of work or effort that went into their progress, you are setting them up to believe that they need to put effort into things, in order to succeed at them.<br /><br /><br />This may not seem like a big deal, but I have worked in childcare for many years, and have learned how elastic children's brains are. You can get them to believe almost anything, or have any opinion, JUST by telling them they have that opinion. Tell a kid they like helping you cook often enough, and they will quickly think that they like helping you cook.<br /><br />For a specific example, I made my first charge like <em>my</em> favorite of the little-kid shows by saying: \"Ooo! Kim Possible is on! You love this show!\" She soon internalized it, and it became one of her favorites. There is of course a limit to this. No amount of saying \"That show is boring\", and \"You don't like that show\" could convince her that Wonderpets was NOT super-awesome.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Q55STnFh6gbSezRuR": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "6YjPdEvqGm2QiRqF5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 51, "baseScore": 46, "extendedScore": null, "score": 9.2e-05, "legacy": true, "legacyId": "11581", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 46, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 94, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 1, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-20T23:37:11.658Z", "modifiedAt": null, "url": null, "title": "Measures, Risk, Death, and War", "slug": "measures-risk-death-and-war", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:03.104Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KgWticBMH2MxgdmYc/measures-risk-death-and-war", "pageUrlRelative": "/posts/KgWticBMH2MxgdmYc/measures-risk-death-and-war", "linkUrl": "https://www.lesswrong.com/posts/KgWticBMH2MxgdmYc/measures-risk-death-and-war", "postedAtFormatted": "Tuesday, December 20th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Measures%2C%20Risk%2C%20Death%2C%20and%20War&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeasures%2C%20Risk%2C%20Death%2C%20and%20War%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKgWticBMH2MxgdmYc%2Fmeasures-risk-death-and-war%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Measures%2C%20Risk%2C%20Death%2C%20and%20War%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKgWticBMH2MxgdmYc%2Fmeasures-risk-death-and-war", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKgWticBMH2MxgdmYc%2Fmeasures-risk-death-and-war", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 2487, "htmlBody": "<p>This is the fourth post of a <a href=\"/lw/8xr/decision_analysis_sequence/\">sequence on decision analysis</a>, preceded by <a href=\"/lw/8uj/compressing_reality_to_math/\">Compressing Reality to Math</a>. It touches on a wide variety of topics which didn't seem to work well as posts of their own, either because they were too short or too long.</p>\n<h1><a id=\"more\"></a>Measures over Prospects</h1>\n<p>So far, we've looked at distinct prospects: different toys, different activities, different life experiences. Those are difficult to compare, and we might actually be unsure about the ordering of some of them. Would I prefer playing Go or chatting more? It takes a bit of effort and imagination to say.</p>\n<p>Oftentimes, though, we face prospects that are measured in the same units. Would you prefer having $10 to having $4? There's no effort or imagination necessary: the answer is yes.<sup>1</sup> Facing wildly different prospects- a vacation to the Bahamas, a new computer, a raise at work- it can be helpful to try and reduce them to <a href=\"/lw/65/money_the_unit_of_caring/\">common units</a>, so that preferences are easy to calculate. This is especially true if the prospects are fungible: you could sell your new computer at some time cost to receive a dollar amount, or could buy one from a store at some dollar cost. It doesn't make sense to value winning a computer higher than the cost to buy one (or gain from selling one, if that manages to be higher), even if you value it much more highly than its cost.<sup>2</sup></p>\n<p>As always, adding uncertainty makes things interesting: would you prefer having {.5 $10, .5 $0} or {1 $4}?<sup>3</sup> The answer depends on your circumstances: if lunch costs $3 and you get $10 worth of value out of eating lunch (the first time), then the certain deal is probably better. If these are your marginal investment dollars, though, a 20% expected return is probably worth jumping on.</p>\n<p>When dealing with a complicated problem with lots of dollar prospects, we could express each one as the <a href=\"/lw/8m5/5_axioms_of_decision_making/\">certain equivalent</a> of a deal between the highest and lowest dollar prospects. If we aren't great at eliciting preferences, though, we might end up with weird results we don't really agree with, and adding a new dollar amount requires eliciting a new preference probability.</p>\n<p>An alternative is to come up with a function that maps the prospects to preference probabilities. The function can be fit with only a few elicited parameters, and then just evaluated for every prospect, making large problems and adding new prospects easy.</p>\n<p>As you've probably guessed, that function is called a <a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">utility function</a>.<sup>4</sup> I haven't brought it up before now because it's not necessary,<sup>5</sup> though a useful computational trick, and it's dangerous to think of utilities as measurable numbers out there, rather than expressions of an individual's preferences.</p>\n<h1>Risk Aversion</h1>\n<p>The primary information encoded by a utility function over one type of prospect is risk sensitivity. We can divide risk sensitivity into risk-averse, risk-neutral, and risk-loving (also called risk-affine)- basically, whether the utility function is concave, flat, or convex.</p>\n<p>&nbsp;</p>\n<p><img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/2a/Riskpremium1.png/275px-Riskpremium1.png\" alt=\"\" width=\"225\" height=\"200\" /><img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Riskpremium2.png/275px-Riskpremium2.png\" alt=\"\" width=\"225\" height=\"200\" /><img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/Riskpremium3.png/275px-Riskpremium3.png\" alt=\"\" width=\"225\" height=\"200\" /></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Averse, Neutral, and Loving. Images from <a href=\"http://en.wikipedia.org/wiki/Risk_aversion\">wikipedia</a>.</p>\n<p>In the risk-averse case, you essentially take some function of the variance off of the expected value of each deal. A risk averse person might rather have 9&plusmn;1 than 10&plusmn;10. Notice that which one they prefer depends on how curved their utility function is, i.e. how much penalty they charge risk. In the risk neutral case, variance is simply unimportant- you only decide based on expected values. In the risk-loving case, you <em>add</em> some function of the variance to the expected value- a risk affine person might prefer 9&plusmn;10 to 10&plusmn;1.</p>\n<p>Globally risk-loving people are hard to come by, although it's easy to imagine a utility function that's locally risk-loving (especially one that's risk-loving up to a certain point). True risk neutrality is also hard to come by- typically, if you multiply the scale by 10 enough times someone becomes risk-averse. Local risk neutrality, though, is the norm- zoom in on any utility function close enough and it'll be <a href=\"http://en.wikipedia.org/wiki/Derivative\">roughly flat</a>.</p>\n<p>So the utility functions we'll look at will be concave. <a href=\"http://www.wolframalpha.com/input/?i=plot+ln%28x%29+from+x%3D0+to+x%3D5\">Log</a> is a common choice, but is sometimes awkward in that log(0) is negative infinity, and log(infinity) is also infinity- it's unbounded both above and below. <a href=\"http://www.wolframalpha.com/input/?i=plot+1-exp%28-x%29+from+x%3D0+to+x%3D5\">Exponential</a> is better behaved- 1-exp(0)=0 and 1-exp(infinity)=1, and so it's bounded both above and below. It also follows what's called the Delta property: if we add a constant amount to every prospect, our behavior doesn't change.<sup>6</sup> The irrelevance of 'money in the bank' is sometimes sensible, but sometimes not- if we re-examine the earlier deal of ({$10, .5; $0, .5} or {$4, 1}), and add $3 to replace it with ({$13, .5; $3, .5} or {$7, 1}), the investor will just up his price by $3, whereas the lunch-buyer might switch from the second choice to the first. Thinking about the delta property- as well as risk premiums (how much would you pay to narrow an outcome uncertainty?) helps determine whether you should use linear, log, or exponential utility functions.</p>\n<p>&nbsp;</p>\n<h1>Micromorts<br /></h1>\n<p>The methodology we've discussed seems like it might have trouble comparing things of wildly different value. Suppose I like reading in the park more than reading in my home, but getting to the park requires traveling, and also suppose that traveling includes some non-zero chance of death. If I had a categorical preference that ranked the continuation of my life first, I would never choose to go to the park.</p>\n<p>But that seems far too cautious. If the difference in enjoyment were large enough - say the choice was between attending my daughter's wedding in the park and reading at home - it seems like I should accept the chance of death and travel. But perhaps that is too bold- if it were almost certain that I would die along the way, I suspect it would be wiser to not go, and others would agree with my assessment. That is, if we adopt categorical preferences (no amount of B could compensate for a reduction in A), we can construct realistic scenarios where we would make regrettable decisions.</p>\n<p>That suggests what we need to do is make a measured tradeoff. If I have a slight preference for living at the park to living at home, and a massive preference for living at home to dying along the way, then in order to go to the park I need it to be almost certain I will arrive alive, but there is some chance of death small enough that I would be willing to accept it.</p>\n<p>How small? The first 'small probability' that comes to mind is 1%, but that would be far, far too large. That's about 600 times riskier than skydiving. I don't expect my mind to process smaller numbers very effectively. When I think of 1 in 10,000 and 1 in 100,000, does the first feel ten times bigger?</p>\n<p>Like we just discussed, the way to deal with this sort of elicitation trouble is to turn to utility functions. Howard outlines an approach in a <a href=\"/dx.doi.org/10.1287%2Fmnsc.30.4.407\">1984 paper</a> which has some sensible features. Given an exponential utility function, there is some maximum probability of death one will accept money for- and in the example they give it's about 10%, though that number will obvious vary from person to person.<sup>7</sup> Anything riskier, and you couldn't be paid enough to accept.</p>\n<p>Conveniently, though, there is a large \"safety region\" where prices are linear with chance of death. That is, the price of an incremental risk doesn't change until the risks get rather severe. To make this easier to handle, consider a one millionth chance of dying: a micromort. That's a fairly convenient unit, as many risky behaviors have easily imagined scales at <a href=\"http://en.wikipedia.org/wiki/Micromort#Additional\">one micromort</a>. For example, walking 17 miles is one micromort; and so going to the park a two miles away and coming back represents a 2.5e-7 chance of dying. (You can calculate your baseline chance of dying <a href=\"http://micromorts.org/MortStats.aspx\">here</a>, though it should be noted by 'baseline' they mean 'average' rather than 'without doing anything.')</p>\n<p>How should we value that incremental amount? Well, it depends on what utility function you want to use, and what you assume about your life. Optimistic singularitarians, for example, should need far more money to accept a chance of dying than others, because they expect their lives to be longer and better than traditional analysis would suggest, but pessimistic singularitarians should need far less money to accept a chance of dying than others, because they expect their lives to be shorter or worse than traditional analysis would suggest.<sup>8</sup> The EPA suggests <a href=\"http://yosemite.epa.gov/ee/epa/eed.nsf/pages/MortalityRiskValuation.html#whatvalue\">$8.24</a> for Americans (in 2011 dollars), but this number should vary based on age, sex, risk attitude, wealth, and other factors. Common values seem to range from $2 to $50; when I ran my numbers a while back I got about $10. If we take the EPA number, it looks like walking to the park will cost me about $2. If I would rather be at the park and $2 poorer than if I were at home, then I should walk over there, even though it brings me a bit closer to death. When considering risky activities like skydiving, I just just adjust the price upwards and decide if I would still want to do it if it cost that much extra, but was safe. (For skydiving, each jump costs about $144 using the EPA micromort value.)</p>\n<p>&nbsp;</p>\n<h1>Adversarial Decision Making<br /></h1>\n<p>So far, we've mostly discussed decision-making under uncertainty by focusing on natural uncertainties- you're not sure if it'll rain or not, you're not sure if you'll win the lottery or not, you're not sure if you'll get involved in an accident on the way to the park or not. That's not the full picture, though: many important decisions include an adversary. Adversaries represent a special kind of uncertainty, because they react to your decisions, have their own uncertainties, and often actively want to make you worse off, rather than just not caring about your preferences.</p>\n<h2>Game Theory</h2>\n<p>Game Theory behaves a lot like the methods we've described before. Take a real situation, turn it into an action-payoff matrix, and find equilibria and mixed strategies. It's a large, rich field and I'm not going to describe how it works in detail, as there are <a href=\"/lw/8nw/link_a_gentle_video_introduction_to_game_theory/\">other resources for that</a>.</p>\n<p>One of the pitfalls with Game Theory, though, is that it requires some strong assumptions about how your opponent makes decisions. Can you really be sure your opponent will play the game-theoretically correct strategy, or that you've determined their payoff matrix correctly?</p>\n<p>For example, consider a game of rock-paper-scissors. Game Theory suggests a mixed strategy of throwing each possibility with 1/3 probability. When playing against <a href=\"http://www.youtube.com/watch?v=NMxzU6hxrNA\">Bart Simpson</a>, you can do better. Even when playing against a normal person, there are <a href=\"http://www.worldrps.com/index.php?option=com_content&amp;task=view&amp;id=256&amp;Itemid=37\">biases you can take advantage of</a>.</p>\n<p>As another example, consider that you run a small firm that's considering entering a market dominated by a large firm. After you choose to enter or not, they can choose whether to cut prices or not. You estimate the dollar payoffs are (yours, theirs):</p>\n<p><img src=\"http://img.photobucket.com/albums/v347/Vaniver/PriceWar1.jpg\" alt=\"\" /></p>\n<p>You see that, regardless of what you do, they earn more not having a price war, and if they don't go for a price war you would prefer entering to not entering. Indeed, (Enter, Don't) is a Nash Equilibrium. But suppose the scenario instead looked like this:</p>\n<p><img src=\"http://img.photobucket.com/albums/v347/Vaniver/PriceWar2.jpg\" alt=\"\" /></p>\n<p>The cells of the matrix all have the same ranking- regardless of whether or not you enter, they earn more by not having a price war. But the difference is much smaller, and the loss to you for entering if they do have a price war is much higher. (This could be because the entire firm will go under if this expansion fails, rather than just losing some money.) Someone might confidently announce that they won't engage in a price war, and so you should enter- but you might want to do a little research first on how strong their preference for dollars are. They might value market share- which isn't included in this payoff matrix- much more highly. That is, the Nash Equilibrium for this matrix (which is still the same cell) might not be the Nash Equilibrium for the real-world scenario.</p>\n<p>You can model this uncertainty about your opponent's strategy explicitly: include it as an uncertainty node that leads to several decision nodes, each operating on different preferences. You might decide, say, that you need &gt;83% confidence that they'll behave selfishly rather than vengefully (when it comes to dollars) in the second scenario, but only &gt;33% confidence that they'll behave selfishly rather than vengefully (when it comes to dollars) in the first scenario.</p>\n<p>&nbsp;</p>\n<hr />\n<p>1. Obviously, this is not true for everything- I might prefer two apples to one apple, but a million apples might be more trouble than they're worth. (Where would I put them?) For dollars, though, more is better in a much more robust way.</p>\n<p>2. This assumes that you'll still be able to buy the computer with whatever option you pick. If I decide to receive non-transferable tickets to a show that I enjoy at $1000 instead of a computer that costs $500 but I enjoy at $3000, and don't have the money to buy the computer, I made a mistake. But if I have at least $500 spare, I can consider myself as already having the first computer- the question is what a second one is worth. Ideally, preferences should be calculated over life experiences, not just events- your future life where you got the computer vs. your future life where you got the tickets.</p>\n<p>3. That is, option A is $10 50% of the time and $0 the other 50% of the time. Option B is $4 every time.</p>\n<p>4. What I described is a special case: a utility function which has a min of 0 and max of 1 on the domain of the problem. General utility functions don't have that constraint.</p>\n<p>5. The Von Neumann-Morgenstern axioms and the 5 axioms I discussed earlier are mostly the same, and so if you have someone willing to make decisions the way I'm describing they should also be willing to construct a utility function and compute expected utilities. Indeed, the processes will be difficult to distinguish, besides vocabulary, and so this is more a statement that the word is unnecessary than that the idea is unnecessary.</p>\n<p>6. It's so called because &Delta; is often used to signify a small amount- this is going through and replacing all prospects x<sub>i</sub> with x<sub>i</sub>+&Delta;.</p>\n<p>7. Incidentally, this is one of the differences between a log utility function and a exponential utility function. Someone with a log utility function would accept an arbitrarily small chance of life with an arbitrarily high wealth- but as wealth is practically bounded (there's only one Earth to own at present) that bounds the maximum chance of death.</p>\n<p>8. Interestingly, cryonics doesn't seem to alter this calculation, unless you think freezing technology will rapidly improve over your lifespan. That said, instead of just tracking risk of death you also need to track risk of not being frozen soon enough, meaning cause of death is much more relevant.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"KoXbd2HmbdRfqLngk": 2, "dPPATLhRmhdJtJM2t": 2, "b8FHrKqyXuYGWc6vn": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KgWticBMH2MxgdmYc", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": 17, "extendedScore": null, "score": 8.183259943268429e-07, "legacy": true, "legacyId": "11466", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 11, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>This is the fourth post of a <a href=\"/lw/8xr/decision_analysis_sequence/\">sequence on decision analysis</a>, preceded by <a href=\"/lw/8uj/compressing_reality_to_math/\">Compressing Reality to Math</a>. It touches on a wide variety of topics which didn't seem to work well as posts of their own, either because they were too short or too long.</p>\n<h1 id=\"Measures_over_Prospects\"><a id=\"more\"></a>Measures over Prospects</h1>\n<p>So far, we've looked at distinct prospects: different toys, different activities, different life experiences. Those are difficult to compare, and we might actually be unsure about the ordering of some of them. Would I prefer playing Go or chatting more? It takes a bit of effort and imagination to say.</p>\n<p>Oftentimes, though, we face prospects that are measured in the same units. Would you prefer having $10 to having $4? There's no effort or imagination necessary: the answer is yes.<sup>1</sup> Facing wildly different prospects- a vacation to the Bahamas, a new computer, a raise at work- it can be helpful to try and reduce them to <a href=\"/lw/65/money_the_unit_of_caring/\">common units</a>, so that preferences are easy to calculate. This is especially true if the prospects are fungible: you could sell your new computer at some time cost to receive a dollar amount, or could buy one from a store at some dollar cost. It doesn't make sense to value winning a computer higher than the cost to buy one (or gain from selling one, if that manages to be higher), even if you value it much more highly than its cost.<sup>2</sup></p>\n<p>As always, adding uncertainty makes things interesting: would you prefer having {.5 $10, .5 $0} or {1 $4}?<sup>3</sup> The answer depends on your circumstances: if lunch costs $3 and you get $10 worth of value out of eating lunch (the first time), then the certain deal is probably better. If these are your marginal investment dollars, though, a 20% expected return is probably worth jumping on.</p>\n<p>When dealing with a complicated problem with lots of dollar prospects, we could express each one as the <a href=\"/lw/8m5/5_axioms_of_decision_making/\">certain equivalent</a> of a deal between the highest and lowest dollar prospects. If we aren't great at eliciting preferences, though, we might end up with weird results we don't really agree with, and adding a new dollar amount requires eliciting a new preference probability.</p>\n<p>An alternative is to come up with a function that maps the prospects to preference probabilities. The function can be fit with only a few elicited parameters, and then just evaluated for every prospect, making large problems and adding new prospects easy.</p>\n<p>As you've probably guessed, that function is called a <a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">utility function</a>.<sup>4</sup> I haven't brought it up before now because it's not necessary,<sup>5</sup> though a useful computational trick, and it's dangerous to think of utilities as measurable numbers out there, rather than expressions of an individual's preferences.</p>\n<h1 id=\"Risk_Aversion\">Risk Aversion</h1>\n<p>The primary information encoded by a utility function over one type of prospect is risk sensitivity. We can divide risk sensitivity into risk-averse, risk-neutral, and risk-loving (also called risk-affine)- basically, whether the utility function is concave, flat, or convex.</p>\n<p>&nbsp;</p>\n<p><img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/2/2a/Riskpremium1.png/275px-Riskpremium1.png\" alt=\"\" width=\"225\" height=\"200\"><img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Riskpremium2.png/275px-Riskpremium2.png\" alt=\"\" width=\"225\" height=\"200\"><img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/Riskpremium3.png/275px-Riskpremium3.png\" alt=\"\" width=\"225\" height=\"200\"></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Averse, Neutral, and Loving. Images from <a href=\"http://en.wikipedia.org/wiki/Risk_aversion\">wikipedia</a>.</p>\n<p>In the risk-averse case, you essentially take some function of the variance off of the expected value of each deal. A risk averse person might rather have 9\u00b11 than 10\u00b110. Notice that which one they prefer depends on how curved their utility function is, i.e. how much penalty they charge risk. In the risk neutral case, variance is simply unimportant- you only decide based on expected values. In the risk-loving case, you <em>add</em> some function of the variance to the expected value- a risk affine person might prefer 9\u00b110 to 10\u00b11.</p>\n<p>Globally risk-loving people are hard to come by, although it's easy to imagine a utility function that's locally risk-loving (especially one that's risk-loving up to a certain point). True risk neutrality is also hard to come by- typically, if you multiply the scale by 10 enough times someone becomes risk-averse. Local risk neutrality, though, is the norm- zoom in on any utility function close enough and it'll be <a href=\"http://en.wikipedia.org/wiki/Derivative\">roughly flat</a>.</p>\n<p>So the utility functions we'll look at will be concave. <a href=\"http://www.wolframalpha.com/input/?i=plot+ln%28x%29+from+x%3D0+to+x%3D5\">Log</a> is a common choice, but is sometimes awkward in that log(0) is negative infinity, and log(infinity) is also infinity- it's unbounded both above and below. <a href=\"http://www.wolframalpha.com/input/?i=plot+1-exp%28-x%29+from+x%3D0+to+x%3D5\">Exponential</a> is better behaved- 1-exp(0)=0 and 1-exp(infinity)=1, and so it's bounded both above and below. It also follows what's called the Delta property: if we add a constant amount to every prospect, our behavior doesn't change.<sup>6</sup> The irrelevance of 'money in the bank' is sometimes sensible, but sometimes not- if we re-examine the earlier deal of ({$10, .5; $0, .5} or {$4, 1}), and add $3 to replace it with ({$13, .5; $3, .5} or {$7, 1}), the investor will just up his price by $3, whereas the lunch-buyer might switch from the second choice to the first. Thinking about the delta property- as well as risk premiums (how much would you pay to narrow an outcome uncertainty?) helps determine whether you should use linear, log, or exponential utility functions.</p>\n<p>&nbsp;</p>\n<h1 id=\"Micromorts\">Micromorts<br></h1>\n<p>The methodology we've discussed seems like it might have trouble comparing things of wildly different value. Suppose I like reading in the park more than reading in my home, but getting to the park requires traveling, and also suppose that traveling includes some non-zero chance of death. If I had a categorical preference that ranked the continuation of my life first, I would never choose to go to the park.</p>\n<p>But that seems far too cautious. If the difference in enjoyment were large enough - say the choice was between attending my daughter's wedding in the park and reading at home - it seems like I should accept the chance of death and travel. But perhaps that is too bold- if it were almost certain that I would die along the way, I suspect it would be wiser to not go, and others would agree with my assessment. That is, if we adopt categorical preferences (no amount of B could compensate for a reduction in A), we can construct realistic scenarios where we would make regrettable decisions.</p>\n<p>That suggests what we need to do is make a measured tradeoff. If I have a slight preference for living at the park to living at home, and a massive preference for living at home to dying along the way, then in order to go to the park I need it to be almost certain I will arrive alive, but there is some chance of death small enough that I would be willing to accept it.</p>\n<p>How small? The first 'small probability' that comes to mind is 1%, but that would be far, far too large. That's about 600 times riskier than skydiving. I don't expect my mind to process smaller numbers very effectively. When I think of 1 in 10,000 and 1 in 100,000, does the first feel ten times bigger?</p>\n<p>Like we just discussed, the way to deal with this sort of elicitation trouble is to turn to utility functions. Howard outlines an approach in a <a href=\"/dx.doi.org/10.1287%2Fmnsc.30.4.407\">1984 paper</a> which has some sensible features. Given an exponential utility function, there is some maximum probability of death one will accept money for- and in the example they give it's about 10%, though that number will obvious vary from person to person.<sup>7</sup> Anything riskier, and you couldn't be paid enough to accept.</p>\n<p>Conveniently, though, there is a large \"safety region\" where prices are linear with chance of death. That is, the price of an incremental risk doesn't change until the risks get rather severe. To make this easier to handle, consider a one millionth chance of dying: a micromort. That's a fairly convenient unit, as many risky behaviors have easily imagined scales at <a href=\"http://en.wikipedia.org/wiki/Micromort#Additional\">one micromort</a>. For example, walking 17 miles is one micromort; and so going to the park a two miles away and coming back represents a 2.5e-7 chance of dying. (You can calculate your baseline chance of dying <a href=\"http://micromorts.org/MortStats.aspx\">here</a>, though it should be noted by 'baseline' they mean 'average' rather than 'without doing anything.')</p>\n<p>How should we value that incremental amount? Well, it depends on what utility function you want to use, and what you assume about your life. Optimistic singularitarians, for example, should need far more money to accept a chance of dying than others, because they expect their lives to be longer and better than traditional analysis would suggest, but pessimistic singularitarians should need far less money to accept a chance of dying than others, because they expect their lives to be shorter or worse than traditional analysis would suggest.<sup>8</sup> The EPA suggests <a href=\"http://yosemite.epa.gov/ee/epa/eed.nsf/pages/MortalityRiskValuation.html#whatvalue\">$8.24</a> for Americans (in 2011 dollars), but this number should vary based on age, sex, risk attitude, wealth, and other factors. Common values seem to range from $2 to $50; when I ran my numbers a while back I got about $10. If we take the EPA number, it looks like walking to the park will cost me about $2. If I would rather be at the park and $2 poorer than if I were at home, then I should walk over there, even though it brings me a bit closer to death. When considering risky activities like skydiving, I just just adjust the price upwards and decide if I would still want to do it if it cost that much extra, but was safe. (For skydiving, each jump costs about $144 using the EPA micromort value.)</p>\n<p>&nbsp;</p>\n<h1 id=\"Adversarial_Decision_Making\">Adversarial Decision Making<br></h1>\n<p>So far, we've mostly discussed decision-making under uncertainty by focusing on natural uncertainties- you're not sure if it'll rain or not, you're not sure if you'll win the lottery or not, you're not sure if you'll get involved in an accident on the way to the park or not. That's not the full picture, though: many important decisions include an adversary. Adversaries represent a special kind of uncertainty, because they react to your decisions, have their own uncertainties, and often actively want to make you worse off, rather than just not caring about your preferences.</p>\n<h2 id=\"Game_Theory\">Game Theory</h2>\n<p>Game Theory behaves a lot like the methods we've described before. Take a real situation, turn it into an action-payoff matrix, and find equilibria and mixed strategies. It's a large, rich field and I'm not going to describe how it works in detail, as there are <a href=\"/lw/8nw/link_a_gentle_video_introduction_to_game_theory/\">other resources for that</a>.</p>\n<p>One of the pitfalls with Game Theory, though, is that it requires some strong assumptions about how your opponent makes decisions. Can you really be sure your opponent will play the game-theoretically correct strategy, or that you've determined their payoff matrix correctly?</p>\n<p>For example, consider a game of rock-paper-scissors. Game Theory suggests a mixed strategy of throwing each possibility with 1/3 probability. When playing against <a href=\"http://www.youtube.com/watch?v=NMxzU6hxrNA\">Bart Simpson</a>, you can do better. Even when playing against a normal person, there are <a href=\"http://www.worldrps.com/index.php?option=com_content&amp;task=view&amp;id=256&amp;Itemid=37\">biases you can take advantage of</a>.</p>\n<p>As another example, consider that you run a small firm that's considering entering a market dominated by a large firm. After you choose to enter or not, they can choose whether to cut prices or not. You estimate the dollar payoffs are (yours, theirs):</p>\n<p><img src=\"http://img.photobucket.com/albums/v347/Vaniver/PriceWar1.jpg\" alt=\"\"></p>\n<p>You see that, regardless of what you do, they earn more not having a price war, and if they don't go for a price war you would prefer entering to not entering. Indeed, (Enter, Don't) is a Nash Equilibrium. But suppose the scenario instead looked like this:</p>\n<p><img src=\"http://img.photobucket.com/albums/v347/Vaniver/PriceWar2.jpg\" alt=\"\"></p>\n<p>The cells of the matrix all have the same ranking- regardless of whether or not you enter, they earn more by not having a price war. But the difference is much smaller, and the loss to you for entering if they do have a price war is much higher. (This could be because the entire firm will go under if this expansion fails, rather than just losing some money.) Someone might confidently announce that they won't engage in a price war, and so you should enter- but you might want to do a little research first on how strong their preference for dollars are. They might value market share- which isn't included in this payoff matrix- much more highly. That is, the Nash Equilibrium for this matrix (which is still the same cell) might not be the Nash Equilibrium for the real-world scenario.</p>\n<p>You can model this uncertainty about your opponent's strategy explicitly: include it as an uncertainty node that leads to several decision nodes, each operating on different preferences. You might decide, say, that you need &gt;83% confidence that they'll behave selfishly rather than vengefully (when it comes to dollars) in the second scenario, but only &gt;33% confidence that they'll behave selfishly rather than vengefully (when it comes to dollars) in the first scenario.</p>\n<p>&nbsp;</p>\n<hr>\n<p>1. Obviously, this is not true for everything- I might prefer two apples to one apple, but a million apples might be more trouble than they're worth. (Where would I put them?) For dollars, though, more is better in a much more robust way.</p>\n<p>2. This assumes that you'll still be able to buy the computer with whatever option you pick. If I decide to receive non-transferable tickets to a show that I enjoy at $1000 instead of a computer that costs $500 but I enjoy at $3000, and don't have the money to buy the computer, I made a mistake. But if I have at least $500 spare, I can consider myself as already having the first computer- the question is what a second one is worth. Ideally, preferences should be calculated over life experiences, not just events- your future life where you got the computer vs. your future life where you got the tickets.</p>\n<p>3. That is, option A is $10 50% of the time and $0 the other 50% of the time. Option B is $4 every time.</p>\n<p>4. What I described is a special case: a utility function which has a min of 0 and max of 1 on the domain of the problem. General utility functions don't have that constraint.</p>\n<p>5. The Von Neumann-Morgenstern axioms and the 5 axioms I discussed earlier are mostly the same, and so if you have someone willing to make decisions the way I'm describing they should also be willing to construct a utility function and compute expected utilities. Indeed, the processes will be difficult to distinguish, besides vocabulary, and so this is more a statement that the word is unnecessary than that the idea is unnecessary.</p>\n<p>6. It's so called because \u0394 is often used to signify a small amount- this is going through and replacing all prospects x<sub>i</sub> with x<sub>i</sub>+\u0394.</p>\n<p>7. Incidentally, this is one of the differences between a log utility function and a exponential utility function. Someone with a log utility function would accept an arbitrarily small chance of life with an arbitrarily high wealth- but as wealth is practically bounded (there's only one Earth to own at present) that bounds the maximum chance of death.</p>\n<p>8. Interestingly, cryonics doesn't seem to alter this calculation, unless you think freezing technology will rapidly improve over your lifespan. That said, instead of just tracking risk of death you also need to track risk of not being frozen soon enough, meaning cause of death is much more relevant.</p>", "sections": [{"title": "Measures over Prospects", "anchor": "Measures_over_Prospects", "level": 1}, {"title": "Risk Aversion", "anchor": "Risk_Aversion", "level": 1}, {"title": "Micromorts", "anchor": "Micromorts", "level": 1}, {"title": "Adversarial Decision Making", "anchor": "Adversarial_Decision_Making", "level": 1}, {"title": "Game Theory", "anchor": "Game_Theory", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "14 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["iWH8Tnh4dBkDpCPws", "2FXtpdzx6uoNRZXjS", "ZpDnRCeef2CLEFeKM", "zFQQEkx4c6bxdshr4", "jNR9WXKAawEfQw5JM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-21T00:12:01.469Z", "modifiedAt": null, "url": null, "title": "Decision Analysis Sequence", "slug": "decision-analysis-sequence", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:09.261Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Vaniver", "createdAt": "2010-10-25T01:59:05.641Z", "isAdmin": true, "displayName": "Vaniver"}, "userId": "fD4ATtTkdQJ4aSpGH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iWH8Tnh4dBkDpCPws/decision-analysis-sequence", "pageUrlRelative": "/posts/iWH8Tnh4dBkDpCPws/decision-analysis-sequence", "linkUrl": "https://www.lesswrong.com/posts/iWH8Tnh4dBkDpCPws/decision-analysis-sequence", "postedAtFormatted": "Wednesday, December 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Decision%20Analysis%20Sequence&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADecision%20Analysis%20Sequence%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiWH8Tnh4dBkDpCPws%2Fdecision-analysis-sequence%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Decision%20Analysis%20Sequence%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiWH8Tnh4dBkDpCPws%2Fdecision-analysis-sequence", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiWH8Tnh4dBkDpCPws%2Fdecision-analysis-sequence", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 280, "htmlBody": "<p>This is the introduction (conclusion) to my decision analysis sequence. It covers (much more quickly and less completely) what you would expect to see in a semester-long course on decision making. The posts are:</p>\n<ol>\n<li><a href=\"/lw/8lb/uncertainty/\"><strong>Uncertainty</strong></a>: the basics of treating uncertainties as probabilities and doing Bayesian math.</li>\n<li><strong><a href=\"/lw/8m5/5_axioms_of_decision_making/\">5 Axioms of Decision Making</a></strong>: the five steps / assumptions that form the foundation of careful decision-making.</li>\n<li><a href=\"/lw/8uj/compressing_reality_to_math/\"><strong>Compressing Reality to Math</strong></a>: how to take a sticky, complicated situation and condense it down to something a calculator can solve, without feeling like you've left something important out.</li>\n<li><a href=\"/lw/8ui/measures_risk_death_and_war/\"><strong>Measures, Risk, Death, and War</strong></a>: how to deal with many similar prospects (utilities), risks of death, and adversaries.</li>\n<li><a href=\"/lw/85x/value_of_information_four_examples/\"><strong>Value of Information: Four Examples</strong></a>: how to value information-gathering activity, like tests or waiting, and incorporate it into your decision-making process.</li>\n</ol>\n<p>I'd like to welcome any comments about the sequence here. What parts did I do well? What parts need work? What parts would you like to see expanded (or removed)?</p>\n<p>One of the difficulties in posting about a topic like this is that it's foundational: basic, but important to get right. The idea of an expected utility calculation is not new (although the approach I take here may be novel for many of you) and, like I say in the VoI post, there's often more benefit in applying the process to examples than repeatedly talking about the process. The case studies I have access to, though, are not ones I can publish online, and I don't think I can construct an example that would work as well as a real one. Do people have problems they would like me to analyze with this framework as examples?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"KoXbd2HmbdRfqLngk": 11}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iWH8Tnh4dBkDpCPws", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 41, "extendedScore": null, "score": 8.3e-05, "legacy": true, "legacyId": "11583", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 41, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["AnbKBK236LtCdXSzj", "zFQQEkx4c6bxdshr4", "2FXtpdzx6uoNRZXjS", "KgWticBMH2MxgdmYc", "vADtvr9iDeYsCDfxd"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 2, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-21T07:15:25.337Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Beautiful Math", "slug": "seq-rerun-beautiful-math", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:09.457Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gZTGubXkHuw8JMB9v/seq-rerun-beautiful-math", "pageUrlRelative": "/posts/gZTGubXkHuw8JMB9v/seq-rerun-beautiful-math", "linkUrl": "https://www.lesswrong.com/posts/gZTGubXkHuw8JMB9v/seq-rerun-beautiful-math", "postedAtFormatted": "Wednesday, December 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Beautiful%20Math&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Beautiful%20Math%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgZTGubXkHuw8JMB9v%2Fseq-rerun-beautiful-math%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Beautiful%20Math%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgZTGubXkHuw8JMB9v%2Fseq-rerun-beautiful-math", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgZTGubXkHuw8JMB9v%2Fseq-rerun-beautiful-math", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 201, "htmlBody": "<p>Today's post, <a href=\"/lw/mq/beautiful_math/\">Beautiful Math</a> was originally published on 10 January 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>The joy of mathematics is inventing mathematical objects, and then noticing that the mathematical objects that you just created have all sorts of wonderful properties that you never intentionally built into them.  It is like building a toaster and then realizing that your invention also, for some unexplained reason, acts as a rocket jetpack and MP3 player.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/8xj/seq_rerun_0_and_1_are_not_probabilities/\">0 And 1 Are Not Probabilities</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gZTGubXkHuw8JMB9v", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 11, "extendedScore": null, "score": 8.184948494333225e-07, "legacy": true, "legacyId": "11595", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Rjw4qhEMvqskhL6Gm", "T3CcPhBqnFv7XjxM7", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-21T23:28:11.129Z", "modifiedAt": null, "url": null, "title": "Meetup : Fort Collins, Colorado Meetup", "slug": "meetup-fort-collins-colorado-meetup-5", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:07.382Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/kmTJNaWwKLmsARFEo/meetup-fort-collins-colorado-meetup-5", "pageUrlRelative": "/posts/kmTJNaWwKLmsARFEo/meetup-fort-collins-colorado-meetup-5", "linkUrl": "https://www.lesswrong.com/posts/kmTJNaWwKLmsARFEo/meetup-fort-collins-colorado-meetup-5", "postedAtFormatted": "Wednesday, December 21st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkmTJNaWwKLmsARFEo%2Fmeetup-fort-collins-colorado-meetup-5%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Fort%20Collins%2C%20Colorado%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkmTJNaWwKLmsARFEo%2Fmeetup-fort-collins-colorado-meetup-5", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FkmTJNaWwKLmsARFEo%2Fmeetup-fort-collins-colorado-meetup-5", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 42, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/5m'>Fort Collins, Colorado Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">04 January 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>A new year, a new opportunity to meet up.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/5m'>Fort Collins, Colorado Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "kmTJNaWwKLmsARFEo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.188535135860365e-07, "legacy": true, "legacyId": "11599", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup\">Discussion article for the meetup : <a href=\"/meetups/5m\">Fort Collins, Colorado Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">04 January 2012 07:00:00PM (-0700)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">144 North College Avenue, Fort Collins, CO 80524</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>A new year, a new opportunity to meet up.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/5m\">Fort Collins, Colorado Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Fort Collins, Colorado Meetup", "anchor": "Discussion_article_for_the_meetup___Fort_Collins__Colorado_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "3 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-22T01:32:06.694Z", "modifiedAt": null, "url": null, "title": "Some thoughts on AI, Philosophy, and Safety", "slug": "some-thoughts-on-ai-philosophy-and-safety", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:22.297Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "paulfchristiano", "createdAt": "2010-07-28T17:04:08.586Z", "isAdmin": false, "displayName": "paulfchristiano"}, "userId": "gb44edJjXhte8DA3A", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/avrEL7t3Ttitrjdwy/some-thoughts-on-ai-philosophy-and-safety", "pageUrlRelative": "/posts/avrEL7t3Ttitrjdwy/some-thoughts-on-ai-philosophy-and-safety", "linkUrl": "https://www.lesswrong.com/posts/avrEL7t3Ttitrjdwy/some-thoughts-on-ai-philosophy-and-safety", "postedAtFormatted": "Thursday, December 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Some%20thoughts%20on%20AI%2C%20Philosophy%2C%20and%20Safety&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASome%20thoughts%20on%20AI%2C%20Philosophy%2C%20and%20Safety%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FavrEL7t3Ttitrjdwy%2Fsome-thoughts-on-ai-philosophy-and-safety%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Some%20thoughts%20on%20AI%2C%20Philosophy%2C%20and%20Safety%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FavrEL7t3Ttitrjdwy%2Fsome-thoughts-on-ai-philosophy-and-safety", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FavrEL7t3Ttitrjdwy%2Fsome-thoughts-on-ai-philosophy-and-safety", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 99, "htmlBody": "<p>I've spent some time over the last two weeks thinking about problems around FAI. I've committed some of these thoughts to writing and put them up <a href=\"http://ordinaryideas.wordpress.com/\">here</a>.</p>\n<p>There are about a dozen real posts and some scraps. I think some of this material will be interesting to certain LWers; there is a lot of discussion of how to write down concepts and instructions formally (which doesn't seem so valuable in itself, but it seems like someone should do it at some point) some review and observations on decision theory, and some random remarks on complexity theory, entropy, and prediction markets.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "avrEL7t3Ttitrjdwy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 19, "baseScore": 17, "extendedScore": null, "score": 8.18899225603396e-07, "legacy": true, "legacyId": "11611", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 23, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-22T01:52:36.971Z", "modifiedAt": null, "url": null, "title": "Meetup : Columbus or Cincinnati Meetup", "slug": "meetup-columbus-or-cincinnati-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:33.162Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gW3kpYRmWbfHNBC8m/meetup-columbus-or-cincinnati-meetup", "pageUrlRelative": "/posts/gW3kpYRmWbfHNBC8m/meetup-columbus-or-cincinnati-meetup", "linkUrl": "https://www.lesswrong.com/posts/gW3kpYRmWbfHNBC8m/meetup-columbus-or-cincinnati-meetup", "postedAtFormatted": "Thursday, December 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Columbus%20or%20Cincinnati%20Meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Columbus%20or%20Cincinnati%20Meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgW3kpYRmWbfHNBC8m%2Fmeetup-columbus-or-cincinnati-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Columbus%20or%20Cincinnati%20Meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgW3kpYRmWbfHNBC8m%2Fmeetup-columbus-or-cincinnati-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FgW3kpYRmWbfHNBC8m%2Fmeetup-columbus-or-cincinnati-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 60, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/5n'>Columbus or Cincinnati Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">22 January 2012 05:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">Wilmington, OH</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Location: Max &amp; Erma's off I-71 in Wilmington</p>\n\n<p>Bring a board game(s).</p>\n\n<p>There is an Ohio LW email list and google group being formed. PM me to get on it.</p>\n\n<p>(Currently expecting ~9 attendees. Carpools available)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/5n'>Columbus or Cincinnati Meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gW3kpYRmWbfHNBC8m", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 5, "extendedScore": null, "score": 8.189067895061803e-07, "legacy": true, "legacyId": "11612", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Columbus_or_Cincinnati_Meetup\">Discussion article for the meetup : <a href=\"/meetups/5n\">Columbus or Cincinnati Meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">22 January 2012 05:00:00PM (-0500)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">Wilmington, OH</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Location: Max &amp; Erma's off I-71 in Wilmington</p>\n\n<p>Bring a board game(s).</p>\n\n<p>There is an Ohio LW email list and google group being formed. PM me to get on it.</p>\n\n<p>(Currently expecting ~9 attendees. Carpools available)</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Columbus_or_Cincinnati_Meetup1\">Discussion article for the meetup : <a href=\"/meetups/5n\">Columbus or Cincinnati Meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Columbus or Cincinnati Meetup", "anchor": "Discussion_article_for_the_meetup___Columbus_or_Cincinnati_Meetup", "level": 1}, {"title": "Discussion article for the meetup : Columbus or Cincinnati Meetup", "anchor": "Discussion_article_for_the_meetup___Columbus_or_Cincinnati_Meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "21 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-22T06:13:22.636Z", "modifiedAt": null, "url": null, "title": "The Magician: A Reductionist's Allegory", "slug": "the-magician-a-reductionist-s-allegory", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:30.944Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "peter_hurford", "createdAt": "2011-07-19T19:05:31.793Z", "isAdmin": false, "displayName": "Peter Wildeford"}, "userId": "FMsXugZ8aB5d8nHsm", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/fJyjCurkpT7QH9a6e/the-magician-a-reductionist-s-allegory", "pageUrlRelative": "/posts/fJyjCurkpT7QH9a6e/the-magician-a-reductionist-s-allegory", "linkUrl": "https://www.lesswrong.com/posts/fJyjCurkpT7QH9a6e/the-magician-a-reductionist-s-allegory", "postedAtFormatted": "Thursday, December 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Magician%3A%20A%20Reductionist's%20Allegory&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Magician%3A%20A%20Reductionist's%20Allegory%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfJyjCurkpT7QH9a6e%2Fthe-magician-a-reductionist-s-allegory%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Magician%3A%20A%20Reductionist's%20Allegory%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfJyjCurkpT7QH9a6e%2Fthe-magician-a-reductionist-s-allegory", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FfJyjCurkpT7QH9a6e%2Fthe-magician-a-reductionist-s-allegory", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1946, "htmlBody": "<p>I like to do a lot of hobbies. &nbsp;One of those is doing magic tricks with a deck of cards. &nbsp;It, along with debate, are the only performance arts I enjoy personally preforming.</p>\n<p>You know you have a great magic trick when you can do it without failing. &nbsp;But it also helps to impress the audience greatly. Truly amazing tricks do something that appears quite literally impossible, or at least downright inexplicable. &nbsp;In the course of doing my tricks, I make a large variety of supernatural claims, such as being able to read your mind by a sheer act of will, or being able to suspend the laws of physics in order to make a Ten of Hearts turn into a Nine of Spades.</p>\n<p>&nbsp;</p>\n<p>Now we know that these tricks are just that: <em>tricks</em>. &nbsp;I am not actually inducing all the atoms in a Ten of Hearts to transmogrify into all the atoms of a Nine of Spades, using supernatural powers to suspend the massive explosions that would take place if done through ...you know... physics. &nbsp;I only make it <em>look</em> like I am.</p>\n<p>In reality, I use techniques like <a href=\"http://en.wikipedia.org/wiki/Double_lift\">double lifts</a>&nbsp;and <a href=\"http://en.wikipedia.org/wiki/Equivocation_(magic)\">card forces</a>&nbsp;that all work well within the known boundaries of physics, no supernatural powers necessary. &nbsp;Every magic trick ever done is actually reducible to these slights of hand, along with a story to keep the trick moving (called \"patter\") and some other types of creativity.</p>\n<p>But this kind of <a href=\"/lw/on/reductionism/\">reductionism</a> in magic -- explaining what looks to be supernatural, paranormal, or just downright impossible in terms of slights of hand and misdirection -- is a great analogy to the claims of magic in real life: the claims that certain parts of the world we know of; most notably the soul, gods, and miracles; are all best explained as the supernatural is very similar to the claim that certain magic tricks are just that... actual magic.</p>\n<p>&nbsp;</p>\n<p>In the South Park episode <a href=\"http://en.wikipedia.org/wiki/Super_Best_Friends\">\"Super Best Friends\"</a>, magician <a href=\"http://en.wikipedia.org/wiki/David_Blaine\">David Blaine</a>&nbsp;visits South Park, impressing the town's residents with street magic, creating a cult that worshiped him as capable of preforming true magic.</p>\n<p>Now imagine that Blainetology was not a cult, but an honest religion that thought they saw genuine miracles. &nbsp;The Blainetologists hold that some of David Blaine's tricks are examples of real, supernatural magic, and have start building churches to worship Blaine, asserting that his final magic trick will be to grant all of his believers everlasting life in a paradise he creates. &nbsp;Some of them have even started suggesting that you need belief in Blaine to be a good person and live a meaningful, purposeful life.</p>\n<p>In response, many people skeptical of the Blainetologist claims have started calling themselves reductionists, saying that not only do they lack a belief in Blaine's powers, but make the additional positive claim that all of Blaine's magic is <em>fully reducible</em> to to slight of hand and misdirection.</p>\n<p>Imagine the kind of arguments it would take for a metaphysical naturalist to persuade a Blainetologist that Blaine did not really have supernatural powers, and the kind of arguments the Blainetologist would offer in return to defend Blainetology as a true religion:</p>\n<p>&nbsp;</p>\n<p><strong>Reductionist:</strong> Our best scientists are thoroughly analyzing more and more of Blaine's tricks and finding them not to be supernatural at all. &nbsp;<a href=\"http://www.youtube.com/watch?v=SWzXtjeuXAk\">Look at this Magic Trick</a> done by a prophet who seems to have the power of David Blaine.</p>\n<p>You claim that there's a lot of magic done in this trick -- he magically is able to channel card spirits to determine three cards at random, and then is able to make them teleport to his hand, and then is able to make them switch with completely different cards. &nbsp;There is so much teleportation and prediction here that it would&nbsp;ordinarily&nbsp;boggle a mind. &nbsp;However, we had our scientist&nbsp;<strong>mismag822</strong>&nbsp;<a href=\"http://www.youtube.com/watch?v=mzkT4fc08fM\">thoroughly analyze Blaine's performance</a>, and he found out that the trick is not supernatural at all, but instead fully reducible to trickery: a pre-arranged deck, bent cards, thumb breaks, pinky breaks, riffle forces, fake flips, fake counts, and double lifts.</p>\n<p>More and more of his tricks like this are being analyzed and found to not be magical, but instead be trickery. &nbsp;Claims of David Blaine having supernatural powers have a track record of failure. &nbsp;If you have one horse that represents reductionism, and one horse that represents Blainetology, and naturalism's horse has won in thousands of races and never lost, whereas Blainetology's horse has lost in thousands of races and never won, who are you going to bet on?</p>\n<p>&nbsp;</p>\n<p><strong>Blainetologist:</strong> That's hardly fair; you're attacking a straw man. &nbsp;Sure we once thought this trick was magic, but modern Blainetology no longer holds this. &nbsp;You forget many of the recent Blainetologist writers have acknowledged that only the select few of Blaine's most magnificent tricks are truly magic. &nbsp;<a href=\"http://www.youtube.com/watch?v=0_6OaP_mbpQ\">Consider&nbsp;this trick</a> by David Blaine himself.</p>\n<p>This level of outright levitation is not explicable by any \"card forces\" or \"misdirection\" -- Blaine gets them to select a card at random and then makes it rise out of the deck, without even touching it himself! &nbsp;It all happens in the hands of an amazed audience! &nbsp;You can't explain that with science!</p>\n<p>&nbsp;</p>\n<p><strong>Reductionist:</strong> Sure, it's inexplicable right now, but our science will eventually catch up and fully explain how David Blaine was able to read her mind. &nbsp;We already can explain how he managed to switch the card in her hand -- even you agree that's not magic. &nbsp;What makes you think we won't be able to eventually explain the mind reading with science? &nbsp;It sounds like you're trying to advance a \"Magic of the Gaps\", where anything about Blaine science can't explain is said to be the realm of Blaine's magic.</p>\n<p>&nbsp;</p>\n<p><strong>Blainetologist:</strong> Again, you're just creating straw men. &nbsp;We have worked hard to create an Inference to the Best Explanation argument here: if Blaine is magical, the existence of his ability to levitate cards is to be expected. &nbsp;However, if Blaine is not magical, then his ability to levitate cards is very surprising and very unlikely. &nbsp;This means it's very likely that Blaine is magical. &nbsp;What you're doing could be the same thing: a Reductionism of the Gaps -- you think whatever we can't explain still must be reducible. &nbsp;What kind of trick needs to be preformed to get you to give it up and admit the existence of magic?</p>\n<p>&nbsp;</p>\n<p><strong>Reductionist:</strong> Hardly. &nbsp;I have no reason to think Blaine is magical because naturalist explanations are doing a great job of explaining Blaine so far, and I see no reason why they won't continue to do so. &nbsp;Also your Inference to the Best Explanation is kind of bogus: you can't properly evaluate how much more likely certain evidence makes a hypothesis without knowing the initial likelihood of that hypothesis. &nbsp;Saying Blaine is magic is a guess at best, and we don't know how likely \"magic\" is, and until we do we can't really make hypotheses about it.</p>\n<p>&nbsp;</p>\n<p><strong>Blainetologist:</strong> The hypothesis that Blaine is magical is simply just the best possible explanation of his card levitation trick. &nbsp;It explains his whole trick in a complete and satisfying manner, therefore having far more explanatory power than your the current naturalist guesses involving \"she might be in on it\" or \"he flipped the cards in a certain way to make her see a card\" -- that's just desperation! &nbsp;It's also a stunningly simple hypothesis -- no complex reductions to many different complex slights of hand are necessary, we just need to suggest the existence of magic. &nbsp;Occam would be proud, no?</p>\n<p>&nbsp;</p>\n<p><strong>Naturalist:</strong> No, I don't think you understand the complexity of what you are suggesting. &nbsp;When you utter that word \"magic\", what do you mean? &nbsp;What experiences do you anticipate having in a world where magic is real, versus one where magic is not? &nbsp;How does magic even work? &nbsp;Does \"it just works, no further explanation needed\"?</p>\n<p>How can Blaine do things just by act of will? &nbsp;Is this not suggesting you can interact with something without having any method of interacting with it? &nbsp;Surely such would be a logical impossibility, the kind of impossibility that not even supernatural powers can fix, because supernatural powers just suspend physics, not logic.</p>\n<p>&nbsp;</p>\n<p><strong>Blainetologist:</strong> Wow, you sure like advancing an army of arguments at once, in an attempt to overwhelm my forces. &nbsp;Why don't we start doing one argument at a time? &nbsp;Magic is what allows David Blaine to levitate cards, if magic didn't exist, I would anticipate David Blaine not being able to preform that levitation trick I talked about.</p>\n<p>And asking how magic works is just silly, because explanations deal with mechanisms and laws of physics, and magic has no mechanisms and operates outside the laws of physics. &nbsp;It's not a logical impossibility, it's just operating outside the laws of causality you are so used to. &nbsp;Besides, what would you accept as proof of magic? &nbsp;You're big on that whole \"anticipate experiences\" thing -- what experiences would you need to see in order to start believing magic is real? &nbsp;Remember that if you don't know of any such experiences, even in principle, that your naturalism is meaningless by your own definitions.</p>\n<p>&nbsp;</p>\n<p><strong>Reductionist:</strong> Fair enough. &nbsp;An experience I could have that would rather conclusively prove that Blaine is magical is for him to do something so out of the ordinary that it would be unthinkable to try to reduce it to slight of hand. &nbsp;Why doesn't Blaine teleport a copy of \"The Holy Book of Blaine\" into every home? &nbsp;Why doesn't Blaine <a href=\"http://www.randi.org/site/index.php/1m-challenge.html\">collect a million dollars from James Randi</a>&nbsp;by doing some more direct and controlled types of mind reading, like just simply saying all of Randi's thoughts as Randi thinks them?</p>\n<p>Such events would not only do much to convince me of Blainetology, but the fact that Blaine has not done this also constitutes a strong argument against Blainetology. &nbsp;Surely Blaine can do much more than just a simple magic trick to a lady on the street. &nbsp;James Randi is one of the most notable ablaineists, so surely he could sort this out.</p>\n<p>&nbsp;</p>\n<p><strong>Blainetologist:</strong> Well why would Blaine want to? &nbsp;That hardly disproves Blaine's magical powers.</p>\n<p>&nbsp;</p>\n<p><strong>Naturalist:</strong> Well, it would end our debate rather soundly. &nbsp;Surely that's in Blaine's interest? &nbsp;Does he not want everyone to accept him as the one true magician and enter into a loving relationship with him by attending his shows? &nbsp;And such an unambiguous miracle would accomplish that, right?</p>\n<p>&nbsp;</p>\n<p><strong>Blainetologist:</strong> Blaine works in mysterious ways.</p>\n<p>&nbsp;</p>\n<p><strong>Reductionist:</strong> Well that's a cop-out, isn't it? &nbsp;We just can't understand Blaine's plan? &nbsp;You are trying my patience.</p>\n<p>&nbsp;</p>\n<p><strong>Blainetologist:</strong>&nbsp;It's not a cop-out, it's a valid unknown purposes defense. &nbsp;Your assuming that if Blaine had a sufficient reason for not proving his supernatural nature, we would see it.</p>\n<p>...</p>\n<p>&nbsp;</p>\n<p>In this allegory, Blainetology is presented as intentionally frustrating because it seems so reasonable and logical despite being so silly. &nbsp;What of the inference that a magic trick is most likely to be explicable by slight of hand instead of actual magical powers? &nbsp;It's an entirely unproven inference, yet we use it every time we see a magician preform. &nbsp;If there's one thing I would want those who invoke supernatural powers to explain, it would be to answer why it is reasonable to inference to a trick with magicians, but not with things like&nbsp;consciousness&nbsp;or the origin on the universe?</p>\n<p>In many ways, major events like the origin of the universe (if there is an origin at all) are the ultimate magic trick. &nbsp;A lot of of events, like lightning and the origin of species, were once mysterious magic tricks, but now have been fully explained by naturalism. &nbsp;Yet the remaining tricks are looked as not puzzles to be figured out, but rather <a href=\"/lw/j2/explainworshipignore/\">stuff to just call magic</a>.</p>\n<p>Surely this discussion and frustration captures a lot of what we talk about on LessWrong without actually having to say the word \"Bayes\".</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "fJyjCurkpT7QH9a6e", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 25, "extendedScore": null, "score": 8.190029921388156e-07, "legacy": true, "legacyId": "11614", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tPqQdLCuxanjhoaNs", "yxvi9RitzZDpqn6Yh"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-22T06:37:49.482Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Expecting Beauty", "slug": "seq-rerun-expecting-beauty", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EdeGsSRtziW5BCv4E/seq-rerun-expecting-beauty", "pageUrlRelative": "/posts/EdeGsSRtziW5BCv4E/seq-rerun-expecting-beauty", "linkUrl": "https://www.lesswrong.com/posts/EdeGsSRtziW5BCv4E/seq-rerun-expecting-beauty", "postedAtFormatted": "Thursday, December 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Expecting%20Beauty&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Expecting%20Beauty%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEdeGsSRtziW5BCv4E%2Fseq-rerun-expecting-beauty%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Expecting%20Beauty%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEdeGsSRtziW5BCv4E%2Fseq-rerun-expecting-beauty", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEdeGsSRtziW5BCv4E%2Fseq-rerun-expecting-beauty", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 179, "htmlBody": "<p>Today's post, <a href=\"/lw/mr/expecting_beauty/\">Expecting Beauty</a> was originally published on 12 January 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Mathematicians expect that if you dig deep enough, a stable, or even beautiful, pattern will emerge. Some people claim that this belief is unfounded. But, we have previously found order in many of the places we've looked for it.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8y3/seq_rerun_beautiful_math/\">Beautiful Math</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EdeGsSRtziW5BCv4E", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 8, "extendedScore": null, "score": 8.190120125455e-07, "legacy": true, "legacyId": "11615", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["uichBYWKcGAqRZZdP", "gZTGubXkHuw8JMB9v", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-22T11:53:32.492Z", "modifiedAt": null, "url": null, "title": "Anything New?", "slug": "anything-new", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "billswift", "createdAt": "2009-02-28T05:59:56.578Z", "isAdmin": false, "displayName": "billswift"}, "userId": "WBoeSNFkZ4q8nRenK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nbv3rdcpfix5YMuaY/anything-new", "pageUrlRelative": "/posts/nbv3rdcpfix5YMuaY/anything-new", "linkUrl": "https://www.lesswrong.com/posts/nbv3rdcpfix5YMuaY/anything-new", "postedAtFormatted": "Thursday, December 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Anything%20New%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAnything%20New%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnbv3rdcpfix5YMuaY%2Fanything-new%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Anything%20New%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnbv3rdcpfix5YMuaY%2Fanything-new", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fnbv3rdcpfix5YMuaY%2Fanything-new", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 101, "htmlBody": "<p>In a discussion on HackerNews that went a little sideways, I left this comment:</p>\n<blockquote>\n<p><span class=\"comment\"><span style=\"color: #000000;\">I haven't seen much original  on the internet.  Almost every \"new\" idea I have seen on the Net I saw  in print in the 1980s or 1990s.  Even if the software is just now  catching up to the ideas.</span></span></p>\n</blockquote>\n<p><span class=\"comment\"><span style=\"color: #000000;\">Thinking about it more, I cannot think of a single new <strong><em>idea</em></strong> that I have read in more than a decade.&nbsp; The closest is Robin's take on the <strong><em>primacy</em></strong> of status and signalling, and even that is really just an extension.</span></span></p>\n<p><span class=\"comment\"><span style=\"color: #000000;\">So what are your candidates for really new ideas?</span></span></p>\n<p><span class=\"comment\"><span style=\"color: #000000;\"><br /></span></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nbv3rdcpfix5YMuaY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 0, "legacy": true, "legacyId": "11616", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-22T21:23:59.188Z", "modifiedAt": null, "url": null, "title": "How to get the most out of the next year", "slug": "how-to-get-the-most-out-of-the-next-year", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:03.950Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alexvermeer", "createdAt": "2010-08-13T16:28:34.576Z", "isAdmin": false, "displayName": "alexvermeer"}, "userId": "3bK6aDQviGG3ovuDJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pAEYZ7MQEyDW3w4bv/how-to-get-the-most-out-of-the-next-year", "pageUrlRelative": "/posts/pAEYZ7MQEyDW3w4bv/how-to-get-the-most-out-of-the-next-year", "linkUrl": "https://www.lesswrong.com/posts/pAEYZ7MQEyDW3w4bv/how-to-get-the-most-out-of-the-next-year", "postedAtFormatted": "Thursday, December 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20get%20the%20most%20out%20of%20the%20next%20year&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20get%20the%20most%20out%20of%20the%20next%20year%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpAEYZ7MQEyDW3w4bv%2Fhow-to-get-the-most-out-of-the-next-year%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20get%20the%20most%20out%20of%20the%20next%20year%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpAEYZ7MQEyDW3w4bv%2Fhow-to-get-the-most-out-of-the-next-year", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpAEYZ7MQEyDW3w4bv%2Fhow-to-get-the-most-out-of-the-next-year", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 948, "htmlBody": "<p>We are not <a onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','lesswrong.com']);\" href=\"/lw/2p5/humans_are_not_automatically_strategic/\">automatically strategic</a>. Since we <a onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','lesswrong.com']);\" href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">want</a> to get stuff done, it only makes sense to <a onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','lesswrong.com']);\" href=\"/lw/ui/use_the_try_harder_luke/\">try harder</a>&mdash;and what better time than the start of a new year?</p>\n<p>In short: the New Year is a great time to do some life review and planning. How are things going? How was the past year? Do a thorough personal review to find out. What are you going to do next year? Make a plan. How can you increase your chances of success? Take all of your plans, projects, and goals and <em>optimize</em> them <a onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','lesswrong.com']);\" href=\"/r/discussion/lw/8vm/the_rationalists_checklist/\">based</a> <a onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','wiki.lesswrong.com']);\" href=\"http://wiki.lesswrong.com/wiki/The_Science_of_Winning_at_Life\">on</a> <a onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','lesswrong.com']);\" href=\"/lw/3nn/scientific_selfhelp_the_state_of_our_knowledge/\">what</a> <a onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','lesswrong.com']);\" href=\"/lw/4su/how_to_be_happy/\">you</a> <a onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','lesswrong.com']);\" href=\"/lw/3w3/how_to_beat_procrastination/\">have</a> <a onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','lesswrong.com']);\" href=\"/lw/1sm/akrasia_tactics_review\">learned</a> <a onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','lesswrong.com']);\" href=\"/lw/1fe/antiakrasia_technique_structured_procrastination/\">on</a> Less Wrong.</p>\n<p>&mdash;</p>\n<p>In the spirit of communal <a onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','wiki.lesswrong.com']);\" href=\"http://wiki.lesswrong.com/wiki/Rationalists_should_win\">winning</a>, here are the details on how I do my yearly review. <a onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','lesswrong.com']);\" href=\"/lw/9v/beware_of_otheroptimizing/\">Maybe</a> you will find this process as useful as I have? I&rsquo;m not quite sure if this is LW quality, so let me know with the karma.</p>\n<p>For starters, I break down my life into fourteen areas, <em>not</em> necessarily of equal importance or in any specific order:</p>\n<p><strong>Worldview &amp; Purpose&nbsp;</strong>&ndash;&nbsp;Do you have <a onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','lesswrong.com']);\" href=\"/lw/nb/something_to_protect/\">something to protect</a>? Your worldview is your complete set of beliefs about everything&mdash;the past, present, and future; what is valuable, virtuous, and just. Do you have clarity as to your existence, purpose, and place in the universe? What is your philosophy of life? What do you want to get out of life?</p>\n<p><strong>Contribution &amp; Impact&nbsp;</strong>&ndash;&nbsp;How are you giving value to the world? Are you making a difference? How much impact does your existence have environmentally, socially, and cognitively? Are you practicing <a onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','lesswrong.com']);\" href=\"/lw/6py/optimal_philanthropy_for_human_beings/\">optimal</a> <a onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','lesswrong.com']);\" href=\"/lw/8j9/new_website_on_careers_for_optimal_philanthropy/\">philanthropy</a>?</p>\n<p><strong>Location &amp; Possessions </strong>&ndash; Are you tied to one location? Are you mobile? This includes your current living situation, where you are in the world; your home, possessions, electronics, toys, and material sufficiency.</p>\n<p><strong>Money &amp; Finances</strong> &ndash; Do you have savings, investments, assets, and debt? Do you have a budget and do you follow it? Are your finances organized and managed? Do you know where you spend your money?</p>\n<p><strong>Career &amp; Work&nbsp;</strong>&ndash;&nbsp;Your work, job, career, or business; your source of income. Is what you do your calling? Are you engaged? Have you <a onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','lesswrong.com']);\" href=\"/lw/43m/optimal_employment/\">optimized your job</a>? Are you networked within your industry?</p>\n<p><strong>Health &amp; Fitness</strong> &ndash; What do you eat? Do you exercise regularly? How often do you get sick? What is your overall energy level and resistance to illness? What are your major health issues and susceptibilities?</p>\n<p><strong>Knowledge &amp; Education </strong>&ndash; What do you know? Are you developing your mind and learning new things? Do you have any talents or skills? Are you being educated?</p>\n<p><strong>Communication</strong>&nbsp;&ndash;&nbsp;Are you spreading ideas? Do you spend time discussing, influencing, persuading, arguing, philosophizing, debating, interacting, writing, or speaking?</p>\n<p><strong>Intimate Relationship</strong> &ndash; The intimate relationship(s) you have or want to have; your partner; the quality of your relationship.</p>\n<p><strong>Social Life</strong>&nbsp;&ndash;&nbsp;This covers your home life, relationships with family members, friends, and social experiences. Are you networking? Are you <a onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','lesswrong.com']);\" href=\"/meetups\">meeting</a> new people? Are you a member of any clubs and organizations?</p>\n<p><strong>Emotions</strong> &ndash; Your general feeling about life. Are you optimistic or pessimistic, positive or negative? Are you aware of your emotions as they are happening?</p>\n<p><strong>Character &amp; Integrity</strong> &ndash; Your intelligence, integrity, honesty, courage, compassion, honor, self-discipline, etc.</p>\n<p><strong>Productivity &amp; Organization</strong> &ndash; Your memorized solutions, daily routine, and schedule. How good is your productivity? Do you act effectively? Are you organized?</p>\n<p><strong>Fun &amp; Adventure</strong>&nbsp;&ndash;&nbsp;Are you experiencing what you want to experience? Are you <em>enjoying life</em>? Are you doing things for fun? Do you have any hobbies or regular recreation? Do you have any creative pursuits?</p>\n<h4>Review and Planning</h4>\n<p>I do four things with the above breakdown, generally over the course of <em>at least</em> a few days, and in all cases using mind maps.</p>\n<p><em>1. Review the current state of my life.</em></p>\n<p>Everything. Every little detail about each area of my life. Spend time in reflection. Gather up what life data I can find. Be honest!</p>\n<p><em>2. Review my ideal future.</em></p>\n<p>How do I want things to look? What do I want to be doing? Long-term and medium-term goals, plans and projects. What would the ideal me look like?</p>\n<p><em>3. Extract the important things I want to work on over the next year.</em></p>\n<p>This is usually in the form of 3-5 &lsquo;major&rsquo; goals. I try to spread them out across various <a onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','lesswrong.com']);\" href=\"/lw/58g/levels_of_action/\">levels</a>: some are life optimizing, others are specific projects.</p>\n<p style=\"padding-left: 30px;\">Example 1: Read <em>all</em> LW posts &ndash; I&rsquo;ve been cherry-picking posts/sequences for a long time. A comprehensive read-through is in order.</p>\n<p style=\"padding-left: 30px;\">Example 2: Lead climb a 5.12c route &ndash; Climbing is amazing: it&rsquo;s social or solo depending on your mood; you make constant, noticeable, incremental progress; it&rsquo;s <em>extremely</em> fun; it uses all of your body and core; it&rsquo;s mentally challenging; and it&rsquo;s exercise to boot!</p>\n<p style=\"padding-left: 30px;\">Example 3: Review and optimize my philanthropy &ndash; Review what I'm giving. Cancel those useless donations. Sort out my money situation. Determine how much I can give away and the most optimal way to do it.</p>\n<p><em>4. Optimize for success!</em></p>\n<p>I used to use an unorganized mish-mash of techniques that I&rsquo;ve picked up over the years, but now that we have a clear outline of <a onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','lesswrong.com']);\" href=\"/lw/4su/how_to_be_happy/\">what makes us happy</a> and <a onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','lesswrong.com']);\" href=\"/lw/3w3/how_to_beat_procrastination/\">why we procrastinate</a>, among other things, this step got a whole lot easier.</p>\n<p>Using what we know, I try to optimize all of my goals to increase their chances of success. For example: break things down into smaller parts when possible; make it clear how my projects tie in with my bigger life goals; make sure my goals are specific and measurable, and so on.</p>\n<p>I also use a <a onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','davidseah.com']);\" href=\"http://davidseah.com/compact-calendar/\">compact year calendar</a> to hold all my big deadlines, project timelines, and special events. This helps me keep a big-picture view of the year. Aside from that, everything is electronic, in calendars, mind maps, documents, etc.</p>\n<p>That pretty much sums it up. Anyone else going to do some reviewing and/or planning for next year? How do you plan on doing it?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pAEYZ7MQEyDW3w4bv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 20, "baseScore": 19, "extendedScore": null, "score": 4.2e-05, "legacy": true, "legacyId": "11618", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PBRWb2Em5SNeWYwwB", "DoLQN5ryZ9XkZjq5h", "fhEPnveFhb9tmd7Pe", "tLR9YZHiNoDE2Czjh", "33KewgYhNSxFpbpXg", "ZbgCx2ntD5eu8Cno9", "RWo4LwFzpHNQCTcYt", "rRmisKb45dN7DK4BW", "n5Yfhygz42QNK2vFe", "6NvbSwuSAooQxxf7f", "SGR4GxFK7KmW7ckCB", "hEqsWLm5zQtsPevd3", "xrNDqL8SpHpeXZdw7", "jtedBLdducritm8y6", "guDcrPqLsnhEjrPZj"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-22T22:47:12.203Z", "modifiedAt": null, "url": null, "title": "Meetup : Melbourne practical rationality meetup", "slug": "meetup-melbourne-practical-rationality-meetup-1", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:02.293Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "matt", "createdAt": "2009-02-24T03:21:23.753Z", "isAdmin": false, "displayName": "matt"}, "userId": "PXCeXYzvwEeqqitqH", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GFikyS77PPyBSoTg3/meetup-melbourne-practical-rationality-meetup-1", "pageUrlRelative": "/posts/GFikyS77PPyBSoTg3/meetup-melbourne-practical-rationality-meetup-1", "linkUrl": "https://www.lesswrong.com/posts/GFikyS77PPyBSoTg3/meetup-melbourne-practical-rationality-meetup-1", "postedAtFormatted": "Thursday, December 22nd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Melbourne%20practical%20rationality%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Melbourne%20practical%20rationality%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGFikyS77PPyBSoTg3%2Fmeetup-melbourne-practical-rationality-meetup-1%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Melbourne%20practical%20rationality%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGFikyS77PPyBSoTg3%2Fmeetup-melbourne-practical-rationality-meetup-1", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGFikyS77PPyBSoTg3%2Fmeetup-melbourne-practical-rationality-meetup-1", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 81, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/5o'>Melbourne practical rationality meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">06 January 2012 07:00:00AM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">55 Walsh St, West Melbourne VIC 3003, Australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical rationality, as distinct from the social and rationality outreach meetups. Look for a social meetup on the 3rd Friday of each month.</p>\n\n<p>Discussion:</p>\n\n<p><a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a> <a href=\"http://www.google.com/moderator/#16/e=6a317\" rel=\"nofollow\">http://www.google.com/moderator/#16/e=6a317</a></p>\n\n<p>This meetup repeats on the 1st Friday of each month.</p>\n\n<p>All welcome from 6pm. Call the phone number on the door and I'll let you in.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/5o'>Melbourne practical rationality meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GFikyS77PPyBSoTg3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 8.19369828078568e-07, "legacy": true, "legacyId": "11619", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Melbourne_practical_rationality_meetup\">Discussion article for the meetup : <a href=\"/meetups/5o\">Melbourne practical rationality meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">06 January 2012 07:00:00AM (+1100)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">55 Walsh St, West Melbourne VIC 3003, Australia</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>Practical rationality, as distinct from the social and rationality outreach meetups. Look for a social meetup on the 3rd Friday of each month.</p>\n\n<p>Discussion:</p>\n\n<p><a href=\"http://groups.google.com/group/melbourne-less-wrong\" rel=\"nofollow\">http://groups.google.com/group/melbourne-less-wrong</a> <a href=\"http://www.google.com/moderator/#16/e=6a317\" rel=\"nofollow\">http://www.google.com/moderator/#16/e=6a317</a></p>\n\n<p>This meetup repeats on the 1st Friday of each month.</p>\n\n<p>All welcome from 6pm. Call the phone number on the door and I'll let you in.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Melbourne_practical_rationality_meetup1\">Discussion article for the meetup : <a href=\"/meetups/5o\">Melbourne practical rationality meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : Melbourne practical rationality meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_practical_rationality_meetup", "level": 1}, {"title": "Discussion article for the meetup : Melbourne practical rationality meetup", "anchor": "Discussion_article_for_the_meetup___Melbourne_practical_rationality_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-23T00:37:33.423Z", "modifiedAt": null, "url": null, "title": "A way of specifying utility functions for UDT", "slug": "a-way-of-specifying-utility-functions-for-udt", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:27.391Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/dsbjbopyxbQMK93om/a-way-of-specifying-utility-functions-for-udt", "pageUrlRelative": "/posts/dsbjbopyxbQMK93om/a-way-of-specifying-utility-functions-for-udt", "linkUrl": "https://www.lesswrong.com/posts/dsbjbopyxbQMK93om/a-way-of-specifying-utility-functions-for-udt", "postedAtFormatted": "Friday, December 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20A%20way%20of%20specifying%20utility%20functions%20for%20UDT&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AA%20way%20of%20specifying%20utility%20functions%20for%20UDT%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdsbjbopyxbQMK93om%2Fa-way-of-specifying-utility-functions-for-udt%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=A%20way%20of%20specifying%20utility%20functions%20for%20UDT%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdsbjbopyxbQMK93om%2Fa-way-of-specifying-utility-functions-for-udt", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FdsbjbopyxbQMK93om%2Fa-way-of-specifying-utility-functions-for-udt", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 497, "htmlBody": "<p>The <a href=\"/lw/15m/towards_a_new_decision_theory/\">original UDT post</a> defined an agent's preferences as a list of programs it cares about, and a utility function over their possible execution histories. That's a good and general formulation, but how do we make a UDT agent care about our real world, seeing as we don't know its true physics yet? It seems difficult to detect humans (or even paperclips) in the execution history of an arbitrary physics program.</p>\n<p>But we can reframe the problem like this: the <a href=\"http://www.vetta.org/documents/disSol.pdf\">universal prior</a> is a probability distribution over all finite bitstrings. If we like some bitstrings more than others, we can ask a UDT agent to&nbsp;<em>influence the universal prior</em> to make those bitstrings more probable. After all, the universal prior is an uncomputable mathematical object that is not completely known to us (and even defined in terms of all possible computer programs, including ours). We <a href=\"/lw/8wc/a_model_of_udt_with_a_halting_oracle/\">already know</a> that UDT agents&nbsp;can control such objects, and it turns out that such control can transfer to the real world.</p>\n<p>For example, you could pick a bitstring that describes the shape of a paperclip, type it into a computer and run a UDT AI with a utility function which says \"maximize the probability of this bitstring under the universal prior, mathematically defined in such-and-such way\". For good measure you could give the AI some actuators, e.g. connect its output to the Internet. The AI would notice our branch of the multiverse somewhere within the universal prior, notice itself running within that branch, seize the opportunity for control, and pave over our world so it looks like a paperclip from as many points of view as possible. The thing is like AIXI in its willingness to kill everyone, but different from AIXI in that it probably won't sabotage itself by mining its own CPU for silicon or becoming <a href=\"/lw/8qy/aixi_and_existential_despair/\">solipsistic</a>.</p>\n<p>This approach does not see the universal prior as a fixed probability distribution over possible universes, instead you could view it as picking from several different possible universal priors. A utility function thus specified doesn't always look like utility maximization from&nbsp;within the universal prior's own multiverse of programs. For example, you could ask the agent to keep a healthy balance between paperclips and staples in the multiverse, i.e. minimize |P(A)-P(B)| where A is a paperclip bitstring, B is a staple bitstring, and P is the universal prior.</p>\n<p>The idea is scary because well-meaning people can be tempted to use it. For example, they could figure out which computational descriptions of the human brain correspond to happiness and ask their AI to maximize the universal prior probability of those. If implemented correctly, that could work and even be preferable to unfriendly AI, but we would still lose many human values. <a href=\"http://wiki.lesswrong.com/wiki/Complexity_of_value\">Eliezer's usual arguments</a> apply in full force here. So if you ever invent a powerful math intuition device, please don't go off building a paperclipper for human happiness. We need to solve the FAI problem properly first.</p>\n<p>Thanks to Paul Christiano and Gary Drescher for ideas that inspired this post.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "dsbjbopyxbQMK93om", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 14, "extendedScore": null, "score": 8.194105789807245e-07, "legacy": true, "legacyId": "11620", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["de3xjFaACCAk6imzv", "Bj244uWzDBXvE2N2S", "AfbY36m8TDYZBjHcu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-23T01:42:39.608Z", "modifiedAt": null, "url": null, "title": "Less Wrong Archives", "slug": "less-wrong-archives", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:02.726Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Grognor", "createdAt": "2011-01-31T02:54:34.463Z", "isAdmin": false, "displayName": "Grognor"}, "userId": "LoykQRMTxJFxwwdPy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/HMm7thMEXwuPxtXN2/less-wrong-archives", "pageUrlRelative": "/posts/HMm7thMEXwuPxtXN2/less-wrong-archives", "linkUrl": "https://www.lesswrong.com/posts/HMm7thMEXwuPxtXN2/less-wrong-archives", "postedAtFormatted": "Friday, December 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Less%20Wrong%20Archives&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALess%20Wrong%20Archives%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHMm7thMEXwuPxtXN2%2Fless-wrong-archives%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Less%20Wrong%20Archives%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHMm7thMEXwuPxtXN2%2Fless-wrong-archives", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FHMm7thMEXwuPxtXN2%2Fless-wrong-archives", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 166, "htmlBody": "<p>The last straw was finding out that the manually-generated (why manual? I don't understand!) \"<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/All_Articles\">All Articles</a>\" article is more than a year out of date. The auto-generated <a href=\"http://www.cs.auckland.ac.nz/~andwhay/postlist.html\">list</a> of Eliezer's Overcoming Bias posts isn't out of date if you only want the posts that were once on Overcoming Bias, but why would you want only those?</p>\n<p>Can we please have a real, automatically generated archives link for Less Wrong? Perhaps one for the Discussion section, too. (And one that skips over meetups. Those don't need to be recorded for antiquity.)</p>\n<p>I'm sincerely confused as to why we don't already have one of these. It seems like such an obviously essential tool. It seems like an <a href=\"http://wiki.lesswrong.com/wiki/Open_problems_on_Less_Wrong\">open problem</a> that nobody has actually thought of. Or maybe I'm just weird and care about archives far more than everyone else does.</p>\n<p>Unfortunately, this is not something I can actually help with. <em>Good luck to any brave soul who also thinks this is a problem but can actually do something to fix it.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"MfpEPj6kJneT9gWT6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "HMm7thMEXwuPxtXN2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 27, "extendedScore": null, "score": 1.4e-05, "legacy": true, "legacyId": "11631", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-23T04:43:03.397Z", "modifiedAt": null, "url": null, "title": "Applied Rationality Practice", "slug": "applied-rationality-practice", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:04.795Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ksvanhorn", "createdAt": "2011-01-10T19:23:33.231Z", "isAdmin": false, "displayName": "ksvanhorn"}, "userId": "fiau5fcXpbad9d6D4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MiSco68qfu558yMjw/applied-rationality-practice", "pageUrlRelative": "/posts/MiSco68qfu558yMjw/applied-rationality-practice", "linkUrl": "https://www.lesswrong.com/posts/MiSco68qfu558yMjw/applied-rationality-practice", "postedAtFormatted": "Friday, December 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Applied%20Rationality%20Practice&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AApplied%20Rationality%20Practice%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMiSco68qfu558yMjw%2Fapplied-rationality-practice%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Applied%20Rationality%20Practice%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMiSco68qfu558yMjw%2Fapplied-rationality-practice", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMiSco68qfu558yMjw%2Fapplied-rationality-practice", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 117, "htmlBody": "<p>It's one thing to read about a subject, but one gains a deeper understanding by seeing it applied to real problems, and an even deeper understanding by applying it yourself. This applies in particular to the closely related subjects of rationality, cognitive biases, and decision theory. With this in mind, I'd like to propose that we create one or more discussion topics each devoted to discussing and analyzing one decision problem of one person, and see how all this theory we've been discussing can help. The person could be either a Less Wrong member or just an acquaintance of one of us.</p>\n<p>I'll commit to actively participating myself. Does anyone want to put forth a problem to discuss?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MiSco68qfu558yMjw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 7, "extendedScore": null, "score": 8.195012489877794e-07, "legacy": true, "legacyId": "11633", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-23T06:40:08.943Z", "modifiedAt": null, "url": null, "title": "Spend Money on Ergonomics", "slug": "spend-money-on-ergonomics", "viewCount": null, "lastCommentedAt": "2020-08-31T20:21:34.978Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Gy8fy7rTgTocNLKfT/spend-money-on-ergonomics", "pageUrlRelative": "/posts/Gy8fy7rTgTocNLKfT/spend-money-on-ergonomics", "linkUrl": "https://www.lesswrong.com/posts/Gy8fy7rTgTocNLKfT/spend-money-on-ergonomics", "postedAtFormatted": "Friday, December 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Spend%20Money%20on%20Ergonomics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASpend%20Money%20on%20Ergonomics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGy8fy7rTgTocNLKfT%2Fspend-money-on-ergonomics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Spend%20Money%20on%20Ergonomics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGy8fy7rTgTocNLKfT%2Fspend-money-on-ergonomics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGy8fy7rTgTocNLKfT%2Fspend-money-on-ergonomics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1466, "htmlBody": "<p><em>Warning: This is an applied rationality post, about rationality applied to a specific area of life, not a generalized rationality post.</em></p>\n<p><a href=\"http://en.wikipedia.org/wiki/Ergonomics\">Ergonomics</a> is incredibly important. Sadly, so many of us in the techno-geek cluster ignore well-defined best practices of ergonomics and develop the infamous hunched back of late night computer toiling.</p>\n<p>Seriously, ergonomics is basically a solved problem. The mathematics of anthropometry in relation to body mechanics and repetive stressors on the body are quite well understood.</p>\n<p>I am here to offer you a basic, incredibly important, yet widely ignored lesson of rationality.</p>\n<p><strong><em><span style=\"text-decoration: underline;\">Spend money on ergonomics!</span></em></strong></p>\n<p>I really can't emphasize this enough. It's such low hanging fruit, yet I know way too many master aspiring rationalists with egregious ergonomic setups.</p>\n<p>It is accepted wisdom on Less Wrong that optimizing your career is important, because you'll spend <a href=\"http://80000hours.org/\">80,000 hours</a> working on your career. Strikingly,&nbsp;ergonomics&nbsp;presents an even larger time-based optimization opportunity. With straightforward monetary investment, you can dramatically improve the next <em>hundreds of thousands</em>&nbsp;of hours of your life. The effect size here is just enormous. Spend money on ergonomics, and you will be less fatigued, more energetic, more productive, and healthier into the later years of your life.</p>\n<p><strong>Chairs</strong></p>\n<p>If you must do your computing while sitting (and do consider alternative <a href=\"http://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&amp;field-keywords=standing+desk&amp;x=0&amp;y=0\">standing desks</a>,&nbsp;<a href=\"http://www.amazon.com/Trek-TD-01-TrekDesk-Treadmill-Desk/dp/B002IYRBI0/ref=sr_1_1?ie=UTF8&amp;qid=1324621808&amp;sr=8-1\">treadmill desks</a>, or a desk suited to <a href=\"http://www.amazon.com/Furinno-Adjustable-Computer-Multifuctional-Ergonomics/dp/B004QXIFCC/ref=sr_1_11?ie=UTF8&amp;qid=1324613792&amp;sr=8-11\">computing while lying in bed</a>), then a good&nbsp;chair is a stunningly good investment. If you make your living while sitting in a chair and computing, what is a $500 investment in your comfort and good health and productivity while sitting? A used Aeron from Craigslist costs around $500 and is the gold standard of ergonomic chair design.&nbsp;</p>\n<p>At the low end of ergnomic chairs, the <a href=\"http://www.ikea.com/us/en/catalog/products/70133761/\">Ikea&nbsp;TORBJ&Ouml;RN</a> gets a hearty recommendation. It's only $39. Buy some extras for your visitors? That's what I did but then they all ended up in the rooms of my roommates. At the midrange, I have recommended the Ikea Verksam, but it appears to be discontinued. I think the current model <a href=\"http://www.ikea.com/us/en/catalog/products/50176959/#/30176955\">Volmar</a> is similar enough though I have not personally sat in it.</p>\n<p>The important thing when getting your chair is to make sure it actually fits your body enough to let you sit in a <a href=\"http://www.osha.gov/SLTC/etools/computerworkstations/positions.html\">proper ergonomic position</a>. Note that the model in these OSHA images is committing an ergonomics no-no by using arm rests. Yes, I know they feel good to rest your arms on, but they're a crutch. Most all of the positions where you are resting your arms on your armrest are really bad for typing 8 hours a day. Just take the armrests off of your chair and start building up your arm strength. Similarly, avoid chairs with head rests.</p>\n<p>&nbsp;</p>\n<p><strong>Keyboard</strong></p>\n<p>Unsurprisingly at this point, I will declare that ergonomic keyboards are just better. They used to be a premium product, but now&nbsp;<a href=\"http://www.amazon.com/Microsoft-Natural-Ergonomic-Keyboard-4000/dp/B000A6PPOK\">Microsoft's entry level ergonomic keyboard is only $25</a>. Also, DVORAK is strictly better than QWERTY, ignoring the inconvenience of being forced to switch back and forth between keysets.</p>\n<p>&nbsp;</p>\n<p><strong>Sleep</strong></p>\n<p>Ironically, given that it is the default environment for computing, sitting is not very good for the body compared to standing or lying. This makes sense in an evolutionary biology sense -- the human body was definitely designed for working while sitting up, and sleeping while lying down. We can hack this a little by working while lying down, though many people have trouble focusing given the implied lack of focus of a lying down position.</p>\n<p>So, a good mattress can be an investment in both your sleeping comfort and your working comfort. I think a good mattress is even more important than a good chair. You spent 1/4-1/3 of your life asleep! I can accomplish no useful work without a good night's sleep.</p>\n<p>If you sleep with (or ever plan on sleeping with) a partner, get a queen size bed. A US full size bed is equal to 1.5 twin beds, which doesn't fit two full size adults. My parents sleep on a full size bed (along with a small dog!) and are plagued by insomnia, not enough space, and bouts of blanket stealing. Apparently, it was not uncommon among their generation to prefer the forced physical closeness of a smaller bed. This is ok sometimes, of course, but when we're talking every night, you'll sleep better when not forced to be crushed up against your partner.&nbsp;</p>\n<p>A king size bed is even better, of course, if your room can fit it. I got a king size bed because my partner and I both like to compute while lying down in bed, and two people plus computers fit much better on a king size bed than a queen size bed.</p>\n<p>I like <a href=\"http://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&amp;field-keywords=memory+foam+mattress&amp;x=0&amp;y=0\">memory foam mattresses</a>. A minority of people really don't. My heuristic on this is that if you think you'll like a memory foam mattress, you will. One nice thing about memory foam is that it doesn't transmit vibrations from one side to the other. This means that you could probably sleep while someone else is jumping on the other side of the bed. That would not work on a conventional spring mattress.&nbsp;I've heard latex mattresses are even better but I'm too cheap to take my own advice to the full logical conclusion.&nbsp;</p>\n<p>Feel free to skip the box spring, unless your bed requires one.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong>Driving</strong></p>\n<p>This is an area where my own ergonomics falls short. I'm 5' 11'' and I just can't quite fit in my Hyundai Elantra. No matter how I adjust the seat, I can't get in a perfectly ergonomic driving position. I refuse to buy another car until I can get one that drives itself, so for now, it seems like I am stuck with a somewhat unergonomic driving experience.&nbsp;</p>\n<p>On hand positioning, note that the 10-2 advocated by some DMV and then driver's ed is basically wrong. Whatever slight advantage it might offer is offset by the risk that your arms are between the airbag and your body during a crash. 9-3 is a new conservative choice.&nbsp;I drive 8 and 4. The <a href=\"http://dmv.ca.gov/pubs/hdbk/signaling.htm\">California DMV manual now supports this</a>.</p>\n<p>&nbsp;</p>\n<p><strong>Fidget more often</strong></p>\n<p>One of the most important points of ergonomics is that injury comes from sustained stress. The body can handle a little bit of a stress for a short period of time without much in the way of problems.&nbsp;People often walk into a room and see me scrunched up in the most awkward seeming, obviously unergonomic and uncomfortable looking positions. Why do I do it? Well, it turns out that your body can tolerate almost any position at all for short periods of time. The important part is to notice when your body is experiencing too much stress and shift positions.</p>\n<p><em>Take a step back from this article and note how your body feels, as you are situated. Do you notice any discomfort or stress in your neck, shoulders, back, or lower body? Try fidgeting into a more comfortable position</em>. <em>Next time you notice stress, fidget again. Repeat for the rest of your life.</em></p>\n<p>The science of fidgeting is still surprisingly undeveloped, though more evidence is coming out in favor of it. <a href=\"http://www.ajcn.org/content/72/6/1451\">Fidgeters are much less likely to be obese than non-fidgeters</a>. Fidgeting also works as a technique to help with focus -- it's <a href=\"http://www.amazon.com/exec/obidos/ASIN/0595350100/\">well documented for ADHD people</a>, but fidgeting doesn't just help ADHD people focus.</p>\n<p><strong>Try barefoot shoes</strong></p>\n<p>Vibram <a href=\"http://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&amp;field-keywords=vibram+five+fingers&amp;x=0&amp;y=0#/ref=nb_sb_noss?url=search-alias%3Daps&amp;field-keywords=vibram+fivefingers&amp;rh=i%3Aaps%2Ck%3Avibram+fivefingers\">Fivefingers</a> are popular enough among aspiring rationalists that I frequently joke about the cult of the toe shoe. The evidence behind barefoot running as strictly superior to conventional running shoes <a href=\"http://en.wikipedia.org/wiki/Barefoot_running\">at this point seems overwhelming</a>. The evidence for barefoot walking as superior to shoe'd walking is less so, but it seems intuitive to me -- when you actually get tactile feedback from your feet painfully thudding against the ground, you're more likely to walk in such a way as to minimize stress on your body.</p>\n<p>I really like Fivefingers, but got annoyed with random passerbys asking me about them everytime I leave my house. Also, they have a tendency to fall apart after heavy use and repeated washings.&nbsp;<br /><br />The cult of the toe shoes seems to be moving onto <a href=\"http://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&amp;field-keywords=zemgear&amp;x=0&amp;y=0#/ref=nb_sb_noss?url=search-alias%3Daps&amp;field-keywords=ninja+zemgear&amp;rh=i%3Aaps%2Ck%3Aninja+zemgear\">Ninja Zemgears</a>. They're also much, much cheaper than Fivefingers, so it's not as big of a deal when they&nbsp;inevitably&nbsp;fall apart. They are also much less intrusive as footwear than Vibrams. People notice them less, and when they do, they think you are wearing comfortable Japanese slippers (Tabi shoes) rather than monstrous toe forms.&nbsp;</p>\n<p><strong><br /></strong></p>\n<p>--</p>\n<p>I've offered a lot of suggestions here for how to actually improve your life. If you do this sort of life-hacking, you will be able to actually notice that you are happier, less fatigued, more energetic, and more productive. Just try it. No one ever regrets improving their ergonomic well-being. You'll get to spend more of your day at your peak level of performance instead of in a tense, fatigued, or uncomfortable state.</p>\n<p>I'm happy to answer specific questions or give product recommendations in the comments.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fkABsGCJZ6y9qConW": 1, "Tg9aFPFCPBHxGABRr": 7, "HLoxy2feb2PYqooom": 1, "XqykXFKL9t38pbSEm": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Gy8fy7rTgTocNLKfT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 60, "baseScore": 73, "extendedScore": null, "score": 0.000156, "legacy": true, "legacyId": "7458", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 74, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>Warning: This is an applied rationality post, about rationality applied to a specific area of life, not a generalized rationality post.</em></p>\n<p><a href=\"http://en.wikipedia.org/wiki/Ergonomics\">Ergonomics</a> is incredibly important. Sadly, so many of us in the techno-geek cluster ignore well-defined best practices of ergonomics and develop the infamous hunched back of late night computer toiling.</p>\n<p>Seriously, ergonomics is basically a solved problem. The mathematics of anthropometry in relation to body mechanics and repetive stressors on the body are quite well understood.</p>\n<p>I am here to offer you a basic, incredibly important, yet widely ignored lesson of rationality.</p>\n<p><strong id=\"Spend_money_on_ergonomics_\"><em><span style=\"text-decoration: underline;\">Spend money on ergonomics!</span></em></strong></p>\n<p>I really can't emphasize this enough. It's such low hanging fruit, yet I know way too many master aspiring rationalists with egregious ergonomic setups.</p>\n<p>It is accepted wisdom on Less Wrong that optimizing your career is important, because you'll spend <a href=\"http://80000hours.org/\">80,000 hours</a> working on your career. Strikingly,&nbsp;ergonomics&nbsp;presents an even larger time-based optimization opportunity. With straightforward monetary investment, you can dramatically improve the next <em>hundreds of thousands</em>&nbsp;of hours of your life. The effect size here is just enormous. Spend money on ergonomics, and you will be less fatigued, more energetic, more productive, and healthier into the later years of your life.</p>\n<p><strong id=\"Chairs\">Chairs</strong></p>\n<p>If you must do your computing while sitting (and do consider alternative <a href=\"http://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&amp;field-keywords=standing+desk&amp;x=0&amp;y=0\">standing desks</a>,&nbsp;<a href=\"http://www.amazon.com/Trek-TD-01-TrekDesk-Treadmill-Desk/dp/B002IYRBI0/ref=sr_1_1?ie=UTF8&amp;qid=1324621808&amp;sr=8-1\">treadmill desks</a>, or a desk suited to <a href=\"http://www.amazon.com/Furinno-Adjustable-Computer-Multifuctional-Ergonomics/dp/B004QXIFCC/ref=sr_1_11?ie=UTF8&amp;qid=1324613792&amp;sr=8-11\">computing while lying in bed</a>), then a good&nbsp;chair is a stunningly good investment. If you make your living while sitting in a chair and computing, what is a $500 investment in your comfort and good health and productivity while sitting? A used Aeron from Craigslist costs around $500 and is the gold standard of ergonomic chair design.&nbsp;</p>\n<p>At the low end of ergnomic chairs, the <a href=\"http://www.ikea.com/us/en/catalog/products/70133761/\">Ikea&nbsp;TORBJ\u00d6RN</a> gets a hearty recommendation. It's only $39. Buy some extras for your visitors? That's what I did but then they all ended up in the rooms of my roommates. At the midrange, I have recommended the Ikea Verksam, but it appears to be discontinued. I think the current model <a href=\"http://www.ikea.com/us/en/catalog/products/50176959/#/30176955\">Volmar</a> is similar enough though I have not personally sat in it.</p>\n<p>The important thing when getting your chair is to make sure it actually fits your body enough to let you sit in a <a href=\"http://www.osha.gov/SLTC/etools/computerworkstations/positions.html\">proper ergonomic position</a>. Note that the model in these OSHA images is committing an ergonomics no-no by using arm rests. Yes, I know they feel good to rest your arms on, but they're a crutch. Most all of the positions where you are resting your arms on your armrest are really bad for typing 8 hours a day. Just take the armrests off of your chair and start building up your arm strength. Similarly, avoid chairs with head rests.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Keyboard\">Keyboard</strong></p>\n<p>Unsurprisingly at this point, I will declare that ergonomic keyboards are just better. They used to be a premium product, but now&nbsp;<a href=\"http://www.amazon.com/Microsoft-Natural-Ergonomic-Keyboard-4000/dp/B000A6PPOK\">Microsoft's entry level ergonomic keyboard is only $25</a>. Also, DVORAK is strictly better than QWERTY, ignoring the inconvenience of being forced to switch back and forth between keysets.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Sleep\">Sleep</strong></p>\n<p>Ironically, given that it is the default environment for computing, sitting is not very good for the body compared to standing or lying. This makes sense in an evolutionary biology sense -- the human body was definitely designed for working while sitting up, and sleeping while lying down. We can hack this a little by working while lying down, though many people have trouble focusing given the implied lack of focus of a lying down position.</p>\n<p>So, a good mattress can be an investment in both your sleeping comfort and your working comfort. I think a good mattress is even more important than a good chair. You spent 1/4-1/3 of your life asleep! I can accomplish no useful work without a good night's sleep.</p>\n<p>If you sleep with (or ever plan on sleeping with) a partner, get a queen size bed. A US full size bed is equal to 1.5 twin beds, which doesn't fit two full size adults. My parents sleep on a full size bed (along with a small dog!) and are plagued by insomnia, not enough space, and bouts of blanket stealing. Apparently, it was not uncommon among their generation to prefer the forced physical closeness of a smaller bed. This is ok sometimes, of course, but when we're talking every night, you'll sleep better when not forced to be crushed up against your partner.&nbsp;</p>\n<p>A king size bed is even better, of course, if your room can fit it. I got a king size bed because my partner and I both like to compute while lying down in bed, and two people plus computers fit much better on a king size bed than a queen size bed.</p>\n<p>I like <a href=\"http://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&amp;field-keywords=memory+foam+mattress&amp;x=0&amp;y=0\">memory foam mattresses</a>. A minority of people really don't. My heuristic on this is that if you think you'll like a memory foam mattress, you will. One nice thing about memory foam is that it doesn't transmit vibrations from one side to the other. This means that you could probably sleep while someone else is jumping on the other side of the bed. That would not work on a conventional spring mattress.&nbsp;I've heard latex mattresses are even better but I'm too cheap to take my own advice to the full logical conclusion.&nbsp;</p>\n<p>Feel free to skip the box spring, unless your bed requires one.&nbsp;</p>\n<p>&nbsp;</p>\n<p><strong id=\"Driving\">Driving</strong></p>\n<p>This is an area where my own ergonomics falls short. I'm 5' 11'' and I just can't quite fit in my Hyundai Elantra. No matter how I adjust the seat, I can't get in a perfectly ergonomic driving position. I refuse to buy another car until I can get one that drives itself, so for now, it seems like I am stuck with a somewhat unergonomic driving experience.&nbsp;</p>\n<p>On hand positioning, note that the 10-2 advocated by some DMV and then driver's ed is basically wrong. Whatever slight advantage it might offer is offset by the risk that your arms are between the airbag and your body during a crash. 9-3 is a new conservative choice.&nbsp;I drive 8 and 4. The <a href=\"http://dmv.ca.gov/pubs/hdbk/signaling.htm\">California DMV manual now supports this</a>.</p>\n<p>&nbsp;</p>\n<p><strong id=\"Fidget_more_often\">Fidget more often</strong></p>\n<p>One of the most important points of ergonomics is that injury comes from sustained stress. The body can handle a little bit of a stress for a short period of time without much in the way of problems.&nbsp;People often walk into a room and see me scrunched up in the most awkward seeming, obviously unergonomic and uncomfortable looking positions. Why do I do it? Well, it turns out that your body can tolerate almost any position at all for short periods of time. The important part is to notice when your body is experiencing too much stress and shift positions.</p>\n<p><em>Take a step back from this article and note how your body feels, as you are situated. Do you notice any discomfort or stress in your neck, shoulders, back, or lower body? Try fidgeting into a more comfortable position</em>. <em>Next time you notice stress, fidget again. Repeat for the rest of your life.</em></p>\n<p>The science of fidgeting is still surprisingly undeveloped, though more evidence is coming out in favor of it. <a href=\"http://www.ajcn.org/content/72/6/1451\">Fidgeters are much less likely to be obese than non-fidgeters</a>. Fidgeting also works as a technique to help with focus -- it's <a href=\"http://www.amazon.com/exec/obidos/ASIN/0595350100/\">well documented for ADHD people</a>, but fidgeting doesn't just help ADHD people focus.</p>\n<p><strong id=\"Try_barefoot_shoes\">Try barefoot shoes</strong></p>\n<p>Vibram <a href=\"http://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&amp;field-keywords=vibram+five+fingers&amp;x=0&amp;y=0#/ref=nb_sb_noss?url=search-alias%3Daps&amp;field-keywords=vibram+fivefingers&amp;rh=i%3Aaps%2Ck%3Avibram+fivefingers\">Fivefingers</a> are popular enough among aspiring rationalists that I frequently joke about the cult of the toe shoe. The evidence behind barefoot running as strictly superior to conventional running shoes <a href=\"http://en.wikipedia.org/wiki/Barefoot_running\">at this point seems overwhelming</a>. The evidence for barefoot walking as superior to shoe'd walking is less so, but it seems intuitive to me -- when you actually get tactile feedback from your feet painfully thudding against the ground, you're more likely to walk in such a way as to minimize stress on your body.</p>\n<p>I really like Fivefingers, but got annoyed with random passerbys asking me about them everytime I leave my house. Also, they have a tendency to fall apart after heavy use and repeated washings.&nbsp;<br><br>The cult of the toe shoes seems to be moving onto <a href=\"http://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&amp;field-keywords=zemgear&amp;x=0&amp;y=0#/ref=nb_sb_noss?url=search-alias%3Daps&amp;field-keywords=ninja+zemgear&amp;rh=i%3Aaps%2Ck%3Aninja+zemgear\">Ninja Zemgears</a>. They're also much, much cheaper than Fivefingers, so it's not as big of a deal when they&nbsp;inevitably&nbsp;fall apart. They are also much less intrusive as footwear than Vibrams. People notice them less, and when they do, they think you are wearing comfortable Japanese slippers (Tabi shoes) rather than monstrous toe forms.&nbsp;</p>\n<p><strong><br></strong></p>\n<p>--</p>\n<p>I've offered a lot of suggestions here for how to actually improve your life. If you do this sort of life-hacking, you will be able to actually notice that you are happier, less fatigued, more energetic, and more productive. Just try it. No one ever regrets improving their ergonomic well-being. You'll get to spend more of your day at your peak level of performance instead of in a tense, fatigued, or uncomfortable state.</p>\n<p>I'm happy to answer specific questions or give product recommendations in the comments.</p>", "sections": [{"title": "Spend money on ergonomics!", "anchor": "Spend_money_on_ergonomics_", "level": 1}, {"title": "Chairs", "anchor": "Chairs", "level": 1}, {"title": "Keyboard", "anchor": "Keyboard", "level": 1}, {"title": "Sleep", "anchor": "Sleep", "level": 1}, {"title": "Driving", "anchor": "Driving", "level": 1}, {"title": "Fidget more often", "anchor": "Fidget_more_often", "level": 1}, {"title": "Try barefoot shoes", "anchor": "Try_barefoot_shoes", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "212 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 212, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-23T06:49:26.051Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Is Reality Ugly?", "slug": "seq-rerun-is-reality-ugly", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yvYJ3Cy2vuoQ4NJmq/seq-rerun-is-reality-ugly", "pageUrlRelative": "/posts/yvYJ3Cy2vuoQ4NJmq/seq-rerun-is-reality-ugly", "linkUrl": "https://www.lesswrong.com/posts/yvYJ3Cy2vuoQ4NJmq/seq-rerun-is-reality-ugly", "postedAtFormatted": "Friday, December 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Is%20Reality%20Ugly%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Is%20Reality%20Ugly%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyvYJ3Cy2vuoQ4NJmq%2Fseq-rerun-is-reality-ugly%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Is%20Reality%20Ugly%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyvYJ3Cy2vuoQ4NJmq%2Fseq-rerun-is-reality-ugly", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyvYJ3Cy2vuoQ4NJmq%2Fseq-rerun-is-reality-ugly", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 212, "htmlBody": "<p>Today's post, <a href=\"/lw/ms/is_reality_ugly/\">Is Reality Ugly?</a> was originally published on 12 January 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>There are three reasons why a world governed by math can still seem messy. First, we may not actually know the math. Secondly, even if we do know all of the math, we may not have enough computing power to do the full calculation. And finally, even if we did know all the math, and we could compute it, we still don't know where in the mathematical system we are living.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/8yn/seq_rerun_expecting_beauty/\">Expecting Beauty</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yvYJ3Cy2vuoQ4NJmq", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.19547930843722e-07, "legacy": true, "legacyId": "11635", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["oKiy7YwGToaYXdvnj", "EdeGsSRtziW5BCv4E", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-23T11:09:21.433Z", "modifiedAt": null, "url": null, "title": "Holiday giving thread", "slug": "holiday-giving-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:08.497Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iCKaM68LZKJ6spMAo/holiday-giving-thread", "pageUrlRelative": "/posts/iCKaM68LZKJ6spMAo/holiday-giving-thread", "linkUrl": "https://www.lesswrong.com/posts/iCKaM68LZKJ6spMAo/holiday-giving-thread", "postedAtFormatted": "Friday, December 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Holiday%20giving%20thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHoliday%20giving%20thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiCKaM68LZKJ6spMAo%2Fholiday-giving-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Holiday%20giving%20thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiCKaM68LZKJ6spMAo%2Fholiday-giving-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiCKaM68LZKJ6spMAo%2Fholiday-giving-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 282, "htmlBody": "<p>Since this is the <a href=\"/lw/8x5/ritual_report_nyc_less_wrong_solstice_celebration/\">season to respect the universe as a worthy foe</a> and remember the challenges ahead of us, it felt appropriate that I should donate some money for a good cause. Then it occurred to me, why not use the opportunity to encourage somebody else to do so as well?</p>\r\n<p>So, if you promise to donate money to either the <a href=\"http://intelligence.org/donate/\">Singularity Institute</a> or the <a href=\"http://www.giving.ox.ac.uk/academic_departments/humanities/extra_pages/future_of_humanity.html\">Future of Humanity Institute</a> before the end of the year,&nbsp;<em>and pledge that you wouldn't have done so without this opportunity</em>, then I will match your donation dollar-for-dollar. Saying \"I would have donated 50 dollars without this opportunity but with it I'll donate 75\" is also fine - in that case I'll match the extra 25. I'll match up to a total of 100 EUR (about 130 USD at today's rate): first-come first-served, so get your matches while they're hot. <em>EDIT:</em> I hit my maximum total, but curiousepic is running a matching as well, see below!</p>\r\n<p>Please feel free to also declare any of your (more or less) completely unrelated donations in this thread, or to set up your own matching pledges.</p>\r\n<p><strong>Matchings:</strong></p>\r\n<p>curiousepic <a href=\"/r/discussion/lw/8z8/holiday_giving_thread/5ifw\">matches</a> up to a total of $1000 on donations to <a href=\"http://intelligence.org/donate/\">SI</a>, <a href=\"http://www.sens.org/donate\">SENS</a>, or <a href=\"http://givewell.org/charities/top-charities\">GiveWell</a>. (Up to <strong>$800</strong>.)</p>\r\n<p>Kaj Sotala matched up to a total of $130 on donations to <a href=\"http://intelligence.org/donate/\">SI</a> or <a href=\"http://www.fhi.ox.ac.uk/donate\">FHI</a>. (Up to <strong>$130</strong>.)</p>\r\n<p><strong>Matched donations:</strong></p>\r\n<p>Barry_Cotter <a href=\"/r/discussion/lw/8z8/holiday_giving_thread/5igg\">donated</a> $25 to SI. (matched $25 each by curiousepic, Kaj Sotala)</p>\r\n<p>Daniel_Burfoot <a href=\"/r/discussion/lw/8z8/holiday_giving_thread/5ijq\">donated</a> $100 to SI. (matched $100 each by curiousepic, Kaj Sotala)</p>\r\n<p>atucker <a href=\"/r/discussion/lw/8z8/holiday_giving_thread/5ikr\">donated</a> $12/month to SI. (matched $5 by Kaj Sotala, $144? by curiousepic)</p>\r\n<p>wmorgan <a href=\"/r/discussion/lw/8z8/holiday_giving_thread/5j6m\">donated</a> $1531 to SI. (<a href=\"/r/discussion/lw/8z8/holiday_giving_thread/5jo1\">matched $531</a> by curiousepic)</p>\r\n<p><strong>Non-matched donations:</strong></p>\r\n<p>Dr_Manhattan <a href=\"/r/discussion/lw/8z8/holiday_giving_thread/5ie1\">donated</a> $100 to SI.</p>\r\n<p>lincolquirk <a href=\"/r/discussion/lw/8z8/holiday_giving_thread/5imc\">donated</a> $700 to SI, $300 to Against Malaria Foundation.</p>\r\n<p>Normal_Anomaly <a href=\"/lw/8z8/holiday_giving_thread/5ips\">donated</a> $300 to Against Malaria Foundation.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iCKaM68LZKJ6spMAo", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 17, "extendedScore": null, "score": 8.196439571556215e-07, "legacy": true, "legacyId": "11636", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 13, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>Since this is the <a href=\"/lw/8x5/ritual_report_nyc_less_wrong_solstice_celebration/\">season to respect the universe as a worthy foe</a> and remember the challenges ahead of us, it felt appropriate that I should donate some money for a good cause. Then it occurred to me, why not use the opportunity to encourage somebody else to do so as well?</p>\n<p>So, if you promise to donate money to either the <a href=\"http://intelligence.org/donate/\">Singularity Institute</a> or the <a href=\"http://www.giving.ox.ac.uk/academic_departments/humanities/extra_pages/future_of_humanity.html\">Future of Humanity Institute</a> before the end of the year,&nbsp;<em>and pledge that you wouldn't have done so without this opportunity</em>, then I will match your donation dollar-for-dollar. Saying \"I would have donated 50 dollars without this opportunity but with it I'll donate 75\" is also fine - in that case I'll match the extra 25. I'll match up to a total of 100 EUR (about 130 USD at today's rate): first-come first-served, so get your matches while they're hot. <em>EDIT:</em> I hit my maximum total, but curiousepic is running a matching as well, see below!</p>\n<p>Please feel free to also declare any of your (more or less) completely unrelated donations in this thread, or to set up your own matching pledges.</p>\n<p><strong id=\"Matchings_\">Matchings:</strong></p>\n<p>curiousepic <a href=\"/r/discussion/lw/8z8/holiday_giving_thread/5ifw\">matches</a> up to a total of $1000 on donations to <a href=\"http://intelligence.org/donate/\">SI</a>, <a href=\"http://www.sens.org/donate\">SENS</a>, or <a href=\"http://givewell.org/charities/top-charities\">GiveWell</a>. (Up to <strong>$800</strong>.)</p>\n<p>Kaj Sotala matched up to a total of $130 on donations to <a href=\"http://intelligence.org/donate/\">SI</a> or <a href=\"http://www.fhi.ox.ac.uk/donate\">FHI</a>. (Up to <strong>$130</strong>.)</p>\n<p><strong id=\"Matched_donations_\">Matched donations:</strong></p>\n<p>Barry_Cotter <a href=\"/r/discussion/lw/8z8/holiday_giving_thread/5igg\">donated</a> $25 to SI. (matched $25 each by curiousepic, Kaj Sotala)</p>\n<p>Daniel_Burfoot <a href=\"/r/discussion/lw/8z8/holiday_giving_thread/5ijq\">donated</a> $100 to SI. (matched $100 each by curiousepic, Kaj Sotala)</p>\n<p>atucker <a href=\"/r/discussion/lw/8z8/holiday_giving_thread/5ikr\">donated</a> $12/month to SI. (matched $5 by Kaj Sotala, $144? by curiousepic)</p>\n<p>wmorgan <a href=\"/r/discussion/lw/8z8/holiday_giving_thread/5j6m\">donated</a> $1531 to SI. (<a href=\"/r/discussion/lw/8z8/holiday_giving_thread/5jo1\">matched $531</a> by curiousepic)</p>\n<p><strong id=\"Non_matched_donations_\">Non-matched donations:</strong></p>\n<p>Dr_Manhattan <a href=\"/r/discussion/lw/8z8/holiday_giving_thread/5ie1\">donated</a> $100 to SI.</p>\n<p>lincolquirk <a href=\"/r/discussion/lw/8z8/holiday_giving_thread/5imc\">donated</a> $700 to SI, $300 to Against Malaria Foundation.</p>\n<p>Normal_Anomaly <a href=\"/lw/8z8/holiday_giving_thread/5ips\">donated</a> $300 to Against Malaria Foundation.</p>", "sections": [{"title": "Matchings:", "anchor": "Matchings_", "level": 1}, {"title": "Matched donations:", "anchor": "Matched_donations_", "level": 1}, {"title": "Non-matched donations:", "anchor": "Non_matched_donations_", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "32 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jES7mcPvKpfmzMTgC"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-23T16:38:04.207Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups: Pittsburgh, Melbourne", "slug": "weekly-lw-meetups-pittsburgh-melbourne", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:02.820Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WW5EYTrTbj3vRsGMA/weekly-lw-meetups-pittsburgh-melbourne", "pageUrlRelative": "/posts/WW5EYTrTbj3vRsGMA/weekly-lw-meetups-pittsburgh-melbourne", "linkUrl": "https://www.lesswrong.com/posts/WW5EYTrTbj3vRsGMA/weekly-lw-meetups-pittsburgh-melbourne", "postedAtFormatted": "Friday, December 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups%3A%20Pittsburgh%2C%20Melbourne&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%3A%20Pittsburgh%2C%20Melbourne%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWW5EYTrTbj3vRsGMA%2Fweekly-lw-meetups-pittsburgh-melbourne%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%3A%20Pittsburgh%2C%20Melbourne%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWW5EYTrTbj3vRsGMA%2Fweekly-lw-meetups-pittsburgh-melbourne", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWW5EYTrTbj3vRsGMA%2Fweekly-lw-meetups-pittsburgh-melbourne", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 348, "htmlBody": "<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/5g\">Pittsburgh Holiday Meetup:&nbsp;<span class=\"date\">16 December 2011 08:00PM</span></a></li>\n<li><a href=\"/meetups/5h\">Indianapolis, Potential:&nbsp;<span class=\"date\">08 January 2012 02:00PM</span></a></li>\n<li><a href=\"/meetups/5j\">Salt Lake City, Late January 2012</a></li>\n<li><a href=\"/meetups/5f\">First Brussels meetup:&nbsp;<span class=\"date\">18 February 2012 11:00AM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/5i\"></a><a href=\"/meetups/5e\">Melbourne social meetup:&nbsp;<span class=\"date\">16 December 2011 07:00PM</span></a></li>\n</ul>\n<p>The Mountain View meetup is taking a hiatus for the rest of December, see the mailing list for details.</p>\n<p>Cities with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>,</strong><strong> <a href=\"/r/discussion/lw/5pd/southern_california_meetup_may_21_weekly_irvine\">Irvine</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison, WI</a></strong>,<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin, CA</a> </strong>(uses the Bay Area List)<strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#San_Francisco\">San Francisco</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>, and <strong><a href=\"/r/discussion/lw/6at/west_la_biweekly_meetups\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong><strong>.</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WW5EYTrTbj3vRsGMA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 8.197654257786463e-07, "legacy": true, "legacyId": "11505", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pAHo9zSFXygp5A5dL", "tHFu6kvy2HMvQBEhW", "d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-23T17:04:22.640Z", "modifiedAt": null, "url": null, "title": "Is every life really worth preserving?", "slug": "is-every-life-really-worth-preserving", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:24.984Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RationallyOptimistic", "createdAt": "2011-12-23T14:04:39.053Z", "isAdmin": false, "displayName": "RationallyOptimistic"}, "userId": "DHnW2jFpRx4Zq3T42", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2D4HCsZLC96k5M5SM/is-every-life-really-worth-preserving", "pageUrlRelative": "/posts/2D4HCsZLC96k5M5SM/is-every-life-really-worth-preserving", "linkUrl": "https://www.lesswrong.com/posts/2D4HCsZLC96k5M5SM/is-every-life-really-worth-preserving", "postedAtFormatted": "Friday, December 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Is%20every%20life%20really%20worth%20preserving%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIs%20every%20life%20really%20worth%20preserving%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2D4HCsZLC96k5M5SM%2Fis-every-life-really-worth-preserving%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Is%20every%20life%20really%20worth%20preserving%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2D4HCsZLC96k5M5SM%2Fis-every-life-really-worth-preserving", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2D4HCsZLC96k5M5SM%2Fis-every-life-really-worth-preserving", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 523, "htmlBody": "<p>Singularitarians frequently lament the irrevocably dead and the lack of widespread application of cryonics. Many cryonocists feel that as many lives as possible should be (and in a more rational world, would be) cryopreserved. Eliezer Yudkowsky, in <a href=\"http://yudkowsky.net/other/yehuda\">an update to the touching note on the death of his younger brother Yehuda</a>, forcefully expressed this sentiment:</p>\n<blockquote>\"I stand there, and instead of reciting Tehillim I look at the outline on the grass of my little brother's grave. Beneath this thin rectangle in the dirt lies my brother's coffin, and within that coffin lie his bones, and perhaps decaying flesh if any remains. There is nothing here or anywhere of my little brother's self. His brain's information is destroyed. Yehuda wasn't signed up for cryonics and his body wasn't identified until three days later; but freezing could have been, should have been standard procedure for anonymous patients. The hospital that should have removed Yehuda's head when his heart stopped beating, and preserved him in liquid nitrogen to await rescue, instead laid him out on a slab. Why is the human species still doing this? Why do we still bury our dead? We have all the information we need in order to know better...\"</blockquote>\n<p>Ignoring the debate concerning the merits of cryopreservation itself and the feasibility of mass cryonics, I would like to question the assumption that every life is worth preserving for posterity.<br /><br />Consider those who have demonstrated through their actions that they are best kept excluded from society at large. John Wayne Gacy and Jeffrey Dahmer would be prime examples. Many people write these villains off as evil and give their condition not a second thought. But it is quite possible that they actually suffer from some sort of mental illness and are thus not fully responsible for their crimes. In fact, there is evidence that the brains of serial killers are measurably different from those of normal people. Far enough in the future, it might be possible to \"cure\" them. However, they will still possess toxic memories and thoughts that would greatly distress them now that they are normal. To truly repair them, they would likely need to have many or all of their memories erased. At that point, with an amnesic brain and a cloned body, are they even really the same person, and if not, what was the point of cryopreserving them?<br /><br />Forming a robust theory of mind and realizing that not everyone thinks or sees the world the same way you do is actually quite difficult. Consider the immense complexity of the world we live in and the staggering scope of thoughts that can possibly be thought as a result. If cryopreservation means first and foremost mind preservation, maybe there are some minds that just shouldn't be preserved. Maybe the future would be a better, happier place without certain thoughts, feelings and memories--and without the minds that harbor them.<br /><br />Personally, I think the assumption of \"better safe than sorry\" is a good-enough justification for mass cryonics (or for cryonics generally), but I think that assumption, like any, should at least be questioned.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2D4HCsZLC96k5M5SM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 2, "extendedScore": null, "score": 8.197751483754141e-07, "legacy": true, "legacyId": "11639", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 78, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-23T19:54:22.907Z", "modifiedAt": null, "url": null, "title": "[LINK] Question Templates", "slug": "link-question-templates", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:03.430Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jimrandomh", "createdAt": "2009-02-27T22:56:02.437Z", "isAdmin": true, "displayName": "jimrandomh"}, "userId": "nLbwLhBaQeG6tCNDN", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/CuDDRpcGPX8c3ibnp/link-question-templates", "pageUrlRelative": "/posts/CuDDRpcGPX8c3ibnp/link-question-templates", "linkUrl": "https://www.lesswrong.com/posts/CuDDRpcGPX8c3ibnp/link-question-templates", "postedAtFormatted": "Friday, December 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Question%20Templates&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Question%20Templates%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCuDDRpcGPX8c3ibnp%2Flink-question-templates%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Question%20Templates%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCuDDRpcGPX8c3ibnp%2Flink-question-templates", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCuDDRpcGPX8c3ibnp%2Flink-question-templates", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 28, "htmlBody": "<p>I have posted an article&nbsp;<a href=\"http://conceptspacecartography.com/question-templates/\">Question Templates</a>&nbsp;on my blog. This is the start of a sequence (of sorts), and it's close to the core of Less Wrong's subject matter.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "CuDDRpcGPX8c3ibnp", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 2, "extendedScore": null, "score": 3e-06, "legacy": true, "legacyId": "11640", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-23T20:34:13.065Z", "modifiedAt": null, "url": null, "title": "Prediction is hard, especially of medicine", "slug": "prediction-is-hard-especially-of-medicine", "viewCount": null, "lastCommentedAt": "2017-06-17T04:30:38.996Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/qNxPRh5jzrLorak6B/prediction-is-hard-especially-of-medicine", "pageUrlRelative": "/posts/qNxPRh5jzrLorak6B/prediction-is-hard-especially-of-medicine", "linkUrl": "https://www.lesswrong.com/posts/qNxPRh5jzrLorak6B/prediction-is-hard-especially-of-medicine", "postedAtFormatted": "Friday, December 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Prediction%20is%20hard%2C%20especially%20of%20medicine&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APrediction%20is%20hard%2C%20especially%20of%20medicine%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqNxPRh5jzrLorak6B%2Fprediction-is-hard-especially-of-medicine%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Prediction%20is%20hard%2C%20especially%20of%20medicine%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqNxPRh5jzrLorak6B%2Fprediction-is-hard-especially-of-medicine", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FqNxPRh5jzrLorak6B%2Fprediction-is-hard-especially-of-medicine", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 6977, "htmlBody": "<blockquote>\n<p>Summary: medical progress has been much slower than even recently predicted.</p>\n</blockquote>\n<p>In the <a href=\"http://www.alcor.org/cryonics/cryonics8802.txt\">February</a> and <a href=\"http://www.alcor.org/cryonics/cryonics8803.txt\">March</a> 1988 issues of <a href=\"http://www.alcor.org/magazine/\"><em>Cryonics</em></a>, <a href=\"http://chronopause.com/\">Mike Darwin</a> (<a href=\"http://en.wikipedia.org/wiki/Mike_Darwin\">Wikipedia</a>/<a href=\"/user/mikedarwin/\">LessWrong</a>) and Steve Harris published a two-part article &ldquo;The Future of Medicine&rdquo; attempting to forecast the medical state of the art for 2008. Darwin has republished it on the <a href=\"http://tech.groups.yahoo.com/group/New_Cryonet/message/1691\">New_Cryonet</a> email list.</p>\n<p>Darwin is a pretty savvy forecaster (who you will remember correctly predicting in 1981 in <a href=\"http://www.alcor.org/cryonics/cryonics8201.txt\">&ldquo;The High Cost of Cryonics&rdquo;</a>/<a href=\"http://www.alcor.org/cryonics/cryonics8202.txt\">part 2</a> ALCOR&rsquo;s recent <a href=\"/lw/8fe/cryonics_costs_given_estimates_are_low/\">troubles with grandfathering</a>), so given my <a href=\"http://www.gwern.net/Prediction%20markets\">standing interests</a> in tracking predictions, I read it with great interest; but they still blew most of them, and not the ones we would prefer them to&rsquo;ve.</p>\n<p>The full essay is ~10k words, so I will excerpt roughly half of it below; feel free to skip to the <a href=\"#reactions\">reactions</a> section and other links.</p>\n<p><a id=\"more\"></a></p>\n<h1 id=\"the-future-of-medicine\"><span class=\"header-section-number\">1</span> The Future of Medicine</h1>\n<h2 id=\"part-1\"><span class=\"header-section-number\">1.1</span> Part 1</h2>\n<blockquote>\n<p>What we hope we are especially good at as cryonicists is predicting the future &mdash; particularly the future of medicine. After all, our lives depend upon it. Because that&rsquo;s what cryonics is about &mdash; tomorrow&rsquo;s medicine today. In order for cryonics to seem reasonable, in order for it to be reasonable, it is necessary to have some idea, at least in broad outline, of where medicine is going and of where it ultimately can go. I think that the cryonicists&rsquo; record on this point in a broad sense has been very good.</p>\n<p>&hellip;One thing which is rarely seen in cryonics publications is an attempt to see the shape of things to come in the near or intermediate future. Oddly enough, that&rsquo;s a far more difficult and dangerous undertaking than predicting ultimates. Nor is this a problem confined to cryonics or the future of medicine. Sadi Carnot (the founder of thermodynamics) could tell you all about the &ldquo;perfect heat engine,&rdquo; but would have no doubt had trouble giving you hard numbers on how well heat engines would be made to perform over the 20 years or so following publication of his work&hellip;.When I look over predictions made in the 1950&rsquo;s or the 1960&rsquo;s about the future of medicine and/or technology, I always chuckle about just how far afield these guys were. A good example is a list of predictions made by Herman Kahn which was summarized in CRYONICS REPORTS in August of 1967 (volume 2, #8). They are reproduced as Table 1 below. Read &rsquo;em and weep &mdash; or laugh if you will!</p>\n<blockquote>\n<p>Table 1. Less Likely But Important Possibilities, from: <em>The Next 33 Years: A Framework For Speculation</em>, by Herman Kahn and Anthony J. Weiner (1967) [predictions for 2000 AD]</p>\n<ul>\n<li>&ldquo;True&rdquo; artificial intelligence</li>\n<li>Practical use of sustained fusion to produce neutrons</li>\n<li>Artificial growth of new limbs and organs</li>\n<li>Room temperature superconductors</li>\n<li>Major use of rockets for transportation (either terrestrial or extraterrestrial)</li>\n<li>Effective chemical or biological treatment for most mental illnesses</li>\n<li>Almost complete control of marginal changes of heredity</li>\n<li>Suspended animation (for years or centuries)</li>\n<li>Practical materials with nearly &ldquo;theoretical limit&rdquo; strengths</li>\n<li>Conversion of mammals (humans?) to fluid breathers</li>\n<li>Direct input into human memory banks</li>\n<li>&hellip;</li>\n</ul>\n</blockquote>\n<p>&hellip;My personal perspective is one of being a hard-core cryonicist who was involved in clinical medicine for the better part of a decade. My biases about predicting the future could probably be summarized as follows: I have a lot of sympathy for the incrementalist view of progress - particularly in the highly regulated area of medicine. It&rsquo;s regulated because it directly and powerfully touches people&rsquo;s well-being and because it is not a very fault-tolerant area &mdash; mistakes are costly and since people like being alive (at least in the short run) they get edgy if an error separates them from their actuarial expectations.</p>\n<p>I thus believe that any predictions about the future of medicine have to include what I call the &ldquo;space program factor&rdquo; (SPF). By this I mean simply that progress in the space program would have proceeded far, far faster (and thus approximated more closely what was theoretically possible) if it were not a high-visibility project with lots of political and social overtones which make it fault-intolerant &mdash; if you could burn up as many astronauts as you do test pilots every month, it would cost a lot less to get where you&rsquo;re going. First-shot fail-safe engineering is costly. Medicine suffers from the same kinds of problems &mdash; witness the FDA as both the solution and the problem.</p>\n</blockquote>\n<h3 id=\"diagnostics\"><span class=\"header-section-number\">1.1.1</span> Diagnostics</h3>\n<blockquote>\n<p>I foresee a veritable explosion of diagnostic techniques and procedures. A large number of illnesses which are poorly understood today will be well-characterized the next decade and will be easy to diagnose very early in their development or even before they develop because they will be found to have direct or indirect genetic causes. Fairly predictive tests for Alzheimer&rsquo;s disease, schizophrenia, depression, some malignancies, heart disease, and most of the rest of the major killers and disablers will probably be in place by 2000 to 2010. Many if not most of these ailments will be assessable in terms of a very sophisticated genetic risk profile which it will be possible to generate in infancy or childhood (or in utero). A wide range of genetic probes for illness-generating genes should be available by the end of the century.</p>\n</blockquote>\n<p>A side-note: genetic associations have been a very fertile field for John Ioannidis, and a big study just blew away a bunch of <a href=\"http://www.arts.cornell.edu/econ/dbenjamin/IQ-SNPs-PsychSci-20111205-accepted.pdf\">SNP-IQ correlations</a>.</p>\n<blockquote>\n<p>Real-time diagnosis will also be revolutionized by the turn of the century. The next 10 to 15 years will see increasing miniaturization of sensors and chemistry packages. Tissue probes or biosensors which can measure a wide array of biological and biochemical factors will be packaged in very small, very stable devices which hold calibration over prolonged periods of time (weeks to months to years) and which can easily be inserted into the patient&rsquo;s body or tissues. For example, I foresee multi-sensor units mounted on very small needle or catheter tips which can be inserted intravenously, intracranially, intra-cerebrally, subcutaneously, and so on.</p>\n<p>These sensors will be able to give real-time measurements of blood gases, pH, electrolytes, enzyme levels, and a host of other biochemical parameters that now involve costly, time-consuming, and/or impossible &ldquo;laboratory studies&rdquo; requiring withdrawal of a sample and processing. Real-time biosensors will revolutionize acute care of critically ill patients.</p>\n<p>&hellip;The first generation of these devices should be in the marketplace somewhere between 1990 and 1995. More sophisticated instruments capable of a wider array of measurements will quickly follow. These sensors will also have a profound impact in acute stabilization of patients in a field setting. It will be possible for paramedical personnel to quickly and effectively insert such instruments in an acutely ill patient &mdash; a victim of cardiac arrest or trauma, and immediately and globally assess that patient&rsquo;s condition, relaying that information to an expert (more on who that expert will be later).</p>\n<p>&hellip;Diagnostic imaging should rapidly come down to a battle between ultrasound and MRI (NMR; (nuclear) magnetic resonance imaging). Because ultrasound units owe their size and weight almost entirely to the computer that processes the information, the size and effectiveness of these units will change on the same rapid exponential curve as the size and power of computers. MRI is a technology which has some other physical limitations, but by the year 2000, even MRI units will be far smaller, less costly, and capable of far, far better results. Bedside units or &ldquo;on floor&rdquo; units (i.e., units in the ICU or CCU) may be available for repeated assessment of the patient&rsquo;s condition. MRI and its grandchildren and cousins should in particular be expected to undergo considerable refinement. Metabolic MRI will also be in wider use, allowing for real-time evaluation of the metabolic and working state of patient&rsquo;s hearts, brains and other organs. By 2000 to 2010 the cost and size of these units may be drastically reduced and they may be in field use for acute metabolic and structural evaluation of patients with trauma or in cardiac arrest.</p>\n</blockquote>\n<p>I recently learned that, besides the usual blame for <a href=\"http://articles.baltimoresun.com/2011-02-22/news/bs-ed-health-costs-20110222_1_health-care-high-quality-care-mri\">increasing medical costs</a>, some categories of doctors have been strenuously urged to <a href=\"http://www.wired.com/magazine/2011/12/ff_causation/all/1#post-50632\">reduce MRI use</a> as actively harmful.</p>\n<blockquote>\n<p>By the late 1990&rsquo;s there should be an answer to this problem in the development of the Portable Doctor or Expert Medical Device (EMD). The EMD will be both a diagnostician and therapist integrated into one unit. In an emergency medical setting (either in an ambulance or in an ICU or CCU) this powerful computer will be directly coupled to a wide array of both simple and complex medical assessment devices&hellip;.EMDs will be a very hot item. Initially (i.e., the 1990&rsquo;s) they will be confined to ambulances and the ICU, CCU, and specialty areas of the hospital, such as radiology and cardiology labs. But there will be powerful incentives for wider application of these devices. As computing capacity drops in cost and increases radically in sophistication (i.e., parallel processors, neural networks, truly massive memories, and so on) expert medical (and other) systems will see increasing application. There will be devices on the market such as a &ldquo;Home Doctor&rdquo; diagnostic program, which will basically be an internal medicine physician in a can.</p>\n<p>&hellip;After 2000, many people will probably have a small sensor array permanently implanted and coupled to telemetry equipment which can be activated to call for help or alert the person that trouble is brewing. People with a known risk of sudden health problems will be the first to use these kinds of devices. With the development of smaller and cheaper telemetry equipment (directly linked to large-antenna satellites), separate telemetry arrangements will disappear. Implantable, computer-controlled defibrillators are already a reality; analogous devices to deliver drugs in case of cardiac or brain infarct (stroke) will eventually become reality.</p>\n</blockquote>\n<h3 id=\"resuscitation\"><span class=\"header-section-number\">1.1.2</span> Resuscitation</h3>\n<blockquote>\n<p>Expect a shift back to open-chest heart massage and away from closed-chest massage in medical and perhaps even paramedical settings. Closed-chest CPR will be realized to be ineffective at maintaining cerebral viability and will be replaced by far more effective open chest methods. In paramedical (i.e., field) settings the emphasis will be on very rapid defibrillation &mdash; or actually &ldquo;leaving the patient alone&rdquo; until circulation can be effectively restored and medications given to inhibit reperfusion injury. Closed chest CPR and restarting circulation by laymen &ldquo;in the field&rdquo; will be realized to be doing more harm than good and there may well be a move away from field CPR, with laymen being instructed to leave the patient without circulation until it can be restarted adequately and under controlled conditions.</p>\n<p>By the late 1990&rsquo;s, extended use of CPR will be a thing of the past and major metropolitan areas will have &ldquo;death reversal units&rdquo; (DRUs) in emergency rooms and perhaps even in larger paramedical units. The DRUs will employ rapid femoral cut-downs and blood-pump/oxygenator supported resuscitation to recover people who have suffered extended periods of ischemia (in the 30 minute to 1 hour range). CPR will be realized very often to be ineffective at recovering patients who are profoundly ischemic and the advent of pharmacologic intervention allowing for cerebral resuscitation will provide tremendous pressure for emergency rooms to develop the capability to very rapidly put an ischemic patient on bypass and completely and adequately support his circulatory and respiratory needs until his brain can recover and/or his heart can be repaired and restarted. An intermediate scenario would be the development of small, flexible impeller pumps that can be collapsed and passed through a large bore percutaneous catheter through the femoral artery and into the abdominal aorta. Such a pump (acting much like the propeller on an outboard boat motor) could then be used to supplement CPR, perhaps providing 2&ndash;3 liters per minute of cardiac output.</p>\n<p>&hellip;Another effect of drugs like the lazaroids and calcium channel blockers will be the more effective treatment of acute injuries to a wide range of tissues such as the spinal cord and brain. Much of the damage that occurs to these tissues is free radical related and can be inhibited by use of these drugs&hellip;Intervention into secondary inflammation will be most important in the brain and spinal cord. Deployment of these techniques will result in the salvage of many spinal cords that would be considered irreversibly injured by today&rsquo;s medicine. There will be far, far fewer paraplegics. However, expect an increase in the number of permanently brain-injured patients and in the number of patients with &ldquo;subtle&rdquo; forms of cerebral injury resembling mild stroke or the cognitive or mood disorders seen in diseases like multiple sclerosis or acute head injury. These disease states will result because people with brain trauma who would have died acutely from secondary free radical mediated injury (cerebral edema and so on) will be saved with lazaroids and other cerebral rescue techniques.</p>\n</blockquote>\n<h3 id=\"antibiotics\"><span class=\"header-section-number\">1.1.3</span> Antibiotics</h3>\n<blockquote>\n<p>The next twenty years should see many powerful new antibiotics engineered directly from knowledge of the structure of the relevant microbial enzyme which it is desired to inhibit. Not only will these antibiotics be more powerful, but because they do not exist in nature, strain resistance will not so easily develop toward them as it has for the antibiotics of today.</p>\n<p>In addition, the next generation of antibiotics will include many which have been designed for effect against viruses, an area where medicine is presently largely powerless.</p>\n</blockquote>\n<p>The pharmaceutical industry and antibiotics have been a case-study in stagnation, failure, and diminishing marginal returns. There is <a href=\"http://en.wikipedia.org/wiki/DRACO_%28antiviral%29\">only one</a>, highly experimental, anti-viral that I have heard of. In a <a href=\"http://tech.groups.yahoo.com/group/New_Cryonet/message/1719\">followup email</a>, Darwin responded to someone else pointing out DRACO:</p>\n<blockquote>\n<p><span style=\"font-family: Arial; color: #000000; font-size: x-small;\"> </span></p>\n<p><span style=\"font-size: small;\">Finally, while Geoff cites this putative advance in antiviral drug therapy, the fact is that my prediction about a plethora of new and highly effective targeted molecular antimicrobials by 2008 was <strong>WRONG</strong>. In fact, antibiotic research is all but dead, and there are virtually no fundamentally new antibiotics in the drug pipeline. This should scare the crap out of all us, because we are rapidly approaching complete antibiotic resistance with a number of common and highly lethal bugs, including staph (MRSA), streptococcus, &nbsp;E. coli, pseudomonas and candida. It is only a matter of months to a few years, at most, &nbsp;before completely antibiotic resistance staph and streptococcus emerge. Pharmaceutical companies have a large <strong>negative</strong> incentive for developing new antimicrobials. At the cost of over a billion dollars a new drug (regulatory) and the high risk of withdrawal of the drug within 5 years (2 out of 3), as well as the near certainty of punishing litigation for adverse effects, antibiotics are not merely uneconomical to develop, they are fiscal suicide. Only drugs that will be chronically used by very large numbers of patients are now worth developing.</span></p>\n<p>&nbsp;</p>\n</blockquote>\n<p><span style=\"font-size: small;\">(This agrees with my own general impressions, which I didn't feel competent to baldly state.)<br /></span></p>\n<h3 id=\"immunology-and-cancer\"><span class=\"header-section-number\">1.1.4</span> Immunology and cancer</h3>\n<blockquote>\n<p>&hellip;Monoclonal and synthetic antibodies carrying toxins or regulatory molecules will be used to turn off or destroy the fraction of immune cells which initially respond and proliferate when a transplant is carried out. More widespread transplantation of tissues will be undertaken, including transplantation of limbs and scalp. Xenografts will be used increasingly in the mid to late 1990&rsquo;s and it will not be uncommon for people to have pancreatic tissue from bovine or porcine sources and perhaps hearts, lungs, and livers from other animals. Expect the first workable transplants to be from great apes (chimps, gorillas, orangutans), with porcine and bovine grafts coming later.</p>\n<p>Immunology and immunotherapy will also be revolutionized by a far more complete understanding of the immune system resulting from the AIDS epidemic and basic research in the immunology of diseases such as multiple sclerosis and aging. The ability to rapidly and cheaply synthesize bioregulatory molecules will open up a wide array of therapeutic possibilities. Expect effective treatments for most autoimmune diseases (lupus, multiple sclerosis, myasthenia gravis, and so on) by the mid to late 1990&rsquo;s. The mid to late 1990&rsquo;s should also see the wider application of immunorestoratives for use with the aged and ill. Cancer therapy will improve considerably as a result of these advances as well as a result of selective targeting techniques. By the early to mid&ndash;1990s the first generations of monoclonal antibodies linked to chemotherapeutic agents or powerful natural toxins will be used against a few cancers.</p>\n</blockquote>\n<h3 id=\"atherosclerosis\"><span class=\"header-section-number\">1.1.5</span> Atherosclerosis</h3>\n<blockquote>\n<p>Atherosclerosis will undergo a very marked but nevertheless gradual reduction in frequency and severity of occurrence as physicians slowly become educated about what is already known and begin to use existing therapeutic modalities more aggressively. By the mid to late 1990&rsquo;s it will be more widely understood that atherosclerosis can be reversed, and there will be wider use of drugs such as lovastatin to reduce serum cholesterol, coupled with sound dietary advice. However, even well into the late 1990&rsquo;s and perhaps beyond, atherosclerotic disease (heart attack, stroke, ischemic limb disease, and so on) will continue to be a serious source of morbidity and mortality. By the late 1990&rsquo;s, 2nd and 3rd generation therapies will be coming on-line which will be able to reverse atherosclerotic disease and more directly inhibit it</p>\n</blockquote>\n<h2 id=\"part-2\"><span class=\"header-section-number\">1.2</span> Part 2</h2>\n<h3 id=\"anesthesia\"><span class=\"header-section-number\">1.2.1</span> Anesthesia</h3>\n<blockquote>\n<p>Expect &ldquo;modular&rdquo; anesthesia by the 1990&rsquo;s to the early 2000&rsquo;s. The development of potent anxieolytics (anxiety removers) which do not depress consciousness and the development of total pain inhibitors will allow for complicated surgical procedures on conscious patients. Expect to see major thoracic and limb surgery on high risk patients (i.e., patients unable to tolerate anesthesia) using such agents.Major abdominal surgery requiring deep muscle relaxation will continue to require skeletal muscle paralysis and general anesthesia. However, expect new drugs in the market place in the late 1990&rsquo;s which induce unconsciousness without respiratory or cardiac depression.</p>\n<p>Surgical and post surgical mortality will decrease sharply due to such anesthetics and the use of real-time physiological and biochemical monitoring during and after surgery using biosensors.</p>\n</blockquote>\n<h3 id=\"surgery\"><span class=\"header-section-number\">1.2.2</span> Surgery</h3>\n<blockquote>\n<p>&hellip;Catheters, laparascopes, and thorascopes with sensors, operating tools, and an impressive array of capabilities will be increasingly used. Abdominal surgery will shift more and more towards the use of the fiberoptic laparascope, endoscope, and laser as miniaturization of tools occurs and disease is diagnosed earlier. Early diagnosis will create the need for less drastic procedures.</p>\n<p>Fine-tuned repair of heart valves and blood vessels, and examination and biopsy of suspected abdominal and retroperitoneal lesions will be early candidates for application of this technology.</p>\n<p>&hellip;In contrast to therapeutic surgery, the frequency of cosmetic surgery will probably increase dramatically as techniques are refined and prosthetics improve in quality and drop in cost. As people live longer, and stay productive longer as well, they will increasingly turn to medicine to maintain not only their health but their appearance. Cosmetic surgery will experience a boom until such time as the fundamental mechanisms underlying the aging process can be brought under control.</p>\n</blockquote>\n<h3 id=\"geriatrics\"><span class=\"header-section-number\">1.2.3</span> Geriatrics</h3>\n<blockquote>\n<p>Advances will be slow here, but significant. Expect increasing understanding and application of trophic factors and bioregulatory compounds. Early candidates for rejuvenation will be the immune system and other stem cell systems or systems with higher cell turnover. By the early decades of 2000, significant rejuvenation and geroprophylaxis of skin, bone, immune, and other &ldquo;high turn- over&rdquo; tissues will be possible as the natural regulatory molecules which control these systems are understood and applied.</p>\n<p>&hellip;By the early years of the 21st century the first generation of compounds effective at &ldquo;rejuvenating&rdquo; (i.e., restoring some degree of normal maintenance and repair to existing brain cells) the central nervous system will be available. These drugs will work by turning on protein synthesis and stimulating natural repair mechanisms.</p>\n<p>However, pathologies of the brain and other non-dividing tissues (renal, cardiac, and musculoskeletal system) will continue to be major sources of morbidity and mortality over the next two decades. As atherosclerosis and immune-related disorders are dealt with more effectively, expect an increasing shift of morbidity and mortality to central nervous system-related causes. Beyond 2000 this may be treated to a limited extent with fetal transplant</p>\n</blockquote>\n<p id=\"psychiatry-behavior\">We all know how well this has worked out. More troubling is that in some respects, we appear <em>further</em> from any solutions or treatments than before; while resveratrol did well in <a href=\"http://pipeline.corante.com/archives/2011/11/10/resveratrol_in_humans_results_of_a_controlled_trial.php\">a recent human trial</a>, the sirtuin research that seemed so promising <a href=\"https://www.sciencemag.org/content/334/6060/1194\">has been battered</a> by null results and failures to replicate. And anti-aging drugs have their own methodological difficulties; from the <a href=\"http://tech.groups.yahoo.com/group/New_Cryonet/message/1719\">followup email</a>:</p>\n<blockquote>\n<p><span style=\"font-size: small;\">Antiaging drugs are unlikely to be free of adverse effects. In fact, it seems very likely that they will be burdened with many adverse effects and that they will even kill a minority of people who use them. The common perception is that antiaging drugs will make people super fit, healthier and more resistant to disease. And yet, in calorie restriction and effective antiaging drug studies there is emerging evidence that slowing aging comes at the cost of interfering with fundamental processes that make organisms fitter for both reproduction and for surviving in a hostile environment. </span></p>\n<p><span style=\"font-size: small;\">Consider the putative antaging drug rapamycin. It seems likely that rapamycin interferes with senesence by affecting the PI3-kinase and TOR: PIKTORing cell growth pathways. This almost certainly means that in some individuals there will serious and even lethal side effects - cancer being one of them. [Persons with a history of promiscuity, and thus a heavy burden of chronic viral infection, and those with certain \"unfavorable\" genotypes will likely be at very high risk.] But, beyond cancer, interfering with these fundamental and deeply evolutionarily conserved pathways is likely to cause a range of adverse effects that negatively (and possibly irreversibly) impact normal body functions, such as energy level, cognition, sexual performance, and so on.. While some people are now using rapamycin as an antiaging drug...</span><span style=\"font-size: small;\">it is virtually inconceivable that any major pharmaceutical company anywhere in the world would (or will) market such a drug for \"normal\" aging. This is important to understand because it gives us basic insight into what will almost certainly be a major barrier to the development and marketing of antiaging drugs: they will necessarily be used by large numbers of people over the course of many decades (and thus millions of drug/person years) and they are incredibly unlikely to be free of adverse, and sometimes even lethal side effects.</span></p>\n</blockquote>\n<h3><span class=\"header-section-number\">1.2.4</span> Psychiatry &amp; Behavior</h3>\n<blockquote>\n<p>Diagnosis by brain scanning (metabolic MRI) and chemical analysis of cerebrospinal fluids will be commonplace in 20 years. As neuroregulatory compounds are better understood and as the biochemistry underlying mental disorders is elucidated there will be more effective treatments. Expect 2nd and 3rd generation drugs and combinations thereof for treatment of depression and psychosis by the late 1990&rsquo;s. There will probably be several very effective therapeutic agents for compulsive disorders in the marketplace by the early to mid 1990&rsquo;s.</p>\n</blockquote>\n<p>From the previously quoted <a href=\"http://tech.groups.yahoo.com/group/New_Cryonet/message/1719\">followup email</a>:</p>\n<blockquote>\n<p><span style=\"font-family: Arial; color: #000000; font-size: x-small;\"> </span></p>\n<p><span style=\"font-size: small;\">Similarly, psychiatric drugs (which <em>are</em> typically chronically used) are no longer economical to develop and market because of the litigation costs associated with them. Widespread chronic use of <strong>any</strong> drug means that the likelihood of adverse conditions that were impossible to detect in the testing phase of the drug development process are almost certain to emerge.&nbsp;Statistics rule in drug development, and a Phase III study that lasts a year and enrolls 5,000 patients is simply not adequately powered to predict what will happen when 5 million patients take&nbsp;a drug for 20 years! The only way to get that data is to do<strong>&nbsp;that</strong> study. And therein lies a&nbsp;powerful caution about antiaging drugs. These drugs will likely need to be taken&nbsp;starting in young adulthood, or in middle age, at latest,&nbsp;and they will need to be taken for a lifetime. Indeed, if they are effective, for a longer lifetime than any but a few super-centenarians &nbsp;has previously lived. </span></p>\n<p>&nbsp;</p>\n</blockquote>\n<h3 id=\"implants-prosthetics\"><span class=\"header-section-number\">1.2.5</span> Implants &amp; Prosthetics</h3>\n<blockquote>\n<p>Early spectacular applications will be small vessel prostheses (wide use by the early to mid 1990&rsquo;s) for use in traumatized and atherosclerotic limbs and organs and venous prostheses (mid to late 1990&rsquo;s) for use in treating traumatic injuries and deep vein incompetence (which results in varicosities, chronic pain, and edema-related skin changes in the leg, often leading to non-healing ulcers or limb loss). Another application of non-thrombogenic surfaces will be a practical artificial heart and more widespread use of extracorporeal support for infants, trauma and cardiac arrest victims, and others where anticoagulation provides a major barrier to the use of artificial circulation.</p>\n<p>&hellip;Good synthetic bone and skin should be available by the late 1990&rsquo;s to early 2000&rsquo;s. Good red cell and plasma substitutes (synthetic blood) should be seen increasing in clinical use throughout the early 1990&rsquo;s and in frequent use by the late 1990&rsquo;s to early 2000&rsquo;s.</p>\n<p>There will be steady improvement in other synthetic materials such as hip, knee, and other joints, as well as in other less dramatic materials such as connective tissue replacements. Expect a slow replacement of prosthetic approaches to therapy as natural repair and regeneration processes are better understood and utilized. Expect to see synthetic connective tissue products for tendon repair which contain bioregulatory molecules (BRMs) that stimulate tendon regeneration. Artificial tendons made of both synthetic and/or natural materials will come into use in the late 1980&rsquo;s to early 1990&rsquo;s. In short, expect stunning advances in tissue replacement technology for all tissues that have primarily structural function and which are not complicated chemical processing plants, such as the liver or kidneys, or mechanically active such as the heart. In addition to connective tissue and bone, a candidate for early (late 1980&rsquo;s to early 1990&rsquo;s) replacement is the cornea. Expect evolution in biocompatible materials to allow for replacement of the cornea with an appropriate plastic, much like the lens of the eye is already replaced with polymer inserts.</p>\n</blockquote>\n<h3 id=\"hemodialysis\"><span class=\"header-section-number\">1.2.6</span> Hemodialysis</h3>\n<blockquote>\n<p>Advances in hemodialysis will also be very incremental. There may be a gradual shift to peritoneal dialysis (PD) if good drugs to block glucosylation of proteins and inhibit cholesterol deposition are available. The major problem with PD today is that it raises blood sugars to astronomical levels, causing diabetic-like side effects. Inhibition of these side effects may lead to renewed application of this modality.</p>\n<p>Direct changes in dialysis are likely to be along the lines of better membrane materials which allow for transport of wastes not currently removable by conventional dialysis and nonthrombogenic surfaces which will reduce the need for anticoagulation. The use of BRMs such as erythropoetin to treat anemia and bone growth factors to treat dialysis bone disease will help to improve the quality and quantity of patient&rsquo;s lives on dialysis.</p>\n<p>Perhaps the biggest advance in this area will be advances in immunology and infectious disease treatment. The ability to administer BRMs to stimulate immune function and improve general health should act to extend dialysis patients&rsquo; lives considerably.</p>\n<p>&hellip;Of course, the biggest improvement in the life expectancy and health of dialysis patients will probably come in the form of the increasing use of transplantation and its application to a wider age range of patients with better long term results.</p>\n<p>The most striking revolution in prosthetics will probably occur in dentistry. Expect a whole family of new materials to enter the dental operatory. A workable vaccine against streptococcus mutans should be available by the mid to late 1990&rsquo;s, greatly reducing the incidence of tooth decay by eliminating the major class of mouth organisms that cause it. Similar advances in prevention and in treatment of gum disease can be expected as well, although probably not as soon. Repairing dental defects will also be revolutionized by the introduction of good, tough, and reliable polymers which will replace metallic amalgams. By the late 1990&rsquo;s to early 2000&rsquo;s biocompatible ceramics and coated polymers will be available that will allow for workable single tooth and multitooth gum-implanted prostheses.</p>\n</blockquote>\n<h3 id=\"organ-preservation\"><span class=\"header-section-number\">1.2.7</span> Organ Preservation</h3>\n<blockquote>\n<p>Ever since the work of people like Mazur, Fahy, and Pegg was published, it has become pretty clear what the constraints are on long term viable cryopreservation of organs: don&rsquo;t form any significant amount of ice; it injures mechanically and it injures chemically. The problem is that water loves to turn into ice when it&rsquo;s cooled below 0&oslash;C. To circumvent this, a lot of very drastic changes have to be made in the system. Whenever you attempt to make a drastic change in a complicated, interdependent living system &mdash; like replacing half the water in it with industrial chemicals &mdash; you are in for trouble. The trouble will come in the form of a very tight or narrow window for success: everything will have to be &ldquo;just right.&rdquo;</p>\n<p>This is where current vitrification technology is now. The existence of such a tight window means that vitrification of large masses will be a technological tour-de-force requiring very sophisticated computer controlled perfusion equipment and exotic and very costly high pressure chambers. Quality control and reliable storage and rewarming of organs will be very costly and difficult.</p>\n<p>The future holds the possibility of developing better solute systems which vitrify more easily and which are less toxic (have a wider window for success). It is difficult to predict the pace of advance in this area since it will be arrived at by a mixture of empirical methods and theoretical insights. A big determining factor will be luck. Will the NIH and the Red Cross continue to fund such efforts? And, more to the point, will technological advances in other areas of organ preservation obviate the need for them? If we were betting men, we&rsquo;d put our dollars on the latter rather than on the former. Major advances in organ preservation (as opposed to cell and tissue preservation) over the next decade will probably be in three areas: 1) Extended hypothermic storage of organs in the 2 to 3 weeks range; 2) Extended normothermic or room temperature storage of organs in the weeks to months range and; 3) mixtures of the above two modalities which yield similar available time courses of storage.</p>\n<p>&hellip;The next 5 to 10 years should also see major advances in our understanding of the effects of deep hypothermia on the tissues and organs of non-hibernating mammals. These advances should be readily translatable into better flush and perfusion storage techniques for organs. A good understanding of lipid metabolism and mechanisms of cell swelling in deep hypothermia may allow for preservation of organs in the 2&oslash;C to 10&oslash;C temperature range for periods of several months &mdash; thus definitively ending the need for long term solid state preservation of transplantable organs.</p>\n</blockquote>\n<h3 id=\"other-approaches-to-organ-preservation\"><span class=\"header-section-number\">1.2.8</span> Other Approaches to Organ Preservation</h3>\n<blockquote>\n<p>One possibility for a major advance over the next two decades is room temperature or hypothermic preservation of organs or organisms using metabolic inhibitors. There have been tantalizing clues in the examination of a wide variety of estivators (animals which go into states of profoundly reduced metabolism at normal temperatures, such as the African lungfish, which can shut off metabolism at temperatures in the range of 30&oslash;C to 40&oslash;C) that anti-metabolite compounds exist which may be able to induce states of profoundly reduced metabolism at ambient (i.e., 70&oslash;F) temperatures.</p>\n</blockquote>\n<h3 id=\"genetic-therapy\"><span class=\"header-section-number\">1.2.9</span> Genetic therapy</h3>\n<blockquote>\n<p>Expect very gradual application of this technology. Early candidates for gene replacement will be in storage diseases such as Lesch-Nyhan, Tay-Sachs, and other &ldquo;single enzyme missing&rdquo; disorders. Later applications will include treatments for hypercholesterolemia, some forms of hypertension, and other congenital missing enzyme syndromes. Very late applications (2000 or later) may be in the treatment of a wide range of mental illnesses and cancers.</p>\n</blockquote>\n<h3 id=\"prevention\"><span class=\"header-section-number\">1.2.10</span> Prevention</h3>\n<blockquote>\n<p>The principal lesson is the lesson of the impact of calorie restriction on overall health, well-being, and lifespan. The basic message here is &ldquo;you are what you eat.&rdquo; In terms of treating atherosclerotic disease, the role of prevention is already clear. By reducing fat intake and decreasing serum cholesterol to below 150 mg/dl, most atherosclerotic disease can be avoided. Similarly, basic changes in nutrition such as trace element and vitamin supplementation can greatly reduce the number of late onset malignancies. Eliminating smoking will also be a major factor in achieving this end&hellip;.Calorie restriction achieved by means of education and therapeutic agents seems the next big area of preventics to be explored by medicine. Expect the development of truly effective anorectics for treatment of gross obesity and eating disorders by the late 1980&rsquo;s and then secondary use of these for treatment of mild obesity and weight control in the normal middle aged. Products with reduced calories employing fat substitutes such as sucrose polyester should also be entering the marketplace in the early 1990&rsquo;s and these will help to reduce the calorie load further.</p>\n</blockquote>\n<h3 id=\"the-downside\"><span class=\"header-section-number\">1.2.11</span> The Downside</h3>\n<blockquote>\n<p>A little information is a dangerous thing, and sometimes a lot of information can be an even more dangerous thing. The reason is that progress in therapeutics, which is relatively difficult, always lags far behind progress in diagnosis, which is relatively easy. This imbalance results in a tension which forces premature treatment which often does more harm than good. It is well to note that each new diagnostic modality brings with it a flood of new information which will at first be grossly misused before anyone understands what it means (Harris&rsquo;s Law of Diagnostics Advance).</p>\n<p>A recent example of this sort of thing is the EKG machine, which for the first time showed that many seemingly normal people had strange cardiac rhythms, some of which were seen also around the time people died suddenly of heart problems. Because of this association, for the last 15 years, a number of very powerful drugs have been used to treat people with such rhythms. Many drug-induced fatalities resulted. Unfortunately, only now is it beginning to be understood that most people with good heart function are less in danger from such rhythms than they are from the drugs used to treat them &mdash; a finding of little consolation to the people already killed by the drugs.</p>\n</blockquote>\n<p>And on to the economics:</p>\n<blockquote>\n<p>There is a second downside to advanced medicine, of course, besides the danger, that is the cost of &ldquo;middlingly advanced technology&rdquo; (such as what we&rsquo;ll see in the next fifty years) in a society takes a socialistic view of health care. Such as ours.</p>\n<p>Non-molecular technology is expensive. It should be obvious to the reader, with a bit of thought that in a world of non-molecular technology, the potential demand for medical care as technology advances, is (for all intents and purposes) Iinfinite. In America, we have adopted the unfortunate policy of letting everyone pay for everyone else&rsquo;s medical care, which has had exactly the same result as if we had let everyone pool their money and pay for each other&rsquo;s lunch: everyone orders lobster. We have paid for the lobster only by spreading the costs around to places where they are not obvious. For instance, when you buy an American car, you pay more money for the health care costs of the people who built it than you do the steel that goes into it. This kind of thing can continue very subtly and very insidiously until a very large fraction of the gross national product is eaten up by health care costs. (In our country, it is already 11% and rising).</p>\n<p>One day, you may find that you have had to forego your family vacation in order to buy Granny that new AUTODOC which measures 245 different chemicals in her blood every minute and transmits all of the results to Medical Multivac in Bethesda. Of course you may not realize this: all you will know is that the vacation went because money is so tight, taxes are so high, and inflation is so bad. But your money went to Granny nevertheless. The only answer to this problem, short of nanotechnology, is rationing.</p>\n<p>But rationing itself becomes the last great social cost of advanced medical technology under socialism, because history shows that it is never done on an individual (person by person) basis. When people do not pay for their own medical care, no one (not doctors, families, or the government) has ever been willing to make the decision of who should benefit from a given technology, and who should not. Therefore, all systems of rationing to control medical costs ultimately have come down in the past to rationing technology across the board&hellip;So all of the rosy predictions made in this article must be tempered with the &ldquo;social&rdquo; realities that medicine will have to deal with in the next 20 years. Many of the advances we have discussed may simply not materialize because we are not wealthy enough to afford them collectively. That will be a great tragedy.</p>\n</blockquote>\n<h1 id=\"reactions\"><span class=\"header-section-number\">2</span> Reactions</h1>\n<p>On reading all the foregoing, I commented: that was a depressing read. As far as I can tell, they were dead on about the dismal economics, somewhat right about the diagnostics, and fairly wrong about everything else. Which is better than the old predictions listed, only one of which struck me as obviously right (but in a useless way, who actually uses <a href=\"http://en.wikipedia.org/wiki/Fluorocarbon\">perfluorocarbons</a> for <a href=\"http://en.wikipedia.org/wiki/Liquid_breathing\">liquid breathing</a>?).</p>\n<p>To which Darwin said:</p>\n<blockquote>\n<p>At the time I wrote it I kept saying to myself, almost none of this stuff is going to happen in 20 years - not here anyway. However, I <em>have</em> to come up with something.</p>\n<p>Ironically, in the area of cerebral resuscitation, where I am a supposed &ldquo;expert,&rdquo; I tried very hard to be realistic and to be both accurate and precise. That was arguably the area where I did the worst - exactly the way other experts fare when they try to predict the future of their fields&hellip;So, here I am, 24 <em>years</em> out from making those predictions and I read the <em>crap</em> posted on Less Wrong and on Cryonet and I don&rsquo;t whether to scream in rage and anger, or weep. How is possible to reach and convince this new generation of cryonics &ldquo;passivists&rdquo; that Yudkowsky and Alcor are breeding and make them understand that progress will continue to be unacceptably slow unless the <em>system</em> itself is changed?</p>\n</blockquote>\n<p>See also <em>Fight Aging!</em>&rsquo;s post, <a href=\"http://www.fightaging.org/archives/2011/12/overestimating-the-near-future.php\">&ldquo;Overestimating the Near Future&rdquo;</a>:</p>\n<blockquote>\n<p>&hellip;Many of the specific predictions in the article were in fact demonstrated in the laboratory to some degree, and were technically feasible to develop as commercial products by the year 2000, and in some cases earlier but at much greater expense. Certainly there are partial hits for many of the predictions by 2010, in the sense of it being possible, somewhat demonstrated, or in the early stages of being shown to be a practical goal. Yet the regulatory environment in much of the developed world essentially rules out any form of adventurous, rapid, highly competitive development in clinical medicine - such as exists in the electrical engineering, computing, and other worlds. We are cursed therefore with the passage of many years between a new medical technology being demonstrated possible and then attempted in the marketplace &hellip; if it ever makes it to the marketplace at all. <a href=\"https://www.opencures.org/content/open-cures-speed-clinical-development-longevity-science\">This must change</a> if we are to see significant progress.</p>\n</blockquote>\n<p>Darwin comments there:</p>\n<blockquote>\n<p>I&rsquo;ve been going over my original manuscript and surfing the web for specific applications (approved or in process) which meet the criteria of my predictions of 24 years ago. While many of my &ldquo;lesser&rdquo; predictions are in fact being realized (often in ways totally unforeseen by us when we wrote the article) overall it is a profoundly depressing experience.</p>\n<p>Perhaps nowhere has that been more true than in the areas of aging and cerebral resuscitation - two fields of endeavor I&rsquo;ve spent a lifetime working on, or intimately involved with those who are. In 1999, <a href=\"http://www.cryonet.org/cgi-bin/dsp.cgi?msg=11238\">we announced</a> that we had achieved repeatable recovery of dogs following 16+ minutes of whole body noromothermic cardiac arrest with no neurological deficit. The enabling molecules and techniques (principally a combination of melatonin, alpha-phenyl-n-tert-butyl-nitrone (PBN), and mild post-cardiac arrest therapeutic hypothermia) all seemed eminently applicable in the (then) immediate future. Indeed, an analog of PBN, 2,3-dihydroxy&ndash;6-nitro&ndash;7-sulfamoyl-benzo(F)quinoxaline (NBQX) had passed Phase I and II clinical trials for the treatment of stroke with flying colors, and seemed destined for approval.</p>\n<p>That was 13 years, ago, and there is still not a single drug available (approved or otherwise) anywhere in the world to treat cerebral ischemia-reperfusion injury - the real killer in cardiac arrest and stroke! Do a <a href=\"http://www.ncbi.nlm.nih.gov/pubmed?term=melatonin%20cerebral%20ischemia\">literature search</a> on Pubmed for melatonin + cerebral ischemia and you will get ~130 hits - almost all of them dramatically positive. Melatonin is a naturally occurring bioregulatory molecule which is inexpensive and freely available as an over the counter &ldquo;nutrient.&rdquo; Even as a stand alone molecule, melatonin is powerfully protective in both global and regional cerebral ischemia, and yet no human application has been forthcoming. It&rsquo;s been 15 years since <a href=\"http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=6&amp;f=G&amp;l=50&amp;co1=AND&amp;d=PTXT&amp;s1=melatonin&amp;s2=federowicz&amp;OS=melatonin+AND+federowicz&amp;RS=melatonin+AND+federowicz\">our patent on melatonin and other cerebroprotective molecules</a> was issued, 17 years since the patent was applied for, and over 20 years since I made the discovery! Indeed, mild therapeutic hypothermia, made the supposed standard of care for post cardiac arrest neuroinjury nearly a decade ago, is still largely ignored and is used well in only a handful of hospitals worldwide.</p>\n<p>What kind of black irony is it that I live in terror of stroke and cardiac arrest (for both myself and my loved ones) and yet the very molecules I discovered to combat them are as unavailable as if they had never been found? Change? Yes, change is certainly needed, and soon.</p>\n</blockquote>\n<h1 id=\"further-reading\"><span class=\"header-section-number\">3</span> Further reading</h1>\n<p>Previous Darwin-related posts:</p>\n<ul>\n<li><a href=\"/lw/6vq/on_the_unpopularity_of_cryonics_life_sucks_but_at/\">&ldquo;On the unpopularity of cryonics: life sucks, but at least then you die&rdquo;</a></li>\n<li><a href=\"/lw/6me/alcor_finances/\">&ldquo;ALCOR finances&rdquo;</a></li>\n<li><a href=\"/lw/7vv/mike_darwin_on_kurzweil_technooptimisim_and/\">&ldquo;Mike Darwin on Kurzweil, Techno-Optimism, and Delusional Stances on Cryonics&rdquo;</a></li>\n<li><a href=\"/lw/7zp/mike_darwin_on_steve_jobss_hypocritical_stance/\">&ldquo;Mike Darwin on Steve Jobs&rsquo;s hypocritical stance towards death&rdquo;</a></li>\n</ul>\n<p>See also Tyler Cowen's <em>The Great Stagnation</em> and <a href=\"/lw/7xm/peter_thiel_warns_of_upcoming_and_current/\">&ldquo;Peter Thiel warns of upcoming (and current) stagnation&rdquo;</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"xHjy88N2uJvGdgzfw": 1, "33BrBRSrRQS4jEHdk": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "qNxPRh5jzrLorak6B", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 49, "baseScore": 72, "extendedScore": null, "score": 0.000145, "legacy": true, "legacyId": "11617", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 72, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<blockquote>\n<p>Summary: medical progress has been much slower than even recently predicted.</p>\n</blockquote>\n<p>In the <a href=\"http://www.alcor.org/cryonics/cryonics8802.txt\">February</a> and <a href=\"http://www.alcor.org/cryonics/cryonics8803.txt\">March</a> 1988 issues of <a href=\"http://www.alcor.org/magazine/\"><em>Cryonics</em></a>, <a href=\"http://chronopause.com/\">Mike Darwin</a> (<a href=\"http://en.wikipedia.org/wiki/Mike_Darwin\">Wikipedia</a>/<a href=\"/user/mikedarwin/\">LessWrong</a>) and Steve Harris published a two-part article \u201cThe Future of Medicine\u201d attempting to forecast the medical state of the art for 2008. Darwin has republished it on the <a href=\"http://tech.groups.yahoo.com/group/New_Cryonet/message/1691\">New_Cryonet</a> email list.</p>\n<p>Darwin is a pretty savvy forecaster (who you will remember correctly predicting in 1981 in <a href=\"http://www.alcor.org/cryonics/cryonics8201.txt\">\u201cThe High Cost of Cryonics\u201d</a>/<a href=\"http://www.alcor.org/cryonics/cryonics8202.txt\">part 2</a> ALCOR\u2019s recent <a href=\"/lw/8fe/cryonics_costs_given_estimates_are_low/\">troubles with grandfathering</a>), so given my <a href=\"http://www.gwern.net/Prediction%20markets\">standing interests</a> in tracking predictions, I read it with great interest; but they still blew most of them, and not the ones we would prefer them to\u2019ve.</p>\n<p>The full essay is ~10k words, so I will excerpt roughly half of it below; feel free to skip to the <a href=\"#reactions\">reactions</a> section and other links.</p>\n<p><a id=\"more\"></a></p>\n<h1 id=\"1_The_Future_of_Medicine\"><span class=\"header-section-number\">1</span> The Future of Medicine</h1>\n<h2 id=\"1_1_Part_1\"><span class=\"header-section-number\">1.1</span> Part 1</h2>\n<blockquote>\n<p>What we hope we are especially good at as cryonicists is predicting the future \u2014 particularly the future of medicine. After all, our lives depend upon it. Because that\u2019s what cryonics is about \u2014 tomorrow\u2019s medicine today. In order for cryonics to seem reasonable, in order for it to be reasonable, it is necessary to have some idea, at least in broad outline, of where medicine is going and of where it ultimately can go. I think that the cryonicists\u2019 record on this point in a broad sense has been very good.</p>\n<p>\u2026One thing which is rarely seen in cryonics publications is an attempt to see the shape of things to come in the near or intermediate future. Oddly enough, that\u2019s a far more difficult and dangerous undertaking than predicting ultimates. Nor is this a problem confined to cryonics or the future of medicine. Sadi Carnot (the founder of thermodynamics) could tell you all about the \u201cperfect heat engine,\u201d but would have no doubt had trouble giving you hard numbers on how well heat engines would be made to perform over the 20 years or so following publication of his work\u2026.When I look over predictions made in the 1950\u2019s or the 1960\u2019s about the future of medicine and/or technology, I always chuckle about just how far afield these guys were. A good example is a list of predictions made by Herman Kahn which was summarized in CRYONICS REPORTS in August of 1967 (volume 2, #8). They are reproduced as Table 1 below. Read \u2019em and weep \u2014 or laugh if you will!</p>\n<blockquote>\n<p>Table 1. Less Likely But Important Possibilities, from: <em>The Next 33 Years: A Framework For Speculation</em>, by Herman Kahn and Anthony J. Weiner (1967) [predictions for 2000 AD]</p>\n<ul>\n<li>\u201cTrue\u201d artificial intelligence</li>\n<li>Practical use of sustained fusion to produce neutrons</li>\n<li>Artificial growth of new limbs and organs</li>\n<li>Room temperature superconductors</li>\n<li>Major use of rockets for transportation (either terrestrial or extraterrestrial)</li>\n<li>Effective chemical or biological treatment for most mental illnesses</li>\n<li>Almost complete control of marginal changes of heredity</li>\n<li>Suspended animation (for years or centuries)</li>\n<li>Practical materials with nearly \u201ctheoretical limit\u201d strengths</li>\n<li>Conversion of mammals (humans?) to fluid breathers</li>\n<li>Direct input into human memory banks</li>\n<li>\u2026</li>\n</ul>\n</blockquote>\n<p>\u2026My personal perspective is one of being a hard-core cryonicist who was involved in clinical medicine for the better part of a decade. My biases about predicting the future could probably be summarized as follows: I have a lot of sympathy for the incrementalist view of progress - particularly in the highly regulated area of medicine. It\u2019s regulated because it directly and powerfully touches people\u2019s well-being and because it is not a very fault-tolerant area \u2014 mistakes are costly and since people like being alive (at least in the short run) they get edgy if an error separates them from their actuarial expectations.</p>\n<p>I thus believe that any predictions about the future of medicine have to include what I call the \u201cspace program factor\u201d (SPF). By this I mean simply that progress in the space program would have proceeded far, far faster (and thus approximated more closely what was theoretically possible) if it were not a high-visibility project with lots of political and social overtones which make it fault-intolerant \u2014 if you could burn up as many astronauts as you do test pilots every month, it would cost a lot less to get where you\u2019re going. First-shot fail-safe engineering is costly. Medicine suffers from the same kinds of problems \u2014 witness the FDA as both the solution and the problem.</p>\n</blockquote>\n<h3 id=\"1_1_1_Diagnostics\"><span class=\"header-section-number\">1.1.1</span> Diagnostics</h3>\n<blockquote>\n<p>I foresee a veritable explosion of diagnostic techniques and procedures. A large number of illnesses which are poorly understood today will be well-characterized the next decade and will be easy to diagnose very early in their development or even before they develop because they will be found to have direct or indirect genetic causes. Fairly predictive tests for Alzheimer\u2019s disease, schizophrenia, depression, some malignancies, heart disease, and most of the rest of the major killers and disablers will probably be in place by 2000 to 2010. Many if not most of these ailments will be assessable in terms of a very sophisticated genetic risk profile which it will be possible to generate in infancy or childhood (or in utero). A wide range of genetic probes for illness-generating genes should be available by the end of the century.</p>\n</blockquote>\n<p>A side-note: genetic associations have been a very fertile field for John Ioannidis, and a big study just blew away a bunch of <a href=\"http://www.arts.cornell.edu/econ/dbenjamin/IQ-SNPs-PsychSci-20111205-accepted.pdf\">SNP-IQ correlations</a>.</p>\n<blockquote>\n<p>Real-time diagnosis will also be revolutionized by the turn of the century. The next 10 to 15 years will see increasing miniaturization of sensors and chemistry packages. Tissue probes or biosensors which can measure a wide array of biological and biochemical factors will be packaged in very small, very stable devices which hold calibration over prolonged periods of time (weeks to months to years) and which can easily be inserted into the patient\u2019s body or tissues. For example, I foresee multi-sensor units mounted on very small needle or catheter tips which can be inserted intravenously, intracranially, intra-cerebrally, subcutaneously, and so on.</p>\n<p>These sensors will be able to give real-time measurements of blood gases, pH, electrolytes, enzyme levels, and a host of other biochemical parameters that now involve costly, time-consuming, and/or impossible \u201claboratory studies\u201d requiring withdrawal of a sample and processing. Real-time biosensors will revolutionize acute care of critically ill patients.</p>\n<p>\u2026The first generation of these devices should be in the marketplace somewhere between 1990 and 1995. More sophisticated instruments capable of a wider array of measurements will quickly follow. These sensors will also have a profound impact in acute stabilization of patients in a field setting. It will be possible for paramedical personnel to quickly and effectively insert such instruments in an acutely ill patient \u2014 a victim of cardiac arrest or trauma, and immediately and globally assess that patient\u2019s condition, relaying that information to an expert (more on who that expert will be later).</p>\n<p>\u2026Diagnostic imaging should rapidly come down to a battle between ultrasound and MRI (NMR; (nuclear) magnetic resonance imaging). Because ultrasound units owe their size and weight almost entirely to the computer that processes the information, the size and effectiveness of these units will change on the same rapid exponential curve as the size and power of computers. MRI is a technology which has some other physical limitations, but by the year 2000, even MRI units will be far smaller, less costly, and capable of far, far better results. Bedside units or \u201con floor\u201d units (i.e., units in the ICU or CCU) may be available for repeated assessment of the patient\u2019s condition. MRI and its grandchildren and cousins should in particular be expected to undergo considerable refinement. Metabolic MRI will also be in wider use, allowing for real-time evaluation of the metabolic and working state of patient\u2019s hearts, brains and other organs. By 2000 to 2010 the cost and size of these units may be drastically reduced and they may be in field use for acute metabolic and structural evaluation of patients with trauma or in cardiac arrest.</p>\n</blockquote>\n<p>I recently learned that, besides the usual blame for <a href=\"http://articles.baltimoresun.com/2011-02-22/news/bs-ed-health-costs-20110222_1_health-care-high-quality-care-mri\">increasing medical costs</a>, some categories of doctors have been strenuously urged to <a href=\"http://www.wired.com/magazine/2011/12/ff_causation/all/1#post-50632\">reduce MRI use</a> as actively harmful.</p>\n<blockquote>\n<p>By the late 1990\u2019s there should be an answer to this problem in the development of the Portable Doctor or Expert Medical Device (EMD). The EMD will be both a diagnostician and therapist integrated into one unit. In an emergency medical setting (either in an ambulance or in an ICU or CCU) this powerful computer will be directly coupled to a wide array of both simple and complex medical assessment devices\u2026.EMDs will be a very hot item. Initially (i.e., the 1990\u2019s) they will be confined to ambulances and the ICU, CCU, and specialty areas of the hospital, such as radiology and cardiology labs. But there will be powerful incentives for wider application of these devices. As computing capacity drops in cost and increases radically in sophistication (i.e., parallel processors, neural networks, truly massive memories, and so on) expert medical (and other) systems will see increasing application. There will be devices on the market such as a \u201cHome Doctor\u201d diagnostic program, which will basically be an internal medicine physician in a can.</p>\n<p>\u2026After 2000, many people will probably have a small sensor array permanently implanted and coupled to telemetry equipment which can be activated to call for help or alert the person that trouble is brewing. People with a known risk of sudden health problems will be the first to use these kinds of devices. With the development of smaller and cheaper telemetry equipment (directly linked to large-antenna satellites), separate telemetry arrangements will disappear. Implantable, computer-controlled defibrillators are already a reality; analogous devices to deliver drugs in case of cardiac or brain infarct (stroke) will eventually become reality.</p>\n</blockquote>\n<h3 id=\"1_1_2_Resuscitation\"><span class=\"header-section-number\">1.1.2</span> Resuscitation</h3>\n<blockquote>\n<p>Expect a shift back to open-chest heart massage and away from closed-chest massage in medical and perhaps even paramedical settings. Closed-chest CPR will be realized to be ineffective at maintaining cerebral viability and will be replaced by far more effective open chest methods. In paramedical (i.e., field) settings the emphasis will be on very rapid defibrillation \u2014 or actually \u201cleaving the patient alone\u201d until circulation can be effectively restored and medications given to inhibit reperfusion injury. Closed chest CPR and restarting circulation by laymen \u201cin the field\u201d will be realized to be doing more harm than good and there may well be a move away from field CPR, with laymen being instructed to leave the patient without circulation until it can be restarted adequately and under controlled conditions.</p>\n<p>By the late 1990\u2019s, extended use of CPR will be a thing of the past and major metropolitan areas will have \u201cdeath reversal units\u201d (DRUs) in emergency rooms and perhaps even in larger paramedical units. The DRUs will employ rapid femoral cut-downs and blood-pump/oxygenator supported resuscitation to recover people who have suffered extended periods of ischemia (in the 30 minute to 1 hour range). CPR will be realized very often to be ineffective at recovering patients who are profoundly ischemic and the advent of pharmacologic intervention allowing for cerebral resuscitation will provide tremendous pressure for emergency rooms to develop the capability to very rapidly put an ischemic patient on bypass and completely and adequately support his circulatory and respiratory needs until his brain can recover and/or his heart can be repaired and restarted. An intermediate scenario would be the development of small, flexible impeller pumps that can be collapsed and passed through a large bore percutaneous catheter through the femoral artery and into the abdominal aorta. Such a pump (acting much like the propeller on an outboard boat motor) could then be used to supplement CPR, perhaps providing 2\u20133 liters per minute of cardiac output.</p>\n<p>\u2026Another effect of drugs like the lazaroids and calcium channel blockers will be the more effective treatment of acute injuries to a wide range of tissues such as the spinal cord and brain. Much of the damage that occurs to these tissues is free radical related and can be inhibited by use of these drugs\u2026Intervention into secondary inflammation will be most important in the brain and spinal cord. Deployment of these techniques will result in the salvage of many spinal cords that would be considered irreversibly injured by today\u2019s medicine. There will be far, far fewer paraplegics. However, expect an increase in the number of permanently brain-injured patients and in the number of patients with \u201csubtle\u201d forms of cerebral injury resembling mild stroke or the cognitive or mood disorders seen in diseases like multiple sclerosis or acute head injury. These disease states will result because people with brain trauma who would have died acutely from secondary free radical mediated injury (cerebral edema and so on) will be saved with lazaroids and other cerebral rescue techniques.</p>\n</blockquote>\n<h3 id=\"1_1_3_Antibiotics\"><span class=\"header-section-number\">1.1.3</span> Antibiotics</h3>\n<blockquote>\n<p>The next twenty years should see many powerful new antibiotics engineered directly from knowledge of the structure of the relevant microbial enzyme which it is desired to inhibit. Not only will these antibiotics be more powerful, but because they do not exist in nature, strain resistance will not so easily develop toward them as it has for the antibiotics of today.</p>\n<p>In addition, the next generation of antibiotics will include many which have been designed for effect against viruses, an area where medicine is presently largely powerless.</p>\n</blockquote>\n<p>The pharmaceutical industry and antibiotics have been a case-study in stagnation, failure, and diminishing marginal returns. There is <a href=\"http://en.wikipedia.org/wiki/DRACO_%28antiviral%29\">only one</a>, highly experimental, anti-viral that I have heard of. In a <a href=\"http://tech.groups.yahoo.com/group/New_Cryonet/message/1719\">followup email</a>, Darwin responded to someone else pointing out DRACO:</p>\n<blockquote>\n<p><span style=\"font-family: Arial; color: #000000; font-size: x-small;\"> </span></p>\n<p><span style=\"font-size: small;\">Finally, while Geoff cites this putative advance in antiviral drug therapy, the fact is that my prediction about a plethora of new and highly effective targeted molecular antimicrobials by 2008 was <strong>WRONG</strong>. In fact, antibiotic research is all but dead, and there are virtually no fundamentally new antibiotics in the drug pipeline. This should scare the crap out of all us, because we are rapidly approaching complete antibiotic resistance with a number of common and highly lethal bugs, including staph (MRSA), streptococcus, &nbsp;E. coli, pseudomonas and candida. It is only a matter of months to a few years, at most, &nbsp;before completely antibiotic resistance staph and streptococcus emerge. Pharmaceutical companies have a large <strong>negative</strong> incentive for developing new antimicrobials. At the cost of over a billion dollars a new drug (regulatory) and the high risk of withdrawal of the drug within 5 years (2 out of 3), as well as the near certainty of punishing litigation for adverse effects, antibiotics are not merely uneconomical to develop, they are fiscal suicide. Only drugs that will be chronically used by very large numbers of patients are now worth developing.</span></p>\n<p>&nbsp;</p>\n</blockquote>\n<p><span style=\"font-size: small;\">(This agrees with my own general impressions, which I didn't feel competent to baldly state.)<br></span></p>\n<h3 id=\"1_1_4_Immunology_and_cancer\"><span class=\"header-section-number\">1.1.4</span> Immunology and cancer</h3>\n<blockquote>\n<p>\u2026Monoclonal and synthetic antibodies carrying toxins or regulatory molecules will be used to turn off or destroy the fraction of immune cells which initially respond and proliferate when a transplant is carried out. More widespread transplantation of tissues will be undertaken, including transplantation of limbs and scalp. Xenografts will be used increasingly in the mid to late 1990\u2019s and it will not be uncommon for people to have pancreatic tissue from bovine or porcine sources and perhaps hearts, lungs, and livers from other animals. Expect the first workable transplants to be from great apes (chimps, gorillas, orangutans), with porcine and bovine grafts coming later.</p>\n<p>Immunology and immunotherapy will also be revolutionized by a far more complete understanding of the immune system resulting from the AIDS epidemic and basic research in the immunology of diseases such as multiple sclerosis and aging. The ability to rapidly and cheaply synthesize bioregulatory molecules will open up a wide array of therapeutic possibilities. Expect effective treatments for most autoimmune diseases (lupus, multiple sclerosis, myasthenia gravis, and so on) by the mid to late 1990\u2019s. The mid to late 1990\u2019s should also see the wider application of immunorestoratives for use with the aged and ill. Cancer therapy will improve considerably as a result of these advances as well as a result of selective targeting techniques. By the early to mid\u20131990s the first generations of monoclonal antibodies linked to chemotherapeutic agents or powerful natural toxins will be used against a few cancers.</p>\n</blockquote>\n<h3 id=\"1_1_5_Atherosclerosis\"><span class=\"header-section-number\">1.1.5</span> Atherosclerosis</h3>\n<blockquote>\n<p>Atherosclerosis will undergo a very marked but nevertheless gradual reduction in frequency and severity of occurrence as physicians slowly become educated about what is already known and begin to use existing therapeutic modalities more aggressively. By the mid to late 1990\u2019s it will be more widely understood that atherosclerosis can be reversed, and there will be wider use of drugs such as lovastatin to reduce serum cholesterol, coupled with sound dietary advice. However, even well into the late 1990\u2019s and perhaps beyond, atherosclerotic disease (heart attack, stroke, ischemic limb disease, and so on) will continue to be a serious source of morbidity and mortality. By the late 1990\u2019s, 2nd and 3rd generation therapies will be coming on-line which will be able to reverse atherosclerotic disease and more directly inhibit it</p>\n</blockquote>\n<h2 id=\"1_2_Part_2\"><span class=\"header-section-number\">1.2</span> Part 2</h2>\n<h3 id=\"1_2_1_Anesthesia\"><span class=\"header-section-number\">1.2.1</span> Anesthesia</h3>\n<blockquote>\n<p>Expect \u201cmodular\u201d anesthesia by the 1990\u2019s to the early 2000\u2019s. The development of potent anxieolytics (anxiety removers) which do not depress consciousness and the development of total pain inhibitors will allow for complicated surgical procedures on conscious patients. Expect to see major thoracic and limb surgery on high risk patients (i.e., patients unable to tolerate anesthesia) using such agents.Major abdominal surgery requiring deep muscle relaxation will continue to require skeletal muscle paralysis and general anesthesia. However, expect new drugs in the market place in the late 1990\u2019s which induce unconsciousness without respiratory or cardiac depression.</p>\n<p>Surgical and post surgical mortality will decrease sharply due to such anesthetics and the use of real-time physiological and biochemical monitoring during and after surgery using biosensors.</p>\n</blockquote>\n<h3 id=\"1_2_2_Surgery\"><span class=\"header-section-number\">1.2.2</span> Surgery</h3>\n<blockquote>\n<p>\u2026Catheters, laparascopes, and thorascopes with sensors, operating tools, and an impressive array of capabilities will be increasingly used. Abdominal surgery will shift more and more towards the use of the fiberoptic laparascope, endoscope, and laser as miniaturization of tools occurs and disease is diagnosed earlier. Early diagnosis will create the need for less drastic procedures.</p>\n<p>Fine-tuned repair of heart valves and blood vessels, and examination and biopsy of suspected abdominal and retroperitoneal lesions will be early candidates for application of this technology.</p>\n<p>\u2026In contrast to therapeutic surgery, the frequency of cosmetic surgery will probably increase dramatically as techniques are refined and prosthetics improve in quality and drop in cost. As people live longer, and stay productive longer as well, they will increasingly turn to medicine to maintain not only their health but their appearance. Cosmetic surgery will experience a boom until such time as the fundamental mechanisms underlying the aging process can be brought under control.</p>\n</blockquote>\n<h3 id=\"1_2_3_Geriatrics\"><span class=\"header-section-number\">1.2.3</span> Geriatrics</h3>\n<blockquote>\n<p>Advances will be slow here, but significant. Expect increasing understanding and application of trophic factors and bioregulatory compounds. Early candidates for rejuvenation will be the immune system and other stem cell systems or systems with higher cell turnover. By the early decades of 2000, significant rejuvenation and geroprophylaxis of skin, bone, immune, and other \u201chigh turn- over\u201d tissues will be possible as the natural regulatory molecules which control these systems are understood and applied.</p>\n<p>\u2026By the early years of the 21st century the first generation of compounds effective at \u201crejuvenating\u201d (i.e., restoring some degree of normal maintenance and repair to existing brain cells) the central nervous system will be available. These drugs will work by turning on protein synthesis and stimulating natural repair mechanisms.</p>\n<p>However, pathologies of the brain and other non-dividing tissues (renal, cardiac, and musculoskeletal system) will continue to be major sources of morbidity and mortality over the next two decades. As atherosclerosis and immune-related disorders are dealt with more effectively, expect an increasing shift of morbidity and mortality to central nervous system-related causes. Beyond 2000 this may be treated to a limited extent with fetal transplant</p>\n</blockquote>\n<p id=\"psychiatry-behavior\">We all know how well this has worked out. More troubling is that in some respects, we appear <em>further</em> from any solutions or treatments than before; while resveratrol did well in <a href=\"http://pipeline.corante.com/archives/2011/11/10/resveratrol_in_humans_results_of_a_controlled_trial.php\">a recent human trial</a>, the sirtuin research that seemed so promising <a href=\"https://www.sciencemag.org/content/334/6060/1194\">has been battered</a> by null results and failures to replicate. And anti-aging drugs have their own methodological difficulties; from the <a href=\"http://tech.groups.yahoo.com/group/New_Cryonet/message/1719\">followup email</a>:</p>\n<blockquote>\n<p><span style=\"font-size: small;\">Antiaging drugs are unlikely to be free of adverse effects. In fact, it seems very likely that they will be burdened with many adverse effects and that they will even kill a minority of people who use them. The common perception is that antiaging drugs will make people super fit, healthier and more resistant to disease. And yet, in calorie restriction and effective antiaging drug studies there is emerging evidence that slowing aging comes at the cost of interfering with fundamental processes that make organisms fitter for both reproduction and for surviving in a hostile environment. </span></p>\n<p><span style=\"font-size: small;\">Consider the putative antaging drug rapamycin. It seems likely that rapamycin interferes with senesence by affecting the PI3-kinase and TOR: PIKTORing cell growth pathways. This almost certainly means that in some individuals there will serious and even lethal side effects - cancer being one of them. [Persons with a history of promiscuity, and thus a heavy burden of chronic viral infection, and those with certain \"unfavorable\" genotypes will likely be at very high risk.] But, beyond cancer, interfering with these fundamental and deeply evolutionarily conserved pathways is likely to cause a range of adverse effects that negatively (and possibly irreversibly) impact normal body functions, such as energy level, cognition, sexual performance, and so on.. While some people are now using rapamycin as an antiaging drug...</span><span style=\"font-size: small;\">it is virtually inconceivable that any major pharmaceutical company anywhere in the world would (or will) market such a drug for \"normal\" aging. This is important to understand because it gives us basic insight into what will almost certainly be a major barrier to the development and marketing of antiaging drugs: they will necessarily be used by large numbers of people over the course of many decades (and thus millions of drug/person years) and they are incredibly unlikely to be free of adverse, and sometimes even lethal side effects.</span></p>\n</blockquote>\n<h3 id=\"1_2_4_Psychiatry___Behavior\"><span class=\"header-section-number\">1.2.4</span> Psychiatry &amp; Behavior</h3>\n<blockquote>\n<p>Diagnosis by brain scanning (metabolic MRI) and chemical analysis of cerebrospinal fluids will be commonplace in 20 years. As neuroregulatory compounds are better understood and as the biochemistry underlying mental disorders is elucidated there will be more effective treatments. Expect 2nd and 3rd generation drugs and combinations thereof for treatment of depression and psychosis by the late 1990\u2019s. There will probably be several very effective therapeutic agents for compulsive disorders in the marketplace by the early to mid 1990\u2019s.</p>\n</blockquote>\n<p>From the previously quoted <a href=\"http://tech.groups.yahoo.com/group/New_Cryonet/message/1719\">followup email</a>:</p>\n<blockquote>\n<p><span style=\"font-family: Arial; color: #000000; font-size: x-small;\"> </span></p>\n<p><span style=\"font-size: small;\">Similarly, psychiatric drugs (which <em>are</em> typically chronically used) are no longer economical to develop and market because of the litigation costs associated with them. Widespread chronic use of <strong>any</strong> drug means that the likelihood of adverse conditions that were impossible to detect in the testing phase of the drug development process are almost certain to emerge.&nbsp;Statistics rule in drug development, and a Phase III study that lasts a year and enrolls 5,000 patients is simply not adequately powered to predict what will happen when 5 million patients take&nbsp;a drug for 20 years! The only way to get that data is to do<strong>&nbsp;that</strong> study. And therein lies a&nbsp;powerful caution about antiaging drugs. These drugs will likely need to be taken&nbsp;starting in young adulthood, or in middle age, at latest,&nbsp;and they will need to be taken for a lifetime. Indeed, if they are effective, for a longer lifetime than any but a few super-centenarians &nbsp;has previously lived. </span></p>\n<p>&nbsp;</p>\n</blockquote>\n<h3 id=\"1_2_5_Implants___Prosthetics\"><span class=\"header-section-number\">1.2.5</span> Implants &amp; Prosthetics</h3>\n<blockquote>\n<p>Early spectacular applications will be small vessel prostheses (wide use by the early to mid 1990\u2019s) for use in traumatized and atherosclerotic limbs and organs and venous prostheses (mid to late 1990\u2019s) for use in treating traumatic injuries and deep vein incompetence (which results in varicosities, chronic pain, and edema-related skin changes in the leg, often leading to non-healing ulcers or limb loss). Another application of non-thrombogenic surfaces will be a practical artificial heart and more widespread use of extracorporeal support for infants, trauma and cardiac arrest victims, and others where anticoagulation provides a major barrier to the use of artificial circulation.</p>\n<p>\u2026Good synthetic bone and skin should be available by the late 1990\u2019s to early 2000\u2019s. Good red cell and plasma substitutes (synthetic blood) should be seen increasing in clinical use throughout the early 1990\u2019s and in frequent use by the late 1990\u2019s to early 2000\u2019s.</p>\n<p>There will be steady improvement in other synthetic materials such as hip, knee, and other joints, as well as in other less dramatic materials such as connective tissue replacements. Expect a slow replacement of prosthetic approaches to therapy as natural repair and regeneration processes are better understood and utilized. Expect to see synthetic connective tissue products for tendon repair which contain bioregulatory molecules (BRMs) that stimulate tendon regeneration. Artificial tendons made of both synthetic and/or natural materials will come into use in the late 1980\u2019s to early 1990\u2019s. In short, expect stunning advances in tissue replacement technology for all tissues that have primarily structural function and which are not complicated chemical processing plants, such as the liver or kidneys, or mechanically active such as the heart. In addition to connective tissue and bone, a candidate for early (late 1980\u2019s to early 1990\u2019s) replacement is the cornea. Expect evolution in biocompatible materials to allow for replacement of the cornea with an appropriate plastic, much like the lens of the eye is already replaced with polymer inserts.</p>\n</blockquote>\n<h3 id=\"1_2_6_Hemodialysis\"><span class=\"header-section-number\">1.2.6</span> Hemodialysis</h3>\n<blockquote>\n<p>Advances in hemodialysis will also be very incremental. There may be a gradual shift to peritoneal dialysis (PD) if good drugs to block glucosylation of proteins and inhibit cholesterol deposition are available. The major problem with PD today is that it raises blood sugars to astronomical levels, causing diabetic-like side effects. Inhibition of these side effects may lead to renewed application of this modality.</p>\n<p>Direct changes in dialysis are likely to be along the lines of better membrane materials which allow for transport of wastes not currently removable by conventional dialysis and nonthrombogenic surfaces which will reduce the need for anticoagulation. The use of BRMs such as erythropoetin to treat anemia and bone growth factors to treat dialysis bone disease will help to improve the quality and quantity of patient\u2019s lives on dialysis.</p>\n<p>Perhaps the biggest advance in this area will be advances in immunology and infectious disease treatment. The ability to administer BRMs to stimulate immune function and improve general health should act to extend dialysis patients\u2019 lives considerably.</p>\n<p>\u2026Of course, the biggest improvement in the life expectancy and health of dialysis patients will probably come in the form of the increasing use of transplantation and its application to a wider age range of patients with better long term results.</p>\n<p>The most striking revolution in prosthetics will probably occur in dentistry. Expect a whole family of new materials to enter the dental operatory. A workable vaccine against streptococcus mutans should be available by the mid to late 1990\u2019s, greatly reducing the incidence of tooth decay by eliminating the major class of mouth organisms that cause it. Similar advances in prevention and in treatment of gum disease can be expected as well, although probably not as soon. Repairing dental defects will also be revolutionized by the introduction of good, tough, and reliable polymers which will replace metallic amalgams. By the late 1990\u2019s to early 2000\u2019s biocompatible ceramics and coated polymers will be available that will allow for workable single tooth and multitooth gum-implanted prostheses.</p>\n</blockquote>\n<h3 id=\"1_2_7_Organ_Preservation\"><span class=\"header-section-number\">1.2.7</span> Organ Preservation</h3>\n<blockquote>\n<p>Ever since the work of people like Mazur, Fahy, and Pegg was published, it has become pretty clear what the constraints are on long term viable cryopreservation of organs: don\u2019t form any significant amount of ice; it injures mechanically and it injures chemically. The problem is that water loves to turn into ice when it\u2019s cooled below 0\u00f8C. To circumvent this, a lot of very drastic changes have to be made in the system. Whenever you attempt to make a drastic change in a complicated, interdependent living system \u2014 like replacing half the water in it with industrial chemicals \u2014 you are in for trouble. The trouble will come in the form of a very tight or narrow window for success: everything will have to be \u201cjust right.\u201d</p>\n<p>This is where current vitrification technology is now. The existence of such a tight window means that vitrification of large masses will be a technological tour-de-force requiring very sophisticated computer controlled perfusion equipment and exotic and very costly high pressure chambers. Quality control and reliable storage and rewarming of organs will be very costly and difficult.</p>\n<p>The future holds the possibility of developing better solute systems which vitrify more easily and which are less toxic (have a wider window for success). It is difficult to predict the pace of advance in this area since it will be arrived at by a mixture of empirical methods and theoretical insights. A big determining factor will be luck. Will the NIH and the Red Cross continue to fund such efforts? And, more to the point, will technological advances in other areas of organ preservation obviate the need for them? If we were betting men, we\u2019d put our dollars on the latter rather than on the former. Major advances in organ preservation (as opposed to cell and tissue preservation) over the next decade will probably be in three areas: 1) Extended hypothermic storage of organs in the 2 to 3 weeks range; 2) Extended normothermic or room temperature storage of organs in the weeks to months range and; 3) mixtures of the above two modalities which yield similar available time courses of storage.</p>\n<p>\u2026The next 5 to 10 years should also see major advances in our understanding of the effects of deep hypothermia on the tissues and organs of non-hibernating mammals. These advances should be readily translatable into better flush and perfusion storage techniques for organs. A good understanding of lipid metabolism and mechanisms of cell swelling in deep hypothermia may allow for preservation of organs in the 2\u00f8C to 10\u00f8C temperature range for periods of several months \u2014 thus definitively ending the need for long term solid state preservation of transplantable organs.</p>\n</blockquote>\n<h3 id=\"1_2_8_Other_Approaches_to_Organ_Preservation\"><span class=\"header-section-number\">1.2.8</span> Other Approaches to Organ Preservation</h3>\n<blockquote>\n<p>One possibility for a major advance over the next two decades is room temperature or hypothermic preservation of organs or organisms using metabolic inhibitors. There have been tantalizing clues in the examination of a wide variety of estivators (animals which go into states of profoundly reduced metabolism at normal temperatures, such as the African lungfish, which can shut off metabolism at temperatures in the range of 30\u00f8C to 40\u00f8C) that anti-metabolite compounds exist which may be able to induce states of profoundly reduced metabolism at ambient (i.e., 70\u00f8F) temperatures.</p>\n</blockquote>\n<h3 id=\"1_2_9_Genetic_therapy\"><span class=\"header-section-number\">1.2.9</span> Genetic therapy</h3>\n<blockquote>\n<p>Expect very gradual application of this technology. Early candidates for gene replacement will be in storage diseases such as Lesch-Nyhan, Tay-Sachs, and other \u201csingle enzyme missing\u201d disorders. Later applications will include treatments for hypercholesterolemia, some forms of hypertension, and other congenital missing enzyme syndromes. Very late applications (2000 or later) may be in the treatment of a wide range of mental illnesses and cancers.</p>\n</blockquote>\n<h3 id=\"1_2_10_Prevention\"><span class=\"header-section-number\">1.2.10</span> Prevention</h3>\n<blockquote>\n<p>The principal lesson is the lesson of the impact of calorie restriction on overall health, well-being, and lifespan. The basic message here is \u201cyou are what you eat.\u201d In terms of treating atherosclerotic disease, the role of prevention is already clear. By reducing fat intake and decreasing serum cholesterol to below 150 mg/dl, most atherosclerotic disease can be avoided. Similarly, basic changes in nutrition such as trace element and vitamin supplementation can greatly reduce the number of late onset malignancies. Eliminating smoking will also be a major factor in achieving this end\u2026.Calorie restriction achieved by means of education and therapeutic agents seems the next big area of preventics to be explored by medicine. Expect the development of truly effective anorectics for treatment of gross obesity and eating disorders by the late 1980\u2019s and then secondary use of these for treatment of mild obesity and weight control in the normal middle aged. Products with reduced calories employing fat substitutes such as sucrose polyester should also be entering the marketplace in the early 1990\u2019s and these will help to reduce the calorie load further.</p>\n</blockquote>\n<h3 id=\"1_2_11_The_Downside\"><span class=\"header-section-number\">1.2.11</span> The Downside</h3>\n<blockquote>\n<p>A little information is a dangerous thing, and sometimes a lot of information can be an even more dangerous thing. The reason is that progress in therapeutics, which is relatively difficult, always lags far behind progress in diagnosis, which is relatively easy. This imbalance results in a tension which forces premature treatment which often does more harm than good. It is well to note that each new diagnostic modality brings with it a flood of new information which will at first be grossly misused before anyone understands what it means (Harris\u2019s Law of Diagnostics Advance).</p>\n<p>A recent example of this sort of thing is the EKG machine, which for the first time showed that many seemingly normal people had strange cardiac rhythms, some of which were seen also around the time people died suddenly of heart problems. Because of this association, for the last 15 years, a number of very powerful drugs have been used to treat people with such rhythms. Many drug-induced fatalities resulted. Unfortunately, only now is it beginning to be understood that most people with good heart function are less in danger from such rhythms than they are from the drugs used to treat them \u2014 a finding of little consolation to the people already killed by the drugs.</p>\n</blockquote>\n<p>And on to the economics:</p>\n<blockquote>\n<p>There is a second downside to advanced medicine, of course, besides the danger, that is the cost of \u201cmiddlingly advanced technology\u201d (such as what we\u2019ll see in the next fifty years) in a society takes a socialistic view of health care. Such as ours.</p>\n<p>Non-molecular technology is expensive. It should be obvious to the reader, with a bit of thought that in a world of non-molecular technology, the potential demand for medical care as technology advances, is (for all intents and purposes) Iinfinite. In America, we have adopted the unfortunate policy of letting everyone pay for everyone else\u2019s medical care, which has had exactly the same result as if we had let everyone pool their money and pay for each other\u2019s lunch: everyone orders lobster. We have paid for the lobster only by spreading the costs around to places where they are not obvious. For instance, when you buy an American car, you pay more money for the health care costs of the people who built it than you do the steel that goes into it. This kind of thing can continue very subtly and very insidiously until a very large fraction of the gross national product is eaten up by health care costs. (In our country, it is already 11% and rising).</p>\n<p>One day, you may find that you have had to forego your family vacation in order to buy Granny that new AUTODOC which measures 245 different chemicals in her blood every minute and transmits all of the results to Medical Multivac in Bethesda. Of course you may not realize this: all you will know is that the vacation went because money is so tight, taxes are so high, and inflation is so bad. But your money went to Granny nevertheless. The only answer to this problem, short of nanotechnology, is rationing.</p>\n<p>But rationing itself becomes the last great social cost of advanced medical technology under socialism, because history shows that it is never done on an individual (person by person) basis. When people do not pay for their own medical care, no one (not doctors, families, or the government) has ever been willing to make the decision of who should benefit from a given technology, and who should not. Therefore, all systems of rationing to control medical costs ultimately have come down in the past to rationing technology across the board\u2026So all of the rosy predictions made in this article must be tempered with the \u201csocial\u201d realities that medicine will have to deal with in the next 20 years. Many of the advances we have discussed may simply not materialize because we are not wealthy enough to afford them collectively. That will be a great tragedy.</p>\n</blockquote>\n<h1 id=\"2_Reactions\"><span class=\"header-section-number\">2</span> Reactions</h1>\n<p>On reading all the foregoing, I commented: that was a depressing read. As far as I can tell, they were dead on about the dismal economics, somewhat right about the diagnostics, and fairly wrong about everything else. Which is better than the old predictions listed, only one of which struck me as obviously right (but in a useless way, who actually uses <a href=\"http://en.wikipedia.org/wiki/Fluorocarbon\">perfluorocarbons</a> for <a href=\"http://en.wikipedia.org/wiki/Liquid_breathing\">liquid breathing</a>?).</p>\n<p>To which Darwin said:</p>\n<blockquote>\n<p>At the time I wrote it I kept saying to myself, almost none of this stuff is going to happen in 20 years - not here anyway. However, I <em>have</em> to come up with something.</p>\n<p>Ironically, in the area of cerebral resuscitation, where I am a supposed \u201cexpert,\u201d I tried very hard to be realistic and to be both accurate and precise. That was arguably the area where I did the worst - exactly the way other experts fare when they try to predict the future of their fields\u2026So, here I am, 24 <em>years</em> out from making those predictions and I read the <em>crap</em> posted on Less Wrong and on Cryonet and I don\u2019t whether to scream in rage and anger, or weep. How is possible to reach and convince this new generation of cryonics \u201cpassivists\u201d that Yudkowsky and Alcor are breeding and make them understand that progress will continue to be unacceptably slow unless the <em>system</em> itself is changed?</p>\n</blockquote>\n<p>See also <em>Fight Aging!</em>\u2019s post, <a href=\"http://www.fightaging.org/archives/2011/12/overestimating-the-near-future.php\">\u201cOverestimating the Near Future\u201d</a>:</p>\n<blockquote>\n<p>\u2026Many of the specific predictions in the article were in fact demonstrated in the laboratory to some degree, and were technically feasible to develop as commercial products by the year 2000, and in some cases earlier but at much greater expense. Certainly there are partial hits for many of the predictions by 2010, in the sense of it being possible, somewhat demonstrated, or in the early stages of being shown to be a practical goal. Yet the regulatory environment in much of the developed world essentially rules out any form of adventurous, rapid, highly competitive development in clinical medicine - such as exists in the electrical engineering, computing, and other worlds. We are cursed therefore with the passage of many years between a new medical technology being demonstrated possible and then attempted in the marketplace \u2026 if it ever makes it to the marketplace at all. <a href=\"https://www.opencures.org/content/open-cures-speed-clinical-development-longevity-science\">This must change</a> if we are to see significant progress.</p>\n</blockquote>\n<p>Darwin comments there:</p>\n<blockquote>\n<p>I\u2019ve been going over my original manuscript and surfing the web for specific applications (approved or in process) which meet the criteria of my predictions of 24 years ago. While many of my \u201clesser\u201d predictions are in fact being realized (often in ways totally unforeseen by us when we wrote the article) overall it is a profoundly depressing experience.</p>\n<p>Perhaps nowhere has that been more true than in the areas of aging and cerebral resuscitation - two fields of endeavor I\u2019ve spent a lifetime working on, or intimately involved with those who are. In 1999, <a href=\"http://www.cryonet.org/cgi-bin/dsp.cgi?msg=11238\">we announced</a> that we had achieved repeatable recovery of dogs following 16+ minutes of whole body noromothermic cardiac arrest with no neurological deficit. The enabling molecules and techniques (principally a combination of melatonin, alpha-phenyl-n-tert-butyl-nitrone (PBN), and mild post-cardiac arrest therapeutic hypothermia) all seemed eminently applicable in the (then) immediate future. Indeed, an analog of PBN, 2,3-dihydroxy\u20136-nitro\u20137-sulfamoyl-benzo(F)quinoxaline (NBQX) had passed Phase I and II clinical trials for the treatment of stroke with flying colors, and seemed destined for approval.</p>\n<p>That was 13 years, ago, and there is still not a single drug available (approved or otherwise) anywhere in the world to treat cerebral ischemia-reperfusion injury - the real killer in cardiac arrest and stroke! Do a <a href=\"http://www.ncbi.nlm.nih.gov/pubmed?term=melatonin%20cerebral%20ischemia\">literature search</a> on Pubmed for melatonin + cerebral ischemia and you will get ~130 hits - almost all of them dramatically positive. Melatonin is a naturally occurring bioregulatory molecule which is inexpensive and freely available as an over the counter \u201cnutrient.\u201d Even as a stand alone molecule, melatonin is powerfully protective in both global and regional cerebral ischemia, and yet no human application has been forthcoming. It\u2019s been 15 years since <a href=\"http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=6&amp;f=G&amp;l=50&amp;co1=AND&amp;d=PTXT&amp;s1=melatonin&amp;s2=federowicz&amp;OS=melatonin+AND+federowicz&amp;RS=melatonin+AND+federowicz\">our patent on melatonin and other cerebroprotective molecules</a> was issued, 17 years since the patent was applied for, and over 20 years since I made the discovery! Indeed, mild therapeutic hypothermia, made the supposed standard of care for post cardiac arrest neuroinjury nearly a decade ago, is still largely ignored and is used well in only a handful of hospitals worldwide.</p>\n<p>What kind of black irony is it that I live in terror of stroke and cardiac arrest (for both myself and my loved ones) and yet the very molecules I discovered to combat them are as unavailable as if they had never been found? Change? Yes, change is certainly needed, and soon.</p>\n</blockquote>\n<h1 id=\"3_Further_reading\"><span class=\"header-section-number\">3</span> Further reading</h1>\n<p>Previous Darwin-related posts:</p>\n<ul>\n<li><a href=\"/lw/6vq/on_the_unpopularity_of_cryonics_life_sucks_but_at/\">\u201cOn the unpopularity of cryonics: life sucks, but at least then you die\u201d</a></li>\n<li><a href=\"/lw/6me/alcor_finances/\">\u201cALCOR finances\u201d</a></li>\n<li><a href=\"/lw/7vv/mike_darwin_on_kurzweil_technooptimisim_and/\">\u201cMike Darwin on Kurzweil, Techno-Optimism, and Delusional Stances on Cryonics\u201d</a></li>\n<li><a href=\"/lw/7zp/mike_darwin_on_steve_jobss_hypocritical_stance/\">\u201cMike Darwin on Steve Jobs\u2019s hypocritical stance towards death\u201d</a></li>\n</ul>\n<p>See also Tyler Cowen's <em>The Great Stagnation</em> and <a href=\"/lw/7xm/peter_thiel_warns_of_upcoming_and_current/\">\u201cPeter Thiel warns of upcoming (and current) stagnation\u201d</a>.</p>", "sections": [{"title": "1 The Future of Medicine", "anchor": "1_The_Future_of_Medicine", "level": 1}, {"title": "1.1 Part 1", "anchor": "1_1_Part_1", "level": 2}, {"title": "1.1.1 Diagnostics", "anchor": "1_1_1_Diagnostics", "level": 3}, {"title": "1.1.2 Resuscitation", "anchor": "1_1_2_Resuscitation", "level": 3}, {"title": "1.1.3 Antibiotics", "anchor": "1_1_3_Antibiotics", "level": 3}, {"title": "1.1.4 Immunology and cancer", "anchor": "1_1_4_Immunology_and_cancer", "level": 3}, {"title": "1.1.5 Atherosclerosis", "anchor": "1_1_5_Atherosclerosis", "level": 3}, {"title": "1.2 Part 2", "anchor": "1_2_Part_2", "level": 2}, {"title": "1.2.1 Anesthesia", "anchor": "1_2_1_Anesthesia", "level": 3}, {"title": "1.2.2 Surgery", "anchor": "1_2_2_Surgery", "level": 3}, {"title": "1.2.3 Geriatrics", "anchor": "1_2_3_Geriatrics", "level": 3}, {"title": "1.2.4 Psychiatry & Behavior", "anchor": "1_2_4_Psychiatry___Behavior", "level": 3}, {"title": "1.2.5 Implants & Prosthetics", "anchor": "1_2_5_Implants___Prosthetics", "level": 3}, {"title": "1.2.6 Hemodialysis", "anchor": "1_2_6_Hemodialysis", "level": 3}, {"title": "1.2.7 Organ Preservation", "anchor": "1_2_7_Organ_Preservation", "level": 3}, {"title": "1.2.8 Other Approaches to Organ Preservation", "anchor": "1_2_8_Other_Approaches_to_Organ_Preservation", "level": 3}, {"title": "1.2.9 Genetic therapy", "anchor": "1_2_9_Genetic_therapy", "level": 3}, {"title": "1.2.10 Prevention", "anchor": "1_2_10_Prevention", "level": 3}, {"title": "1.2.11 The Downside", "anchor": "1_2_11_The_Downside", "level": 3}, {"title": "2 Reactions", "anchor": "2_Reactions", "level": 1}, {"title": "3 Further reading", "anchor": "3_Further_reading", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "48 comments"}], "headingsCount": 23}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 48, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dQWCDERoZjLLHWpys", "mkrvsNi8cYGSjGqkh", "iPK8AezgksW8N7N5Z", "P4LakhT54cLwtonnn", "38bhCn3sqoLYNzwQB", "GKqBQDMsAFdgHfd49"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 4, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-23T21:41:34.021Z", "modifiedAt": null, "url": null, "title": "Historical examples of flinching away", "slug": "historical-examples-of-flinching-away", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:07.691Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YpXsZ4H67MaQsoyzz/historical-examples-of-flinching-away", "pageUrlRelative": "/posts/YpXsZ4H67MaQsoyzz/historical-examples-of-flinching-away", "linkUrl": "https://www.lesswrong.com/posts/YpXsZ4H67MaQsoyzz/historical-examples-of-flinching-away", "postedAtFormatted": "Friday, December 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Historical%20examples%20of%20flinching%20away&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHistorical%20examples%20of%20flinching%20away%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYpXsZ4H67MaQsoyzz%2Fhistorical-examples-of-flinching-away%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Historical%20examples%20of%20flinching%20away%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYpXsZ4H67MaQsoyzz%2Fhistorical-examples-of-flinching-away", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYpXsZ4H67MaQsoyzz%2Fhistorical-examples-of-flinching-away", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 83, "htmlBody": "<p>I'm looking for historical examples of \"flinching away,\" so I can illustrate the concept to others and talk about <a href=\"http://wiki.lesswrong.com/wiki/Motivated_cognition\">motivated cognition</a> and <a href=\"/lw/o4/leave_a_line_of_retreat/\">leaving a line of retreat</a> and so on.</p>\n<p>The ideal example would be one of motivated skepticism with grave consequences. Like, a military commander who shied away from believing certain reports because they implied something huge and scary was about to happen, and then the huge and scary thing happened and caused great damage. Something like that.</p>\n<p>What examples can you think of?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YpXsZ4H67MaQsoyzz", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 13, "extendedScore": null, "score": 8.198776045799054e-07, "legacy": true, "legacyId": "11641", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 50, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["3XgYbghWruBMrPTAL"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-23T23:24:16.633Z", "modifiedAt": null, "url": null, "title": "East Coast Megameetup II: Electric Boogalloo", "slug": "east-coast-megameetup-ii-electric-boogalloo", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:06.858Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "atucker", "createdAt": "2010-08-07T03:49:28.822Z", "isAdmin": false, "displayName": "atucker"}, "userId": "hJiWvoMeXCqB3gTMx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9DTQRmv4uLxbLtKe2/east-coast-megameetup-ii-electric-boogalloo", "pageUrlRelative": "/posts/9DTQRmv4uLxbLtKe2/east-coast-megameetup-ii-electric-boogalloo", "linkUrl": "https://www.lesswrong.com/posts/9DTQRmv4uLxbLtKe2/east-coast-megameetup-ii-electric-boogalloo", "postedAtFormatted": "Friday, December 23rd 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20East%20Coast%20Megameetup%20II%3A%20Electric%20Boogalloo&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEast%20Coast%20Megameetup%20II%3A%20Electric%20Boogalloo%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9DTQRmv4uLxbLtKe2%2Feast-coast-megameetup-ii-electric-boogalloo%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=East%20Coast%20Megameetup%20II%3A%20Electric%20Boogalloo%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9DTQRmv4uLxbLtKe2%2Feast-coast-megameetup-ii-electric-boogalloo", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9DTQRmv4uLxbLtKe2%2Feast-coast-megameetup-ii-electric-boogalloo", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 208, "htmlBody": "<p>The last <a href=\"/lw/6f5/meetup_east_coast_super_fun_rationalists/\">East Coast Megameetup</a> was awesome, and people wanted to do another one.</p>\n<p>So let's do another one?</p>\n<p>Vision:</p>\n<p>\n<ul>\n<li>Primarily a social event, with some skill transfer</li>\n<li>Meet people within the LW community, make friends, get contacts</li>\n<li>Get people together for our mutual benefit</li>\n<li>Informal focused discussion/teaching, people in the community know a lot about stuff that's useful to know</li>\n<li>Help lone rationalists be part of the community</li>\n</ul>\n<div>Geoff Anders has kindly agreed to host at the Leverage House.</div>\n</p>\n<p>What I want from you, East Coaster:</p>\n<p>\n<ul>\n<li>Bring this up at your meetup or mailing list</li>\n<li>Select someone in your meetup group as a point of contact for the group. They should be able to represent and effectively communicate with the rest of the meetup group.</li>\n<li>Have them email me at aarondtucker &lt;at&gt; gmail.com</li>\n<li>You can always arbitrarily designate yourself to be that point of contact to keep things moving, and then figure out with the group who it should actually be.</li>\n<li>Try to figure out if you want to attend.</li>\n</ul>\n<div>I want this to happen around the end of January.</div>\n<div>If you don't have a meetup group, you are still very much invited</div>\n<div>Thoughts?</div>\n</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9DTQRmv4uLxbLtKe2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 9, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "11642", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 6, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["mr6XWhrWxnoncwopb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-24T05:33:36.277Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Beautiful Probability", "slug": "seq-rerun-beautiful-probability", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:03.812Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xCnNbCR27dbLjjifT/seq-rerun-beautiful-probability", "pageUrlRelative": "/posts/xCnNbCR27dbLjjifT/seq-rerun-beautiful-probability", "linkUrl": "https://www.lesswrong.com/posts/xCnNbCR27dbLjjifT/seq-rerun-beautiful-probability", "postedAtFormatted": "Saturday, December 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Beautiful%20Probability&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Beautiful%20Probability%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxCnNbCR27dbLjjifT%2Fseq-rerun-beautiful-probability%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Beautiful%20Probability%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxCnNbCR27dbLjjifT%2Fseq-rerun-beautiful-probability", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FxCnNbCR27dbLjjifT%2Fseq-rerun-beautiful-probability", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 167, "htmlBody": "<p>Today's post, <a href=\"/lw/mt/beautiful_probability/\">Beautiful Probability</a> was originally published on 14 January 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Bayesians expect probability theory, and rationality itself, to be math. Self consistent, neat, even beautiful. This is why Bayesians think that Cox's theorems are so important.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/8z7/seq_rerun_is_reality_ugly/\">Is Reality Ugly?</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xCnNbCR27dbLjjifT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.200521333904484e-07, "legacy": true, "legacyId": "11654", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bkSkRwo9SRYxJMiSY", "yvYJ3Cy2vuoQ4NJmq", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-24T21:21:35.674Z", "modifiedAt": null, "url": null, "title": "Would you like to give me feedback for \"On What is a Self\" ", "slug": "would-you-like-to-give-me-feedback-for-on-what-is-a-self", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/9aey9ri5469ob9Pag/would-you-like-to-give-me-feedback-for-on-what-is-a-self", "pageUrlRelative": "/posts/9aey9ri5469ob9Pag/would-you-like-to-give-me-feedback-for-on-what-is-a-self", "linkUrl": "https://www.lesswrong.com/posts/9aey9ri5469ob9Pag/would-you-like-to-give-me-feedback-for-on-what-is-a-self", "postedAtFormatted": "Saturday, December 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Would%20you%20like%20to%20give%20me%20feedback%20for%20%22On%20What%20is%20a%20Self%22%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWould%20you%20like%20to%20give%20me%20feedback%20for%20%22On%20What%20is%20a%20Self%22%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9aey9ri5469ob9Pag%2Fwould-you-like-to-give-me-feedback-for-on-what-is-a-self%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Would%20you%20like%20to%20give%20me%20feedback%20for%20%22On%20What%20is%20a%20Self%22%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9aey9ri5469ob9Pag%2Fwould-you-like-to-give-me-feedback-for-on-what-is-a-self", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F9aey9ri5469ob9Pag%2Fwould-you-like-to-give-me-feedback-for-on-what-is-a-self", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 47, "htmlBody": "<p>Hi, I'm going to publish soon here a study of CEV composed of two texts \"On What is a Self\" and \"Troubles with CEV\"&nbsp; Would you like to give feedback prior to publication?&nbsp;</p>\n<p>If so, please provide your e-mail address and I will send you the text.</p>\n<p>Merry Newtonmas</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "9aey9ri5469ob9Pag", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": -11, "extendedScore": null, "score": 8.204028377145981e-07, "legacy": true, "legacyId": "11658", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-24T21:22:27.015Z", "modifiedAt": null, "url": null, "title": "Would you like to give me feedback for \"Troubles With CEV\"", "slug": "would-you-like-to-give-me-feedback-for-troubles-with-cev", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:03.521Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "diegocaleiro", "createdAt": "2009-07-27T10:36:18.861Z", "isAdmin": false, "displayName": "diegocaleiro"}, "userId": "6tTwQ8Rdp2uhK5NL3", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Cuzo9axug5vwCKozY/would-you-like-to-give-me-feedback-for-troubles-with-cev", "pageUrlRelative": "/posts/Cuzo9axug5vwCKozY/would-you-like-to-give-me-feedback-for-troubles-with-cev", "linkUrl": "https://www.lesswrong.com/posts/Cuzo9axug5vwCKozY/would-you-like-to-give-me-feedback-for-troubles-with-cev", "postedAtFormatted": "Saturday, December 24th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Would%20you%20like%20to%20give%20me%20feedback%20for%20%22Troubles%20With%20CEV%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWould%20you%20like%20to%20give%20me%20feedback%20for%20%22Troubles%20With%20CEV%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCuzo9axug5vwCKozY%2Fwould-you-like-to-give-me-feedback-for-troubles-with-cev%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Would%20you%20like%20to%20give%20me%20feedback%20for%20%22Troubles%20With%20CEV%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCuzo9axug5vwCKozY%2Fwould-you-like-to-give-me-feedback-for-troubles-with-cev", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FCuzo9axug5vwCKozY%2Fwould-you-like-to-give-me-feedback-for-troubles-with-cev", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 47, "htmlBody": "<div id=\"entry_t3_8zu\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<p>Hi, I'm going to publish soon here a study of CEV composed of  two texts \"On What is a Self\" and \"Troubles with CEV\"&nbsp; Would you like to  give feedback prior to publication?&nbsp;</p>\n<p>If so, please provide your e-mail address and I will send you the text.</p>\n<p>Merry Newtonmas</p>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Cuzo9axug5vwCKozY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 14, "baseScore": -13, "extendedScore": null, "score": 8.204031543534091e-07, "legacy": true, "legacyId": "11659", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-25T00:17:11.104Z", "modifiedAt": null, "url": null, "title": "Details of lab-made bird flu won't be revealed [link]", "slug": "details-of-lab-made-bird-flu-won-t-be-revealed-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:08.384Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kevin", "createdAt": "2009-03-01T08:53:06.623Z", "isAdmin": false, "displayName": "Kevin"}, "userId": "8GnKujYLZ2ZZLs5zk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/tdkv6cNRpNdG778Jj/details-of-lab-made-bird-flu-won-t-be-revealed-link", "pageUrlRelative": "/posts/tdkv6cNRpNdG778Jj/details-of-lab-made-bird-flu-won-t-be-revealed-link", "linkUrl": "https://www.lesswrong.com/posts/tdkv6cNRpNdG778Jj/details-of-lab-made-bird-flu-won-t-be-revealed-link", "postedAtFormatted": "Sunday, December 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Details%20of%20lab-made%20bird%20flu%20won't%20be%20revealed%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ADetails%20of%20lab-made%20bird%20flu%20won't%20be%20revealed%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftdkv6cNRpNdG778Jj%2Fdetails-of-lab-made-bird-flu-won-t-be-revealed-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Details%20of%20lab-made%20bird%20flu%20won't%20be%20revealed%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftdkv6cNRpNdG778Jj%2Fdetails-of-lab-made-bird-flu-won-t-be-revealed-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Ftdkv6cNRpNdG778Jj%2Fdetails-of-lab-made-bird-flu-won-t-be-revealed-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p><a href=\"http://news.yahoo.com/details-lab-made-bird-flu-wont-revealed-223114982.html\">http://news.yahoo.com/details-lab-made-bird-flu-wont-revealed-223114982.html</a></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "tdkv6cNRpNdG778Jj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 11, "extendedScore": null, "score": 8.204678256465391e-07, "legacy": true, "legacyId": "11660", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-25T02:26:13.738Z", "modifiedAt": null, "url": null, "title": "[Link] Correlation Graphs Reveal Shocking Information", "slug": "link-correlation-graphs-reveal-shocking-information", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:05.489Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Alicorn", "createdAt": "2009-03-17T18:52:42.458Z", "isAdmin": false, "displayName": "Alicorn"}, "userId": "iPdmf2tiNRtfJbvdQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/vyx5RF4ZTsxGmTkne/link-correlation-graphs-reveal-shocking-information", "pageUrlRelative": "/posts/vyx5RF4ZTsxGmTkne/link-correlation-graphs-reveal-shocking-information", "linkUrl": "https://www.lesswrong.com/posts/vyx5RF4ZTsxGmTkne/link-correlation-graphs-reveal-shocking-information", "postedAtFormatted": "Sunday, December 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Correlation%20Graphs%20Reveal%20Shocking%20Information&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Correlation%20Graphs%20Reveal%20Shocking%20Information%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvyx5RF4ZTsxGmTkne%2Flink-correlation-graphs-reveal-shocking-information%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Correlation%20Graphs%20Reveal%20Shocking%20Information%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvyx5RF4ZTsxGmTkne%2Flink-correlation-graphs-reveal-shocking-information", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fvyx5RF4ZTsxGmTkne%2Flink-correlation-graphs-reveal-shocking-information", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 20, "htmlBody": "<p><a href=\"http://www.cbc.ca/strombo/Images/etc_correlation50__01__960.jpg\">Babies named Ava caused the housing bubble</a>, and other intriguing data.</p>\n<p>More illustrative than the usual \"correlation is not causation\" mantra.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "vyx5RF4ZTsxGmTkne", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 19, "extendedScore": null, "score": 8.205155920148522e-07, "legacy": true, "legacyId": "11661", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 13, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-25T06:55:23.733Z", "modifiedAt": null, "url": null, "title": "[LINK] Lincoln on Rationality", "slug": "link-lincoln-on-rationality", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:04.089Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "Q9oWZLLfJtXqhi5fq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hTS9zp4Saz58z26HG/link-lincoln-on-rationality", "pageUrlRelative": "/posts/hTS9zp4Saz58z26HG/link-lincoln-on-rationality", "linkUrl": "https://www.lesswrong.com/posts/hTS9zp4Saz58z26HG/link-lincoln-on-rationality", "postedAtFormatted": "Sunday, December 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20Lincoln%20on%20Rationality&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20Lincoln%20on%20Rationality%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhTS9zp4Saz58z26HG%2Flink-lincoln-on-rationality%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20Lincoln%20on%20Rationality%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhTS9zp4Saz58z26HG%2Flink-lincoln-on-rationality", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhTS9zp4Saz58z26HG%2Flink-lincoln-on-rationality", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 168, "htmlBody": "<p><a href=\"http://opinionator.blogs.nytimes.com/2011/12/24/a-fateful-christmas-meeting/\">http://opinionator.blogs.nytimes.com/2011/12/24/a-fateful-christmas-meeting/</a></p>\n<p>&nbsp;</p>\n<blockquote>\n<p>But Lincoln, like Chase, harbored doubts about taking a conciliatory approach. At the conclusion of what became a four-hour meeting, the president proposed to Seward that they both draft arguments supporting their positions. He and Seward would then face off the next day like lawyers competing in court to win the jury&rsquo;s verdict.</p>\n<p>The meeting on Dec. 26 proved to be both shorter and less combative, with the cabinet quickly agreeing to the release of Mason and Slidell. Lincoln, despite his position the day before, raised no objections. Surprised that his fiercest opponent had changed his mind, Seward asked the president what had prompted the reversal. &ldquo;I found that I could not make an argument that would satisfy my own mind, and that proved to me your ground was the right one,&rdquo; replied the president.</p>\n</blockquote>\n<div>This reminded me of <a href=\"http://www.amazon.com/Groupthink-Psychological-Studies-Decisions-Fiascoes/dp/0395317045/ref=sr_1_1?ie=UTF8&amp;qid=1324795684&amp;sr=8-1\">Groupthink</a> by&nbsp;Irving L. Janis (also see <a href=\"http://en.wikipedia.org/wiki/Groupthink\">Wikipedia's article</a>), which analyzed how bad decisions are made (e.g. the <a href=\"http://en.wikipedia.org/wiki/Bay_of_Pigs_Invasion\">Bay of Pigs Invasion</a>) and avoided (e.g. the <a href=\"http://en.wikipedia.org/wiki/Cuban_Missile_Crisis\">Cuban Missile Crisis</a>).</div>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hTS9zp4Saz58z26HG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 15, "extendedScore": null, "score": 8.206152414726478e-07, "legacy": true, "legacyId": "11662", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 10, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-25T07:28:08.110Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Trust in Math", "slug": "seq-rerun-trust-in-math", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bw3A4PRXPqoWWA98N/seq-rerun-trust-in-math", "pageUrlRelative": "/posts/bw3A4PRXPqoWWA98N/seq-rerun-trust-in-math", "linkUrl": "https://www.lesswrong.com/posts/bw3A4PRXPqoWWA98N/seq-rerun-trust-in-math", "postedAtFormatted": "Sunday, December 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Trust%20in%20Math&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Trust%20in%20Math%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbw3A4PRXPqoWWA98N%2Fseq-rerun-trust-in-math%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Trust%20in%20Math%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbw3A4PRXPqoWWA98N%2Fseq-rerun-trust-in-math", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fbw3A4PRXPqoWWA98N%2Fseq-rerun-trust-in-math", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 201, "htmlBody": "<p>Today's post, <a href=\"/lw/mu/trust_in_math/\">Trust in Math</a> was originally published on 15 January 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>When you find a seeming inconsistency in the rules of math, or logic, or probability theory, you might do well to consider that math has rightfully earned a bit more credibility than that. Check the proof. It is more likely that you have made a mistake in algebra, than that you have just discovered a fatal flaw in math itself.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/8zq/seq_rerun_beautiful_probability/\">Beautiful Probability</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bw3A4PRXPqoWWA98N", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.206273636158703e-07, "legacy": true, "legacyId": "11663", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2MqXKvBym3kRxvJMv", "xCnNbCR27dbLjjifT", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-25T09:37:00.410Z", "modifiedAt": "2019-11-10T17:36:23.197Z", "url": null, "title": "Just another day in utopia", "slug": "just-another-day-in-utopia", "viewCount": null, "lastCommentedAt": "2020-11-24T08:54:41.388Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Stuart_Armstrong", "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/sMsvcdxbK2Xqx8EHr/just-another-day-in-utopia", "pageUrlRelative": "/posts/sMsvcdxbK2Xqx8EHr/just-another-day-in-utopia", "linkUrl": "https://www.lesswrong.com/posts/sMsvcdxbK2Xqx8EHr/just-another-day-in-utopia", "postedAtFormatted": "Sunday, December 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Just%20another%20day%20in%20utopia&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJust%20another%20day%20in%20utopia%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsMsvcdxbK2Xqx8EHr%2Fjust-another-day-in-utopia%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Just%20another%20day%20in%20utopia%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsMsvcdxbK2Xqx8EHr%2Fjust-another-day-in-utopia", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FsMsvcdxbK2Xqx8EHr%2Fjust-another-day-in-utopia", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3901, "htmlBody": "<html><head></head><body><p><i>(Reposted from discussion at commentator suggestion)</i></p><p>Thinking of Eliezer's <a href=\"/lw/wv/prolegomena_to_a_theory_of_fun/\">fun theory</a> and the challenge of creating actual utopias where people would like to live, I tried to write a light utopia for my friends around Christmas, and thought it might be worth sharing. It's a techno-utopia, but (considering my audience) it's only a short <a href=\"/lw/kg/expecting_short_inferential_distances\">inferential distance</a> from normality.</p><h1>Just another day in Utopia</h1><p>Ishtar went to sleep in the arms of her lover Ted, and awoke locked in a safe, in a cargo hold of a triplane spiralling towards a collision with the reconstructed temple of Solomon.</p><p>Again! Sometimes she wished that a whole week would go by without something like that happening. But then, she had chosen a high excitement existence (not maximal excitement, of course \u2013 <i>that</i> was for complete masochists), so she couldn\u2019t complain. She closed her eyes for a moment and let the thrill and the adrenaline warp her limbs and mind, until she felt transformed, yet again, into a demi-goddess of adventure. Drugs couldn\u2019t have that effect on her, she knew; only real danger and challenge could do that.</p><p>Right. First, the safe. She gave the inner door a firm thud, felt it ring like a bell, heard the echo return \u2013 and felt the tumblers move. So, sound controlled lock, then. A search through her shoes produced a small pebble which sparked as she dashed it against the metal. Trying to ignore the ominous vibration as the triplane motor shook itself to pieces, she constructed a mental image of the safe\u2019s inside from the brief flashes of light. Symmetric gold and gilded extravagances festooned her small prison \u2013 French Baroque decorations, but not yet Roccoco. So Louis XIV period. She gave the less visited parts of her mind a good dusting, trying to remember the tunes of Jean Batiste Lully, the period\u2019s most influential composer. She hoped it wasn\u2019t any of his ballets; she was much better with his operas. The decorations looked vaguely snake-like; so she guessed Lully\u2019s \u2018Pers\u00e9e\u2019 opera, about the death of the medusa.</p><p>The engine creaked to a worrying silence as she was half-way through humming the Gorgon theme from the opera. Rushing the rest of the composition, she felt the door shift, finally, to a ten-times speeded up version of Andromeda\u2019s response to Perseus\u2019s proposal. She kicked the door open, exploded from the safe, took in the view of the temple of Solomon rushing up towards her, seconds away, grabbed a picture from the floor, grabbed an axe from the wall, hacked off one of the wings with three violent cuts, and jumped out of the plane after it.</p><p>Behind her, the plane disintegrated in midair as the temple lasers cut it to shreds and she fell through space, buffeted by the wind, not losing her grip on to the mangled wing. She had maybe thirty seconds to tie herself to the wing, using the object\u2019s own canvas as binding, and she rushed through that. The Machines wouldn\u2019t allow the fall to kill her, of course, but it would hurt quite a bit (another of her choices \u2013 she\u2019d allowed herself to feel moderate amounts of pain), put back her attempts to ever find Ted, and, most importantly of all, be crushingly embarrassing socially.</p><p>Once she was lashed to the plummeting piece of wood and canvas, and she was reasonably confident that the fall was slow enough, and her knots secure enough, she finally looked at the photograph she had grabbed during her explosive exit from the plane. It showed Ted, trussed up in chains but smiling and evidently enjoying the novel experience. Underneath was finely engraved note saying \u201cIf you ever want to see your lover again, bring me the missing Stradivarius by noon tomorrow. Nero the 2nd\u201d. Each capital letter was beautifully decorated with heads on spikes.</p><p>So! It seemed that her magnificent enemy Nero had resorted to kidnapping in order to get his way. It wasn\u2019t like Nero could actually harm Ted \u2013 unlike Ishtar, her lover had never chosen to accept any level of pain above mild, brief discomfort. But if he was \u2018killed\u2019, Ted would feel honour-bound to never see her again, something she wasn\u2019t ready to cope with yet. On the other hand, if she gave Nero her last Stradivarius, he might destroy it for good. It was her own choice: she had requested that her adventures have real meaning, hence real consequences. If she failed, and if Nero so choose, a small piece of humanity\u2019s cultural history could be destroyed forever, permanently stymying her attempts to reconstruct Stradivarius\u2019s violin-making techniques for the modern world. Culture or love, what to choose? Those were her final thoughts before she crashed into an oak tree shaped like a duck.</p><p>She returned to bleary consciousness fifteen minutes later. Her fainting was a sign the Machines were only granting her partial success in her escape attempt; she would have to try harder next time. In the meantime, however, she would have to deal with shotgun pressed into her face and the gorgeous man at the other side of it shouting \u201cGet off my property!\u201d.</p><p>\u201cPause,\u201d she said softly. The man nodded; she had temporarily paused her adventure, so that she wouldn\u2019t have to deal with danger or pursuit for the next few minutes, and so that this guy wouldn\u2019t have to get her away immediately to protect his property from collateral damage. Most Adventurers disdained the use of the pause, claiming it ruined the purity of their experience. But Ishtar liked it; it gave her the opportunity, as now, of getting to know the people she bumped into. And this person definitely seemed to be in the \u2018worth getting to know\u2019 category. He put down his shotgun without a word and picked up his paintbrush, applying a few more touches to the canvas in front of him.&nbsp;</p><p>After disengaging herself from both the mangled wing and the duck-shaped tree (she\u2019d have a dramatic scar from that crash, if she choose to), she worked her way round to what he was painting. It was a rather good neo-impressionistic canvas of her, unconscious in the tree, pieces of torn canvas around her, framed by broken branches and a convenient setting moon. Even with his main subject out of the frame, as it were, he still seemed intent on finishing his painting.&nbsp;</p><p>\u201cWhy did you splice your tree\u2019s genes to make it look like a duck?\u201d she asked, when the silence had gone on, in her estimation, for ten times as long as it should have. He had done a pretty good job with that oak, in fact; the feathers and the features were clear and distinct amongst the wood \u2013 or had been, until someone had crashed a triplane wing into the middle of it.</p><p>\u201cI didn\u2019t,\u201d he said. \u201cThat\u2019s normal oak; I just trim and tie it.\u201d</p><p>\u201cBut...\u201d she looked at it again in astonishment; the amount of work involved to get that detail from natural wood was beyond belief. And oak wasn\u2019t exactly a fast growing plant... \u201cIt must have taken you decades!\u201d</p><p>\u201cTwo centuries,\u201d he answered with dour satisfaction. \u201cAll natural, no help from the Machines.\u201d He waved his hand up the side of the hill. \u201cI\u2019m making the perfect landscape. And then, I shall paint it.\u201d</p><p>&nbsp;</p><p>The layout was a tapestry of secret themes. Hedges, streams, tree-rows, pathways, ridges and twined lianas carved the landscape into hidden pockets of beauty. Each such pocket seemed to be a private retreat, cut off from the others and from the rest of the world \u2013 and yet all were visible at once, the layout a cunning display of multiple intimacy. Here and there were formal gardens, with lines of flowers all at attention, row after row, shading across colour and size from huge orchids to tiny snowdrops. Some pockets were carefully dishevelled, mini deserts or prairies or jungles, perfect fragments of wild untamed nature that could only exist at the cost of supreme artifice. There were herb gardens, rock gardens, orchards, water parks and vineyards; modelled on ancient Persia, England, Japan, France, Korea, Spain, the Inca and Roman empires \u2013 of those she could immediately recognise.</p><p>And then a few touches of fancy, such as the segment they were in, with the oaks shaped into animals. Further off, a dramatic slew of moss-coated sculptures, with water pouring out from every nook and cranny. Then a <i>dynamic</i> garden, with plants blasting each other with discharges of pollen, set-up in a simple eight-beat rhythm. And a massive Baobab, its limbs plated with a forest of tiny bonsai trees.</p><p>\u201cWhat\u2019s your safety level for all this?\u201d she asked. If he\u2019d chosen total safety, he wouldn\u2019t have needed her off his property, as the Machines wouldn\u2019t have allowed his creations to be damaged by her adventure. But surely he wouldn\u2019t have left such artistic creation vulnerable to the fallout of Adventurers or random accidents...</p><p>&nbsp;\u201cZero,\u201d he said.</p><p>\u201cWhat?\u201d No-one choose zero safety; it just wasn\u2019t done.</p><p>\u201cAs I said, no help from the Machines.\u201d He looked at her somewhat shyly, as she stared in disbelief. \u201cIt\u2019s been destroyed twice so far, but I\u2019ll see it out to the end.\u201d</p><p>No wonder he\u2019d wanted her out... He only had himself to count on for protection, so had to chase out any potential disturbances. She felt deeply moved by the whole grandiose, proud and quixotic project of his. Acting almost \u2013 almost \u2013 without thinking, she drew out a battered papyrus scroll: \u201cCan you keep this for me?\u201d</p><p>\u201cWhat is it?\u201d he asked, before frowning and tearing up his painting with a sigh. Only then did he look at the scroll, and at her.</p><p>\u201cIt\u2019s my grandfather\u2019s diary,\u201d she said, \u201cwith my own annotations. It\u2019s been of great use and significance to me.\u201d Of course it had been \u2013 the Machines would have gone to great pains to integrate such a personal and significant item deeply into her adventures. \u201cCould you keep it for my children?\u201d When she finally found the right person to have them with, she added mentally. Ever since her split with Albert... No, that was definitely not what she needed to be thinking right now. Focus instead on this gorgeous painter, name still unknown, and his impossible dreams.</p><p>\u201cWhat was he like?\u201d he asked.</p><p>\u201cMy grandfather? Odd, and a bit traditional. He brought me up. And when we were all grown up, all his grandchildren, he decided we needed, like in ancient times, to lose our eldest generation.\u201d</p><p>\u201cHe died?\u201d The painter sounded sceptical; there were a few people choosing to die, of course, but those events were immensely rare and widely publicised.</p><p>\u201cNo, he simply had his intelligence boosted. Recursively. And he withdrew from human society, to have direct philosophical conversations with the Machines.\u201d</p><p>He thought for a while, then took the scroll from her, deliberately brushing her fingers as he did so. \u201cI\u2019ll keep this. And I\u2019m sure your children will find their ways to me.\u201d An artefact, handed down and annotated through the generations, and entrusted in a quirky landscape artist who laboured obsessively with zero safety level? It was such a beautiful story hook, there was no way the Machines wouldn\u2019t make use of it. As long as one of her children had the slightest adventurous streak, they\u2019d end up here.</p><p>\u201cThis feels rather planned,\u201d he said. \u201cI expect it\u2019s not exactly a coincidence you ended up here.\u201d</p><p>\u201cOf course not.\u201d He was reclusive, brilliant, prickly; Ishtar realised a subtle seduction would be a waste of time. \u201cShall we make love?\u201d, she asked directly.</p><p>\u201cOf course.\u201d He motioned her towards a bed of soft blue moss that grew in the midst of the orchids. \u201cI have to warn you, I insist that the pleasure-enhancing drugs we use be entirely natural, and picked from my garden. Let me show you around first, and you can make your choice.\u201d They wandered together through the garden, shedding their clothes and choosing their pleasures.</p><p>Later, after love, she murmured \u201cunpause\u201d before the moment could fade. \u201cGet off my property!\u201d he murmured, then kissed her for the last time. She dived away, running from the vineyard and onto the street, bullets exploding overhead and at her feet.</p><p>Three robot gangsters roared through the street in a 1920 vintage car, spraying bullets from their Tommy guns. The bullets ricocheted off the crystal pavement and gently moving wind-houses, causing the passer-by\u2019s (all of whom had opted for slight excitement that week) to duck enthusiastically to the floor, with the bullets carefully and barely missing them. Diving round a conveniently placed market stall a few seconds before it exploded in a hail of hurtling lead, she called up her friend:</p><p>\u201cSigsimund, bit busy to talk now, but can you meet me in the Temple of Tea in about five...\u201d a laser beam from a circling drone sliced off the pavement she was standing on, while three robot samurai rose to bar her passage, katanas drawn (many humans were eager and enthusiastic to have a go at being evil masterminds, but few would settle for being minions). \u201c...in about ten minutes? Lovely, see ya there!\u201d</p><p>It actually took her twelve minutes to reach the Temple (she\u2019d paused to vote \u2018yes\u2019 on the question as to whether to bring back extinct species to the new Amazonian Rainforests, and to do some light research on the Stradivari). It was nearly-safe ground, meaning that adventures were only very rarely permitted to intrude on it, just enough to give a slight frisson of background excitement. She would certainly be safe for the duration of her conversation.</p><p>The priest, in gold and white robes with huge translucent butterfly wings, bowed to her as she entered. \u201cI shall need to Know all about you,\u201d he intoned, to her nodded agreement.</p><p>Sigsimund waved at her from a floating table that was making its way serenely through the temple\u2019s many themed rooms, floating on a river that brought them through the Seventy-Seven Stages of Civilization. Ishtar swam out to join her, taking her seat at the gondola-shaped table.</p><p>\u201cBy your current lack of clothes,\u201d Sigsimund said, \u201cI take it you\u2019ve been putting my advice into practice.\u201d</p><p>Sigsimund was one of those who wished above all else to help their fellow human beings. In a world without poverty, disease or death, she specialised in the remaining areas of personal pain: relationship difficulties, jealousies and emotional turmoil. It was quite a popular and respected role, since most humans were unwilling to get rid of those negative emotions artificially, lest they become less than human; but at the same time, they appreciated those who ensured they didn\u2019t have to suffer the full sting of these painful experiences unaided.</p><p>Sigsimund had first developed an interest in Ishtar when her long term relationship with Albert had fallen apart. Albert was a physicist (by mutual agreement with the Machines, physics was one of the areas where research was reserved to humans; so all fundamental new discoveries about the nature of reality were entirely triumphs of the unaided human mind), and his need for monogamy had ultimately proved incompatible with Ishtar\u2019s desires. In Sigsimund\u2019s expert analysis, the first stage in Ishtar\u2019s recuperation was a lot of casual sex; she disapproved of Ted for this reason, feeling her friend wasn\u2019t leaving enough time for play before starting another serious relationship (she dismissed comparisons with her own 78-year relationship, started two days after her previous one ended, with the line \u201cwe ain\u2019t all the same, you know\u201d).</p><p>So as Ishtar recounted her adventure, while strategically wrapping herself in an embroidered sarong that fell from the temple\u2019s sarong-tree, Sigsimund started positively glowing.</p><p>\u201cFabulous!\u201d she said. \u201cI couldn\u2019t have designed it better. And, even more perfect, you\u2019ll never see or hear from him again, and didn\u2019t even get his name. Maybe we can move on to the next step of my recuperation curriculum?\u201d</p><p>\u201cGo on\u201d, Ishtar said suspiciously.</p><p>\u201cHave you considered spending some time as a man? It would broaden your perspectives on things.\u201d</p><p>Ishtar stared fixedly for a full twenty seconds, hoping to convey the full ridiculousness of the suggestion. \u201cI am entirely convinced,\u201d she said, \u201cthat that would be entirely unhelpful.\u201d</p><p>\u201cAs someone who has been mending people\u2019s psyches for a hundred and seven years, and who has access to your full psychological profile and detailed recordings of your activities and emotions for the last decade, let me say that I am entirely convinced that it would be entirely helpful.\u201d A passing clockwork insect dropped a plump apple-strawberry into her hand, and she devoured it. \u201cYou should learn to live a little.\u201d</p><p>\u201cWhy don\u2019t you ever have Adventures?\u201d, Ishtar asked. \u201cYou\u2019re the one who should live a bit.\u201d</p><p>\u201cOh, just let me continue spreading happiness and healing pain all around me. Adventures aren\u2019t really my thing.\u201d</p><p>\u201c99.7% of people have had adventures,\u201d Ishtar said, lifting a lime sherbet from a leaf floating past. \u201cThat makes you, my friend, a member of a tiny and dweebish minority.\u201d</p><p>\u201cYes, but most people just have short adventures when they\u2019re teenagers or on honeymoons. Only...\u201d she let the thought out to the world, and the answer appeared in her mind a second later: \u201c...only 32% of people have adventures as a major part of their lifestyle. And as for people like you, whose whole lives revolve around adventures, the number drops precipitously... J\u2019accuse <i>you</i> of being the member of a tiny and dweebish minority. Also, I need time to learn Akkadian properly, if I\u2019m going to be any use in my next dig. Incidentally, what do you think of my new face? You haven\u2019t commented on it.\u201d</p><p>\u201cI like it,\u201d Ishtar said, politely. \u201cVery... colourful. Ethnic, even.\u201d Though of no ethnicity known to man or beast, she added mentally, and the universe is very thankful for that fact. Though maybe some of the more brightly coloured lizards could find some small aspects of it alluring, she conceded. In dim light. If they didn\u2019t have to see it all at once.</p><p>\u201cI find it brings out the best in my friends,\u201d Sigsimund said with a huge rainbow grin. \u201cNobody likes it, nobody dares say anything.\u201d</p><p>\u201cWhereas I hope that is not the verdict you shall give on my tea,\u201d said the temple priest, holding a tiny cup aloft as his ivory throne descended lazily from the sky. \u201cMadame Ishtar, I have downloaded your full history, biological records, run thousands of simulations with models of your taste buds, Glossopharyngeal nerve and brain stem; looked through your history for all pleasant and unpleasant taste associations I could find, analysed your stomach contents and recent consumptions, cast your horoscope, computed your chi, and peered deep into your chakras. And added a bit of flair and feeling. I believe this is the best cup of tea you ever had.\u201d</p><p>The smell hit her before her hand even touched the cup, a light scent of burning grass that catapulted her into her childhood, dancing with her sisters in front of the traditional forest fires. It was the young girl and the woman who both clasped their finger across the cup, reunited across time by the single perfect aroma. She wasn\u2019t conscious of drinking the tea, but she must have, for it exploded in her mouth, hot, spicy, cool, fruity, chocolaty and lemony. The tastes chased each other across her tongue and nerves, alternating with rapid and smooth transitions. She had just enough time to appreciate one taste combination, register a dozen half-formed marvellous impressions, feel that her joy was about to peak \u2013 but already the transition had happened, and she was in a new taste-world. She lost the consciousness of her tongue and nose; the sensations were applied directly to her brain, the mediating machinery stripped away.</p><p>And then, in three glorious seconds, it was over, and only one word could describe her feelings with sufficient poetry and precision:</p><p>\u201cWow!\u201d</p><p>She also recognised the tell-tale sign of her dopamine system being inhibited. This was an essential precaution with any super-stimulus, to prevent addiction: though she <i>liked</i> the tea more than anything in recent memory, she wasn\u2019t filled with an irresistible <i>want</i> for it. It was just a perfect moment for her to treasure.</p><p>\u201cIt is traditional,\u201d the priest intoned, \u201cfor guests to leave a little something in exchange.\u201d</p><p>Ishtar thought deeply. She wasn\u2019t that used to ritual situations, and she couldn\u2019t think of anything she had of comparable value to exchange. \u201cWell,\u201d she said, \u201cI did spend two decades as house-wife, a while back.\u201d The priest\u2019s expression didn\u2019t change. \u201cOne of the things I became good at was... baking cookies.\u201d Still no sign as to what the priest was thinking, in any direction. \u201cI did a whole lot of chemical research, of course, and some of them turned out sublime... One batch in particular, took my breath away and pounded my lungs with the sheer joy of being alive and tasting existence itself. And chocolate. I can... I can share the memory of that with you. It\u2019s... very private, so please don\u2019t go bandying it around to anyone...\u201d</p><p>\u201cThat is...\u201d, the priest said, as the memory was downloaded into his mind, \u201c...generous.\u201d. He bowed and his throne levitated away.</p><p>They sat in silence for a minute, until Sigsimund felt it was time to return their thoughts to trivialities. \u201cBy the way, I had a chat with Nero,\u201d she said.</p><p>\u201cYou did?\u201d Ishtar blinked, struggling to conceive of Nero as anything else but the magnificent and constant bane of her existence, the perfect enemy.</p><p>\u201cOh yes,\u201d Sigsimund said, \u201cHe\u2019s another one of my friends; he\u2019s doing quite well, in fact, and is trending happy and well balanced and looking around for a healthy, low hassle relationship.\u201d</p><p>\u201cWell, I\u2019m happy for him, I suppose...\u201d</p><p>\u201cIn fact,\u201d Sigsimund said, wagging eyebrows the size of maple leaves as suggestively as she could, \u201cI would go so far as to say, that if you\u2019re interested (and I recommend you to be interested) the rivalry between you might be amenable to... ending up in the traditional way, if you catch my drift. No pressure, just a thought to keep in mind when you both end up sweaty and wrestling over an exploding volcano.\u201d</p><p>\u201cThat\u2019s interesting,\u201d Ishtar allowed, grudgingly. She sat in silence for a while, a stray thought nagging the rest of her brain for attention. She brought back the memory of the picture of Ted in chains. They had felt a little odd and fake, like they were made of plastic. Or maybe not weighing as much as they should. Like there wasn\u2019t enough gravity. Not space or the moon but maybe...</p><p>\u201cFor the moment, I need a bit of a break,\u201d she said. \u201cYou want to go skiing?\u201d Sigsimund shook her head. \u201cWith rockets?\u201d Still more head shaking. \u201cOn the mountains of Mars?\u201d</p><p>\u201cNow you\u2019re talking!\u201d Then Sigsimund\u2019s gaze grew a little more serious, staring over her friend\u2019s shoulder. \u201cThough I see you\u2019ll have to take the long route?\u201d</p><p>Ishtar turned round. There, paddling slowly towards her through the stream, was the largest mechanical tiger she\u2019d ever seen, its diamond-and-steel jaws glowing in the light of the temple as the other guests arranged themselves around it to witness the spectacle. Smoke belched from its nostrils alongside a tinny synthesised version of Nero 2\u2019s laughter. Ishtar\u2019s hand reached for her weapon, which she didn\u2019t have, so it closed around a cheese knife instead. \u201cIf I make it, meet you tomorrow at the little Italian starport at the foot of Olympus Mons, okay?\u201d</p><p>\u201cSee ya there,\u201d Sigsimund said, and saluted, as her friend gave a blood re-curdling scream and launched herself over a fleet of tiny sailing ships battling each other, cheese knife pointed directly at the tiger\u2019s clockwork heart.</p></body></html>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"etDohXtBrXd8WqCtR": 12}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "sMsvcdxbK2Xqx8EHr", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": null}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "neutral", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 111, "baseScore": 132, "extendedScore": null, "score": 0.000262, "legacy": true, "legacyId": "11656", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 132, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 118, "af": false, "version": "1.2.0", "pingbacks": {"Posts": ["pK4HTxuv6mftHXWC3", "HLqWn5LASfhhArZ7w"]}, "moderationGuidelinesVersion": "1.1.0", "customHighlightVersion": null, "afDate": null, "afBaseScore": 13, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-25T15:41:42.370Z", "modifiedAt": null, "url": null, "title": "Merry Newtonmas LW. Have some rationalist music.", "slug": "merry-newtonmas-lw-have-some-rationalist-music", "viewCount": null, "lastCommentedAt": "2021-05-31T21:46:12.327Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Spurlock", "createdAt": "2010-03-24T17:13:19.572Z", "isAdmin": false, "displayName": "Spurlock"}, "userId": "mK7rKWbkuoDsm3aQb", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/293SmHN2mozWzHYxG/merry-newtonmas-lw-have-some-rationalist-music", "pageUrlRelative": "/posts/293SmHN2mozWzHYxG/merry-newtonmas-lw-have-some-rationalist-music", "linkUrl": "https://www.lesswrong.com/posts/293SmHN2mozWzHYxG/merry-newtonmas-lw-have-some-rationalist-music", "postedAtFormatted": "Sunday, December 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Merry%20Newtonmas%20LW.%20Have%20some%20rationalist%20music.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMerry%20Newtonmas%20LW.%20Have%20some%20rationalist%20music.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F293SmHN2mozWzHYxG%2Fmerry-newtonmas-lw-have-some-rationalist-music%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Merry%20Newtonmas%20LW.%20Have%20some%20rationalist%20music.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F293SmHN2mozWzHYxG%2Fmerry-newtonmas-lw-have-some-rationalist-music", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F293SmHN2mozWzHYxG%2Fmerry-newtonmas-lw-have-some-rationalist-music", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 99, "htmlBody": "<p><strong>Related to:</strong> <a href=\"/lw/5h8/so_youve_changed_your_mind/\">So You've Changed Your Mind</a></p>\n<p>Basically <a href=\"http://www.squidrock.com/\">my band</a> did an album whose theme was \"change your mind\", largely inspired by <a href=\"http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind\">the LW sequence</a>. It's not Bayes Theorem in rhyming form, but the subject matter and spirit of it should (hopefully) resonate with LW readers.</p>\n<p>Anyway, for obvious reasons I'm curious what you'll think of it, and so in the spirit of giving here's a <a href=\"http://www.squidrock.com/album_lw.zip\">direct link to download the album for free</a>. If you don't want to download the whole thing immediately, you can also stream each song via <a href=\"http://doctorsquid.bandcamp.com/album/doctor-squid-changes-the-channel\">Bandcamp</a>.</p>\n<p>Sound-wise, you'll probably like it if you like 90s rock/pop/alternative.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"AXhEhCkTrHZbjXXu3": 1, "KDpqtN3MxHSmD4vcB": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "293SmHN2mozWzHYxG", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 29, "baseScore": 31, "extendedScore": null, "score": 6.8e-05, "legacy": true, "legacyId": "11664", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 27, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 8, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["W2ZymNJFWbirwctxo"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-25T23:14:04.223Z", "modifiedAt": "2021-12-01T23:27:40.003Z", "url": null, "title": "[Link, Humor] The Best Christmas Ever", "slug": "link-humor-the-best-christmas-ever", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:04.624Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "pjeby", "createdAt": "2009-02-27T23:51:22.854Z", "isAdmin": false, "displayName": "pjeby"}, "userId": "Zzxr5JZpkitaNxL4Q", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Qgv4RrQSb68fgqpXW/link-humor-the-best-christmas-ever", "pageUrlRelative": "/posts/Qgv4RrQSb68fgqpXW/link-humor-the-best-christmas-ever", "linkUrl": "https://www.lesswrong.com/posts/Qgv4RrQSb68fgqpXW/link-humor-the-best-christmas-ever", "postedAtFormatted": "Sunday, December 25th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%2C%20Humor%5D%20The%20Best%20Christmas%20Ever&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%2C%20Humor%5D%20The%20Best%20Christmas%20Ever%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQgv4RrQSb68fgqpXW%2Flink-humor-the-best-christmas-ever%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%2C%20Humor%5D%20The%20Best%20Christmas%20Ever%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQgv4RrQSb68fgqpXW%2Flink-humor-the-best-christmas-ever", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQgv4RrQSb68fgqpXW%2Flink-humor-the-best-christmas-ever", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 23, "htmlBody": "<p>In the holiday spirit, here's a comic that many LWers may appreciate...\u00a0 and perhaps even find useful for illustrating a certain, ah, topic:</p>\n<p><a href=\"http://web.archive.org/web/20100131211832/http://www.strippycomics.com/2010/01/05/gifto/\">http://www.strippycomics.com/2010/01/05/gifto/</a></p>\n<p>Enjoy!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Qgv4RrQSb68fgqpXW", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 18, "baseScore": 11, "extendedScore": null, "score": 8.209777419776196e-07, "legacy": true, "legacyId": "11665", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 5, "af": false, "version": "1.1.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-26T04:30:46.000Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The Allais Paradox", "slug": "seq-rerun-the-allais-paradox", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:03.977Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ratyky2Q2Ap3iPh5d/seq-rerun-the-allais-paradox", "pageUrlRelative": "/posts/ratyky2Q2Ap3iPh5d/seq-rerun-the-allais-paradox", "linkUrl": "https://www.lesswrong.com/posts/ratyky2Q2Ap3iPh5d/seq-rerun-the-allais-paradox", "postedAtFormatted": "Monday, December 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20Allais%20Paradox&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20Allais%20Paradox%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fratyky2Q2Ap3iPh5d%2Fseq-rerun-the-allais-paradox%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20Allais%20Paradox%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fratyky2Q2Ap3iPh5d%2Fseq-rerun-the-allais-paradox", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fratyky2Q2Ap3iPh5d%2Fseq-rerun-the-allais-paradox", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 151, "htmlBody": "<p>Today's post, <a href=\"/lw/my/the_allais_paradox/\">The Allais Paradox</a> was originally published on 19 January 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Offered choices between gambles, people make decision-theoretically inconsistent decisions.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/r/discussion/lw/8zz/seq_rerun_trust_in_math/\">Trust in Math</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ratyky2Q2Ap3iPh5d", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 8, "extendedScore": null, "score": 8.210951063885969e-07, "legacy": true, "legacyId": "11681", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 3, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["zJZvoiwydJ5zvzTHK", "bw3A4PRXPqoWWA98N", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-26T10:34:52.053Z", "modifiedAt": null, "url": null, "title": " Welcome to Less Wrong!, part 2?", "slug": "welcome-to-less-wrong-part-2", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:03.967Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EduL5N2FSGHxLqeFE/welcome-to-less-wrong-part-2", "pageUrlRelative": "/posts/EduL5N2FSGHxLqeFE/welcome-to-less-wrong-part-2", "linkUrl": "https://www.lesswrong.com/posts/EduL5N2FSGHxLqeFE/welcome-to-less-wrong-part-2", "postedAtFormatted": "Monday, December 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%20Welcome%20to%20Less%20Wrong!%2C%20part%202%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%20Welcome%20to%20Less%20Wrong!%2C%20part%202%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEduL5N2FSGHxLqeFE%2Fwelcome-to-less-wrong-part-2%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%20Welcome%20to%20Less%20Wrong!%2C%20part%202%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEduL5N2FSGHxLqeFE%2Fwelcome-to-less-wrong-part-2", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEduL5N2FSGHxLqeFE%2Fwelcome-to-less-wrong-part-2", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 71, "htmlBody": "<p><a href=\"/lw/b9/introductory_thread/\">Welcome to Less Wrong!</a> has over 1300 comments, but the display only offers 500 newest or 500 oldest, which makes it difficult if anyone wants to look at the whole thing. At a minimum, I recommend a second Welcome! post, but it might also be good to break up the first one (Welcome 1a and 1b, perhaps), and have a policy of a new Welcome post every 700 comments or so.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EduL5N2FSGHxLqeFE", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 15, "extendedScore": null, "score": 8.212300754320568e-07, "legacy": true, "legacyId": "11683", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["CG9AEXwSjdrXPBEZ9"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-26T14:44:01.763Z", "modifiedAt": null, "url": null, "title": "Future of Moral Machines - New York Times [link]", "slug": "future-of-moral-machines-new-york-times-link", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:04.593Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dr_Manhattan", "createdAt": "2010-12-16T13:46:11.412Z", "isAdmin": false, "displayName": "Dr_Manhattan"}, "userId": "rhNqxRkdTL5KSCuJk", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/o3MxeCnooNRXm8yNB/future-of-moral-machines-new-york-times-link", "pageUrlRelative": "/posts/o3MxeCnooNRXm8yNB/future-of-moral-machines-new-york-times-link", "linkUrl": "https://www.lesswrong.com/posts/o3MxeCnooNRXm8yNB/future-of-moral-machines-new-york-times-link", "postedAtFormatted": "Monday, December 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Future%20of%20Moral%20Machines%20-%20New%20York%20Times%20%5Blink%5D&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFuture%20of%20Moral%20Machines%20-%20New%20York%20Times%20%5Blink%5D%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo3MxeCnooNRXm8yNB%2Ffuture-of-moral-machines-new-york-times-link%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Future%20of%20Moral%20Machines%20-%20New%20York%20Times%20%5Blink%5D%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo3MxeCnooNRXm8yNB%2Ffuture-of-moral-machines-new-york-times-link", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo3MxeCnooNRXm8yNB%2Ffuture-of-moral-machines-new-york-times-link", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>http://opinionator.blogs.nytimes.com/2011/12/25/the-future-of-moral-machines/</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "o3MxeCnooNRXm8yNB", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 1, "extendedScore": null, "score": 8.213224602280125e-07, "legacy": true, "legacyId": "11684", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 10, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-26T16:05:23.386Z", "modifiedAt": null, "url": null, "title": "Organizing the FOOM debate", "slug": "organizing-the-foom-debate", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:04.146Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Curiouskid", "createdAt": "2011-05-13T23:30:04.967Z", "isAdmin": false, "displayName": "Curiouskid"}, "userId": "Y4xxvuP743fwKcZQY", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iZQRkEJ5yQ8Hb3N3n/organizing-the-foom-debate", "pageUrlRelative": "/posts/iZQRkEJ5yQ8Hb3N3n/organizing-the-foom-debate", "linkUrl": "https://www.lesswrong.com/posts/iZQRkEJ5yQ8Hb3N3n/organizing-the-foom-debate", "postedAtFormatted": "Monday, December 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Organizing%20the%20FOOM%20debate&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AOrganizing%20the%20FOOM%20debate%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiZQRkEJ5yQ8Hb3N3n%2Forganizing-the-foom-debate%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Organizing%20the%20FOOM%20debate%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiZQRkEJ5yQ8Hb3N3n%2Forganizing-the-foom-debate", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiZQRkEJ5yQ8Hb3N3n%2Forganizing-the-foom-debate", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 39, "htmlBody": "<p>Why was the<a href=\"http://www.google.com/url?q=http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate&amp;sa=U&amp;ei=NJr4Tuq8D-adiAK8vLCnDg&amp;ved=0CAQQFjAA&amp;client=internal-uds-cse&amp;usg=AFQjCNFlCAtoiFHI5cTLeOkKOZqNXAnQWA\"> FOOM debate</a> not conducted using any <a href=\"http://wiki.lesswrong.com/wiki/Debate_tools\">debate tools</a>? Perhaps that would enable agreement theorems to actually work and the debate to be resolved. Would someone want to go through the debate and try to organize it?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iZQRkEJ5yQ8Hb3N3n", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 9, "baseScore": 6, "extendedScore": null, "score": 8.213526311954407e-07, "legacy": true, "legacyId": "11686", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-26T16:29:46.539Z", "modifiedAt": null, "url": null, "title": "Summary of \"The Straw Vulcan\"", "slug": "summary-of-the-straw-vulcan", "viewCount": null, "lastCommentedAt": "2020-01-23T18:41:23.212Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "alexvermeer", "createdAt": "2010-08-13T16:28:34.576Z", "isAdmin": false, "displayName": "alexvermeer"}, "userId": "3bK6aDQviGG3ovuDJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/z9hfbWhRrY2Pwwrgi/summary-of-the-straw-vulcan", "pageUrlRelative": "/posts/z9hfbWhRrY2Pwwrgi/summary-of-the-straw-vulcan", "linkUrl": "https://www.lesswrong.com/posts/z9hfbWhRrY2Pwwrgi/summary-of-the-straw-vulcan", "postedAtFormatted": "Monday, December 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Summary%20of%20%22The%20Straw%20Vulcan%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASummary%20of%20%22The%20Straw%20Vulcan%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz9hfbWhRrY2Pwwrgi%2Fsummary-of-the-straw-vulcan%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Summary%20of%20%22The%20Straw%20Vulcan%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz9hfbWhRrY2Pwwrgi%2Fsummary-of-the-straw-vulcan", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fz9hfbWhRrY2Pwwrgi%2Fsummary-of-the-straw-vulcan", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1664, "htmlBody": "<p><strong>Followup to:&nbsp;</strong><a href=\"/lw/8ko/communicating_rationality_to_the_public_julia/\">Communicating rationality to the public: Julia Galef's \"The Straw Vulcan\"</a></p>\n<p><img style=\"float: right;\" src=\"http://alexvermeer.com/wp-content/uploads/spock-scarecrow-final-2-168x300.jpg\" alt=\"The Straw Vulcan\" width=\"168\" height=\"300\" />I wrote a summary of Julia Galef's \"The Straw Vulcan\" presentation from Skepticon 4. Note that it is written in my own words, but all of the ideas should be credited to Julia and her presentation (unless I unintentionally misrepresent any of them!).</p>\n<p>---</p>\n<p>The classic Hollywood example of rationality is the Vulcans from Star Trek. They are depicted as an ultra-rational race that has eschewed all emotion from their lives.</p>\n<p>But is this truly rational? What <em>is</em> rationality?</p>\n<p>A &ldquo;Straw Vulcan&rdquo;&mdash;an idea originally defined on&nbsp;<a onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','tvtropes.org']);\" href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/StrawVulcan\">TV Tropes</a>&mdash;is a straw man used to show that emotion is better than logic. Traditionally, you have your &lsquo;rational&rsquo; character who thinks perfectly &lsquo;logically&rsquo;, but then ends up running into trouble, having problems, or failing to achieve what they were trying to achieve.</p>\n<p>These characters have a sort of <em>fake</em> rationality. They don&rsquo;t fail because rationality failed, but because they <em>aren&rsquo;t actually being rational. </em>Straw Vulcan rationality is <em>not</em> the same thing as actual rationality.</p>\n<h2>What is real rationality?</h2>\n<p>There are two different concepts that we refer to when we use the word &lsquo;rationality&rsquo;:</p>\n<blockquote>\n<p>1. The method of obtaining an accurate view of reality. (<strong>Epistemic Rationality</strong>) &mdash; Learning new things, updating your beliefs based on the evidence, being as accurate as possible, being as close to what is true as possible, etc.</p>\n</blockquote>\n<blockquote>\n<p>2. The method of achieving your goals. (<strong>Instrumental Rationality</strong>) &mdash; Whatever your goals are, be them selfish or altruistic, there are better and worse ways to achieve them, and instrumental rationality helps you figure this out.</p>\n</blockquote>\n<p>These two concepts are obviously related. You want a clear model of the world to be able to achieve your goals. You also may have goals related to obtaining an accurate model of the world.</p>\n<p>How do these concepts of rationality relate to Straw Vulcan rationality? What is the Straw Vulcan conception of rationality?</p>\n<h2>&ldquo;Straw Vulcan&rdquo; Rationality Principles</h2>\n<h3>Straw Vulcan&nbsp;Principle #1: Being rational means expecting other people to be rational too.</h3>\n<p>Galef uses an example from Star Trek where Spock, in an attempt to protect the crew of the crashed ship, decides to show aggression against the local aliens so that they will be scared and run away. Instead, they are angered by the display of aggression and attack even more fiercely, much to Spock&rsquo;s dismay and confusion.</p>\n<p>But this isn&rsquo;t being rational! Spock&rsquo;s model of the world is severely tarnished by his silly expectation for everyone else to be as rational as he would be. Real rationality would require you to try to understand all aspects of the situation and act accordingly.</p>\n<h3>Straw Vulcan&nbsp;Principle #2: Being rational means never making a decision until you have all the information.</h3>\n<p>This seems to assume that the only important criteria for making decisions is that you make the best one given <em>all</em> the information. But what about things like <em>time</em> and <em>risk</em>? Surely those should factor into your decisions too.</p>\n<p>We know intuitively that this is true. If you want a really awesome sandwich you may be willing to pay an extra $1.00 for some cheese, but you wouldn&rsquo;t pay $300 for a small increase in the quality of a sandwich. You want the <em>best possible outcome</em>, but this requires simultaneously weighing various things like time, cost, value, and risk.</p>\n<p>What is the most rational way to find a partner? Take this example from Gerd Gigerenzer, a well-respected psychology describing how a rationalist would find a partner:</p>\n<blockquote>\n<p>&ldquo;He would have to look at the probabilities of various consequences of marrying each of them&mdash;whether the woman would still talk to him after they&rsquo;re married, whether she&rsquo;d take care of their children, whatever is important to him&mdash;and the utilities of each of these&hellip;After many years of research he&rsquo;d probably find out that his final choice had already married another person who didn&rsquo;t do these computations, and actually just fell in love with her.&rdquo;</p>\n</blockquote>\n<p>But clearly this isn&rsquo;t optimal decision making. The <em>rational</em> thing to do isn&rsquo;t to merely wait until you have as much information as you can possibly have. You need to factor in things like how long the research is taking, the decreasing number of available partners as time passes, etc.</p>\n<h3>Straw Vulcan&nbsp;Principle #3: Being rational means never relying on intuition.</h3>\n<p>Straw Vulcan rationality says that anything intuition-based is illogical. But what <em>is</em> intuition?</p>\n<p>We have two systems in our brains, which have been unexcitingly called System 1 and System 2.</p>\n<blockquote>\n<p><strong>System 1</strong>&mdash;the <strong>intuitive system</strong>&mdash;is the older of the two and allows us to make quick, automatic judgments using shortcuts (i.e. heuristics) that are usually good most of the time, all while requiring very little of your time and attention.</p>\n<p><strong>System 2</strong>&mdash;the <strong>deliberative system</strong>&mdash;is the newer of the two and allows us to do things like abstract hypothetical thinking and make models that explain unexpected events. System 2 tends to do better when you have more resources and more time and worse when there are many factors to consider and you have limited time.</p>\n</blockquote>\n<p>Take a sample puzzle: A bat and ball together cost $1.10. If the bat costs $1 more than the ball, how much does the ball cost?</p>\n<p>When a group of Princeton students were given this question, about 50% of them got it wrong. The correct answer is $0.05, since then the bat would cost $1.05 for a total of $1.10. The <em>wrong</em> answer of $0.10 is easily generated (incorrectly) by our System 1, and our System 2 accepts it without question.</p>\n<p>Your System 1 is prone to biases, and it is also incredibly powerful. Our intuition tends to do well with purchasing decisions or other choices about our personal lives. System 1 is also very powerful for an expert. Chess grandmasters can glance at a chessboard and say, &ldquo;white checkmates in three moves,&rdquo; because of the vast amount of time and mental effort spent playing chess and building up a mental knowledge base about it.</p>\n<p>Intuition can be <em>bad</em> and less reliable when based on something not relevant to the task at hand or when you don&rsquo;t have expert knowledge on the topic. You opinions of AI may be heavily influenced by scifi movies that have little basis in reality.</p>\n<p><strong>The main thing to take away from this System 1 and 2 split is that both systems have strengths and weaknesses,</strong> and rationality is about finding the best path&mdash;using both systems at the right times&mdash;to epistemic and instrumental rationality.</p>\n<p>Being &ldquo;too rational&rdquo; usually means you are using your System 2 brain intentionally but poorly. For example, teenagers were criticized in an article for being &ldquo;too rational&rdquo; because they could reason themselves into things like drugs and speeding. But this isn&rsquo;t a problem with being too rational; it&rsquo;s a problem with being very <em>bad</em> at System 2 reasoning!</p>\n<h3>Straw Vulcan&nbsp;Principle #4: Being rational means not having emotions.</h3>\n<p>Rationality and emotions are often portrayed in a certain way in Straw Vulcan rationalists, such as when Spock is excited to see that Captain Kirk isn&rsquo;t dead, and then quickly covers up his emotions. The simplistic Hollywood portrayal of emotions and rationality is as follows:</p>\n<p><img title=\"screen-shot-2011-11-26-at-3-58-00-pm\" src=\"http://alexvermeer.com/wp-content/uploads/screen-shot-2011-11-26-at-3-58-00-pm-300x156.png\" alt=\"\" width=\"300\" height=\"156\" /></p>\n<p>Note that emotions <em>can</em> get in the way of taking actions on our goals. For example, anxiety causes us to overestimate risks; depression causes us to underestimate how much we will enjoy an activity; and feeling threatened or vulnerable causes us to exhibit more superstitious behavior and and likely to see patterns that don&rsquo;t exist.</p>\n<p>But emotions are also important for making the decisions themselves. Without having any emotional desires we would have no reason to have goals in the first place. You would have no motivations to choose between a calm beach and a nuclear waste site for your vacation. Emotions are <em>necessary</em> for forming goals; rationality is lame without them!</p>\n<p>[Galef noted in a <a onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','lesswrong.com']);\" href=\"/lw/8ko/communicating_rationality_to_the_public_julia/5br9\">comment</a> that the intended meaning is in line with &ldquo;Emotions are necessary for forming goals among humans, rationality has no normative value to humans without goals.&rdquo;]</p>\n<p>This leaves us with a more accurate portrayal of the relationship between emotions and rationality:</p>\n<p><img title=\"screen-shot-2011-11-26-at-3-58-14-pm\" src=\"http://alexvermeer.com/wp-content/uploads/screen-shot-2011-11-26-at-3-58-14-pm-300x156.png\" alt=\"\" width=\"300\" height=\"156\" /></p>\n<p>How do emotions make us irrational? Emotions can be epistemically irrational if they are based on a false model of the world. You can be angry at your husband for not asking how your presentation at work went, but then upon reflection realize you never told him about it so how would he know it happened? Your anger was based on a false model of reality.</p>\n<p>Emotions can be instrumentally irrational if they get in the way of you achieving your goals. If you feel things are hopeless and there are no ways to change the situation, you may be <em>wrong</em> about that. Your emotions may prevent you from taking necessary actions.</p>\n<p>Our emotions also influence each other. If you have a desire to be liked by others and a desire to sit on a couch all day, you may run into problems. These desires may influence and conflict with each other.</p>\n<p>We can also <em>change </em>our emotions. For example, cognitive behavioral therapy has many exercises and techniques (e.g. Thought Records) for changing your emotions by changing your beliefs.</p>\n<h3>Straw Vulcan Principle #5: Being rational means valuing only quantifiable things, like money, efficiency, or productivity.</h3>\n<p>If it isn&rsquo;t concrete and measurable then there is no reason to value it, right? Things like beauty, love, or joy are just irrational emotions, right?</p>\n<p>What are the problems with this? For starters, money <em>can&rsquo;t</em> be valuable in and of itself, because it is only a means to obtain other valued things. Also, there is no reason to <em>assume</em> that money and productivity are the only things of value.</p>\n<h2>The Main Takeaway</h2>\n<p>Galef finishes off with this final message:</p>\n<blockquote>\n<p>&ldquo;If you think you&rsquo;re acting rationally but you consistently keep getting the wrong answer, and you consistently keep ending worse off than you could be, then the conclusion you should draw from that <strong>is not that rationality is bad, it&rsquo;s that you&rsquo;re bad at rationality.</strong></p>\n<p><strong>In other words, you&rsquo;re doing it wrong!</strong>&rdquo;</p>\n</blockquote>\n<p><img src=\"http://alexvermeer.com/wp-content/uploads/funny-picture-marine-machine-gun-crossbow-fail-420x360.jpg\" alt=\"You're Doing It Wrong!\" width=\"540\" height=\"434\" /></p>\n<p><em>First three images are from measureofdoubt.com &gt;&nbsp;<a href=\"http://measureofdoubt.com/2011/11/26/the-straw-vulcan-hollywoods-illogical-approach-to-logical-decisionmaking/\">The Straw Vulcan: Hollywood&rsquo;s illogical approach to logical decisionmaking</a>.<br />You're Doing It Wrong image from evilbomb.com.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Ng8Gice9KNkncxqcj": 2, "9YFoDPFwMoWthzgkY": 3, "8SfkJYYMe75MwjHzN": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "z9hfbWhRrY2Pwwrgi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 49, "extendedScore": null, "score": 0.000111, "legacy": true, "legacyId": "11687", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 36, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong>Followup to:&nbsp;</strong><a href=\"/lw/8ko/communicating_rationality_to_the_public_julia/\">Communicating rationality to the public: Julia Galef's \"The Straw Vulcan\"</a></p>\n<p><img style=\"float: right;\" src=\"http://alexvermeer.com/wp-content/uploads/spock-scarecrow-final-2-168x300.jpg\" alt=\"The Straw Vulcan\" width=\"168\" height=\"300\">I wrote a summary of Julia Galef's \"The Straw Vulcan\" presentation from Skepticon 4. Note that it is written in my own words, but all of the ideas should be credited to Julia and her presentation (unless I unintentionally misrepresent any of them!).</p>\n<p>---</p>\n<p>The classic Hollywood example of rationality is the Vulcans from Star Trek. They are depicted as an ultra-rational race that has eschewed all emotion from their lives.</p>\n<p>But is this truly rational? What <em>is</em> rationality?</p>\n<p>A \u201cStraw Vulcan\u201d\u2014an idea originally defined on&nbsp;<a onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','tvtropes.org']);\" href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/StrawVulcan\">TV Tropes</a>\u2014is a straw man used to show that emotion is better than logic. Traditionally, you have your \u2018rational\u2019 character who thinks perfectly \u2018logically\u2019, but then ends up running into trouble, having problems, or failing to achieve what they were trying to achieve.</p>\n<p>These characters have a sort of <em>fake</em> rationality. They don\u2019t fail because rationality failed, but because they <em>aren\u2019t actually being rational. </em>Straw Vulcan rationality is <em>not</em> the same thing as actual rationality.</p>\n<h2 id=\"What_is_real_rationality_\">What is real rationality?</h2>\n<p>There are two different concepts that we refer to when we use the word \u2018rationality\u2019:</p>\n<blockquote>\n<p>1. The method of obtaining an accurate view of reality. (<strong>Epistemic Rationality</strong>) \u2014 Learning new things, updating your beliefs based on the evidence, being as accurate as possible, being as close to what is true as possible, etc.</p>\n</blockquote>\n<blockquote>\n<p>2. The method of achieving your goals. (<strong>Instrumental Rationality</strong>) \u2014 Whatever your goals are, be them selfish or altruistic, there are better and worse ways to achieve them, and instrumental rationality helps you figure this out.</p>\n</blockquote>\n<p>These two concepts are obviously related. You want a clear model of the world to be able to achieve your goals. You also may have goals related to obtaining an accurate model of the world.</p>\n<p>How do these concepts of rationality relate to Straw Vulcan rationality? What is the Straw Vulcan conception of rationality?</p>\n<h2 id=\"_Straw_Vulcan__Rationality_Principles\">\u201cStraw Vulcan\u201d Rationality Principles</h2>\n<h3 id=\"Straw_Vulcan_Principle__1__Being_rational_means_expecting_other_people_to_be_rational_too_\">Straw Vulcan&nbsp;Principle #1: Being rational means expecting other people to be rational too.</h3>\n<p>Galef uses an example from Star Trek where Spock, in an attempt to protect the crew of the crashed ship, decides to show aggression against the local aliens so that they will be scared and run away. Instead, they are angered by the display of aggression and attack even more fiercely, much to Spock\u2019s dismay and confusion.</p>\n<p>But this isn\u2019t being rational! Spock\u2019s model of the world is severely tarnished by his silly expectation for everyone else to be as rational as he would be. Real rationality would require you to try to understand all aspects of the situation and act accordingly.</p>\n<h3 id=\"Straw_Vulcan_Principle__2__Being_rational_means_never_making_a_decision_until_you_have_all_the_information_\">Straw Vulcan&nbsp;Principle #2: Being rational means never making a decision until you have all the information.</h3>\n<p>This seems to assume that the only important criteria for making decisions is that you make the best one given <em>all</em> the information. But what about things like <em>time</em> and <em>risk</em>? Surely those should factor into your decisions too.</p>\n<p>We know intuitively that this is true. If you want a really awesome sandwich you may be willing to pay an extra $1.00 for some cheese, but you wouldn\u2019t pay $300 for a small increase in the quality of a sandwich. You want the <em>best possible outcome</em>, but this requires simultaneously weighing various things like time, cost, value, and risk.</p>\n<p>What is the most rational way to find a partner? Take this example from Gerd Gigerenzer, a well-respected psychology describing how a rationalist would find a partner:</p>\n<blockquote>\n<p>\u201cHe would have to look at the probabilities of various consequences of marrying each of them\u2014whether the woman would still talk to him after they\u2019re married, whether she\u2019d take care of their children, whatever is important to him\u2014and the utilities of each of these\u2026After many years of research he\u2019d probably find out that his final choice had already married another person who didn\u2019t do these computations, and actually just fell in love with her.\u201d</p>\n</blockquote>\n<p>But clearly this isn\u2019t optimal decision making. The <em>rational</em> thing to do isn\u2019t to merely wait until you have as much information as you can possibly have. You need to factor in things like how long the research is taking, the decreasing number of available partners as time passes, etc.</p>\n<h3 id=\"Straw_Vulcan_Principle__3__Being_rational_means_never_relying_on_intuition_\">Straw Vulcan&nbsp;Principle #3: Being rational means never relying on intuition.</h3>\n<p>Straw Vulcan rationality says that anything intuition-based is illogical. But what <em>is</em> intuition?</p>\n<p>We have two systems in our brains, which have been unexcitingly called System 1 and System 2.</p>\n<blockquote>\n<p><strong>System 1</strong>\u2014the <strong>intuitive system</strong>\u2014is the older of the two and allows us to make quick, automatic judgments using shortcuts (i.e. heuristics) that are usually good most of the time, all while requiring very little of your time and attention.</p>\n<p><strong>System 2</strong>\u2014the <strong>deliberative system</strong>\u2014is the newer of the two and allows us to do things like abstract hypothetical thinking and make models that explain unexpected events. System 2 tends to do better when you have more resources and more time and worse when there are many factors to consider and you have limited time.</p>\n</blockquote>\n<p>Take a sample puzzle: A bat and ball together cost $1.10. If the bat costs $1 more than the ball, how much does the ball cost?</p>\n<p>When a group of Princeton students were given this question, about 50% of them got it wrong. The correct answer is $0.05, since then the bat would cost $1.05 for a total of $1.10. The <em>wrong</em> answer of $0.10 is easily generated (incorrectly) by our System 1, and our System 2 accepts it without question.</p>\n<p>Your System 1 is prone to biases, and it is also incredibly powerful. Our intuition tends to do well with purchasing decisions or other choices about our personal lives. System 1 is also very powerful for an expert. Chess grandmasters can glance at a chessboard and say, \u201cwhite checkmates in three moves,\u201d because of the vast amount of time and mental effort spent playing chess and building up a mental knowledge base about it.</p>\n<p>Intuition can be <em>bad</em> and less reliable when based on something not relevant to the task at hand or when you don\u2019t have expert knowledge on the topic. You opinions of AI may be heavily influenced by scifi movies that have little basis in reality.</p>\n<p><strong>The main thing to take away from this System 1 and 2 split is that both systems have strengths and weaknesses,</strong> and rationality is about finding the best path\u2014using both systems at the right times\u2014to epistemic and instrumental rationality.</p>\n<p>Being \u201ctoo rational\u201d usually means you are using your System 2 brain intentionally but poorly. For example, teenagers were criticized in an article for being \u201ctoo rational\u201d because they could reason themselves into things like drugs and speeding. But this isn\u2019t a problem with being too rational; it\u2019s a problem with being very <em>bad</em> at System 2 reasoning!</p>\n<h3 id=\"Straw_Vulcan_Principle__4__Being_rational_means_not_having_emotions_\">Straw Vulcan&nbsp;Principle #4: Being rational means not having emotions.</h3>\n<p>Rationality and emotions are often portrayed in a certain way in Straw Vulcan rationalists, such as when Spock is excited to see that Captain Kirk isn\u2019t dead, and then quickly covers up his emotions. The simplistic Hollywood portrayal of emotions and rationality is as follows:</p>\n<p><img title=\"screen-shot-2011-11-26-at-3-58-00-pm\" src=\"http://alexvermeer.com/wp-content/uploads/screen-shot-2011-11-26-at-3-58-00-pm-300x156.png\" alt=\"\" width=\"300\" height=\"156\"></p>\n<p>Note that emotions <em>can</em> get in the way of taking actions on our goals. For example, anxiety causes us to overestimate risks; depression causes us to underestimate how much we will enjoy an activity; and feeling threatened or vulnerable causes us to exhibit more superstitious behavior and and likely to see patterns that don\u2019t exist.</p>\n<p>But emotions are also important for making the decisions themselves. Without having any emotional desires we would have no reason to have goals in the first place. You would have no motivations to choose between a calm beach and a nuclear waste site for your vacation. Emotions are <em>necessary</em> for forming goals; rationality is lame without them!</p>\n<p>[Galef noted in a <a onclick=\"javascript:_gaq.push(['_trackEvent','outbound-article','lesswrong.com']);\" href=\"/lw/8ko/communicating_rationality_to_the_public_julia/5br9\">comment</a> that the intended meaning is in line with \u201cEmotions are necessary for forming goals among humans, rationality has no normative value to humans without goals.\u201d]</p>\n<p>This leaves us with a more accurate portrayal of the relationship between emotions and rationality:</p>\n<p><img title=\"screen-shot-2011-11-26-at-3-58-14-pm\" src=\"http://alexvermeer.com/wp-content/uploads/screen-shot-2011-11-26-at-3-58-14-pm-300x156.png\" alt=\"\" width=\"300\" height=\"156\"></p>\n<p>How do emotions make us irrational? Emotions can be epistemically irrational if they are based on a false model of the world. You can be angry at your husband for not asking how your presentation at work went, but then upon reflection realize you never told him about it so how would he know it happened? Your anger was based on a false model of reality.</p>\n<p>Emotions can be instrumentally irrational if they get in the way of you achieving your goals. If you feel things are hopeless and there are no ways to change the situation, you may be <em>wrong</em> about that. Your emotions may prevent you from taking necessary actions.</p>\n<p>Our emotions also influence each other. If you have a desire to be liked by others and a desire to sit on a couch all day, you may run into problems. These desires may influence and conflict with each other.</p>\n<p>We can also <em>change </em>our emotions. For example, cognitive behavioral therapy has many exercises and techniques (e.g. Thought Records) for changing your emotions by changing your beliefs.</p>\n<h3 id=\"Straw_Vulcan_Principle__5__Being_rational_means_valuing_only_quantifiable_things__like_money__efficiency__or_productivity_\">Straw Vulcan Principle #5: Being rational means valuing only quantifiable things, like money, efficiency, or productivity.</h3>\n<p>If it isn\u2019t concrete and measurable then there is no reason to value it, right? Things like beauty, love, or joy are just irrational emotions, right?</p>\n<p>What are the problems with this? For starters, money <em>can\u2019t</em> be valuable in and of itself, because it is only a means to obtain other valued things. Also, there is no reason to <em>assume</em> that money and productivity are the only things of value.</p>\n<h2 id=\"The_Main_Takeaway\">The Main Takeaway</h2>\n<p>Galef finishes off with this final message:</p>\n<blockquote>\n<p>\u201cIf you think you\u2019re acting rationally but you consistently keep getting the wrong answer, and you consistently keep ending worse off than you could be, then the conclusion you should draw from that <strong>is not that rationality is bad, it\u2019s that you\u2019re bad at rationality.</strong></p>\n<p><strong>In other words, you\u2019re doing it wrong!</strong>\u201d</p>\n</blockquote>\n<p><img src=\"http://alexvermeer.com/wp-content/uploads/funny-picture-marine-machine-gun-crossbow-fail-420x360.jpg\" alt=\"You're Doing It Wrong!\" width=\"540\" height=\"434\"></p>\n<p><em>First three images are from measureofdoubt.com &gt;&nbsp;<a href=\"http://measureofdoubt.com/2011/11/26/the-straw-vulcan-hollywoods-illogical-approach-to-logical-decisionmaking/\">The Straw Vulcan: Hollywood\u2019s illogical approach to logical decisionmaking</a>.<br>You're Doing It Wrong image from evilbomb.com.</em></p>", "sections": [{"title": "What is real rationality?", "anchor": "What_is_real_rationality_", "level": 1}, {"title": "\u201cStraw Vulcan\u201d Rationality Principles", "anchor": "_Straw_Vulcan__Rationality_Principles", "level": 1}, {"title": "Straw Vulcan\u00a0Principle #1: Being rational means expecting other people to be rational too.", "anchor": "Straw_Vulcan_Principle__1__Being_rational_means_expecting_other_people_to_be_rational_too_", "level": 2}, {"title": "Straw Vulcan\u00a0Principle #2: Being rational means never making a decision until you have all the information.", "anchor": "Straw_Vulcan_Principle__2__Being_rational_means_never_making_a_decision_until_you_have_all_the_information_", "level": 2}, {"title": "Straw Vulcan\u00a0Principle #3: Being rational means never relying on intuition.", "anchor": "Straw_Vulcan_Principle__3__Being_rational_means_never_relying_on_intuition_", "level": 2}, {"title": "Straw Vulcan\u00a0Principle #4: Being rational means not having emotions.", "anchor": "Straw_Vulcan_Principle__4__Being_rational_means_not_having_emotions_", "level": 2}, {"title": "Straw Vulcan Principle #5: Being rational means valuing only quantifiable things, like money, efficiency, or productivity.", "anchor": "Straw_Vulcan_Principle__5__Being_rational_means_valuing_only_quantifiable_things__like_money__efficiency__or_productivity_", "level": 2}, {"title": "The Main Takeaway", "anchor": "The_Main_Takeaway", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "27 comments"}], "headingsCount": 10}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 27, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["zuJmtSqt3TsnBTYyu"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-26T22:57:21.157Z", "modifiedAt": "2021-10-28T23:51:20.017Z", "url": null, "title": "Welcome to Less Wrong! (2012)", "slug": "welcome-to-less-wrong-2012", "viewCount": null, "lastCommentedAt": "2021-05-06T21:47:58.869Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "orthonormal", "createdAt": "2009-03-22T16:06:51.665Z", "isAdmin": false, "displayName": "orthonormal"}, "userId": "4fh2AAe3n7oBviyxx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Jztohmx2anxGnp7g9/welcome-to-less-wrong-2012", "pageUrlRelative": "/posts/Jztohmx2anxGnp7g9/welcome-to-less-wrong-2012", "linkUrl": "https://www.lesswrong.com/posts/Jztohmx2anxGnp7g9/welcome-to-less-wrong-2012", "postedAtFormatted": "Monday, December 26th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Welcome%20to%20Less%20Wrong!%20(2012)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWelcome%20to%20Less%20Wrong!%20(2012)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJztohmx2anxGnp7g9%2Fwelcome-to-less-wrong-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Welcome%20to%20Less%20Wrong!%20(2012)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJztohmx2anxGnp7g9%2Fwelcome-to-less-wrong-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FJztohmx2anxGnp7g9%2Fwelcome-to-less-wrong-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1444, "htmlBody": "<div id=\"entry_t3_2ku\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div id=\"entry_t3_b9\" class=\"content clear\">\n<div class=\"md\">\n<div>If you've recently joined the <a href=\"/lw/1/about_less_wrong\">Less Wrong community</a>, please leave a comment here and introduce yourself. We'd love to know who you are, what you're doing, what you value, <a href=\"/lw/2/tell_your_rationalist_origin_story\">how you came to identify as a rationalist</a> or how you found us. You can <a href=\"/lw/90l/welcome_to_less_wrong_2012/#comments\">skip right to that</a> if you like; the rest of this post consists of a few things you might find helpful. More can be found at the <a href=\"http://wiki.lesswrong.com/wiki/FAQ\">FAQ</a>.<br /></div>\n<div>(This is the third incarnation of&nbsp;the welcome thread, the first two of which which now have too many comments to show all at once.)</div>\n<h4><a id=\"more\"></a>A few notes about the site mechanics<br /></h4>\n<div>Less Wrong&nbsp; <strong>comments are threaded</strong>&nbsp; for easy following of multiple conversations. To respond to any comment, click the \"Reply\" link at the bottom of that comment's box. Within the comment box, links and formatting are achieved via&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Comment_formatting\">Markdown syntax</a>&nbsp; (you can click the \"Help\" link below the text box to bring up a primer).</div>\n<div class=\"md\">You may have noticed that all the posts and comments on this site have buttons to <strong>vote them up or down</strong>, and all the users have \"karma\" scores which come from the sum of all their comments and posts. This immediate easy feedback mechanism helps keep arguments from turning into flamewars and helps make the best posts more visible; it's part of what makes discussions on Less Wrong look different from those anywhere else on the Internet.</div>\n<div class=\"md\">However, it can feel really irritating to get downvoted, especially if one doesn't know why. It happens to all of us sometimes, and it's perfectly acceptable to ask for an explanation. (Sometimes it's the unwritten LW etiquette; we have different norms than other forums.) Take note when you're downvoted a lot on one topic, as it often means that several members of the community think you're missing an important point or making a mistake in reasoning&mdash; not just that they disagree with you!<strong> If you've any questions about karma or voting, please feel free to ask here.</strong></div>\n<div class=\"md\"><strong>Replies</strong> to your comments across the site, plus <strong>private messages</strong> from other users, will show up in your <a href=\"/message/inbox\">inbox</a>. You can reach it via the little mail icon beneath your karma score on the upper right of most pages. When you have a new reply or message, it glows red. You can also click on any user's name to view all of their comments and posts.</div>\n<div class=\"md\">It's definitely worth your time <strong>commenting on old posts</strong>; veteran users look through the <a href=\"/comments\">recent comments thread</a> quite often (there's a separate <a href=\"/r/discussion/comments\">recent comments thread for the Discussion section</a>, for whatever reason), and a conversation begun anywhere will pick up contributors that way.&nbsp; There's also a succession of <a href=\"/tag/open_thread\">open comment threads</a> for discussion of anything remotely related to rationality.</div>\n<div class=\"md\">Discussions on Less Wrong tend to end differently than in most other forums; a surprising number end when one participant changes their mind, or when multiple people clarify their views enough and reach agreement. More commonly, though, people will just stop when they've better identified their deeper disagreements, or simply <strong>\"tap out\" of a discussion</strong> that's stopped being productive. (Seriously, you can just write \"I'm tapping out of this thread.\") This is absolutely OK, and it's one good way to avoid the flamewars that plague many sites.<br /></div>\n<div class=\"md\"><strong>EXTRA FEATURES:</strong><br /></div>\n<div class=\"md\">There's actually more than meets the eye here: look near the top of the page for the \"WIKI\", \"DISCUSSION\" and \"SEQUENCES\" links.</div>\n<div class=\"md\"><strong>LW WIKI:</strong> This is our attempt to make searching by topic feasible, as well as to store information like <a href=\"http://wiki.lesswrong.com/wiki/Acronyms_used_on_Less_Wrong\">common abbreviations</a> and idioms. It's a good place to look if someone's speaking Greek to you.<br /></div>\n<div class=\"md\"><strong>LW DISCUSSION:</strong> This is a forum just like the top-level one, with two key differences: in the top-level forum, posts require the author to have 20 karma in order to publish, and any upvotes or downvotes on the post are multiplied by 10. Thus there's a lot more informal dialogue in the Discussion section, including some of the more fun conversations here.</div>\n<div class=\"md\"><strong>SEQUENCES:</strong> A <em>huge</em> corpus of material mostly written by Eliezer Yudkowsky in his days of blogging at Overcoming Bias, before Less Wrong was started. Much of the discussion here will casually depend on or refer to ideas brought up in those posts, so reading them can really help with present discussions. Besides which, they're pretty engrossing in my opinion.<br /></div>\n<div>\n<h4>A few notes about the community<br /></h4>\n<div>If you've come to Less Wrong to&nbsp; <strong>discuss a particular topic</strong>, this thread would be a great place to start the conversation. By commenting here, and checking the responses, you'll probably get a good read on what, if anything, has already been said here on that topic, what's widely understood and what you might still need to take some time explaining.</div>\n<div><strong>If your welcome comment starts a huge discussion</strong>, then please move to the next step and&nbsp; <strong>create a LW Discussion post to continue the conversation</strong>; we can fit many more welcomes onto each thread if fewer of them sprout 400+ comments. (To do this: click \"Create new areticle\" in the upper right corner next to your username, then write the article, then at the bottom take the menu \"Post to\" and change it from \"Drafts\" to \"Less Wrong Discussion\". Then click \"Submit\". When you edit a published post, clicking \"Save and continue\" does correctly update the post.)<br /></div>\n<div>If you want to write a post about a LW-relevant topic, awesome!&nbsp; <strong>I highly recommend you submit your first post to Less Wrong Discussion</strong>; don't worry, you can later promote it from there to the main page if it's well-received. (It's much better to get some feedback before every vote counts for 10 karma- honestly, you don't know what you don't know about the community norms here.)<br /></div>\n<div>If you'd like to connect with other LWers in real life, we have&nbsp; <strong>meetups&nbsp;</strong> in various parts of the world. Check the&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups\">wiki page for places with regular meetups</a>, or the&nbsp;<a href=\"/meetups\">upcoming (irregular) meetups page</a>.<br /></div>\n<div>There's also a&nbsp;<a href=\"http://www.facebook.com/home.php#/group.php?gid=144017955332&amp;ref=ts\">Facebook group</a>.&nbsp; If you've your own blog or other online presence, please feel free to link it.</div>\n<p><strong>If English is not your first language</strong>, don't let that make you afraid to post or comment. You can get English help on Discussion- or Main-level posts by sending a PM to one of the following users (use the \"send message\" link on the upper right of their user page). Either put the text of the post in the PM, or just say that you'd like English help and you'll get a response with an email address. <br /> * <a href=\"/user/Normal_Anomaly\">Normal_Anomaly</a> <br /> * <a href=\"/user/Randaly\">Randaly</a> <br /> * <a href=\"/user/shokwave\">shokwave</a> <br /> * <a href=\"/user/Barry_Cotter\">Barry Cotter</a></p>\n<p><strong>A note for theists</strong>: you will find the Less Wrong community to be predominantly atheist, though not completely so, and most of us are genuinely respectful of religious people who keep the usual community norms. It's worth saying that we might think religion is off-topic in some places where you think it's on-topic, so be thoughtful about where and how you start explicitly talking about it; some of us are happy to talk about religion, some of us aren't interested. Bear in mind that many of us really, truly have given full consideration to theistic claims and found them to be false, so starting with the most common arguments is pretty likely just to annoy people. Anyhow, it's absolutely OK to mention that you're religious in your welcome post and to invite a discussion there.</p>\n<h4>A list of some posts that are pretty awesome<br /></h4>\n<p>I recommend the&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Sequences\">major sequences</a>&nbsp; to everybody, but I realize how daunting they look at first. So for purposes of immediate gratification, the following posts are particularly interesting/illuminating/provocative and don't require any previous reading:</p>\n<ul>\n<li><a href=\"/lw/2bu/your_intuitions_are_not_magic\">Your Intuitions are Not Magic</a></li>\n<li><a href=\"/lw/20/the_apologist_and_the_revolutionary\">The Apologist and the Revolutionary</a></li>\n<li><a href=\"/lw/jr/how_to_convince_me_that_2_2_3\">How to Convince Me that 2 + 2 = 3</a></li>\n<li><a href=\"/lw/vo/lawful_uncertainty\">Lawful Uncertainty</a></li>\n<li><a href=\"/lw/jg/planning_fallacy\">The Planning Fallacy</a></li>\n<li><a href=\"/lw/hw/scope_insensitivity\">Scope Insensitivity</a></li>\n<li><a href=\"/lw/my/the_allais_paradox\">The Allais Paradox</a>&nbsp; (with&nbsp;<a href=\"/lw/mz/zut_allais\">two</a>&nbsp;<a href=\"/lw/n1/allais_malaise\">followups</a>)</li>\n<li><a href=\"/lw/jx/we_change_our_minds_less_often_than_we_think\">We Change Our Minds Less Often Than We Think</a></li>\n<li><a href=\"/lw/2k/the_least_convenient_possible_world\">The Least Convenient Possible World</a></li>\n<li><a href=\"/lw/hu/the_third_alternative\">The Third Alternative</a></li>\n<li><a href=\"/lw/116/the_domain_of_your_utility_function\">The Domain of Your Utility Function</a></li>\n<li><a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality\">Newcomb's Problem and Regret of Rationality</a></li>\n<li><a href=\"/lw/tn/the_true_prisoners_dilemma\">The True Prisoner's Dilemma</a></li>\n<li><a href=\"/lw/kw/the_tragedy_of_group_selectionism\">The Tragedy of Group Selectionism</a></li>\n<li><a href=\"/lw/gz/policy_debates_should_not_appear_onesided\">Policy Debates Should Not Appear One-Sided</a></li>\n<li><a href=\"/lw/qk/that_alien_message\">That Alien Message</a></li>\n</ul>\n<p>More suggestions are welcome! Or just check out the&nbsp;<a href=\"/top/?t=all\">top-rated posts from the history of Less Wrong</a>. Most posts at +50 or more are well worth your time.</p>\n<p>Welcome to Less Wrong, and we look forward to hearing from you throughout the site.</p>\n<p>(Note from orthonormal: MBlume and other contributors wrote the original version of this welcome message, and I've stolen heavily from it.)</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Jztohmx2anxGnp7g9", "schemaVersion": 1, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 31, "extendedScore": null, "score": 9.1e-05, "legacy": true, "legacyId": "11685", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": "", "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<div id=\"entry_t3_2ku\" class=\"content clear\">\n<div class=\"md\">\n<div>\n<div id=\"entry_t3_b9\" class=\"content clear\">\n<div class=\"md\">\n<div>If you've recently joined the <a href=\"/lw/1/about_less_wrong\">Less Wrong community</a>, please leave a comment here and introduce yourself. We'd love to know who you are, what you're doing, what you value, <a href=\"/lw/2/tell_your_rationalist_origin_story\">how you came to identify as a rationalist</a> or how you found us. You can <a href=\"/lw/90l/welcome_to_less_wrong_2012/#comments\">skip right to that</a> if you like; the rest of this post consists of a few things you might find helpful. More can be found at the <a href=\"http://wiki.lesswrong.com/wiki/FAQ\">FAQ</a>.<br></div>\n<div>(This is the third incarnation of&nbsp;the welcome thread, the first two of which which now have too many comments to show all at once.)</div>\n<h4 id=\"A_few_notes_about_the_site_mechanics\"><a id=\"more\"></a>A few notes about the site mechanics<br></h4>\n<div>Less Wrong&nbsp; <strong>comments are threaded</strong>&nbsp; for easy following of multiple conversations. To respond to any comment, click the \"Reply\" link at the bottom of that comment's box. Within the comment box, links and formatting are achieved via&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Comment_formatting\">Markdown syntax</a>&nbsp; (you can click the \"Help\" link below the text box to bring up a primer).</div>\n<div class=\"md\">You may have noticed that all the posts and comments on this site have buttons to <strong>vote them up or down</strong>, and all the users have \"karma\" scores which come from the sum of all their comments and posts. This immediate easy feedback mechanism helps keep arguments from turning into flamewars and helps make the best posts more visible; it's part of what makes discussions on Less Wrong look different from those anywhere else on the Internet.</div>\n<div class=\"md\">However, it can feel really irritating to get downvoted, especially if one doesn't know why. It happens to all of us sometimes, and it's perfectly acceptable to ask for an explanation. (Sometimes it's the unwritten LW etiquette; we have different norms than other forums.) Take note when you're downvoted a lot on one topic, as it often means that several members of the community think you're missing an important point or making a mistake in reasoning\u2014 not just that they disagree with you!<strong> If you've any questions about karma or voting, please feel free to ask here.</strong></div>\n<div class=\"md\"><strong>Replies</strong> to your comments across the site, plus <strong>private messages</strong> from other users, will show up in your <a href=\"/message/inbox\">inbox</a>. You can reach it via the little mail icon beneath your karma score on the upper right of most pages. When you have a new reply or message, it glows red. You can also click on any user's name to view all of their comments and posts.</div>\n<div class=\"md\">It's definitely worth your time <strong>commenting on old posts</strong>; veteran users look through the <a href=\"/comments\">recent comments thread</a> quite often (there's a separate <a href=\"/r/discussion/comments\">recent comments thread for the Discussion section</a>, for whatever reason), and a conversation begun anywhere will pick up contributors that way.&nbsp; There's also a succession of <a href=\"/tag/open_thread\">open comment threads</a> for discussion of anything remotely related to rationality.</div>\n<div class=\"md\">Discussions on Less Wrong tend to end differently than in most other forums; a surprising number end when one participant changes their mind, or when multiple people clarify their views enough and reach agreement. More commonly, though, people will just stop when they've better identified their deeper disagreements, or simply <strong>\"tap out\" of a discussion</strong> that's stopped being productive. (Seriously, you can just write \"I'm tapping out of this thread.\") This is absolutely OK, and it's one good way to avoid the flamewars that plague many sites.<br></div>\n<div class=\"md\"><strong>EXTRA FEATURES:</strong><br></div>\n<div class=\"md\">There's actually more than meets the eye here: look near the top of the page for the \"WIKI\", \"DISCUSSION\" and \"SEQUENCES\" links.</div>\n<div class=\"md\"><strong>LW WIKI:</strong> This is our attempt to make searching by topic feasible, as well as to store information like <a href=\"http://wiki.lesswrong.com/wiki/Acronyms_used_on_Less_Wrong\">common abbreviations</a> and idioms. It's a good place to look if someone's speaking Greek to you.<br></div>\n<div class=\"md\"><strong>LW DISCUSSION:</strong> This is a forum just like the top-level one, with two key differences: in the top-level forum, posts require the author to have 20 karma in order to publish, and any upvotes or downvotes on the post are multiplied by 10. Thus there's a lot more informal dialogue in the Discussion section, including some of the more fun conversations here.</div>\n<div class=\"md\"><strong>SEQUENCES:</strong> A <em>huge</em> corpus of material mostly written by Eliezer Yudkowsky in his days of blogging at Overcoming Bias, before Less Wrong was started. Much of the discussion here will casually depend on or refer to ideas brought up in those posts, so reading them can really help with present discussions. Besides which, they're pretty engrossing in my opinion.<br></div>\n<div>\n<h4 id=\"A_few_notes_about_the_community\">A few notes about the community<br></h4>\n<div>If you've come to Less Wrong to&nbsp; <strong>discuss a particular topic</strong>, this thread would be a great place to start the conversation. By commenting here, and checking the responses, you'll probably get a good read on what, if anything, has already been said here on that topic, what's widely understood and what you might still need to take some time explaining.</div>\n<div><strong>If your welcome comment starts a huge discussion</strong>, then please move to the next step and&nbsp; <strong>create a LW Discussion post to continue the conversation</strong>; we can fit many more welcomes onto each thread if fewer of them sprout 400+ comments. (To do this: click \"Create new areticle\" in the upper right corner next to your username, then write the article, then at the bottom take the menu \"Post to\" and change it from \"Drafts\" to \"Less Wrong Discussion\". Then click \"Submit\". When you edit a published post, clicking \"Save and continue\" does correctly update the post.)<br></div>\n<div>If you want to write a post about a LW-relevant topic, awesome!&nbsp; <strong>I highly recommend you submit your first post to Less Wrong Discussion</strong>; don't worry, you can later promote it from there to the main page if it's well-received. (It's much better to get some feedback before every vote counts for 10 karma- honestly, you don't know what you don't know about the community norms here.)<br></div>\n<div>If you'd like to connect with other LWers in real life, we have&nbsp; <strong>meetups&nbsp;</strong> in various parts of the world. Check the&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups\">wiki page for places with regular meetups</a>, or the&nbsp;<a href=\"/meetups\">upcoming (irregular) meetups page</a>.<br></div>\n<div>There's also a&nbsp;<a href=\"http://www.facebook.com/home.php#/group.php?gid=144017955332&amp;ref=ts\">Facebook group</a>.&nbsp; If you've your own blog or other online presence, please feel free to link it.</div>\n<p><strong>If English is not your first language</strong>, don't let that make you afraid to post or comment. You can get English help on Discussion- or Main-level posts by sending a PM to one of the following users (use the \"send message\" link on the upper right of their user page). Either put the text of the post in the PM, or just say that you'd like English help and you'll get a response with an email address. <br> * <a href=\"/user/Normal_Anomaly\">Normal_Anomaly</a> <br> * <a href=\"/user/Randaly\">Randaly</a> <br> * <a href=\"/user/shokwave\">shokwave</a> <br> * <a href=\"/user/Barry_Cotter\">Barry Cotter</a></p>\n<p><strong>A note for theists</strong>: you will find the Less Wrong community to be predominantly atheist, though not completely so, and most of us are genuinely respectful of religious people who keep the usual community norms. It's worth saying that we might think religion is off-topic in some places where you think it's on-topic, so be thoughtful about where and how you start explicitly talking about it; some of us are happy to talk about religion, some of us aren't interested. Bear in mind that many of us really, truly have given full consideration to theistic claims and found them to be false, so starting with the most common arguments is pretty likely just to annoy people. Anyhow, it's absolutely OK to mention that you're religious in your welcome post and to invite a discussion there.</p>\n<h4 id=\"A_list_of_some_posts_that_are_pretty_awesome\">A list of some posts that are pretty awesome<br></h4>\n<p>I recommend the&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Sequences\">major sequences</a>&nbsp; to everybody, but I realize how daunting they look at first. So for purposes of immediate gratification, the following posts are particularly interesting/illuminating/provocative and don't require any previous reading:</p>\n<ul>\n<li><a href=\"/lw/2bu/your_intuitions_are_not_magic\">Your Intuitions are Not Magic</a></li>\n<li><a href=\"/lw/20/the_apologist_and_the_revolutionary\">The Apologist and the Revolutionary</a></li>\n<li><a href=\"/lw/jr/how_to_convince_me_that_2_2_3\">How to Convince Me that 2 + 2 = 3</a></li>\n<li><a href=\"/lw/vo/lawful_uncertainty\">Lawful Uncertainty</a></li>\n<li><a href=\"/lw/jg/planning_fallacy\">The Planning Fallacy</a></li>\n<li><a href=\"/lw/hw/scope_insensitivity\">Scope Insensitivity</a></li>\n<li><a href=\"/lw/my/the_allais_paradox\">The Allais Paradox</a>&nbsp; (with&nbsp;<a href=\"/lw/mz/zut_allais\">two</a>&nbsp;<a href=\"/lw/n1/allais_malaise\">followups</a>)</li>\n<li><a href=\"/lw/jx/we_change_our_minds_less_often_than_we_think\">We Change Our Minds Less Often Than We Think</a></li>\n<li><a href=\"/lw/2k/the_least_convenient_possible_world\">The Least Convenient Possible World</a></li>\n<li><a href=\"/lw/hu/the_third_alternative\">The Third Alternative</a></li>\n<li><a href=\"/lw/116/the_domain_of_your_utility_function\">The Domain of Your Utility Function</a></li>\n<li><a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality\">Newcomb's Problem and Regret of Rationality</a></li>\n<li><a href=\"/lw/tn/the_true_prisoners_dilemma\">The True Prisoner's Dilemma</a></li>\n<li><a href=\"/lw/kw/the_tragedy_of_group_selectionism\">The Tragedy of Group Selectionism</a></li>\n<li><a href=\"/lw/gz/policy_debates_should_not_appear_onesided\">Policy Debates Should Not Appear One-Sided</a></li>\n<li><a href=\"/lw/qk/that_alien_message\">That Alien Message</a></li>\n</ul>\n<p>More suggestions are welcome! Or just check out the&nbsp;<a href=\"/top/?t=all\">top-rated posts from the history of Less Wrong</a>. Most posts at +50 or more are well worth your time.</p>\n<p>Welcome to Less Wrong, and we look forward to hearing from you throughout the site.</p>\n<p>(Note from orthonormal: MBlume and other contributors wrote the original version of this welcome message, and I've stolen heavily from it.)</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>", "sections": [{"title": "A few notes about the site mechanics", "anchor": "A_few_notes_about_the_site_mechanics", "level": 1}, {"title": "A few notes about the community", "anchor": "A_few_notes_about_the_community", "level": 1}, {"title": "A list of some posts that are pretty awesome", "anchor": "A_list_of_some_posts_that_are_pretty_awesome", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1440 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1444, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2om7AHEHtbogJmT5s", "BHMBBFupzb4s8utts", "Psp8ZpYLCDJjshpRb", "ZiQqsgGX6a42Sfpii", "6FmqiAgS8h4EJm86s", "msJA6B9ZjiiZxT6EZ", "CPm5LTwHrvBJCa9h5", "2ftJ38y9SRBCBsCzy", "zJZvoiwydJ5zvzTHK", "zNcLnqHF5rvrTsQJx", "knpAQ4F3gmguxy39z", "buixYfcXBah9hbSNZ", "neQ7eXuaXpiYw7SBy", "erGipespbbzdG5zYb", "xgicQnkrdA5FehhnQ", "6ddcsdA2c2XpNpE5x", "HFyWNBnDNEDsDNLrZ", "QsMJQSFj7WfoTMNgW", "PeSzc9JTBxhaYRp9b", "5wMcKNAwB6X4mp9og"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": "2011-12-26T22:57:21.157Z", "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-27T02:33:39.021Z", "modifiedAt": null, "url": null, "title": "If You Were Brilliant When You Were Ten...", "slug": "if-you-were-brilliant-when-you-were-ten", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:29.599Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "AspiringKnitter", "createdAt": "2011-12-19T07:07:58.702Z", "isAdmin": false, "displayName": "AspiringKnitter"}, "userId": "wyuRmEzfzp3zGgSNK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iFae3EYaqDfSMsnfi/if-you-were-brilliant-when-you-were-ten", "pageUrlRelative": "/posts/iFae3EYaqDfSMsnfi/if-you-were-brilliant-when-you-were-ten", "linkUrl": "https://www.lesswrong.com/posts/iFae3EYaqDfSMsnfi/if-you-were-brilliant-when-you-were-ten", "postedAtFormatted": "Tuesday, December 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20If%20You%20Were%20Brilliant%20When%20You%20Were%20Ten...&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIf%20You%20Were%20Brilliant%20When%20You%20Were%20Ten...%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiFae3EYaqDfSMsnfi%2Fif-you-were-brilliant-when-you-were-ten%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=If%20You%20Were%20Brilliant%20When%20You%20Were%20Ten...%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiFae3EYaqDfSMsnfi%2Fif-you-were-brilliant-when-you-were-ten", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiFae3EYaqDfSMsnfi%2Fif-you-were-brilliant-when-you-were-ten", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 277, "htmlBody": "<p>(If I do anything wrong here, please tell me. I don't know what I'm doing and would benefit from being told what I've got wrong, if anything. I've never made a top-level post here before.)</p>\n<p>So, it seems like most people here are really smart. And a lot of us, I'm betting, will have been identified as smart when we were children, and gotten complimented on it a lot. And it's pretty common for that to really mess you up, and then you don't end up reaching your full potential.&nbsp;Admittedly, maybe only people who've gotten past all that read Less Wrong. Maybe I'm the exception. But somehow I doubt that very much.</p>\n<p>So here's the only thing I can think of to say if this is your situation: <strong>ask stupid questions.</strong></p>\n<p>Seriously, even if it shows that you have no clue what was just said. (Especially if it shows that. You don't want to continue not understanding.) You can optimize for being smart, you can optimize for seeming smart, but sometimes you need to pick which one to optimize for. It may make you uncomfortable to admit to not knowing something. It may make you feel like the people around you will stop thinking you're all-knowing. But if you don't know how to ask stupid questions, and you just keep pretending to understand, you'll fall behind and eventually be outed as being really, really stupid, instead of just pretty normal. Which sounds worse?</p>\n<p>Here, let me demonstrate: so, what tags go on this post and how would I know?</p>\n<p>So, anyone else know of any similar things to do, to get back to optimizing for being smart instead of for seeming smart?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"WqLn4pAWi5hn6McHQ": 1, "2EFq8dJbxKNzforjM": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iFae3EYaqDfSMsnfi", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 32, "extendedScore": null, "score": 8.215856751862549e-07, "legacy": true, "legacyId": "11700", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 69, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-27T02:39:13.616Z", "modifiedAt": null, "url": null, "title": "Philosophy that can be \"taken seriously by computer scientists\"", "slug": "philosophy-that-can-be-taken-seriously-by-computer", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:57.984Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Stync46vwxGXRidNK/philosophy-that-can-be-taken-seriously-by-computer", "pageUrlRelative": "/posts/Stync46vwxGXRidNK/philosophy-that-can-be-taken-seriously-by-computer", "linkUrl": "https://www.lesswrong.com/posts/Stync46vwxGXRidNK/philosophy-that-can-be-taken-seriously-by-computer", "postedAtFormatted": "Tuesday, December 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Philosophy%20that%20can%20be%20%22taken%20seriously%20by%20computer%20scientists%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0APhilosophy%20that%20can%20be%20%22taken%20seriously%20by%20computer%20scientists%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FStync46vwxGXRidNK%2Fphilosophy-that-can-be-taken-seriously-by-computer%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Philosophy%20that%20can%20be%20%22taken%20seriously%20by%20computer%20scientists%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FStync46vwxGXRidNK%2Fphilosophy-that-can-be-taken-seriously-by-computer", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FStync46vwxGXRidNK%2Fphilosophy-that-can-be-taken-seriously-by-computer", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 157, "htmlBody": "<p>I've long held <a href=\"http://www.hss.cmu.edu/philosophy/\">CMU's philosophy department</a> in high regard. One of their leading lights, <a href=\"http://en.wikipedia.org/wiki/Clark_Glymour\">Clark Glymour</a>, recently published a short&nbsp;<a href=\"http://choiceandinference.com/2011/12/23/in-light-of-some-recent-discussion-over-at-new-apps-i-bring-you-clark-glymours-manifesto/\">manifesto</a>, which Brian Leiter <a href=\"http://leiterreports.typepad.com/blog/2011/12/speaking-of-making-things-up.html\">summed up</a> as saying that \"the measure of value for philosophy departments is whether they are taken seriously by computer scientists.\"</p>\n<p>Selected quote from Glymour's manifesto:</p>\n<blockquote>\n<p>Were I a university administrator facing a contracting budget, I would not look to eliminate biosciences or computer engineering. I would notice that the philosophers seem smart, but their writings are tediously incestuous and of no influence except among themselves, and I would conclude that my academy could do without such a department... But not if I found that my philosophy department retrieved a million dollars a year in grants and fellowships, and contained members whose work is cited and used in multiple subjects, and whose faculty taught the traditional subject well to the university&rsquo;s undergraduates.</p>\n</blockquote>\n<p>Also see the critique <a href=\"http://www.newappsblog.com/2011/12/glymour-against-philosophy.html\">here</a>, but I'd like to have Glymour working on FAI.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"GLykb6NukBeBQtDvQ": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Stync46vwxGXRidNK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 30, "extendedScore": null, "score": 6.4e-05, "legacy": true, "legacyId": "11703", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 26, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-27T04:51:35.384Z", "modifiedAt": null, "url": null, "title": "What causes burnout?", "slug": "what-causes-burnout", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:24.610Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "juliawise", "createdAt": "2011-07-18T13:52:30.717Z", "isAdmin": false, "displayName": "juliawise"}, "userId": "JtChJYGsjzgAh5Ag8", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/MsEXP5WucW8trMYPJ/what-causes-burnout", "pageUrlRelative": "/posts/MsEXP5WucW8trMYPJ/what-causes-burnout", "linkUrl": "https://www.lesswrong.com/posts/MsEXP5WucW8trMYPJ/what-causes-burnout", "postedAtFormatted": "Tuesday, December 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20causes%20burnout%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20causes%20burnout%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMsEXP5WucW8trMYPJ%2Fwhat-causes-burnout%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20causes%20burnout%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMsEXP5WucW8trMYPJ%2Fwhat-causes-burnout", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FMsEXP5WucW8trMYPJ%2Fwhat-causes-burnout", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 160, "htmlBody": "<p>When I try to figure out how to balance personal happiness with saving the world, I think a lot about burnout.&nbsp; If I make a major change - e.g. changing careers - what are the chances I'll burn out and become a lot less useful as a result?</p>\n<p>I've never burned out, so I don't know where that edge is for me (plus I suspect the edge moves around depending on circumstances).&nbsp; I'm obviously biased on the topic: there's the temptation to tell myself \"This will prevent burnout and make me more effective in the long run\" every time I want to do something.</p>\n<p>Some things people here have described as causing burnout:</p>\n<p><a href=\"/lw/2ku/welcome_to_less_wrong_20102011/5g5i\">Going through the motions of a religion you don't believe in</a></p>\n<p><a href=\"/lw/8gv/the_curse_of_identity\">Training yourself to feel guilty whenever you relax</a></p>\n<p><a href=\"/lw/38u/best_career_models_for_doing_research/33et\">Pursuing altruism too exclusively</a>&nbsp; (That post suggests Bostrom's <a href=\"http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html\">parliamentary model</a> as an antidote).</p>\n<p>Do you have more examples?&nbsp; Have you burned out?&nbsp; Are there things that you think have kept you from burning out?&nbsp;&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "MsEXP5WucW8trMYPJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 22, "extendedScore": null, "score": 8.216368574442081e-07, "legacy": true, "legacyId": "11704", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 14, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 16, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["tAXrD8Y6hcJ8dt6Nt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-27T05:13:38.129Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Zut Allais!", "slug": "seq-rerun-zut-allais", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:05.800Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4PFM7dQXtcj4QWEJw/seq-rerun-zut-allais", "pageUrlRelative": "/posts/4PFM7dQXtcj4QWEJw/seq-rerun-zut-allais", "linkUrl": "https://www.lesswrong.com/posts/4PFM7dQXtcj4QWEJw/seq-rerun-zut-allais", "postedAtFormatted": "Tuesday, December 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Zut%20Allais!&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Zut%20Allais!%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4PFM7dQXtcj4QWEJw%2Fseq-rerun-zut-allais%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Zut%20Allais!%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4PFM7dQXtcj4QWEJw%2Fseq-rerun-zut-allais", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4PFM7dQXtcj4QWEJw%2Fseq-rerun-zut-allais", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 150, "htmlBody": "<p>Today's post, <a href=\"/lw/mz/zut_allais/\">Zut Allais!</a> was originally published on 20 January 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Offered choices between gambles, people make decision-theoretically inconsistent decisions.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/90h/seq_rerun_the_allais_paradox/\">The Allais Paradox</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4PFM7dQXtcj4QWEJw", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.216450380215141e-07, "legacy": true, "legacyId": "11705", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["zNcLnqHF5rvrTsQJx", "ratyky2Q2Ap3iPh5d", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-27T06:05:19.646Z", "modifiedAt": null, "url": null, "title": "Meetup : Portland Meetup?", "slug": "meetup-portland-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:27.292Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "obfuscate", "createdAt": "2011-12-27T04:32:41.471Z", "isAdmin": false, "displayName": "obfuscate"}, "userId": "mHLgDB9MRXoEGYEem", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iCYBkNyxGpeXa3kSy/meetup-portland-meetup", "pageUrlRelative": "/posts/iCYBkNyxGpeXa3kSy/meetup-portland-meetup", "linkUrl": "https://www.lesswrong.com/posts/iCYBkNyxGpeXa3kSy/meetup-portland-meetup", "postedAtFormatted": "Tuesday, December 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20Portland%20Meetup%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20Portland%20Meetup%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiCYBkNyxGpeXa3kSy%2Fmeetup-portland-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20Portland%20Meetup%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiCYBkNyxGpeXa3kSy%2Fmeetup-portland-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiCYBkNyxGpeXa3kSy%2Fmeetup-portland-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 116, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/5p'>Portland Meetup?</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">14 January 2012 12:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">1005 West Burnside, Portland, OR 97209</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>(Edited after looking over some other meetup posts.) This is a tentative attempt at a meetup in Portland. I think Powell's books is a decent place for a meeting place, and from there if people want to migrate to someplace to sit down and eat, etc., that works well too. It has the added advantage that if it turns out that nobody shows up, hey, I am in Powell's. It's a win anyways. I am still working out the details of all this. It's a bit subject to change.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/5p'>Portland Meetup?</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iCYBkNyxGpeXa3kSy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 8.216642200882564e-07, "legacy": true, "legacyId": "11706", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___Portland_Meetup_\">Discussion article for the meetup : <a href=\"/meetups/5p\">Portland Meetup?</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">14 January 2012 12:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">1005 West Burnside, Portland, OR 97209</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>(Edited after looking over some other meetup posts.) This is a tentative attempt at a meetup in Portland. I think Powell's books is a decent place for a meeting place, and from there if people want to migrate to someplace to sit down and eat, etc., that works well too. It has the added advantage that if it turns out that nobody shows up, hey, I am in Powell's. It's a win anyways. I am still working out the details of all this. It's a bit subject to change.</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___Portland_Meetup_1\">Discussion article for the meetup : <a href=\"/meetups/5p\">Portland Meetup?</a></h2>", "sections": [{"title": "Discussion article for the meetup : Portland Meetup?", "anchor": "Discussion_article_for_the_meetup___Portland_Meetup_", "level": 1}, {"title": "Discussion article for the meetup : Portland Meetup?", "anchor": "Discussion_article_for_the_meetup___Portland_Meetup_1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "12 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-27T13:29:03.419Z", "modifiedAt": null, "url": null, "title": "How to Draw Conclusions Like Sherlock Holmes", "slug": "how-to-draw-conclusions-like-sherlock-holmes", "viewCount": null, "lastCommentedAt": "2017-06-17T04:04:35.779Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "abcd_z", "createdAt": "2011-06-26T00:41:40.672Z", "isAdmin": false, "displayName": "abcd_z"}, "userId": "ntrr3JGG5fDueLnND", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/wTfKDqve3zYoy7rH3/how-to-draw-conclusions-like-sherlock-holmes", "pageUrlRelative": "/posts/wTfKDqve3zYoy7rH3/how-to-draw-conclusions-like-sherlock-holmes", "linkUrl": "https://www.lesswrong.com/posts/wTfKDqve3zYoy7rH3/how-to-draw-conclusions-like-sherlock-holmes", "postedAtFormatted": "Tuesday, December 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20to%20Draw%20Conclusions%20Like%20Sherlock%20Holmes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20to%20Draw%20Conclusions%20Like%20Sherlock%20Holmes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwTfKDqve3zYoy7rH3%2Fhow-to-draw-conclusions-like-sherlock-holmes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20to%20Draw%20Conclusions%20Like%20Sherlock%20Holmes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwTfKDqve3zYoy7rH3%2Fhow-to-draw-conclusions-like-sherlock-holmes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FwTfKDqve3zYoy7rH3%2Fhow-to-draw-conclusions-like-sherlock-holmes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 558, "htmlBody": "<p>&nbsp;</p>\n<p>Eliezer Yudkowsky once wrote that</p>\n<blockquote>\n<p>[...] when you look at what Sherlock Holmes does - you can't go out and do it at home. &nbsp;Sherlock Holmes is not really operating by any sort of reproducible method. &nbsp;He is operating by magically finding the right clues and carrying out magically correct complicated chains of deduction. &nbsp;Maybe it's just me, but it seems to me that reading Sherlock Holmes does not inspire you to go and do likewise. &nbsp;Holmes is a mutant superhero. &nbsp;And even if you did try to imitate him, it would never work in real life.</p>\n</blockquote>\n<p>&nbsp;</p>\n<p>A few days ago I was at an acquaintance's house after watching the Sherlock miniseries on Netflix. My mind whirling with the abilities displayed by the titular character and I wandered around the house while others were making small talk. I stopped by a large oil painting on one wall that was decent but had obvious problems with perspective. Additionally, it was missing a signature in the lower-right corner.</p>\n<p>&nbsp;</p>\n<p>ANALYSIS:</p>\n<p>Sub-par paintings don't generally get put on the market.</p>\n<p>If the hostess thought it was worth putting on the wall, it was most likely because she had an emotional attachment to the piece.</p>\n<p>Painters place their signatures in the corner of the painting to identify themselves as the creator. If the painter didn't bother leaving their mark, it was because they were confident that they didn't need to.</p>\n<p>The conclusion I drew from this was that the painter was either the hostess herself or somebody very close to her. As it turns out, it was the hostess.</p>\n<p>&nbsp;</p>\n<p>Now, this anecdote hardly proves anything. &nbsp;Still, I think it's a fun little thing and the ability to show off like that, even a small percentage of the time, is too good to pass up. &nbsp;So I present my analysis of How to Become a Regular Sherlock Holmes.</p>\n<p>&nbsp;</p>\n<p>1) Pay attention to details. Look around you at your environment. &nbsp;A scratch on a wall, a limp in somebody's walk, a smudge on somebody's cheek. &nbsp;At this point it's probably hard to tell what details are important, so pay attention to everything.</p>\n<p>&nbsp;</p>\n<p>2) Answer these two questions:</p>\n<p>\"What am I looking at?\" and</p>\n<p>\"What could it mean (if anything)?\"</p>\n<p>&nbsp;</p>\n<p>3) Check your guesses.</p>\n<p>This is an important step. It's easy to make any sort of judgments about the details and what they mean, but if you accept your own conclusions without checking the facts, you're likely to create false assumptions and associations that you take as fact. &nbsp;That's the opposite of what we're trying to do here.</p>\n<p>Fortunately, checking your guesses is very easy to do in most situations with another person. Just state what you've noticed and ask for information on the context. &nbsp;For example, \"I've noticed a large scratch on your end-table. Do you know how it happened?\"</p>\n<p>A follow-up question might be \"why haven't you changed it out for another one?\", but only if you think getting the information is more important than the possibility of being seen as rude and the potential consequences thereof.</p>\n<p>&nbsp;</p>\n<p>In Summary:</p>\n<p>&nbsp;</p>\n<p>Pay attention to details</p>\n<p>\"What am I looking at?\"</p>\n<p>\"What could it mean?\"</p>\n<p>Check your guesses</p>\n<p>&nbsp;</p>\n<p>Oh, and the painting I mentioned at the beginning? I actually didn't figure it out until she told me. I just about kicked myself when I realized I could have figured it out myself and pulled off a really cool Sherlock Summation if I hadn't asked first. C'est la vie.</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "wTfKDqve3zYoy7rH3", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": -6, "extendedScore": null, "score": 8.218287385613776e-07, "legacy": true, "legacyId": "11709", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 28, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-27T21:24:30.416Z", "modifiedAt": null, "url": null, "title": "Singularity Institute $100,000 end-of-year fundraiser only 20% filled so far", "slug": "singularity-institute-usd100-000-end-of-year-fundraiser-only", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:57.662Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Louie", "createdAt": "2010-05-10T21:41:14.619Z", "isAdmin": false, "displayName": "Louie"}, "userId": "JPwZspDjBcfwwuy7W", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YNBamqCgmWWsipk2F/singularity-institute-usd100-000-end-of-year-fundraiser-only", "pageUrlRelative": "/posts/YNBamqCgmWWsipk2F/singularity-institute-usd100-000-end-of-year-fundraiser-only", "linkUrl": "https://www.lesswrong.com/posts/YNBamqCgmWWsipk2F/singularity-institute-usd100-000-end-of-year-fundraiser-only", "postedAtFormatted": "Tuesday, December 27th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Singularity%20Institute%20%24100%2C000%20end-of-year%20fundraiser%20only%2020%25%20filled%20so%20far&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASingularity%20Institute%20%24100%2C000%20end-of-year%20fundraiser%20only%2020%25%20filled%20so%20far%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYNBamqCgmWWsipk2F%2Fsingularity-institute-usd100-000-end-of-year-fundraiser-only%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Singularity%20Institute%20%24100%2C000%20end-of-year%20fundraiser%20only%2020%25%20filled%20so%20far%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYNBamqCgmWWsipk2F%2Fsingularity-institute-usd100-000-end-of-year-fundraiser-only", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYNBamqCgmWWsipk2F%2Fsingularity-institute-usd100-000-end-of-year-fundraiser-only", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 498, "htmlBody": "<p><em>** cross-posted from <a href=\"http://intelligence.org/2011winterfundraiser/\">http://singinst.org/2011winterfundraiser/</a> **</em></p>\n<p><em>Contains detailed info about accomplishments and plans at SI. Thanks for supporting our work! &nbsp;-Louie</em></p>\n<p><em><br /></em></p>\n<h3><span>ARTIFICIAL INTELLIGENCE MORE RELEVANT THAN EVER</span></h3>\n<p>Recent books like <em><a href=\"http://www.amazon.com/Machine-Ethics-Michael-Anderson/dp/0521112354/\">Machine Ethics</a></em> from Cambridge University Press and <em><a href=\"http://www.amazon.com/Robot-Ethics-Implications-Intelligent-Autonomous/dp/0262016664/\">Robot Ethics</a></em> from MIT Press, along with the U.S. military-funded research that resulted in <em><a href=\"http://www.amazon.com/Governing-Lethal-Behavior-Autonomous-Robots/dp/1420085948/\">Governing Lethal Behavior in Autonomous Robots</a></em> show that the world is waking up to the challenges of building safe and ethical AI. But these projects focus on limited AI applications and fail to address the most important concern: how to ensure that <em>smarter-than-human</em> AI benefits humanity. The Singularity Institute has been working on that problem longer than anybody, a full decade before the Singularity landed on the <a href=\"http://www.time.com/time/magazine/article/0,9171,2048299,00.html\">cover of <em>TIME</em> magazine</a>.</p>\n<h3><br /></h3>\n<h3><span>ACCOMPLISHMENTS IN 2011</span></h3>\n<p>2011 was our biggest year yet. Since the year began, we have:</p>\n<ul>\n<li>Held our annual <a href=\"http://www.singularitysummit.com/\">Singularity Summit</a> in New York City, with more than 900 in attendance. Speakers included inventor and futurist Ray Kurzweil, economist Tyler Cowen, PayPal co-founder Peter Thiel, <em>Skeptic</em> publisher Michael Shermer, Mathematica and WolframAlpha creator Stephen Wolfram, neuroscientist Christof Koch, MIT physicist Max Tegmark, and famed <em>Jeopardy!</em> contestant Ken Jennings.</li>\n<li>Held a smaller Singularity Summit in Salt Lake City.</li>\n<li>Held a one-week Rationality Minicamp and a ten-week Rationality Boot Camp.</li>\n<li>Created the <a href=\"http://intelligence.org/aboutus/researchassociates\">Research Associates</a> program, which currently has 7 researchers coordinating with Singularity Institute.</li>\n<li>Published our <a href=\"http://intelligence.org/singularityfaq\">Singularity FAQ</a>, <a href=\"http://intelligenceexplosion.com/\">IntelligenceExplosion.com</a>, and <a href=\"http://friendly-ai.com/\">Friendly-AI.com</a>.</li>\n<li>Wrote three chapters for Springer's upcoming volume <em><a href=\"http://singularityhypothesis.blogspot.com/\">The Singularity Hypothesis</a></em>, along with four other research papers.</li>\n<li>Began work on a new, clearer website design with lots of new content, which should go live Q1 2012.</li>\n<li>Began <a href=\"http://lukeprog.com/SaveTheWorld.html\">outlining open problems in Singularity research</a> to help outside collaborators better understand our research priorities.</li>\n</ul>\n<p>&nbsp;</p>\n<h3><span>FUTURE PLANS YOU CAN HELP SUPPORT</span></h3>\n<p>In the coming year, we plan to do the following:</p>\n<ul>\n<li>Hold our annual Singularity Summit, in San Francisco this year.</li>\n<li>Improve organizational transparency by creating a simpler, easier-to-use website that includes Singularity Institute planning and policy documents.</li>\n<li>Publish a document of open research problems in Singularity Research, to clarify the research space and encourage other researchers to contribute to our mission.</li>\n<li>Add additional skilled researchers to our Research Associates program.</li>\n<li>Publish a well-researched document making the case for existential risk reduction as optimal philanthropy.</li>\n<li>Diversify our funding sources by applying for targeted grants and advertising our affinity credit card program.</li>\n<br /><br />\n<p><em>We appreciate your support</em> for our high-impact work. As Skype co-founder Jaan Tallinn said:</p>\n<blockquote>We became the dominant species on this planet by being the most intelligent species around. This century we are going to cede that crown to machines&hellip; Since we have only one shot at getting this transition right, the importance of Singularity Institute's work cannot be overestimated.</blockquote>\n</ul>\n<p>&nbsp;</p>\n<p><em><br />Now is your last chance to make a tax-deductible donation in 2011.</em></p>\n<p><strong>If you'd like to support our work: <a href=\"http://intelligence.org/donate\">please donate now</a>!</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YNBamqCgmWWsipk2F", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 8, "extendedScore": null, "score": 8.220054457529028e-07, "legacy": true, "legacyId": "11714", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>** cross-posted from <a href=\"http://intelligence.org/2011winterfundraiser/\">http://singinst.org/2011winterfundraiser/</a> **</em></p>\n<p><em>Contains detailed info about accomplishments and plans at SI. Thanks for supporting our work! &nbsp;-Louie</em></p>\n<p><em><br></em></p>\n<h3 id=\"ARTIFICIAL_INTELLIGENCE_MORE_RELEVANT_THAN_EVER\"><span>ARTIFICIAL INTELLIGENCE MORE RELEVANT THAN EVER</span></h3>\n<p>Recent books like <em><a href=\"http://www.amazon.com/Machine-Ethics-Michael-Anderson/dp/0521112354/\">Machine Ethics</a></em> from Cambridge University Press and <em><a href=\"http://www.amazon.com/Robot-Ethics-Implications-Intelligent-Autonomous/dp/0262016664/\">Robot Ethics</a></em> from MIT Press, along with the U.S. military-funded research that resulted in <em><a href=\"http://www.amazon.com/Governing-Lethal-Behavior-Autonomous-Robots/dp/1420085948/\">Governing Lethal Behavior in Autonomous Robots</a></em> show that the world is waking up to the challenges of building safe and ethical AI. But these projects focus on limited AI applications and fail to address the most important concern: how to ensure that <em>smarter-than-human</em> AI benefits humanity. The Singularity Institute has been working on that problem longer than anybody, a full decade before the Singularity landed on the <a href=\"http://www.time.com/time/magazine/article/0,9171,2048299,00.html\">cover of <em>TIME</em> magazine</a>.</p>\n<h3><br></h3>\n<h3 id=\"ACCOMPLISHMENTS_IN_2011\"><span>ACCOMPLISHMENTS IN 2011</span></h3>\n<p>2011 was our biggest year yet. Since the year began, we have:</p>\n<ul>\n<li>Held our annual <a href=\"http://www.singularitysummit.com/\">Singularity Summit</a> in New York City, with more than 900 in attendance. Speakers included inventor and futurist Ray Kurzweil, economist Tyler Cowen, PayPal co-founder Peter Thiel, <em>Skeptic</em> publisher Michael Shermer, Mathematica and WolframAlpha creator Stephen Wolfram, neuroscientist Christof Koch, MIT physicist Max Tegmark, and famed <em>Jeopardy!</em> contestant Ken Jennings.</li>\n<li>Held a smaller Singularity Summit in Salt Lake City.</li>\n<li>Held a one-week Rationality Minicamp and a ten-week Rationality Boot Camp.</li>\n<li>Created the <a href=\"http://intelligence.org/aboutus/researchassociates\">Research Associates</a> program, which currently has 7 researchers coordinating with Singularity Institute.</li>\n<li>Published our <a href=\"http://intelligence.org/singularityfaq\">Singularity FAQ</a>, <a href=\"http://intelligenceexplosion.com/\">IntelligenceExplosion.com</a>, and <a href=\"http://friendly-ai.com/\">Friendly-AI.com</a>.</li>\n<li>Wrote three chapters for Springer's upcoming volume <em><a href=\"http://singularityhypothesis.blogspot.com/\">The Singularity Hypothesis</a></em>, along with four other research papers.</li>\n<li>Began work on a new, clearer website design with lots of new content, which should go live Q1 2012.</li>\n<li>Began <a href=\"http://lukeprog.com/SaveTheWorld.html\">outlining open problems in Singularity research</a> to help outside collaborators better understand our research priorities.</li>\n</ul>\n<p>&nbsp;</p>\n<h3 id=\"FUTURE_PLANS_YOU_CAN_HELP_SUPPORT\"><span>FUTURE PLANS YOU CAN HELP SUPPORT</span></h3>\n<p>In the coming year, we plan to do the following:</p>\n<ul>\n<li>Hold our annual Singularity Summit, in San Francisco this year.</li>\n<li>Improve organizational transparency by creating a simpler, easier-to-use website that includes Singularity Institute planning and policy documents.</li>\n<li>Publish a document of open research problems in Singularity Research, to clarify the research space and encourage other researchers to contribute to our mission.</li>\n<li>Add additional skilled researchers to our Research Associates program.</li>\n<li>Publish a well-researched document making the case for existential risk reduction as optimal philanthropy.</li>\n<li>Diversify our funding sources by applying for targeted grants and advertising our affinity credit card program.</li>\n<br><br>\n<p><em>We appreciate your support</em> for our high-impact work. As Skype co-founder Jaan Tallinn said:</p>\n<blockquote>We became the dominant species on this planet by being the most intelligent species around. This century we are going to cede that crown to machines\u2026 Since we have only one shot at getting this transition right, the importance of Singularity Institute's work cannot be overestimated.</blockquote>\n</ul>\n<p>&nbsp;</p>\n<p><em><br>Now is your last chance to make a tax-deductible donation in 2011.</em></p>\n<p><strong id=\"If_you_d_like_to_support_our_work__please_donate_now_\">If you'd like to support our work: <a href=\"http://intelligence.org/donate\">please donate now</a>!</strong></p>", "sections": [{"title": "ARTIFICIAL INTELLIGENCE MORE RELEVANT THAN EVER", "anchor": "ARTIFICIAL_INTELLIGENCE_MORE_RELEVANT_THAN_EVER", "level": 1}, {"title": "ACCOMPLISHMENTS IN 2011", "anchor": "ACCOMPLISHMENTS_IN_2011", "level": 1}, {"title": "FUTURE PLANS YOU CAN HELP SUPPORT", "anchor": "FUTURE_PLANS_YOU_CAN_HELP_SUPPORT", "level": 1}, {"title": "If you'd like to support our work: please donate now!", "anchor": "If_you_d_like_to_support_our_work__please_donate_now_", "level": 2}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "47 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 47, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-28T04:25:56.380Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Allais Malaise", "slug": "seq-rerun-allais-malaise", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/xccwx8SPGH99Yhosu/seq-rerun-allais-malaise", "pageUrlRelative": "/posts/xccwx8SPGH99Yhosu/seq-rerun-allais-malaise", "linkUrl": "https://www.lesswrong.com/posts/xccwx8SPGH99Yhosu/seq-rerun-allais-malaise", "postedAtFormatted": "Wednesday, December 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Allais%20Malaise&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Allais%20Malaise%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxccwx8SPGH99Yhosu%2Fseq-rerun-allais-malaise%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Allais%20Malaise%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxccwx8SPGH99Yhosu%2Fseq-rerun-allais-malaise", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fxccwx8SPGH99Yhosu%2Fseq-rerun-allais-malaise", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 149, "htmlBody": "<p>Today's post, <a href=\"/lw/n1/allais_malaise/\">Allais Malaise</a> was originally published on 21 January 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Offered choices between gambles, people make decision-theoretically inconsistent decisions.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/915/seq_rerun_zut_allais/\">Zut Allais!</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "xccwx8SPGH99Yhosu", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 8.22161977087724e-07, "legacy": true, "legacyId": "11729", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["knpAQ4F3gmguxy39z", "4PFM7dQXtcj4QWEJw", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-28T05:32:39.839Z", "modifiedAt": null, "url": null, "title": "Best of Rationality Quotes, 2011 Edition", "slug": "best-of-rationality-quotes-2011-edition", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:27.324Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DanielVarga", "createdAt": "2009-09-16T22:21:30.125Z", "isAdmin": false, "displayName": "DanielVarga"}, "userId": "rqE4DaRxHwBpQXj96", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/P8frBu55n5FkGPm9i/best-of-rationality-quotes-2011-edition", "pageUrlRelative": "/posts/P8frBu55n5FkGPm9i/best-of-rationality-quotes-2011-edition", "linkUrl": "https://www.lesswrong.com/posts/P8frBu55n5FkGPm9i/best-of-rationality-quotes-2011-edition", "postedAtFormatted": "Wednesday, December 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Best%20of%20Rationality%20Quotes%2C%202011%20Edition&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABest%20of%20Rationality%20Quotes%2C%202011%20Edition%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP8frBu55n5FkGPm9i%2Fbest-of-rationality-quotes-2011-edition%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Best%20of%20Rationality%20Quotes%2C%202011%20Edition%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP8frBu55n5FkGPm9i%2Fbest-of-rationality-quotes-2011-edition", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP8frBu55n5FkGPm9i%2Fbest-of-rationality-quotes-2011-edition", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 213, "htmlBody": "<p>I created a 2011 update to last year's Best of Rationality Quotes collection. (<a href=\"/lw/3cn/best_of_rationality_quotes_20092010/\">Here is the original.</a>)</p>\n<p><strong><a title=\"Best of Rationality Quotes 2011\" href=\"http://people.mokk.bme.hu/%7Edaniel/rationality_quotes_2011/rq_only2011.html\">Best of Rationality Quotes 2011</a></strong>&nbsp;(360kB page, 352 quotes)</p>\n<div>and&nbsp;<strong><a title=\"Best of Rationality Quotes 2009-2011\" href=\"http://people.mokk.bme.hu/%7Edaniel/rationality_quotes_2011/rq.html\">Best of Rationality Quotes 2009-2011</a></strong> (700kB page, 706 quotes)</div>\n<p><a id=\"more\"></a></p>\n<p>The page was built by a short script (<a title=\"source code\" href=\"http://people.mokk.bme.hu/%7Edaniel/rationality_quotes_2011/\">source code here</a>) from all the LW Rationality Quotes threads so far. (We had such a thread each month since April 2009.) The script collects all comments with karma score 10 or more, and sorts them by score. Replies are not collected, only top-level comments.</p>\n<p>A year ago commenters asked for various statistics and top-lists based on the data. This year I re-ran the scripts I wrote to answer those requests. (Source code for these is also at the above link, see the README.) I added the results as comments to this post:</p>\n<ul>\n<li><a href=\"/lw/91s/best_of_rationality_quotes_2011_edition/5jbp\">Top quote contributors by total karma score collected</a></li>\n<li><a href=\"/lw/91s/best_of_rationality_quotes_2011_edition/5jbq\">Top quote contributors by karma score collected in 2011<br /></a></li>\n<li><a href=\"/lw/91s/best_of_rationality_quotes_2011_edition/5jbu\">Top quote contributors by statistical significance level</a>&nbsp; (See&nbsp; <a href=\"/lw/3cn/best_of_rationality_quotes_20092010/37ej\">this comment</a>&nbsp; for a description of this metric.)<a href=\"/lw/3cn/best_of_rationality_quotes_20092010/37ej\"><br /></a></li>\n<li><a href=\"/lw/91s/best_of_rationality_quotes_2011_edition/5jbs\">Top original authors by number of quotes</a></li>\n<li><a href=\"/lw/91s/best_of_rationality_quotes_2011_edition/5jbt\">Top original authors by total karma score collected</a></li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 3}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "P8frBu55n5FkGPm9i", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 30, "baseScore": 41, "extendedScore": null, "score": 8.221867650491476e-07, "legacy": true, "legacyId": "11728", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 29, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZFMqBSX8CnpAwmWes"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-28T09:42:41.794Z", "modifiedAt": null, "url": null, "title": "Eutopia is Scary - for the author", "slug": "eutopia-is-scary-for-the-author", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:15.682Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Stuart_Armstrong", "createdAt": "2009-03-26T10:25:39.189Z", "isAdmin": false, "displayName": "Stuart_Armstrong"}, "userId": "uCfjEXpnchoqDWNoL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PQqKR3mCfK7Ny2jg2/eutopia-is-scary-for-the-author", "pageUrlRelative": "/posts/PQqKR3mCfK7Ny2jg2/eutopia-is-scary-for-the-author", "linkUrl": "https://www.lesswrong.com/posts/PQqKR3mCfK7Ny2jg2/eutopia-is-scary-for-the-author", "postedAtFormatted": "Wednesday, December 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Eutopia%20is%20Scary%20-%20for%20the%20author&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEutopia%20is%20Scary%20-%20for%20the%20author%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPQqKR3mCfK7Ny2jg2%2Feutopia-is-scary-for-the-author%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Eutopia%20is%20Scary%20-%20for%20the%20author%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPQqKR3mCfK7Ny2jg2%2Feutopia-is-scary-for-the-author", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPQqKR3mCfK7Ny2jg2%2Feutopia-is-scary-for-the-author", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 412, "htmlBody": "<p>As Eliezer <a href=\"/lw/xl/eutopia_is_scary/\">makes the point</a> that real utopias will be scary - certainly more scary that my latest <a href=\"/lw/8zs/just_another_day_in_utopia/\">attempt</a>. Mainly they will be scary because they'll be different, and humans don't like different, and it's vital that the authors realise this if they want to create a realistic scenario. It's necessary to craft a world where we would be out of place.</p>\n<p>But it's important to remember that utopias will not be scary for the people living there - the aspects that we find scary at the beginning of the 21st century are not what the locals will be afraid of (put your hand up if you are currently terrified that the majority of women can vote in modern democracies). Scary is in the observer, not the territory.</p>\n<p>This is a special challenge when writing a fictional utopia. Dystopias and flawed utopias are much easier to write than utopias; when you can <a href=\"http://www.fanfiction.net/s/5782108/55/Harry_Potter_and_the_Methods_of_Rationality\">drop an anvil</a> on your protagonist whenever you feel like it, then the tension and interest are much easier to sustain. And the scary parts of utopia are a cheap and easy way of dropping anvils: the reader thrills to this frightening and interesting concept, start objecting/agreeing/thinking about and with it. But it's all ok, you think, it's not dystopia, it's just a scary utopia; you can get your thrills without going astray.</p>\n<p>But all that detracts from your real mission, which is to write a utopia that is genuinely good for the people in it, and would be genuinely interesting to read about <em>even if it weren't scary</em>. I found this particularly hard, and I'd recommend that those who write utopias do a first draft or summary without any scary bits in it - if this doesn't feel interesting on its own, then you've failed.</p>\n<p>Then when you do add the scary bits, make sure they don't suck all the energy out of your story, and make sure you emphasise that the protagonists find these aspects commonplace rather than&nbsp;frightening. There is a length issue - if your story is long, you can afford to put more scary bits in, and even make the reader start seeing them just as the locals do, without the main point being swallowed up. If your story's short, however, I'd cut down on the scary radically: if \"<a href=\"/lw/y8/interlude_with_the_confessor_48/\">rape is legal</a>\" and you only have a few pages, then that's what most people are going to remember about your story. The scariness is a flavouring, not the main dish.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PQqKR3mCfK7Ny2jg2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 15, "extendedScore": null, "score": 3.2e-05, "legacy": true, "legacyId": "11736", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 15, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 41, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["hQSaMafoizBSa3gFR", "sMsvcdxbK2Xqx8EHr", "bojLBvsYck95gbKNM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-28T10:12:25.545Z", "modifiedAt": null, "url": null, "title": "Religious dogma as group identity", "slug": "religious-dogma-as-group-identity", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:38.033Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "uzalud", "createdAt": "2011-07-16T12:16:46.510Z", "isAdmin": false, "displayName": "uzalud"}, "userId": "LtYeek58ACZWdRy8n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/STHhxoodxWjEgR4cj/religious-dogma-as-group-identity", "pageUrlRelative": "/posts/STHhxoodxWjEgR4cj/religious-dogma-as-group-identity", "linkUrl": "https://www.lesswrong.com/posts/STHhxoodxWjEgR4cj/religious-dogma-as-group-identity", "postedAtFormatted": "Wednesday, December 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Religious%20dogma%20as%20group%20identity&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AReligious%20dogma%20as%20group%20identity%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSTHhxoodxWjEgR4cj%2Freligious-dogma-as-group-identity%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Religious%20dogma%20as%20group%20identity%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSTHhxoodxWjEgR4cj%2Freligious-dogma-as-group-identity", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FSTHhxoodxWjEgR4cj%2Freligious-dogma-as-group-identity", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 307, "htmlBody": "<p>I was reading the \"<a href=\"/lw/i6/professing_and_cheering\">Professing and Cheering</a>\" article and it reminded me about some of my own ideas about the role of religious dogma as group identity badges. Here's the gist of it:</p>\n<p style=\"padding-left: 30px;\">Religious and other <strong>dogmas need not make sense</strong>. Indeed, they may work better if they are not logical. Logical and useful ideas pop-up independently and spread easily, and widely accepted ideas are not very good <em>badges</em>. You need a <em>unique </em>idea to <strong>identify </strong>your group. It helps to have a somewhat costly idea as a dogma, because they are hard to fake and hard to deny. People would need to invest in these bad ideas, so they would be less likely to&nbsp;leave the group and&nbsp;confront the sunk cost. Also, it's harder to deny allegiance to the group afterwards, because no one in their right minds would accept an idea that bad for any other reason.</p>\n<p style=\"padding-left: 30px;\">If you have a naive interpretation of the dogma, which regards it as an objective statement about the world, you will tend to question it. When you&rsquo;re contesting the dogma, people won&rsquo;t judge your argument on its merits: they will look at it as an <strong>in-group power struggle</strong>. Either you want to install your own dogma, which makes you a pretender, or you&rsquo;re accepted a competing dogma, which makes you a traitor. Even if they accept that you just don&rsquo;t want to yield to the authority behind the dogma, that makes you a rebel. Dogmas are just off-limits to criticism.</p>\n<p style=\"padding-left: 30px;\">Public display of dismissive attitude to your questioning is also important. Taking it into consideration is in itself a form of treason, as it is interpreted as entertaining the option of joining you against the authority. So it&rsquo;s best to dismiss the heresy quickly and loudly, without&nbsp;<em>thinking </em>about it.</p>\n<p>Do you know of some other texts which shed some light on this idea?</p>\n<p>&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "STHhxoodxWjEgR4cj", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 10, "extendedScore": null, "score": 8.222907106407035e-07, "legacy": true, "legacyId": "11737", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 20, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["RmCjazjupRGcHSm5N"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-28T12:59:35.505Z", "modifiedAt": null, "url": null, "title": "Which biases would you like to keep?", "slug": "which-biases-would-you-like-to-keep", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:05.309Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "cousin_it", "createdAt": "2009-03-26T19:57:07.970Z", "isAdmin": false, "displayName": "cousin_it"}, "userId": "Ht6GLzmaxbXmR6fgy", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/GqbNdhxAThYyAL7B2/which-biases-would-you-like-to-keep", "pageUrlRelative": "/posts/GqbNdhxAThYyAL7B2/which-biases-would-you-like-to-keep", "linkUrl": "https://www.lesswrong.com/posts/GqbNdhxAThYyAL7B2/which-biases-would-you-like-to-keep", "postedAtFormatted": "Wednesday, December 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Which%20biases%20would%20you%20like%20to%20keep%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhich%20biases%20would%20you%20like%20to%20keep%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGqbNdhxAThYyAL7B2%2Fwhich-biases-would-you-like-to-keep%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Which%20biases%20would%20you%20like%20to%20keep%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGqbNdhxAThYyAL7B2%2Fwhich-biases-would-you-like-to-keep", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FGqbNdhxAThYyAL7B2%2Fwhich-biases-would-you-like-to-keep", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 94, "htmlBody": "<p>It probably wouldn't be an unalloyed good for me to overcome ALL the biases.&nbsp;I <a href=\"http://en.wikipedia.org/wiki/Affective_forecasting\">overestimate my future emotional reactions</a> to winning or losing, and I'd like to keep that bias because it helps me win and not lose.&nbsp;I <a href=\"http://en.wikipedia.org/wiki/Rosy_retrospection\">remember only good things about the past</a>, and I'd like to keep that bias because it makes me smile.&nbsp;I <a href=\"http://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect#Supporting_studies\">underestimate my relative competence</a> at things I'm genuinely good at, and I'd like to keep that bias because it encourages me to improve. Which biases would you prefer to keep even though you know you have them?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "GqbNdhxAThYyAL7B2", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 3, "extendedScore": null, "score": 1e-06, "legacy": true, "legacyId": "11738", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-28T13:18:42.536Z", "modifiedAt": null, "url": null, "title": "[Humor] [Link] Eclipse Maid, a posthuman maid role-playing game", "slug": "humor-link-eclipse-maid-a-posthuman-maid-role-playing-game", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:31.268Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Kaj_Sotala", "createdAt": "2009-02-27T19:11:58.811Z", "isAdmin": false, "displayName": "Kaj_Sotala"}, "userId": "qxJ28GN72aiJu96iF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/KZv2fSmgk9vYp9XdS/humor-link-eclipse-maid-a-posthuman-maid-role-playing-game", "pageUrlRelative": "/posts/KZv2fSmgk9vYp9XdS/humor-link-eclipse-maid-a-posthuman-maid-role-playing-game", "linkUrl": "https://www.lesswrong.com/posts/KZv2fSmgk9vYp9XdS/humor-link-eclipse-maid-a-posthuman-maid-role-playing-game", "postedAtFormatted": "Wednesday, December 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BHumor%5D%20%5BLink%5D%20Eclipse%20Maid%2C%20a%20posthuman%20maid%20role-playing%20game&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BHumor%5D%20%5BLink%5D%20Eclipse%20Maid%2C%20a%20posthuman%20maid%20role-playing%20game%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKZv2fSmgk9vYp9XdS%2Fhumor-link-eclipse-maid-a-posthuman-maid-role-playing-game%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BHumor%5D%20%5BLink%5D%20Eclipse%20Maid%2C%20a%20posthuman%20maid%20role-playing%20game%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKZv2fSmgk9vYp9XdS%2Fhumor-link-eclipse-maid-a-posthuman-maid-role-playing-game", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FKZv2fSmgk9vYp9XdS%2Fhumor-link-eclipse-maid-a-posthuman-maid-role-playing-game", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 448, "htmlBody": "<p>What do you get when you put together <a href=\"http://www.eclipsephase.com/\">Eclipse Phase</a>, the science-fiction RPG of posthuman horror, and <a href=\"http://maidrpg.com/\">Maid</a>, the light comedy anime-themed RPG? The answer, of course, is... <a href=\"http://www.penguinking.com/files/pdf/eclipse-maid.pdf\">Eclipse Maid</a>.</p>\n<blockquote>\n<p>In the distant future, humanity's age has passed. Runaway technological development has led to the obsolescence of the human race, and the Solar System is now ruled by vast, posthuman intelligences that explore realms of science and philosophy unimaginable to the unenhanced mind. Their most idle musings spawn computational vistas more complex than entire human civilsations as they plumb the very secrets of the cosmos.<br /><br />Incomprehensibly sophisticated as they may be, however, the posthumans have difficulty dealing with what they euphemistically term the &ldquo;analogue world&rdquo;. To be blunt, they're really quite hopeless when it comes to physical matters. For all their cognitive puissance, they haven't yet freed themselves from certain physical needs &ndash; energy, security, computational machinery on which to run &ndash; and so they create servants to carry out their will, defend their physical forms from rivals and hostile Outsiders, and generally keep things tidy.<br /><br />Thus, even in the age of humanity's eclipse, there are maids.</p>\n</blockquote>\n<p>The Ego (mind) Origins Table contains entries such as Blank (\"You're a brand-new digital sentience, created from scratch to serve your Master\"), Fork (\"You're a scaled-down copy of your Master's own program. You have <em>so many</em> identity issues\"), Uplift (\"The Master gave you intelligence to serve him. Were you animal, or something weird like a plant?\"), and Offspring (\"You're actually a larval posthuman AI, serving your \"parent\" or another Master as a form of vocational training\").</p>\n<p>The selection of Morphs (physical bodies) includes ones such as Chibimorph, Giant Flying Space Whale, Spideroid (\"This Morph resembles an armoured crab or spider the size of a small car. They're designed for combat and reconnaissance, but a hardware glitch causes Egos sleeved into them to become curious and philosophical\"), Braincase (\"A brain in a jar; you communicate using a built-in video screen with a picture of your face on it. While sleeved into this Morph, your intellect is vastly expanded, but you're easily tipped over\"), Nekomorph, and Spectator (\"A hovering metallic sphere with numerous camera-eyes mounted on prehensile robotic stalks. It's equipped with eye lasers for self-defence\"). Special Morph qualities range from Blushes Easily (\"This Morph turns red at the least provocation - even if this <em>makes no sense whatsoever</em>\") to Solar Powered (\"Efficient, environmentally friendly, and useless in the dark\").</p>\n<p>Possible Masters for your maids range from sapient starships to planetary minds to hive minds. You might enjoy reading the PDF even if you didn't know anything about role-playing games.</p>\n<p>Thanks to <a href=\"/user/Risto_Saarelma\">Risto Saarelma</a> for the pointer.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "KZv2fSmgk9vYp9XdS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 17, "baseScore": 12, "extendedScore": null, "score": 8.223599373390401e-07, "legacy": true, "legacyId": "11739", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 21, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-28T13:59:40.790Z", "modifiedAt": null, "url": null, "title": "Can you recognize a random generator?", "slug": "can-you-recognize-a-random-generator", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:20.260Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "uzalud", "createdAt": "2011-07-16T12:16:46.510Z", "isAdmin": false, "displayName": "uzalud"}, "userId": "LtYeek58ACZWdRy8n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/jzSnxprNbnHJj7rmr/can-you-recognize-a-random-generator", "pageUrlRelative": "/posts/jzSnxprNbnHJj7rmr/can-you-recognize-a-random-generator", "linkUrl": "https://www.lesswrong.com/posts/jzSnxprNbnHJj7rmr/can-you-recognize-a-random-generator", "postedAtFormatted": "Wednesday, December 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Can%20you%20recognize%20a%20random%20generator%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ACan%20you%20recognize%20a%20random%20generator%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjzSnxprNbnHJj7rmr%2Fcan-you-recognize-a-random-generator%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Can%20you%20recognize%20a%20random%20generator%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjzSnxprNbnHJj7rmr%2Fcan-you-recognize-a-random-generator", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FjzSnxprNbnHJj7rmr%2Fcan-you-recognize-a-random-generator", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 291, "htmlBody": "<p>I can't seem to get my head around a simple issue of judging probability. Perhaps someone here can point to an obvious flaw in my thinking.</p>\n<p>Let's say we have a binary generator, a machine that outputs a required sequence of ones and zeros according to some internally encapsulated rule (deterministic or probabilistic). All binary generators look alike and you can only infer (a probability of) a rule by looking at its output.</p>\n<p>You have two binary generators: A and B. One of these is a true random generator (fair coin tosser). The other one is a biased random generator: stateless (each digit is independently calculated from those given before), with probability of outputting zero p(0) somewhere between zero and one, but NOT 0.5 - let's say it's uniformly distributed in the range [0; .5) U (.5; 1]. At this point, chances that A is a true random generator are 50%.</p>\n<p>Now you read the output of first ten digits generated by these machines. Machine A outputs 0000000000. Machine B outputs 0010111101. Knowing this, is the probability of machine A being a true random generator now less than 50%?</p>\n<p>My intuition says yes.</p>\n<p>But the probability that a true random generator will output&nbsp;0000000000 should be the same as the probability that it will output&nbsp;0010111101, because all sequences of equal length are equally likely. The biased random generator is also&nbsp;just as likely to output 0000000000 as it is&nbsp;0010111101.</p>\n<p>So there seems to be no reason to think that a machine outputting a sequence of zeros of <em>any size</em>&nbsp;is any more likely to be a biased stateless random generator than it is to be a true random generator.</p>\n<p>I know that you can never <em>know </em>that the generator is&nbsp;truly&nbsp;random. But surely you can statistically discern between random and non-random generators?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "jzSnxprNbnHJj7rmr", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 1, "extendedScore": null, "score": 8.223751643214751e-07, "legacy": true, "legacyId": "11740", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 55, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-28T15:37:24.077Z", "modifiedAt": null, "url": null, "title": "[LINK] G\u00f6del, Escher, Bach read through starting on Reddit", "slug": "link-goedel-escher-bach-read-through-starting-on-reddit", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:24.445Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Karmakaiser", "createdAt": "2011-08-19T20:23:06.809Z", "isAdmin": false, "displayName": "Karmakaiser"}, "userId": "35k6aTWG8i43xjoqR", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hDexMaoqchP7Jvkiv/link-goedel-escher-bach-read-through-starting-on-reddit", "pageUrlRelative": "/posts/hDexMaoqchP7Jvkiv/link-goedel-escher-bach-read-through-starting-on-reddit", "linkUrl": "https://www.lesswrong.com/posts/hDexMaoqchP7Jvkiv/link-goedel-escher-bach-read-through-starting-on-reddit", "postedAtFormatted": "Wednesday, December 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20G%C3%B6del%2C%20Escher%2C%20Bach%20read%20through%20starting%20on%20Reddit&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20G%C3%B6del%2C%20Escher%2C%20Bach%20read%20through%20starting%20on%20Reddit%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhDexMaoqchP7Jvkiv%2Flink-goedel-escher-bach-read-through-starting-on-reddit%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20G%C3%B6del%2C%20Escher%2C%20Bach%20read%20through%20starting%20on%20Reddit%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhDexMaoqchP7Jvkiv%2Flink-goedel-escher-bach-read-through-starting-on-reddit", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhDexMaoqchP7Jvkiv%2Flink-goedel-escher-bach-read-through-starting-on-reddit", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 221, "htmlBody": "<p style=\"text-align: justify;\"><a href=\"http://www.reddit.com/r/GEB/comments/nmy4p/starting_a_readthrough_january_17/\">http://www.reddit.com/r/GEB/comments/nmy4p/starting_a_readthrough_january_17/</a></p>\n<p style=\"text-align: justify;\"><span style=\"background-color: #fafafa; font-family: verdana, arial, helvetica, sans-serif;\">[</span><span style=\"background-color: #fafafa; font-family: verdana, arial, helvetica, sans-serif;\">Context:</span><span style=\"background-color: #fafafa; font-family: verdana, arial, helvetica, sans-serif;\">&nbsp;</span><span class=\" keyNavAnnotation\" style=\"background-color: #fafafa; font-family: verdana, arial, helvetica, sans-serif;\">[1]&nbsp;</span><a class=\" hasListener imgScanned\" style=\"background-color: #fafafa; font-family: verdana, arial, helvetica, sans-serif; text-decoration: none; color: #336699;\" href=\"http://www.reddit.com/r/cogsci/comments/nljge/mit_open_courseware_offers_a_class_called_g%C3%B6del/\">waingro and I want to start a Reddit read-through of GEB</a><span style=\"background-color: #fafafa; font-family: verdana, arial, helvetica, sans-serif;\">.</span></p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; font-family: verdana, arial, helvetica, sans-serif; background-color: #fafafa; padding: 0px;\">I've done an&nbsp;<span class=\" keyNavAnnotation\">[2]&nbsp;</span><a class=\" hasListener imgScanned\" style=\"text-decoration: none; color: #336699;\" href=\"http://ocw.mit.edu/courses/special-programs/sp-258-goedel-escher-bach-spring-2007/\">in-person MIT seminar</a>&nbsp;where we read through the book twice before. I think a subreddit would be a great way to make the same experience available to anyone on the Internet!</p>\n<p style=\"margin-top: 5px; margin-right: 0px; margin-bottom: 5px; margin-left: 0px; font-family: verdana, arial, helvetica, sans-serif; background-color: #fafafa; padding: 0px;\">My plan for when to start would be around January 17. Yes, that's almost a month from now, but it allows time for:</p>\n<ul style=\"font-family: verdana, arial, helvetica, sans-serif; background-color: #fafafa; padding: 0px;\">\n<li style=\"padding: 0px; margin: 0px;\">Publicizing this subreddit</li>\n<li style=\"padding: 0px; margin: 0px;\">Allowing people to find copies of the book if they don't own it</li>\n<li style=\"padding: 0px; margin: 0px;\">Most importantly, it's after the&nbsp;<span class=\" keyNavAnnotation\">[3]&nbsp;</span><a class=\" hasListener imgScanned\" style=\"text-decoration: none; color: #336699;\" href=\"http://en.wikipedia.org/wiki/MIT_Mystery_Hunt\">MIT Mystery Hunt</a>&nbsp;which will be consuming all of my recreational brainpower until then.]</li>\n</ul>\n<p style=\"text-align: justify;\">The read through is organized and run by Rob Speer who taught a&nbsp;seminar&nbsp;on GEB once as a senior and once as a grad student at MIT [1](<a href=\"http://ocw.mit.edu/courses/special-programs/sp-258-goedel-escher-bach-spring-2007/\">http://ocw.mit.edu/courses/special-programs/sp-258-goedel-escher-bach-spring-2007/</a>). GEB is occasionally referenced on LessWrong and is considered and&nbsp;influential&nbsp;book by Eliezer Yudkowsky, the subject of a short review by LukeProg who recently claimed that it \"<span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">[...] defied summary more than all the other books I had previously said \"defied summary.\"</span><span style=\"text-align: justify;\"><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">&nbsp;If you are interested in reading GEB but have not taken the time to do so, I do not need to cite the&nbsp;research on&nbsp;how mechanisms such as commitment contracts are helpful to reaching goals. Joining this group would make the goal read&nbsp;</span></span></span><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\">G&ouml;del, Escher, Bach more reachable than it otherwise would have been.&nbsp;</span></span></p>\n<p style=\"text-align: justify;\">&nbsp;</p>\n<p style=\"text-align: justify;\"><span style=\"font-family: Arial, Helvetica, sans-serif;\"><span style=\"line-height: 19px;\"><br /></span></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hDexMaoqchP7Jvkiv", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 8, "extendedScore": null, "score": 8.224114848991931e-07, "legacy": true, "legacyId": "11741", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 14, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-28T18:15:11.598Z", "modifiedAt": null, "url": null, "title": "[Link] Opacity as a defense against bias?", "slug": "link-opacity-as-a-defense-against-bias", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:05.479Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "malthrin", "createdAt": "2011-03-22T15:23:59.536Z", "isAdmin": false, "displayName": "malthrin"}, "userId": "5b5DcLkcYGD9YGRfF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nmqJ6iuDbP82GprYR/link-opacity-as-a-defense-against-bias", "pageUrlRelative": "/posts/nmqJ6iuDbP82GprYR/link-opacity-as-a-defense-against-bias", "linkUrl": "https://www.lesswrong.com/posts/nmqJ6iuDbP82GprYR/link-opacity-as-a-defense-against-bias", "postedAtFormatted": "Wednesday, December 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Opacity%20as%20a%20defense%20against%20bias%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Opacity%20as%20a%20defense%20against%20bias%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnmqJ6iuDbP82GprYR%2Flink-opacity-as-a-defense-against-bias%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Opacity%20as%20a%20defense%20against%20bias%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnmqJ6iuDbP82GprYR%2Flink-opacity-as-a-defense-against-bias", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnmqJ6iuDbP82GprYR%2Flink-opacity-as-a-defense-against-bias", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 131, "htmlBody": "<p><a href=\"http://www.interfluidity.com/v2/2669.html\">This article</a> makes the claim that financial systems are opaque in order to help us overcome our risk aversion:</p>\n<blockquote>\n<p><span style=\"font-family: optima, verdana, helvetica, arial, sans-serif; font-size: 15px; line-height: 21px; text-align: justify;\">Opacity is not something that can be reformed away, because it is essential to banks&rsquo; economic function of mobilizing the risk-bearing capacity of people who, if fully informed, wouldn&rsquo;t bear the risk. Societies that lack opaque, faintly fraudulent, financial systems fail to develop and prosper. Insufficient economic risks are taken to sustain growth and development. You can have opacity and an industrial economy, or you can have transparency and herd goats.</span></p>\n</blockquote>\n<p>I don't entirely agree. There's also a risk-pooling aspect (similar to insurance) and a middleman research value-add (individuals don't have to study thousands of possible investments). Still, it's an interesting idea. What other areas of the economy could make a similar argument?</p>\n<p><span style=\"font-family: optima, verdana, helvetica, arial, sans-serif; font-size: 15px; line-height: 21px; text-align: justify;\"><br /></span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nmqJ6iuDbP82GprYR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 7, "baseScore": 5, "extendedScore": null, "score": 8.22470138224589e-07, "legacy": true, "legacyId": "11742", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-28T21:42:58.466Z", "modifiedAt": null, "url": null, "title": "For-Profit Rationality Training", "slug": "for-profit-rationality-training", "viewCount": null, "lastCommentedAt": "2012-08-03T01:12:31.773Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "ksvanhorn", "createdAt": "2011-01-10T19:23:33.231Z", "isAdmin": false, "displayName": "ksvanhorn"}, "userId": "fiau5fcXpbad9d6D4", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/P7PqtLBwzfxKXWMp9/for-profit-rationality-training", "pageUrlRelative": "/posts/P7PqtLBwzfxKXWMp9/for-profit-rationality-training", "linkUrl": "https://www.lesswrong.com/posts/P7PqtLBwzfxKXWMp9/for-profit-rationality-training", "postedAtFormatted": "Wednesday, December 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20For-Profit%20Rationality%20Training&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AFor-Profit%20Rationality%20Training%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP7PqtLBwzfxKXWMp9%2Ffor-profit-rationality-training%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=For-Profit%20Rationality%20Training%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP7PqtLBwzfxKXWMp9%2Ffor-profit-rationality-training", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FP7PqtLBwzfxKXWMp9%2Ffor-profit-rationality-training", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 959, "htmlBody": "<p>As I've been reading through various articles and their comments on Less Wrong, I've noticed a theme that has appeared repeatedly: a frustration that we are not seeing more practical benefits from studying rationality. For example, Eliezer writes in <a href=\"/lw/2c/a_sense_that_more_is_possible/\">A Sense that More Is Possible</a>,</p>\n<blockquote>\n<p>Why aren't \"rationalists\" surrounded by a visible aura of formidability? Why aren't they found at the top level of every elite selected on any basis that has anything to do with thought? Why do most \"rationalists\" just seem like ordinary people...</p>\n</blockquote>\n<p>Yvain writes in <a href=\"/lw/9p/rationality_its_not_that_great/\">Extreme Rationality: It's Not That Great</a>,</p>\n<blockquote>\n<p>...I've gotten countless clarity-of-mind benefits from Overcoming Bias' x-rationality, but practical benefits? Aside from some peripheral disciplines, I can't think of any.</p>\n</blockquote>\n<p>patrissimo wrote in a <a href=\"/lw/2p5/humans_are_not_automatically_strategic/2l5h\">comment</a> on another article,</p>\n<blockquote>\n<p>Sorry, folks, but compared to the self-help/self-development community, Less Wrong is currently UTTERLY LOSING at self-improvement and life optimization.</p>\n</blockquote>\n<p>These writers have also offered some suggestions for improving the situation. Eliezer <a href=\"/lw/2c/a_sense_that_more_is_possible/\">writes</a>,</p>\n<blockquote>\n<p>Of this [question] there are several answers; but one of them, surely, is that they have received less systematic training of rationality in a less systematic context than a first-dan black belt gets in hitting people.</p>\n</blockquote>\n<p>patrissimo <a href=\"/lw/2po/selfimprovement_or_shiny_distraction_why_less/\">describes</a> what he thinks an effective rationality practice would look like.</p>\n<blockquote><ol>\n<li>It is a group of people who gather in person to train specific skills.</li>\n<li>While there are some theoreticians of the art, most people participate by learning it and doing it, not theorizing about it.</li>\n<li>Thus the main focus is on local practice groups, along with the global coordination to maximize their effectiveness (marketing, branding, integration of knowledge, common infrastructure). As a result, <em>it is driven by the needs of the learners</em> [emphasis added].</li>\n<li>You have to sweat, but the result is you get stronger.</li>\n<li>You improve by learning from those better than you, competing with those at your level, and teaching those below you.</li>\n<li>It is run by a professional, or at least <em>someone getting paid</em> [emphasis added] for their hobby. The practicants receive personal benefit from their practice, in particular from the value-added of the coach, enough to pay for talented coaches.</li>\n</ol></blockquote>\n<p>Dan Nuffer and I have decided that it's time to stop talking and start doing. We are in the very early stages of creating a business to help people improve their lives by training them in instrumental rationality. We've done some preliminary market research to get an idea of where the opportunities might lie. In fact, this venture got started when, on a whim, I ran a poll on <a href=\"http://ask500people.com\">ask500people.com</a> asking,</p>\n<blockquote>\n<p>Would you pay $75 for an interactive online course teaching effective decision-making skills?</p>\n</blockquote>\n<p>I got 299 responses in total. These are the numbers that responded with \"likely\" or \"very likely\":</p>\n<ul>\n<li>23.4% (62) overall.</li>\n<li>49% (49 of 100) of the respondents from India.</li>\n<li>10.6% (21 of 199) of the respondents <em>not</em> from India.</li>\n<li>9.0% (8 of 89) of the respondents from the U.S.</li>\n</ul>\n<p>These numbers were much higher than I expected, especially the numbers from India, which still puzzle me. Googling around a bit, though, I found an instructor-led online decision-making course for $130, and a one-day decision-making workshop offered in the UK for &pound;200 (over $350)... and the Google keyword tool returns a large number of search terms (800) related to \"decision-making\", many of them with a high number of monthly searches.</p>\n<p>So it appears that there may be a market for training in effective decision-making -- something that could be the first step towards a more comprehensive training program in instrumental rationality. Some obvious market segments to consider are business decision makers, small business owners, and intelligent people of an analytical bent (e.g., the kind of people who find Less Wrong interesting). An important subset of this last group are INTJ personality types; I don't know if there is an effective way to find and market to specific Meyers-Briggs personality types, but I'm looking into it.</p>\n<p>\"Life coaching\" is a proven business, and its growing popularity suggests the potential for a \"decision coaching\" service; in fact, helping people with big decisions is one of the things a life coach does. One life coach of 12 years described a typical client as age 35 to 55, who is \"at a crossroads, must make a decision and is sick of choosing out of safety and fear.\" Life coaches working with individuals typically charge around $100 to $300 per hour. As far as I can tell, training in decision analysis / instrumental rationality is not commonly found among life coaches. Surely we can do better.</p>\n<p>Can we do effective training online? patrissimo thinks that gathering in person is <a href=\"/lw/2po/selfimprovement_or_shiny_distraction_why_less/\">necessary</a>, but I'm not so sure. His evidence is that \"all the people who have replied to me so far saying they get useful rationality practice out of the LW community said the growth came through attending local meetups.\" To me this is weak evidence -- it seems to say more about the effectiveness of local meetups vs. just reading about rationality. In any event, it's worth testing whether online training can work, since</p>\n<ul>\n<li>not everyone can go to meetups,</li>\n<li>it should be easier to scale up, and</li>\n<li>not to put too fine a point on it, but online training is probably more profitable.</li>\n</ul>\n<p>To conclude, one of the things an entrepreneur needs to do is \"get out of the building\" and talk to members of the target market. We're interested in hearing what you think. What ideas do you think would be most effective in training for instrumental rationality, and why? What would you personally want from a rationality training program? What kinds of products / services related to rationality training would you be interesting in buying?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 1, "xexCWMyds6QLWognu": 1, "Ng8Gice9KNkncxqcj": 1, "3QnDqGSdRMA5mdMM6": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "P7PqtLBwzfxKXWMp9", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 27, "baseScore": 31, "extendedScore": null, "score": 8.225473844686884e-07, "legacy": true, "legacyId": "11743", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 24, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 39, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["Nu3wa6npK4Ry66vFp", "LgavAYtzFQZKg95WC", "uFYQaGCRwt3wKtyZP"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-28T22:02:40.566Z", "modifiedAt": null, "url": null, "title": "Meetup : San Diego experimental meetup", "slug": "meetup-san-diego-experimental-meetup", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:23.890Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Mercurial", "createdAt": "2011-04-21T03:59:51.257Z", "isAdmin": false, "displayName": "Mercurial"}, "userId": "2dGsX6cZSR9PmQyBq", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/EMp29u53ncm4WGAqQ/meetup-san-diego-experimental-meetup", "pageUrlRelative": "/posts/EMp29u53ncm4WGAqQ/meetup-san-diego-experimental-meetup", "linkUrl": "https://www.lesswrong.com/posts/EMp29u53ncm4WGAqQ/meetup-san-diego-experimental-meetup", "postedAtFormatted": "Wednesday, December 28th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20San%20Diego%20experimental%20meetup&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20San%20Diego%20experimental%20meetup%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEMp29u53ncm4WGAqQ%2Fmeetup-san-diego-experimental-meetup%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20San%20Diego%20experimental%20meetup%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEMp29u53ncm4WGAqQ%2Fmeetup-san-diego-experimental-meetup", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FEMp29u53ncm4WGAqQ%2Fmeetup-san-diego-experimental-meetup", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 363, "htmlBody": "<h2>Discussion article for the meetup : <a href='/meetups/5q'>San Diego experimental meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong>&#32;\n\t\t    \t<span class=\"date\">15 January 2012 01:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong>&#32;\n\t\t    \t<span class=\"address\">6380 Del Cerro Blvd. San Diego, CA 92120</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're having a meetup in <a href=\"http://knbwinecellars.com/\" rel=\"nofollow\">our usual haunt</a> on Sunday, January 15th at 1pm. Food and drink are available for purchase, though you'll need your ID to get anything alcoholic.</p>\n\n<p>In the spirit of developing <a href=\"http://lesswrong.com/lw/8g0/rationality_dojo_examples/\">Rationality Dojo</a> curricula, we're going to test a newly developed training session. We want to see how it works and to get your feedback on how it came across to you. And I think we'll have good fun in the process. :-)</p>\n\n<p>If there's time and interest, I'm also willing to continue the discussion from <a href=\"http://lesswrong.com/lw/8eq/meetup_san_diego_meetup/\">last time</a> by giving a presentation on what I call the Enneagram keys. These are guidelines for interacting with the types that let you (a) build a good relationship with someone of a known type, (b) hit their hot buttons like nothing else (which is really meant to help you know why they get hurt or angry and how to avoid doing that), and (c) open communications and build rapport. Because each Ennea-type has a relatively specific way of reacting to specific keys, you can also use the keys to test hypotheses about someone's type. For instance, if you don't know whether you're dealing with a <a href=\"http://www.enneagraminstitute.com/TypeFour.asp\" rel=\"nofollow\">Four</a> or a <a href=\"http://www.enneagraminstitute.com/TypeNine.asp\" rel=\"nofollow\">Nine</a>, you can try suggesting something optimistic about their future (\"Today might be a bit drab, but tomorrow is a new day!\"). A Nine will typically respond with something positive (a spacey smile if they're withdrawn or a big grin and nods if they're feeling confident), whereas a Four will typically turn cold and might even give you an eyeroll. So the keys are where the Enneagram most blatantly <a href=\"http://lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">pays rent</a>.</p>\n\n<p>But I'm definitely open to other options! I could stand to train my calibration better, and I have as yet to play any of the calibration games others have brought on occasion. (I'm looking at you, Jennifer!)</p>\n\n<p>So! Please, show up, help us develop a solid Rationality Dojo curriculum by joining us in <a href=\"http://lesswrong.com/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">getting stronger</a>, and join in on some delightful conversation!</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2>Discussion article for the meetup : <a href='/meetups/5q'>San Diego experimental meetup</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "EMp29u53ncm4WGAqQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 1, "baseScore": 1, "extendedScore": null, "score": 8.225547095137416e-07, "legacy": true, "legacyId": "11744", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 0, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___San_Diego_experimental_meetup\">Discussion article for the meetup : <a href=\"/meetups/5q\">San Diego experimental meetup</a></h2>\n\t\t  <div class=\"meetup-meta\">\n\t\t\t\t<p>\n\t\t    \t<strong>WHEN:</strong> \n\t\t    \t<span class=\"date\">15 January 2012 01:00:00PM (-0800)</span><br>\n\t\t\t\t</p>\n\t\t\t\t<p>\n\t\t    \t<strong>WHERE:</strong> \n\t\t    \t<span class=\"address\">6380 Del Cerro Blvd. San Diego, CA 92120</span>\n\t\t\t\t</p>\n\t\t  </div><!-- .meta -->\n\t\t  <div id=\"\" class=\"content\">\n\t\t    <div class=\"md\"><p>We're having a meetup in <a href=\"http://knbwinecellars.com/\" rel=\"nofollow\">our usual haunt</a> on Sunday, January 15th at 1pm. Food and drink are available for purchase, though you'll need your ID to get anything alcoholic.</p>\n\n<p>In the spirit of developing <a href=\"http://lesswrong.com/lw/8g0/rationality_dojo_examples/\">Rationality Dojo</a> curricula, we're going to test a newly developed training session. We want to see how it works and to get your feedback on how it came across to you. And I think we'll have good fun in the process. :-)</p>\n\n<p>If there's time and interest, I'm also willing to continue the discussion from <a href=\"http://lesswrong.com/lw/8eq/meetup_san_diego_meetup/\">last time</a> by giving a presentation on what I call the Enneagram keys. These are guidelines for interacting with the types that let you (a) build a good relationship with someone of a known type, (b) hit their hot buttons like nothing else (which is really meant to help you know why they get hurt or angry and how to avoid doing that), and (c) open communications and build rapport. Because each Ennea-type has a relatively specific way of reacting to specific keys, you can also use the keys to test hypotheses about someone's type. For instance, if you don't know whether you're dealing with a <a href=\"http://www.enneagraminstitute.com/TypeFour.asp\" rel=\"nofollow\">Four</a> or a <a href=\"http://www.enneagraminstitute.com/TypeNine.asp\" rel=\"nofollow\">Nine</a>, you can try suggesting something optimistic about their future (\"Today might be a bit drab, but tomorrow is a new day!\"). A Nine will typically respond with something positive (a spacey smile if they're withdrawn or a big grin and nods if they're feeling confident), whereas a Four will typically turn cold and might even give you an eyeroll. So the keys are where the Enneagram most blatantly <a href=\"http://lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">pays rent</a>.</p>\n\n<p>But I'm definitely open to other options! I could stand to train my calibration better, and I have as yet to play any of the calibration games others have brought on occasion. (I'm looking at you, Jennifer!)</p>\n\n<p>So! Please, show up, help us develop a solid Rationality Dojo curriculum by joining us in <a href=\"http://lesswrong.com/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">getting stronger</a>, and join in on some delightful conversation!</p>\n\n<p>See you there!</p></div>\n\t\t  </div><!-- .content -->\n\t\t<h2 id=\"Discussion_article_for_the_meetup___San_Diego_experimental_meetup1\">Discussion article for the meetup : <a href=\"/meetups/5q\">San Diego experimental meetup</a></h2>", "sections": [{"title": "Discussion article for the meetup : San Diego experimental meetup", "anchor": "Discussion_article_for_the_meetup___San_Diego_experimental_meetup", "level": 1}, {"title": "Discussion article for the meetup : San Diego experimental meetup", "anchor": "Discussion_article_for_the_meetup___San_Diego_experimental_meetup1", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "1 comment"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SwGhpL2uk3JzrkD2c", "KFAHuWkeFcFxBM3nz", "a7n8GdKiAZRX86T5A", "DoLQN5ryZ9XkZjq5h"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-29T00:10:58.774Z", "modifiedAt": null, "url": null, "title": "Less Wrong mentoring thread", "slug": "less-wrong-mentoring-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:17:09.878Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "John_Maxwell_IV", "createdAt": "2009-02-27T05:45:59.993Z", "isAdmin": false, "displayName": "John_Maxwell"}, "userId": "mcKSiwq2TBrTMZS6X", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cWFRxGamBQcWRBpvL/less-wrong-mentoring-thread", "pageUrlRelative": "/posts/cWFRxGamBQcWRBpvL/less-wrong-mentoring-thread", "linkUrl": "https://www.lesswrong.com/posts/cWFRxGamBQcWRBpvL/less-wrong-mentoring-thread", "postedAtFormatted": "Thursday, December 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Less%20Wrong%20mentoring%20thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALess%20Wrong%20mentoring%20thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcWFRxGamBQcWRBpvL%2Fless-wrong-mentoring-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Less%20Wrong%20mentoring%20thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcWFRxGamBQcWRBpvL%2Fless-wrong-mentoring-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcWFRxGamBQcWRBpvL%2Fless-wrong-mentoring-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 238, "htmlBody": "<p>I just had a 17-year-old Less Wronger e-mail me for advice regarding the <a href=\"http://www.thielfellowship.org/\">Thiel Fellowship</a>&nbsp;after reading my <a href=\"/lw/3iq/being_rational_and_being_productive_similar_core/\">application essay</a>&nbsp;from last year when I was 19. We had a long instant message conversation where I gave him a lot of advice which he seemed to find highly useful (my biggest piece of advice was to start teaching himself programming using <a href=\"http://learnpythonthehardway.org/\">Learn Python the Hard Way</a>, shamelessly asking for help using a pseudonym on IRC channels, forums, and <a href=\"http://stackoverflow.com/\">Stack Overflow</a>&nbsp;if he got stuck).</p>\n<p>It seems likely that there are other Less Wrong users who still live with their parents who could benefit from life and career advice. I'm especially interested in reaching those who see reducing <a href=\"/lw/8f0/existential_risk/\">existential risk</a>&nbsp;as a major life goal.</p>\n<p>A related idea is for people who have some goal they want to achieve, like having a romantic relationship with someone of their preferred gender or being admitted to a prestigious graduate school, to pair up with someone who has accomplished that goal.</p>\n<p>So if you're a young person who would like advice, an older person who would like to give advice, a person who wants to accomplish a goal, or a person who has accomplished a goal and is willing to help others accomplish that goal, consider leaving a comment on this post so you can find your counterpart.</p>\n<p>I realize this post is a bit open ended--consider it an experiment in tapping Less Wrong's social capital in a novel fashion.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"izp6eeJJEg9v5zcur": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cWFRxGamBQcWRBpvL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 35, "baseScore": 43, "extendedScore": null, "score": 8.9e-05, "legacy": true, "legacyId": "11745", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 44, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 92, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["keieeJWh4aNjvqPcn", "FGTgeweYNxmMBx4fz"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 5, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-29T05:16:35.754Z", "modifiedAt": null, "url": null, "title": "Meetup : First Philadelphia Meetup of 2012", "slug": "meetup-first-philadelphia-meetup-of-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:24.853Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "DasAllFolks", "createdAt": "2011-12-23T17:34:00.896Z", "isAdmin": false, "displayName": "DasAllFolks"}, "userId": "s7hnECaJ7cv5oJApQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/iWRXRfHdozPtxg9eR/meetup-first-philadelphia-meetup-of-2012", "pageUrlRelative": "/posts/iWRXRfHdozPtxg9eR/meetup-first-philadelphia-meetup-of-2012", "linkUrl": "https://www.lesswrong.com/posts/iWRXRfHdozPtxg9eR/meetup-first-philadelphia-meetup-of-2012", "postedAtFormatted": "Thursday, December 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meetup%20%3A%20First%20Philadelphia%20Meetup%20of%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeetup%20%3A%20First%20Philadelphia%20Meetup%20of%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiWRXRfHdozPtxg9eR%2Fmeetup-first-philadelphia-meetup-of-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meetup%20%3A%20First%20Philadelphia%20Meetup%20of%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiWRXRfHdozPtxg9eR%2Fmeetup-first-philadelphia-meetup-of-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FiWRXRfHdozPtxg9eR%2Fmeetup-first-philadelphia-meetup-of-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 229, "htmlBody": "<h2>Discussion article for the meetup : <a href=\"/meetups/5r\">First Philadelphia Meetup of 2012</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">08 January 2012 02:00:00PM (-0500)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">8620 Germantown Ave Philadelphia, PA 19118</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Hello Philadelphian Less-Wrongers!</p>\n<p>A couple of people have expressed interest in restarting regular Philadelphia-area Less Wrong meetups, so I thought it would be fun to hold one once the December holiday season has wound to a close.</p>\n<p>Tentative date is Sunday, January 8th at 2 P.M.; if you're interested in coming, please fill out the Doodle poll at <a href=\"http://www.doodle.com/s2afikbngrz3d42z\" target=\"_blank\">http://www.doodle.com/s2afikbngrz3d42z</a> to give me an idea of how your schedule looks for the week (may be handy in determining if a better time will work after all).</p>\n<p>Location is also flexible; the meetup is currently scheduled at the Chestnut Hill Coffee Company for its pleasant atmosphere, proximity to free Chestnut Hill parking, and - as the name might suggest - excellent coffee, but other ideas are also quite welcome.</p>\n<p>Formal agenda will include general introductions, picking a time and place for regular meetups, and deciding what content we want to pursue at them: I was thinking that a combination of discussions of rational methods, project planning, and self-improvement a la Overcoming Bias NYC might be a good place to start, but we can obviously discuss this more in person.</p>\n<p>Have a very Happy New Year, and I look forward to meeting all of you soon!</p>\n</div>\n</div>\n<!-- .content -->\n<h2>Discussion article for the meetup : <a href=\"/meetups/5r\">First Philadelphia Meetup of 2012</a></h2>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "iWRXRfHdozPtxg9eR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": 6, "extendedScore": null, "score": 8.227160700651644e-07, "legacy": true, "legacyId": "11758", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<h2 id=\"Discussion_article_for_the_meetup___First_Philadelphia_Meetup_of_2012\">Discussion article for the meetup : <a href=\"/meetups/5r\">First Philadelphia Meetup of 2012</a></h2>\n<div class=\"meetup-meta\">\n<p><strong>WHEN:</strong> <span class=\"date\">08 January 2012 02:00:00PM (-0500)</span></p>\n<p><strong>WHERE:</strong> <span class=\"address\">8620 Germantown Ave Philadelphia, PA 19118</span></p>\n</div>\n<!-- .meta -->\n<div class=\"content\">\n<div class=\"md\">\n<p>Hello Philadelphian Less-Wrongers!</p>\n<p>A couple of people have expressed interest in restarting regular Philadelphia-area Less Wrong meetups, so I thought it would be fun to hold one once the December holiday season has wound to a close.</p>\n<p>Tentative date is Sunday, January 8th at 2 P.M.; if you're interested in coming, please fill out the Doodle poll at <a href=\"http://www.doodle.com/s2afikbngrz3d42z\" target=\"_blank\">http://www.doodle.com/s2afikbngrz3d42z</a> to give me an idea of how your schedule looks for the week (may be handy in determining if a better time will work after all).</p>\n<p>Location is also flexible; the meetup is currently scheduled at the Chestnut Hill Coffee Company for its pleasant atmosphere, proximity to free Chestnut Hill parking, and - as the name might suggest - excellent coffee, but other ideas are also quite welcome.</p>\n<p>Formal agenda will include general introductions, picking a time and place for regular meetups, and deciding what content we want to pursue at them: I was thinking that a combination of discussions of rational methods, project planning, and self-improvement a la Overcoming Bias NYC might be a good place to start, but we can obviously discuss this more in person.</p>\n<p>Have a very Happy New Year, and I look forward to meeting all of you soon!</p>\n</div>\n</div>\n<!-- .content -->\n<h2 id=\"Discussion_article_for_the_meetup___First_Philadelphia_Meetup_of_20121\">Discussion article for the meetup : <a href=\"/meetups/5r\">First Philadelphia Meetup of 2012</a></h2>", "sections": [{"title": "Discussion article for the meetup : First Philadelphia Meetup of 2012", "anchor": "Discussion_article_for_the_meetup___First_Philadelphia_Meetup_of_2012", "level": 1}, {"title": "Discussion article for the meetup : First Philadelphia Meetup of 2012", "anchor": "Discussion_article_for_the_meetup___First_Philadelphia_Meetup_of_20121", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "11 comments"}], "headingsCount": 4}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-29T06:00:42.463Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Against Discount Rates", "slug": "seq-rerun-against-discount-rates", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5dnAcAGd9bcXAkDzM/seq-rerun-against-discount-rates", "pageUrlRelative": "/posts/5dnAcAGd9bcXAkDzM/seq-rerun-against-discount-rates", "linkUrl": "https://www.lesswrong.com/posts/5dnAcAGd9bcXAkDzM/seq-rerun-against-discount-rates", "postedAtFormatted": "Thursday, December 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Against%20Discount%20Rates&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Against%20Discount%20Rates%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5dnAcAGd9bcXAkDzM%2Fseq-rerun-against-discount-rates%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Against%20Discount%20Rates%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5dnAcAGd9bcXAkDzM%2Fseq-rerun-against-discount-rates", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5dnAcAGd9bcXAkDzM%2Fseq-rerun-against-discount-rates", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 155, "htmlBody": "<p>Today's post, <a href=\"/lw/n2/against_discount_rates/\">Against Discount Rates</a> was originally published on 21 January 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>We really shouldn't care less about the future than we do about the present.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/91t/seq_rerun_allais_malaise/\">Allais Malaise</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5dnAcAGd9bcXAkDzM", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 8.227324768500175e-07, "legacy": true, "legacyId": "11761", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["AvJeJw52NL9y7RJDJ", "xccwx8SPGH99Yhosu", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-29T10:14:30.847Z", "modifiedAt": null, "url": null, "title": "Negentropy Overrated?", "slug": "negentropy-overrated", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:07.169Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "paulfchristiano", "createdAt": "2010-07-28T17:04:08.586Z", "isAdmin": false, "displayName": "paulfchristiano"}, "userId": "gb44edJjXhte8DA3A", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/2BJBZh7Rvxr6GaLQs/negentropy-overrated", "pageUrlRelative": "/posts/2BJBZh7Rvxr6GaLQs/negentropy-overrated", "linkUrl": "https://www.lesswrong.com/posts/2BJBZh7Rvxr6GaLQs/negentropy-overrated", "postedAtFormatted": "Thursday, December 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Negentropy%20Overrated%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANegentropy%20Overrated%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2BJBZh7Rvxr6GaLQs%2Fnegentropy-overrated%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Negentropy%20Overrated%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2BJBZh7Rvxr6GaLQs%2Fnegentropy-overrated", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F2BJBZh7Rvxr6GaLQs%2Fnegentropy-overrated", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 121, "htmlBody": "<p><a href=\"http://ordinaryideas.wordpress.com/2011/12/21/entropy-reversibility-and-uncomputation/\">This</a>&nbsp;post may be interesting to some LWers.</p>\n<p>In summary: it looks like our universe can support reversible computers which don't create entropy.&nbsp;Reversible computers can simulate irreversible computers, with pretty mild time and space blowup.&nbsp;So if moral value comes from computation, negentropy probably won't be such an important resource for distant future folks, and if the universe lasts a long time we may be able to simulate astronomically long-lived civilizations (easily 10^(10^25) clock cycles, using current estimates and neglecting other obstructions).&nbsp;</p>\n<p>Has this been discussed before, and/or is there some reason that it doesn't work or isn't relevant? I suspect that this consideration won't matter in the long run, but it is at least interesting and seems to significantly deflate (long-run) concerns about entropy.&nbsp;</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"csMv9MvvjYJyeHqoo": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "2BJBZh7Rvxr6GaLQs", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 33, "extendedScore": null, "score": 6.8e-05, "legacy": true, "legacyId": "11764", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 33, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 32, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 6, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-29T11:49:19.077Z", "modifiedAt": null, "url": null, "title": "Why would a free human society be in agreement on how to alter itself?", "slug": "why-would-a-free-human-society-be-in-agreement-on-how-to", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:08.432Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Multiheaded", "createdAt": "2011-07-02T10:10:20.692Z", "isAdmin": false, "displayName": "Multiheaded"}, "userId": "moGiw35FowgiAnzfg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/oGAGDAk6zCoRdGr9v/why-would-a-free-human-society-be-in-agreement-on-how-to", "pageUrlRelative": "/posts/oGAGDAk6zCoRdGr9v/why-would-a-free-human-society-be-in-agreement-on-how-to", "linkUrl": "https://www.lesswrong.com/posts/oGAGDAk6zCoRdGr9v/why-would-a-free-human-society-be-in-agreement-on-how-to", "postedAtFormatted": "Thursday, December 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20would%20a%20free%20human%20society%20be%20in%20agreement%20on%20how%20to%20alter%20itself%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20would%20a%20free%20human%20society%20be%20in%20agreement%20on%20how%20to%20alter%20itself%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoGAGDAk6zCoRdGr9v%2Fwhy-would-a-free-human-society-be-in-agreement-on-how-to%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20would%20a%20free%20human%20society%20be%20in%20agreement%20on%20how%20to%20alter%20itself%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoGAGDAk6zCoRdGr9v%2Fwhy-would-a-free-human-society-be-in-agreement-on-how-to", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FoGAGDAk6zCoRdGr9v%2Fwhy-would-a-free-human-society-be-in-agreement-on-how-to", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 330, "htmlBody": "<p>So I've been grappling with <a href=\"/lw/y8/interlude_with_the_confessor_48/\">these</a> <a href=\"/lw/1f4/less_wrong_qa_with_eliezer_yudkowsky_ask_your/18yw\">discussions</a> on the plausibility or implausibility of redefining cultural boundaries of several sorts to (as the radical example of self-alteration by Eliezer goes) make non-consensual sex as tolerated and tolerable as non-consensual conversation. Said discussions being quite separate from EY's largely accepted underlying statement of principle; \"Something in a world better than ours is likely to creep us out, at least purely emotionally.\"</p>\r\n<p>Then I suddenly saw a plot hole in the source material. The story outright mentions that internal disagreement was present at all points in its culture during the transition from 20th century sexual mores to the end result. <em>So</em>... why didn't anyone who saw the trend of self-modification and couldn't accept it for themselves or their children, <em>DO</em> anything about it? To use another locally popular example, if Gandhi knew that his children, whom he had begun to raise in pacifism, were to be given pills that'd make them enjoy killing in self-defense*, why the hell wouldn't he and his followers split off? And, supposing that a <strong>large</strong> amount of Indians and others at the time disliked that slippery slope, wouldn't their forked culture make a considerable impact on the future's \"mainstream\" society, such as a <strong>much</strong> greater level of catering to pacifists in all areas of life than e.g. the visibility of Jewish kosher goods and services in today's West?</p>\r\n<p>In other words, why didn't the story mention its (wealthy, permissive, libertarian) society having other arrangements in such a contentious matter - including, with statistical near-certainty, one of the half-dosen characters on the bridge of the Impossible Possible World? I strongly suspect that this blow to suspension of disbelief, while swallowed by most commenters who began talking about the implied binary choice (modify entire culture to tolerate rape/society at large stays frozen in stone), added a great deal of needless, irrational controversy, fueling the already incensed emotions and such.</p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p>\r\n<p>*(I know the real Gandhi didn't totally abhor self-defense, but that's beside the point)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "oGAGDAk6zCoRdGr9v", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": -2, "extendedScore": null, "score": 8.228621590476052e-07, "legacy": true, "legacyId": "11766", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["bojLBvsYck95gbKNM"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-29T13:09:35.483Z", "modifiedAt": null, "url": null, "title": "Rationality of sometimes missing the point of the stated question, and of certain type of defensive reasoning", "slug": "rationality-of-sometimes-missing-the-point-of-the-stated", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:24.230Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dmytry", "createdAt": "2009-12-03T17:11:53.492Z", "isAdmin": false, "displayName": "Dmytry"}, "userId": "AjtmA2qtA8sdiMbru", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/YCGoBNMGoseamBpRg/rationality-of-sometimes-missing-the-point-of-the-stated", "pageUrlRelative": "/posts/YCGoBNMGoseamBpRg/rationality-of-sometimes-missing-the-point-of-the-stated", "linkUrl": "https://www.lesswrong.com/posts/YCGoBNMGoseamBpRg/rationality-of-sometimes-missing-the-point-of-the-stated", "postedAtFormatted": "Thursday, December 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20of%20sometimes%20missing%20the%20point%20of%20the%20stated%20question%2C%20and%20of%20certain%20type%20of%20defensive%20reasoning&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20of%20sometimes%20missing%20the%20point%20of%20the%20stated%20question%2C%20and%20of%20certain%20type%20of%20defensive%20reasoning%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYCGoBNMGoseamBpRg%2Frationality-of-sometimes-missing-the-point-of-the-stated%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20of%20sometimes%20missing%20the%20point%20of%20the%20stated%20question%2C%20and%20of%20certain%20type%20of%20defensive%20reasoning%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYCGoBNMGoseamBpRg%2Frationality-of-sometimes-missing-the-point-of-the-stated", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FYCGoBNMGoseamBpRg%2Frationality-of-sometimes-missing-the-point-of-the-stated", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1048, "htmlBody": "<p>Imagine that you are being asked a question; a moral question involving an imaginary world. From the prior experience with people, you have learnt that people behave in a certain way; people are, for the most part, applied thinkers and whatever is your answer, it will become a <a href=\"/lw/k5/cached_thoughts/\">cached thought</a> that will be applied in the real world, should the situation arise. The whole rationale behind thinking of imaginary worlds may be to create cached thoughts.</p>\n<p>Your answer probably won't stay segregated in the well defined imaginary world for any longer than it takes the person who asked the question to switch the topic; it is the real world consequences you should be most concerned about.</p>\n<p>Given this, would it not be rational to perhaps miss the point but answer that sort of question in the real world way?</p>\n<p>To give a specific example, consider this question from <a href=\"/lw/2k/the_least_convenient_possible_world/\">The Least Convenient Possible World</a> :</p>\n<blockquote>\n<p>You are a doctor in a small rural hospital. You have ten patients, each of whom is dying for the lack of a separate organ; that is, one person needs a heart transplant, another needs a lung transplant, another needs a kidney transplant, and so on. A traveller walks into the hospital, mentioning how he has no family and no one knows that he's there. All of his organs seem healthy. You realize that by killing this traveller and distributing his organs among your patients, you could save ten lives. Would this be moral or not?</p>\n</blockquote>\n<p>First of all, note that the question is not abstract \"If <em>[you are absolutely certain that]</em> the only way to save 10 innocent people is to kill 1 innocent person, is it moral to kill?\" . There's a lot of details. We are even told that this 1 is a traveller, I am not exactly sure why but I would think that it references kin selection related instincts; the traveller has lower utility to the village than a resident.</p>\n<p>In light of how people process answers to such detailed questions, and how the answers are incorporated into the thought patterns - which might end up used in the real world - is it not in fact most rational not to address that kind of question exactly as specified, but to point out that one of the patients could be taken apart for the best of other 9 ? And to point out the poor quality of life and life expectancy of the surviving patients?</p>\n<p>Indeed, as a solution one could gather all the patients and let them discuss how they solve the problem; perhaps one will decide to be terminated, perhaps they will decide to draw straws, perhaps those with the worst prognosis will draw the straws. If they're comatose one could have a panel of 12 peers make the decision. There could easily be <em>trillions</em> of possible solutions to this not-so-abstract problem, and the trillions is not a figure of speech here. Privileging one solution is similar to <a href=\"/lw/19m/privileging_the_hypothesis/\">privileging a hypothesis</a>.</p>\n<p>In this example, the utility of any villager can be higher to the doctor than of the traveller who will never return, and hence the doctor would opt to take apart the traveller for the spare parts, instead of picking one of the patients based on some cost-benefit metric and sacrificing that patient for the best of the others. The choice we're asked about turn out to be just one of the options, chosen selfishly; it is deep selfishness of the doctor that makes him realize that killing the traveller may be justified, but not realize the same about one of the patients, for the selfishness did bias his thought towards exploring one line of reasoning but not the other.</p>\n<p>Of course one can say that I missed the point, and one can employ backward reasoning and tweak the example by stating that those people are aliens, and the traveller is totally histocompatible with each patient, but none of the patients are compatible with each other (that's how alien immune systems work: there are some rare mutant aliens whose tissues are not at all rejected by any other).</p>\n<p>But to do so would be to completely lose the point of why we should expend mental effort to search for alternative solutions. Yes it is defensive thinking - what does it defend us from though? In this case it defends us from making a decision based on incomplete reasoning or a faulty model. All real world decisions are, too, made in imaginary worlds - in what we imagine the world to be.</p>\n<p>Morality requires a sort of 'due process'; the good faith reasoning effort to find the best solution rather than the first solution that the selfish subroutines conveniently present for consideration; to explore the models for faults; to try and think outside the highly abbreviated version of the real world one might initially construct when considering the circumstances.</p>\n<p>The imaginary world situation here is just an example; and so is the answer an example of reasoning that should be applied to such situations - the reasoning that strives to explore the solution space and test the model for accuracy.</p>\n<p>Something else which is tangential to the main point of this article. If I had 10 differently broken cars and 1 working one, I wouldn't even think of taking apart the working one for spare parts, I'd take apart one of the broken ones for spare parts. Same would apply to e.g. having 11 children, 1 healthy, 10 in need of replacement of different organs. The option that one would be thinking of is to take the one that's least likely to survive, sacrifice for other 9; no one in their mind would even think of taking apart the healthy one unless there's very compelling prior reasons. This seem to be something that we would only consider for any time for a stranger. There may be hidden kin selection based cognitive biases that affect our moral reasoning.</p>\n<p>edit: I don't know if it is OK to be editing published articles but I'm a bit of obsessively compulsive perfectionist and I plan on improving it for publishing it in lesswrong (edit: i mean not lesswrong discussion), so I am going to take liberty at improving some of the points but perhaps also removing the duplicate argumentation and cutting down the verbosity.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "YCGoBNMGoseamBpRg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 22, "extendedScore": null, "score": 6.3e-05, "legacy": true, "legacyId": "11765", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["2MD3NMLBPCqPfnfre", "neQ7eXuaXpiYw7SBy", "X2AD2LgtKgkRNPj2a"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-29T14:51:27.834Z", "modifiedAt": null, "url": null, "title": "Should we discount extraordinary implications?", "slug": "should-we-discount-extraordinary-implications", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:24.118Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ZLHKNrCH5A9tJvrax/should-we-discount-extraordinary-implications", "pageUrlRelative": "/posts/ZLHKNrCH5A9tJvrax/should-we-discount-extraordinary-implications", "linkUrl": "https://www.lesswrong.com/posts/ZLHKNrCH5A9tJvrax/should-we-discount-extraordinary-implications", "postedAtFormatted": "Thursday, December 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Should%20we%20discount%20extraordinary%20implications%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AShould%20we%20discount%20extraordinary%20implications%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZLHKNrCH5A9tJvrax%2Fshould-we-discount-extraordinary-implications%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Should%20we%20discount%20extraordinary%20implications%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZLHKNrCH5A9tJvrax%2Fshould-we-discount-extraordinary-implications", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FZLHKNrCH5A9tJvrax%2Fshould-we-discount-extraordinary-implications", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1113, "htmlBody": "<p>(Spawned by an <a href=\"/r/discussion/lw/91e/singularity_institute_100000_endofyear_fundraiser/5jih\">exchange</a> between Louie Helm and Holden Karnofsky.)</p>\n<p><strong><a href=\"http://www.urbandictionary.com/define.php?term=tl%3Bdr\">tl;dr: </a></strong></p>\n<p>The field of formal rationality is relatively new and I believe that we would be well-advised to discount some of its logical implications that advocate extraordinary actions.<br /><br />Our current methods might turn out to be biased in new and unexpected ways. <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">Pascal's mugging</a>, the <a href=\"/lw/17h/the_lifespan_dilemma/\">Lifespan Dilemma</a>, blackmailing and the wrath of L&ouml;b's theorem are just <a href=\"/lw/8ts/open_problems_related_to_the_singularity_draft_1/\">a few examples</a> on how an agent build according to our current understanding of rationality could fail. <br /><br />Bayes&rsquo; Theorem, the expected utility formula, and Solomonoff induction are all reasonable heuristics. Yet those theories are not enough to build an agent that will be reliably in helping us to achieve our values, even if those values were thoroughly defined. <br /><br />If we wouldn't trust a superhuman agent equipped with our current grasp of rationality to be reliably in extrapolating our volition, how can we trust ourselves to arrive at correct answers given what we know?</p>\n<p>We should of course <a href=\"/lw/s0/where_recursive_justification_hits_bottom/\">continue to</a> use our best methods to decide what to do. But I believe that we should also draw a line somewhere when it comes to extraordinary implications.</p>\n<h3>Intuition, Rationality and Extraordinary Implications<br /></h3>\n<blockquote>\n<p>It doesn't feel to me like 3^^^^3 lives are <em>really</em> at stake, even at very tiny probability.&nbsp; I'd sooner question my grasp of \"rationality\" than give five dollars to a Pascal's Mugger because I thought it was \"rational\". &mdash; Eliezer Yudkowsky</p>\n</blockquote>\n<p>Holden Karnofsky <a href=\"/r/discussion/lw/91e/singularity_institute_100000_endofyear_fundraiser/5jjw\">is suggesting</a> that in some cases we should follow the simple rule that <em>\"extraordinary claims require extraordinary evidence\"</em>.</p>\n<p>I think that we should sometimes demand particular proof P; and if proof P is not available, then we should discount seemingly absurd or undesirable consequences even if <a href=\"/lw/1ph/youre_entitled_to_arguments_but_not_that\">our theories disagree</a>.</p>\n<p>I am not referring to the weirdness of the conclusions but the foreseeable scope of the consequences of being wrong about them. We should be careful in using the <em>implied</em> scope of certain conclusions to outweigh their low probability. I <em>feel</em> we should put more weight to the consequences of our conclusions being wrong than being right.</p>\n<p>As an example take the idea of <a href=\"http://www.paul-almond.com/CivilizationLevelQuantumSuicide.pdf\">quantum suicide</a> and assume it would make sense under certain circumstances. I wouldn&rsquo;t commit quantum suicide even given a high confidence in the <a href=\"/lw/r8/and_the_winner_is_manyworlds\">many-worlds interpretation of quantum mechanics</a> being true. Logical implications just don&rsquo;t seem enough in some cases.</p>\n<p>To be clear, extrapolations work and often are the best we can do. But since there are problems such as the above, that we perceive to be undesirable and that lead to absurd actions and their consequences, I think it is reasonable to ask for some upper and lower bounds regarding the use and scope of certain heuristics.</p>\n<p>We are not going to stop pursuing whatever terminal goal we have chosen just because someone promises us even more utility if we do what that person wants. We are not going to stop loving our girlfriend just because there are other people who do not approve our relationship and who together would experience more happiness if we divorced than the combined happiness of us and our girlfriend being in love. Therefore we already informally established some upper and lower bounds.</p>\n<p>I have read about people who became very disturbed and depressed <a href=\"/lw/38u/best_career_models_for_doing_research/344l\">taking ideas too seriously</a>. That way madness lies, and I am not willing to choose that path yet.</p>\n<p>Maybe I am simply biased and have been unable to overcome it yet. But my best guess right now is <em></em>that we simply have to <em>draw a lot of arbitrary lines</em> and <a href=\"/lw/17h/the_lifespan_dilemma/13pq\"><em>arbitrarily refuse some steps</em></a>.</p>\n<p>Taking into account considerations of vast utility or low probability quickly leads to chaos theoretic considerations like the <a rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Butterfly_effect\">butterfly effect</a>. As a computationally bounded and psychical unstable agent I am unable to cope with that. Consequently I see no other way than to neglect the <a href=\"http://d.repec.org/n?u=RePEc:crf:wpaper:10-14&amp;r=exp\">moral impossibility</a> of extreme uncertainty.</p>\n<p>Until the problems are resolved, or rationality is sufficiently established, I will continue to put vastly more weight on empirical evidence and my intuition than on logical implications, if only because I still lack the necessary educational background to trust my comprehension and judgement of the various underlying concepts and methods used to arrive at those implications.</p>\n<h3>Expected Utility Maximization and Complex Values</h3>\n<p>One of the problems with my current grasp of rationality that I perceive to be unacknowledged are the consequences of expected utility maximization with respect to human nature and our complex values.</p>\n<p>I am still genuinely confused about what a person <em>should</em> do. I don't even know how much sense that concept makes. Does expected utility maximization <a href=\"/lw/6oo/to_what_degree_do_we_have_goals/\">has anything to do</a> with being human?</p>\n<p>Those people who take existential risks seriously and who are currently involved in their mitigation seem to be disregarding many other activities that humans usually deem valuable because the expected utility of saving the world does outweigh the pursuit of other goals. I do not disagree with that assessment but find it troubling.</p>\n<p>The problem is, will there ever be anything but a single goal, a goal that can <span class=\"zj\"> either be more effectively realized and optimized to yield the most utility or whose associated expected utility simply outweighs all other values?</span></p>\n<p><span class=\"zj\">Assume that humanity managed to create a friendly AI (FAI). Given the enormous amount of resources that each human is poised to consume until <a href=\"http://en.wikipedia.org/wiki/Future_of_an_expanding_universe#Timeline\">the dark era of the universe</a>, wouldn't the same arguments that now suggest that we should contribute money to existential risk charities then suggest that we should donate our resources to the friendly AI? Our resources could enable it to find a way to either travel back in time, leave the universe or hack the matrix. Anything that could avert the <a href=\"http://en.wikipedia.org/wiki/Heat_death_of_the_universe\">end of the universe</a> and allow the FAI to support many more agents has effectively infinite expected utility.</span></p>\n<p><span class=\"zj\">The sensible decision would be to concentrate on those </span><span class=\"zj\">scenarios </span><span class=\"zj\">with the highest expected utility now, e.g. solving friendly AI, and worry about those problems later. But not only does the same argument always work but the question is also relevant to the nature of friendly AI and our ultimate goals. Is expected utility maximization even compatible with our nature? Does expected utility maximization lead to world states in which <a href=\"http://wireheading.com/\">wireheading</a> is favored, either directly or indirectly by focusing solely on a single high-utility goal that does outweigh all other goals?</span></p>\n<h3><span class=\"zj\">Conclusion<br /></span></h3>\n<ol>\n<li><span class=\"zj\">Being able to prove something mathematically doesn't prove its relation to reality.</span></li>\n<li><span class=\"zj\">R</span>elativity is <em>less wrong</em> than Newtonian mechanics but it still breaks down in describing singularities including the very beginning of the universe. </li>\n</ol>\n<p><span class=\"zj\">It seems to me that our notion of rationality is not the last word on the topic and that we shouldn't act as if it was.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ZLHKNrCH5A9tJvrax", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 25, "baseScore": 4, "extendedScore": null, "score": 3e-05, "legacy": true, "legacyId": "11767", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>(Spawned by an <a href=\"/r/discussion/lw/91e/singularity_institute_100000_endofyear_fundraiser/5jih\">exchange</a> between Louie Helm and Holden Karnofsky.)</p>\n<p><strong id=\"tl_dr__\"><a href=\"http://www.urbandictionary.com/define.php?term=tl%3Bdr\">tl;dr: </a></strong></p>\n<p>The field of formal rationality is relatively new and I believe that we would be well-advised to discount some of its logical implications that advocate extraordinary actions.<br><br>Our current methods might turn out to be biased in new and unexpected ways. <a href=\"/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\">Pascal's mugging</a>, the <a href=\"/lw/17h/the_lifespan_dilemma/\">Lifespan Dilemma</a>, blackmailing and the wrath of L\u00f6b's theorem are just <a href=\"/lw/8ts/open_problems_related_to_the_singularity_draft_1/\">a few examples</a> on how an agent build according to our current understanding of rationality could fail. <br><br>Bayes\u2019 Theorem, the expected utility formula, and Solomonoff induction are all reasonable heuristics. Yet those theories are not enough to build an agent that will be reliably in helping us to achieve our values, even if those values were thoroughly defined. <br><br>If we wouldn't trust a superhuman agent equipped with our current grasp of rationality to be reliably in extrapolating our volition, how can we trust ourselves to arrive at correct answers given what we know?</p>\n<p>We should of course <a href=\"/lw/s0/where_recursive_justification_hits_bottom/\">continue to</a> use our best methods to decide what to do. But I believe that we should also draw a line somewhere when it comes to extraordinary implications.</p>\n<h3 id=\"Intuition__Rationality_and_Extraordinary_Implications\">Intuition, Rationality and Extraordinary Implications<br></h3>\n<blockquote>\n<p>It doesn't feel to me like 3^^^^3 lives are <em>really</em> at stake, even at very tiny probability.&nbsp; I'd sooner question my grasp of \"rationality\" than give five dollars to a Pascal's Mugger because I thought it was \"rational\". \u2014 Eliezer Yudkowsky</p>\n</blockquote>\n<p>Holden Karnofsky <a href=\"/r/discussion/lw/91e/singularity_institute_100000_endofyear_fundraiser/5jjw\">is suggesting</a> that in some cases we should follow the simple rule that <em>\"extraordinary claims require extraordinary evidence\"</em>.</p>\n<p>I think that we should sometimes demand particular proof P; and if proof P is not available, then we should discount seemingly absurd or undesirable consequences even if <a href=\"/lw/1ph/youre_entitled_to_arguments_but_not_that\">our theories disagree</a>.</p>\n<p>I am not referring to the weirdness of the conclusions but the foreseeable scope of the consequences of being wrong about them. We should be careful in using the <em>implied</em> scope of certain conclusions to outweigh their low probability. I <em>feel</em> we should put more weight to the consequences of our conclusions being wrong than being right.</p>\n<p>As an example take the idea of <a href=\"http://www.paul-almond.com/CivilizationLevelQuantumSuicide.pdf\">quantum suicide</a> and assume it would make sense under certain circumstances. I wouldn\u2019t commit quantum suicide even given a high confidence in the <a href=\"/lw/r8/and_the_winner_is_manyworlds\">many-worlds interpretation of quantum mechanics</a> being true. Logical implications just don\u2019t seem enough in some cases.</p>\n<p>To be clear, extrapolations work and often are the best we can do. But since there are problems such as the above, that we perceive to be undesirable and that lead to absurd actions and their consequences, I think it is reasonable to ask for some upper and lower bounds regarding the use and scope of certain heuristics.</p>\n<p>We are not going to stop pursuing whatever terminal goal we have chosen just because someone promises us even more utility if we do what that person wants. We are not going to stop loving our girlfriend just because there are other people who do not approve our relationship and who together would experience more happiness if we divorced than the combined happiness of us and our girlfriend being in love. Therefore we already informally established some upper and lower bounds.</p>\n<p>I have read about people who became very disturbed and depressed <a href=\"/lw/38u/best_career_models_for_doing_research/344l\">taking ideas too seriously</a>. That way madness lies, and I am not willing to choose that path yet.</p>\n<p>Maybe I am simply biased and have been unable to overcome it yet. But my best guess right now is <em></em>that we simply have to <em>draw a lot of arbitrary lines</em> and <a href=\"/lw/17h/the_lifespan_dilemma/13pq\"><em>arbitrarily refuse some steps</em></a>.</p>\n<p>Taking into account considerations of vast utility or low probability quickly leads to chaos theoretic considerations like the <a rel=\"nofollow\" href=\"http://en.wikipedia.org/wiki/Butterfly_effect\">butterfly effect</a>. As a computationally bounded and psychical unstable agent I am unable to cope with that. Consequently I see no other way than to neglect the <a href=\"http://d.repec.org/n?u=RePEc:crf:wpaper:10-14&amp;r=exp\">moral impossibility</a> of extreme uncertainty.</p>\n<p>Until the problems are resolved, or rationality is sufficiently established, I will continue to put vastly more weight on empirical evidence and my intuition than on logical implications, if only because I still lack the necessary educational background to trust my comprehension and judgement of the various underlying concepts and methods used to arrive at those implications.</p>\n<h3 id=\"Expected_Utility_Maximization_and_Complex_Values\">Expected Utility Maximization and Complex Values</h3>\n<p>One of the problems with my current grasp of rationality that I perceive to be unacknowledged are the consequences of expected utility maximization with respect to human nature and our complex values.</p>\n<p>I am still genuinely confused about what a person <em>should</em> do. I don't even know how much sense that concept makes. Does expected utility maximization <a href=\"/lw/6oo/to_what_degree_do_we_have_goals/\">has anything to do</a> with being human?</p>\n<p>Those people who take existential risks seriously and who are currently involved in their mitigation seem to be disregarding many other activities that humans usually deem valuable because the expected utility of saving the world does outweigh the pursuit of other goals. I do not disagree with that assessment but find it troubling.</p>\n<p>The problem is, will there ever be anything but a single goal, a goal that can <span class=\"zj\"> either be more effectively realized and optimized to yield the most utility or whose associated expected utility simply outweighs all other values?</span></p>\n<p><span class=\"zj\">Assume that humanity managed to create a friendly AI (FAI). Given the enormous amount of resources that each human is poised to consume until <a href=\"http://en.wikipedia.org/wiki/Future_of_an_expanding_universe#Timeline\">the dark era of the universe</a>, wouldn't the same arguments that now suggest that we should contribute money to existential risk charities then suggest that we should donate our resources to the friendly AI? Our resources could enable it to find a way to either travel back in time, leave the universe or hack the matrix. Anything that could avert the <a href=\"http://en.wikipedia.org/wiki/Heat_death_of_the_universe\">end of the universe</a> and allow the FAI to support many more agents has effectively infinite expected utility.</span></p>\n<p><span class=\"zj\">The sensible decision would be to concentrate on those </span><span class=\"zj\">scenarios </span><span class=\"zj\">with the highest expected utility now, e.g. solving friendly AI, and worry about those problems later. But not only does the same argument always work but the question is also relevant to the nature of friendly AI and our ultimate goals. Is expected utility maximization even compatible with our nature? Does expected utility maximization lead to world states in which <a href=\"http://wireheading.com/\">wireheading</a> is favored, either directly or indirectly by focusing solely on a single high-utility goal that does outweigh all other goals?</span></p>\n<h3 id=\"Conclusion\"><span class=\"zj\">Conclusion<br></span></h3>\n<ol>\n<li><span class=\"zj\">Being able to prove something mathematically doesn't prove its relation to reality.</span></li>\n<li><span class=\"zj\">R</span>elativity is <em>less wrong</em> than Newtonian mechanics but it still breaks down in describing singularities including the very beginning of the universe. </li>\n</ol>\n<p><span class=\"zj\">It seems to me that our notion of rationality is not the last word on the topic and that we shouldn't act as if it was.</span></p>", "sections": [{"title": "tl;dr: ", "anchor": "tl_dr__", "level": 2}, {"title": "Intuition, Rationality and Extraordinary Implications", "anchor": "Intuition__Rationality_and_Extraordinary_Implications", "level": 1}, {"title": "Expected Utility Maximization and Complex Values", "anchor": "Expected_Utility_Maximization_and_Complex_Values", "level": 1}, {"title": "Conclusion", "anchor": "Conclusion", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "104 comments"}], "headingsCount": 6}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 107, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["a5JAiTdytou3Jg749", "9RCoE7jmmvGd5Zsh2", "p9DdaSFzJHW9At3JM", "C8nEXTcjZb9oauTCW", "vqbieD9PHG8RRJddu", "9cgBF6BQ2TRB3Hy4E", "ePA4NDzZkunz98tLx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-29T16:52:57.817Z", "modifiedAt": null, "url": null, "title": "Are Yearly/Monthly Book Suggestion Threads a Good Idea?", "slug": "are-yearly-monthly-book-suggestion-threads-a-good-idea", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:07.341Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "RobertLumley", "createdAt": "2011-04-28T23:53:16.950Z", "isAdmin": false, "displayName": "RobertLumley"}, "userId": "KXJjaWHDF4HJ2DF7a", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/E8gc4H7Xorx4ebq8d/are-yearly-monthly-book-suggestion-threads-a-good-idea", "pageUrlRelative": "/posts/E8gc4H7Xorx4ebq8d/are-yearly-monthly-book-suggestion-threads-a-good-idea", "linkUrl": "https://www.lesswrong.com/posts/E8gc4H7Xorx4ebq8d/are-yearly-monthly-book-suggestion-threads-a-good-idea", "postedAtFormatted": "Thursday, December 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Are%20Yearly%2FMonthly%20Book%20Suggestion%20Threads%20a%20Good%20Idea%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAre%20Yearly%2FMonthly%20Book%20Suggestion%20Threads%20a%20Good%20Idea%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE8gc4H7Xorx4ebq8d%2Fare-yearly-monthly-book-suggestion-threads-a-good-idea%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Are%20Yearly%2FMonthly%20Book%20Suggestion%20Threads%20a%20Good%20Idea%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE8gc4H7Xorx4ebq8d%2Fare-yearly-monthly-book-suggestion-threads-a-good-idea", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FE8gc4H7Xorx4ebq8d%2Fare-yearly-monthly-book-suggestion-threads-a-good-idea", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 158, "htmlBody": "<p>I thought I had seen a thread recently asking for book recommendations, and I had a recommendation to post there, but <a href=\"/lw/2kk/book_recommendations/\">the thread I found</a> is from about a year and a half ago. I didn't want to make an entire thread for my book suggestion, that would be a bit extreme (I will post it in the comments though). So I was wondering what people's thoughts were on a yearly or monthly discussion thread recommending good new books, perhaps with a brief synopsis or explanation. Would we have enough new recommendations to fill one? (I'm still astonished there are as few repeats in the quote threads as there are) Would we have so many that monthly would be a good idea? Should there be any guidelines like there are for the quotes threads? The only one I can think of would be just requiring a brief summary and why one thinks LWers might be interested in it.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "E8gc4H7Xorx4ebq8d", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 31, "baseScore": 4, "extendedScore": null, "score": 8.229751440776029e-07, "legacy": true, "legacyId": "11768", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["uWhcKdToYEMPArHsv"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-29T20:37:02.626Z", "modifiedAt": null, "url": null, "title": "Smart and under 20? Peter Thiel wants to pay you to not go to school.", "slug": "smart-and-under-20-peter-thiel-wants-to-pay-you-to-not-go-to", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:21.392Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/hK7cWNoEoCFqNwapK/smart-and-under-20-peter-thiel-wants-to-pay-you-to-not-go-to", "pageUrlRelative": "/posts/hK7cWNoEoCFqNwapK/smart-and-under-20-peter-thiel-wants-to-pay-you-to-not-go-to", "linkUrl": "https://www.lesswrong.com/posts/hK7cWNoEoCFqNwapK/smart-and-under-20-peter-thiel-wants-to-pay-you-to-not-go-to", "postedAtFormatted": "Thursday, December 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Smart%20and%20under%2020%3F%20Peter%20Thiel%20wants%20to%20pay%20you%20to%20not%20go%20to%20school.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASmart%20and%20under%2020%3F%20Peter%20Thiel%20wants%20to%20pay%20you%20to%20not%20go%20to%20school.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhK7cWNoEoCFqNwapK%2Fsmart-and-under-20-peter-thiel-wants-to-pay-you-to-not-go-to%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Smart%20and%20under%2020%3F%20Peter%20Thiel%20wants%20to%20pay%20you%20to%20not%20go%20to%20school.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhK7cWNoEoCFqNwapK%2Fsmart-and-under-20-peter-thiel-wants-to-pay-you-to-not-go-to", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FhK7cWNoEoCFqNwapK%2Fsmart-and-under-20-peter-thiel-wants-to-pay-you-to-not-go-to", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 353, "htmlBody": "<p>Peter Thiel is offering another round of \"20 under 20\" Fellowships. The application deadline is December 31st. We know many of the current Thiel fellows here in the Bay Area, and it's a great opportunity. Here's the official letter from the Thiel Foundation:</p>\n<p>&nbsp;</p>\n<blockquote>\n<p>\n<p>We are delighted to announce that we are now accepting applications for the 2012 class of Thiel Fellows. &nbsp;The 20 Under 20 Thiel Fellowship is a no-strings-attached grant of $100,000 that lets extraordinary young adults skip college and focus on their work, their research, and their self-education. The deadline is December 31st.</p>\n<p>The 2011 class of Fellows includes 24 people who are tackling breakthroughs in hardware and robotics, making energy plentiful, making markets more effective, challenging the notion that there is only one way to get an education, and extending the human lifespan. Several of them have already launched companies, secured financing, and won prestigious awards. As they&rsquo;re demonstrating, you don&rsquo;t need college to invent the future (you can read about their progress in a <a href=\"http://techcrunch.com/2011/12/06/thiel-fellows-are-making-the-grade/\">recent article in TechCrunch</a>).</p>\n<p>If you&rsquo;re under twenty and love science or technology, we hope you'll consider joining the 2012 class. Go to <a href=\"http://www.thielfellowship.org/\">ThielFellowship.org</a> and apply to change the world. There&rsquo;s no cost to apply. Fellows will be appointed this spring and begin two-year fellowships in the summer of 2012.</p>\n<p>If you&rsquo;re twenty or over, we have a different request. Think of the smartest, most creative person you know who&rsquo;s 19 or younger. Sit down and talk with that person about her or his goals and interests. For some people, such as future doctors, the time and cost of four years of college may be worth it. But for those who plan to invent things or start companies, starting now may make more sense. Please send such visionaries and tinkerers our way.</p>\n<p>Millions of people enjoy a higher quality of life because smart people like Steve Jobs, Muriel Siebert, Benjamin Franklin, Mark Zuckerberg, and hundreds of others skipped college to start a project that couldn&rsquo;t wait.</p>\n<p>We hope you&rsquo;ll help us spread the word.</p>\n<p>Thanks,</p>\n<p>The Thiel Foundation</p>\n</p>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "hK7cWNoEoCFqNwapK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 10, "extendedScore": null, "score": 2.3e-05, "legacy": true, "legacyId": "11769", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 22, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-29T22:25:22.710Z", "modifiedAt": null, "url": null, "title": "Ambient control of the arithmetical hierarchy?", "slug": "ambient-control-of-the-arithmetical-hierarchy", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:08.755Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Will_Newsome", "createdAt": "2010-02-25T03:52:25.697Z", "isAdmin": false, "displayName": "Will_Newsome"}, "userId": "CxM9n2EDSn4AYgLdi", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Tq2DkTyB3TnAwtM9i/ambient-control-of-the-arithmetical-hierarchy", "pageUrlRelative": "/posts/Tq2DkTyB3TnAwtM9i/ambient-control-of-the-arithmetical-hierarchy", "linkUrl": "https://www.lesswrong.com/posts/Tq2DkTyB3TnAwtM9i/ambient-control-of-the-arithmetical-hierarchy", "postedAtFormatted": "Thursday, December 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Ambient%20control%20of%20the%20arithmetical%20hierarchy%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AAmbient%20control%20of%20the%20arithmetical%20hierarchy%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTq2DkTyB3TnAwtM9i%2Fambient-control-of-the-arithmetical-hierarchy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Ambient%20control%20of%20the%20arithmetical%20hierarchy%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTq2DkTyB3TnAwtM9i%2Fambient-control-of-the-arithmetical-hierarchy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FTq2DkTyB3TnAwtM9i%2Fambient-control-of-the-arithmetical-hierarchy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 150, "htmlBody": "<p>Responding to this:&nbsp;<a href=\"http://lesswrong.com/r/discussion/lw/8ys/a_way_of_specifying_utility_functions_for_udt/\">http://lesswrong.com/r/discussion/lw/8ys/a_way_of_specifying_utility_functions_for_udt/</a></p>\n<p>I had a similar idea a few months ago that highlights different aspects of the problem which I find confusing. In my version the UDT agent controls bits of Chaitin's constant instead of the universal prior directly, seeing as one of the programs that the oracle (which you can derive from Chaitin's omega) has to solve the halting problem for is the UDT agent's. But since the oracle for the oracle you get from Chaitin's constant depends on the latter oracle's bits, you seem to be able to ambiently control THE ENTIRE ARITHMETICAL HIERARCHY SAY WHAT!? That's the confusing part; isn't your one true oracle supposed to screen you off from higher oracles? Or is that only insofar as you can computably verify?</p>\n<p>Anyway I like this theme of controlling computational contexts as it forms a tight loop between agent and environment, something currently lacking. Keep it up comrades!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Tq2DkTyB3TnAwtM9i", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 24, "baseScore": 6, "extendedScore": null, "score": 8.230988654389025e-07, "legacy": true, "legacyId": "11771", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 11, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["dsbjbopyxbQMK93om"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-29T22:33:17.497Z", "modifiedAt": null, "url": null, "title": "\"Misbehaving Machines: The Emulated Brains of Transhumanist Dreams\", Corry Shores", "slug": "misbehaving-machines-the-emulated-brains-of-transhumanist", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:19.504Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/zswEixLariPTgxLcC/misbehaving-machines-the-emulated-brains-of-transhumanist", "pageUrlRelative": "/posts/zswEixLariPTgxLcC/misbehaving-machines-the-emulated-brains-of-transhumanist", "linkUrl": "https://www.lesswrong.com/posts/zswEixLariPTgxLcC/misbehaving-machines-the-emulated-brains-of-transhumanist", "postedAtFormatted": "Thursday, December 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%22Misbehaving%20Machines%3A%20The%20Emulated%20Brains%20of%20Transhumanist%20Dreams%22%2C%20Corry%20Shores&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%22Misbehaving%20Machines%3A%20The%20Emulated%20Brains%20of%20Transhumanist%20Dreams%22%2C%20Corry%20Shores%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzswEixLariPTgxLcC%2Fmisbehaving-machines-the-emulated-brains-of-transhumanist%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%22Misbehaving%20Machines%3A%20The%20Emulated%20Brains%20of%20Transhumanist%20Dreams%22%2C%20Corry%20Shores%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzswEixLariPTgxLcC%2Fmisbehaving-machines-the-emulated-brains-of-transhumanist", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FzswEixLariPTgxLcC%2Fmisbehaving-machines-the-emulated-brains-of-transhumanist", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 4389, "htmlBody": "<p><a href=\"http://jetpress.org/v22/shores.htm\">&ldquo;Misbehaving Machines: The Emulated Brains of Transhumanist Dreams&rdquo;</a>, by Corry Shores (grad student; <a href=\"https://twitter.com/#!/corryshores\">Twitter</a>, <a href=\"http://piratesandrevolutionaries.blogspot.com/\">blog</a>) is another recent JET paper. Abstract:</p>\n<blockquote>\n<p>Enhancement technologies may someday grant us capacities far beyond what we now consider humanly possible. Nick Bostrom and Anders Sandberg suggest that we might survive the deaths of our physical bodies by living as computer emulations.&shy;&shy; In 2008, they issued a report, or &ldquo;roadmap,&rdquo; from a conference where experts in all relevant fields collaborated to determine the path to &ldquo;whole brain emulation.&rdquo; Advancing this technology could also aid philosophical research. Their &ldquo;roadmap&rdquo; defends certain philosophical assumptions required for this technology&rsquo;s success, so by determining the reasons why it succeeds or fails, we can obtain empirical data for philosophical debates regarding our mind and selfhood. The scope ranges widely, so I merely survey some possibilities, namely, I argue that this technology could help us determine</p>\n<ol style=\"list-style-type: decimal\">\n<li>if the mind is an emergent phenomenon,</li>\n<li>if analog technology is necessary for brain emulation, and</li>\n<li>if neural randomness is so wild that a complete emulation is impossible.</li>\n</ol></blockquote>\n<h1 id=\"introduction\"><a id=\"more\"></a></h1>\n<h1><a href=\"#TOC\"><span class=\"header-section-number\">1</span> Introduction</a></h1>\n<blockquote>\n<p>Whole brain emulation succeeds if it merely replicates human neural functioning. Yet for Nick Bostrom and Anders Sandberg, its success increases when it perfectly replicates a specific person&rsquo;s brain&hellip;In 2008, Nick Bostrom and Anders Sandberg <a href=\"http://www.fhi.ox.ac.uk/reports/2008-3.pdf\">compiled the findings</a> from a conference of philosophers, technicians and other experts who had gathered to formulate a &ldquo;roadmap&rdquo; of the individual steps and requirements that could plausibly develop this technology&hellip;As I proceed, I will look more closely at these philosophical assumptions individually. For now let it suffice to say that I will adopt the basic framework of their philosophy of mind.</p>\n<p>&hellip;I will explore research that calls into question certain other ones. For example, although the authors diminish the importance of analog computation and noise interference, there are findings and compelling arguments that suggest otherwise. As well, there is reason to think that the brain&rsquo;s computational dynamics would not call for Bostrom&rsquo;s and Sandberg&rsquo;s hierarchical model for the mind&rsquo;s emergence. And finally, I will argue on these bases that if brain emulation were to be carried out to its ultimate end of replicating some specific person&rsquo;s mind, the resulting replica would still over time develop divergently from its original.</p>\n</blockquote>\n<h1 id=\"we-are-such-stuff-as-digital-dreams-are-made-on\"><a href=\"#TOC\"><span class=\"header-section-number\">2</span> We are such stuff as digital dreams are made on</a></h1>\n<blockquote>\n<p>Moravec believes that our minds can be transferred this way, because he does not adopt what he calls the <em>body-identity position</em>, which holds that the human individual can only be preserved if the continuity of its &ldquo;body stuff&rdquo; is maintained. He proposes instead what he terms the <em>pattern-identity</em> theory, which defines the essence of personhood as &ldquo;the <em>pattern</em> and the <em>process</em> going on in my head and body, not the machinery supporting that process. If the process is preserved, I am preserved. The rest is mere jelly&rdquo; (Moravec 1988, 108&ndash;109). He explains that over the course of our lives, our bodies regenerate themselves</p>\n<p>&hellip;For N. Katherine Hayles, Moravec&rsquo;s description of mind transfer is a nightmare. She observes that mental uploading presupposes a cybernetic concept. Our selfhood extends into intersubjective systems lying beyond our body&rsquo;s bounds (Hayles 1999, 2). For example, Picasso in a sense places himself into his paintings, and then they reflect and communicate his identity to other selves. This could have been more fully accomplished if we precisely emulated his brain processes&hellip;These thinkers whom Krueger refers to as posthumanists would like to overcome the realms of matter and corporeality in which the body resides so as to enter into a pure mental sphere that secures their immortality. They propose that the human mind be &ldquo;scanned as a perfect simulation&rdquo; so it may continue forever inside computer hardware (Krueger 2005, 77). In fact, Krueger explains, because posthumanist philosophy seeks the annihilation of biological evolution in favor of computer and machine evolution, their philosophy necessitates there be an immortal existence, and hence, &ldquo;the idea of uploading human beings into an absolute virtual existence inside the storage of a computer takes the center stage of the posthumanist philosophy&rdquo; (Krueger 2005, 80).</p>\n</blockquote>\n<h1 id=\"encoding-all-the-sparks-of-nature\"><a href=\"#TOC\"><span class=\"header-section-number\">3</span> Encoding all the sparks of nature</a></h1>\n<blockquote>\n<p>Bostrom and Sandberg do not favor Moravec&rsquo;s &ldquo;invasive&rdquo; sort of mind replication that involves surgery and the destruction of brain tissue (Bostrom and Sandberg 2008, 27). They propose instead <em>whole brain emulation</em>. To emulate someone&rsquo;s neural patterns, we first scan a particular brain to obtain precise detail of its structures and their interactions. Using this data, we program an emulation that will behave essentially the same as the original brain&hellip;The emulation will mimic the human brain&rsquo;s functioning on the cellular level, and then automatically, higher and higher orders of organization should spontaneously arise. Finally human consciousness might emerge at the highest level of organization.</p>\n<p>&hellip;There are various levels of successfully attaining a functionally isomorphic mind, beginning with a simple &ldquo;parts list&rdquo; of the brain&rsquo;s components along with the ways they interact. Yet, the highest levels are the most philosophically interesting, write Bostrom and Sandberg. When the technology achieves <em>individual brain emulation</em>, it produces emergent activity characteristic of that of one particular (fully functioning) brain. It is more similar to the activity of the original brain than any other brain. The highest form is a <em>personal identity emulation</em>: &ldquo;a continuation of the original mind; either as numerically the same person, or as a surviving continuer thereof,&rdquo; and we achieve such an emulation when it becomes rationally self-concerned for the brain it emulates (Bostrom and Sandberg 2008, 11).</p>\n</blockquote>\n<h1 id=\"arising-minds\"><a href=\"#TOC\"><span class=\"header-section-number\">4</span> Arising minds</a></h1>\n<blockquote>\n<p>Bostrom&rsquo;s and Sandberg&rsquo;s &ldquo;Roadmap&rdquo; presupposes a physicalist standpoint&hellip;Bostrom and Sandberg write that &ldquo;sufficient apparent success with [whole brain emulation] would provide persuasive evidence for <em>multiple realizability</em>&rdquo; (Bostrom and Sandberg 2008, 14).</p>\n<p>&hellip;Our minds emerge from the complex dynamic pattern of all our neurons communicating and computing in parallel. Roger Sperry offers compelling evidence. There are &ldquo;split brain&rdquo; patients whose right and left brain hemispheres are disconnected from one another, and nonetheless, they have maintained unified consciousness. However, there is no good account for this on the basis of neurological activity, because there is no longer normal communication between the two brain-halves (Clayton 2006, 20). For this reason, Sperry concludes that mental phenomena are emergent properties that &ldquo;govern the flow of nerve impulse traffic.&rdquo; According to Sperry, &ldquo;Individual nerve impulses and other excitatory components of a cerebral activity pattern are simply carried along or shunted this way and that by the prevailing overall dynamics of the whole active process&rdquo; (Sperry quoted in Clayton 2006, 20). Yet it works the other way as well:</p>\n<p>The conscious properties of cerebral patterns are directly dependent on the action of the component neural elements. Thus, a mutual interdependence is recognized between the sustaining physico-chemical processes and the enveloping conscious qualities. The neurophysiology, in other words, controls the mental effects, and the mental properties in turn control the neurophysiology. (Sperry quoted in Clayton 2006, 20)</p>\n<p>&hellip;Now let&rsquo;s suppose that whole brain emulation continually fails to produce emergent mental phenomena, despite having developed incredible computational resources for doing so. This might lead us to favor Todd Feinberg&rsquo;s argument that the mind does not emerge from the brain to a higher order. He builds his argument in part upon Searle&rsquo;s distinction between two varieties of conscious emergence. Searle first has us consider a system made of a set of components, for example, a rock made up of a conglomerate of molecules. The rock will have features not found in any individual molecule; its weight of ten pounds is not found entirely in any molecular part. However, we can deduce or calculate the weight of the rock on the basis of the weights of its molecules. Yet, what about the solidity of the rock? This is an example of an emergent property that can be explained only in terms of the interactions among the elements (Searle 1992, 111). Consciousness, he argues, is an emergent property based on the interactions of neurons, but he disputes a more &ldquo;adventurous conception,&rdquo; which holds that emergent consciousness has capacities not explainable on the basis of the neurons&rsquo; interactivity: &ldquo;the na&iuml;ve idea here is that consciousness gets squirted out by the behaviour of the neurons in the brain, but once it has been squirted out, then it has a life of its own&rdquo; (Searle 1992, 112). Feinberg will build from Searle&rsquo;s position in order to argue for a non-hierarchical conception of mental emergence. So while Feinberg does in fact think consciousness results from the interaction of many complex layers of neural organization, no level emerges to a superior status. He offers the example of visual recognition and has us consider when we recognize our grandmother. One broad layer of neurons transmits information about the whole visual field. Another more selective layer picks-out lines. Then an even narrower layer detects shapes. Finally the information arrives at the &ldquo;grandmother cell,&rdquo; which only fires when she is the one we see. But this does not make the grandmother cell emergently higher. Rather, all the neural layers of organization must work together simultaneously to achieve this recognition. The brain is a vast network of interconnected circuits, so we cannot say that any layer of organization emerges over-and-above the others (Feinberg 2001, 130&ndash;31)&hellip;Nonetheless, his objection may still be problematic for whole brain emulation, because Bostrom and Sandberg write:</p>\n<p>An important hypothesis for [whole brain emulation] is that in order to emulate the brain we do not need to understand the whole system, but rather we just need a database containing all necessary low-level information about the brain and knowledge of the local update rules that change brain states from moment to moment. (Bostrom and Sandberg 2008, 8)</p>\n<p>&hellip;If consciousness emerges from neural activity, perhaps it does so in a way that is not perfectly suited to the sort of emergentism that Bostrom and Sandberg use in their roadmap. Hence, pursuing the development of whole brain emulation might provide evidence indicating whether and how our minds relate to our brains.</p>\n</blockquote>\n<h1 id=\"mental-waves-and-pulses-analog-vs.digital-computation\"><a href=\"#TOC\"><span class=\"header-section-number\">5</span> Mental waves and pulses: analog vs.&nbsp;digital computation</a></h1>\n<blockquote>\n<p>&hellip;One notable advantage of analog is its &ldquo;density&rdquo; (Goodman 1968, 160&ndash;161). Between any two variables can be found another, but digital variables will always have gaps between them. For this reason, analog can compute an infinity of different values found within a finite range, while digital will always be missing variables between its units. In fact, Hava Siegelmann argues that analog is capable of a hyper-computation that no digital computer could possibly accomplish (Siegelmann 2003, 109).</p>\n<p>&hellip;A &ldquo;relevant property&rdquo; of an audiophile&rsquo;s brain is its ability to discern analog from digital, and prefer one to the other. However, a digital emulation of the audiophile&rsquo;s brain might not be able to share its appreciation for analog, and also, perhaps digital emulations might even produce a mental awareness quite foreign to what humans normally experience. Bostrom&rsquo;s and Sandberg&rsquo;s brain emulation exclusively uses digital computation. Yet, they acknowledge that some argue analog and digital are qualitatively different, and they even admit that implementing analog in brain emulation could present profound difficulties (Bostrom and Sandberg 2008, 39). Nonetheless, they think there is no need to worry.</p>\n<p>They first argue that brains are made of discrete atoms that must obey quantum mechanical rules, which force the atoms into discrete energy states. Moreover, these states could be limited by a discrete time-space (Bostrom and Sandberg 2008, 38). Although I am unable to comment on issues of quantum physics, let&rsquo;s presume for argument&rsquo;s sake that the world is fundamentally made-up of discrete parts. Bostrom and Sandberg also say that whole brain emulation&rsquo;s development would be profoundly hindered if quantum computation were needed to compute such incredibly tiny variations (Bostrom and Sandberg 2008, 39); however, this is where analog now already has the edge (Siegelmann 2003, 111).</p>\n<p>Yet their next argument calls even that notion into question. They pose what is called &ldquo;the argument from noise.&rdquo; Analog devices always take some physical form, and it is unavoidable that interferences and irregularities, called noise, will make the analog device imprecise. So analog might be capable of taking on an infinite range of variations; however, it will never be absolutely accurate, because noise always causes it to veer-off slightly from where it should be&hellip;However, soon the magnitude between digital&rsquo;s smallest values will equal the magnitude that analog veers away from its proper course. Digital&rsquo;s blind spots would then be no greater than analog&rsquo;s smallest inaccuracies. So, we only need to wait for digital technology to improve enough so that it can compute the same values with equivalent precision. Both will be equally inaccurate, but for fundamentally different reasons.</p>\n<p>&hellip;Note first that our nervous system&rsquo;s electrical signals are discrete pulses, like Morse code. In that sense they are digital. However, the frequency of the pulses can vary continuously (Jackendoff 1987, 33); for, the interval between two impulses may take any value (M&uuml;ller et al. 1995, 5). This applies as well to our sense signals: as the stimulus varies continuously, the signal&rsquo;s frequency and voltage changes proportionally (Marieb and Hoehn 2007, 401). As well, there are many other neural quantities that are analog in this way. Recent research suggests that the signal&rsquo;s amplitude is also graded and hence is analog (McCormick et.al 2006, 761). Also consider that our brains learn by adjusting the &ldquo;weight&rdquo; or computational significance of certain signal channels. A neuron&rsquo;s signal-inputs are summed, and when it reaches a specific threshold, the neuron fires its own signal, which then travels to other neurons where the process is repeated. Another way the neurons adapt is by altering this input threshold. Both these adjustments may take on a continuous range of values; hence analog computation seems fundamental to learning (Mead 1989, 353&ndash;54).</p>\n</blockquote>\n<p>At this point, Shores summarizes an argument from Fred Dretske which seems to me so blatantly false that it could not possibly be what Dretske meant, so I will refrain from excerpting it and passing on my own misconceptions. Continuing on:</p>\n<blockquote>\n<p>&hellip;and yet like Bostrom and Sandberg, Schonbein critiques analog using the argument from noise (Schonbein 2005, 60). He says that analog computers are more powerful only in theory, but as soon as we build them, noise from the physical environment diminishes their accuracy (Schonbein 2005, 65&ndash;66). Curiously, he concludes that we should not for that reason dismiss analog but instead claims that analog neural networks, &ldquo;while not offering greater computational power, may nonetheless offer something else&rdquo; (2005, 68). However, he leaves it for another effort to say exactly what might be the unique value of analog computation.</p>\n<p>A.F. Murray&rsquo;s research on neural-network learning supplies an answer: analog noise interference is significantly more effective than digital at aiding adaptation, because being &ldquo;wrong&rdquo; allows neurons to explore new possibilities for weights and connections (Murray 1991, 1547). This enables us to learn and adapt to a chaotically changing environment. So using digitally-simulated neural noise might be inadequate. Analog is better, because it affords our neurons an infinite array of alternate configurations (1991, 1547&ndash;1548). Hence in response to Bostrom&rsquo;s and Sandberg&rsquo;s argument from noise, I propose this argument <em>for</em> noise. Analog&rsquo;s inaccuracies take the form of continuous variation, and in my view, this is precisely what makes it necessary for whole brain emulation.</p>\n</blockquote>\n<p>This is worth examining in more depth. The citation Murray 1991 is <a href=\"http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=132798\">&ldquo;Analogue noise-enhanced learning in neural network circuits&rdquo;</a>; the PDF is not online and the abstract is not particularly helpful:</p>\n<blockquote>\n<p>Experiments are reported which demonstrate that, whereas digital inaccuracy in neural arithmetic, in the form of bit-length limitation, degrades neural learning, analogue noise enhances it dramatically. The classification task chosen is that of vowel recognition within a multilayer perceptron network, but the findings seem to be perfectly general in the neural context, and have ramifications for all learning processes where weights evolve incrementally, and slowly.</p>\n</blockquote>\n<p>Fortunately, Shores has excerpted key bits on his blog in <a href=\"http://piratesandrevolutionaries.blogspot.com/2009/04/deleuzes-guattaris-neurophysiology-and.html\">&ldquo;Deleuze&rsquo;s &amp; Guattari&rsquo;s Neurophysiology and Neurocomputation&rdquo;</a>; I will reproduce them below:</p>\n<blockquote>\n<p>Analogue techniques allow the essential neural functions of addition and multiplication to be mapped elegantly into small analogue circuits. The compactness introduced allows the use of massively parallel arrays of such operators, but analogue circuits are noise-prone. This is widely held to be tolerable during neural computation, but not during learning. In arriving at this conclusion, parallels are drawn between analogue &lsquo;noise&rsquo; uncertainty, and digital inaccuracy, limited by bit length. This, coupled with dogma which holds that high (\u224316 bit) accuracy is needed during neural learning, has discouraged attempts to develop analogue learning circuitry, although some recent work on hybrid analogue/digital systems suggests that learning is possible, if perhaps not optimal, in low (digital) precision networks. In this Letter results are presented which demonstrate that <em>analogue noise, far from impeding neural learning, actually enhances it</em>. (Murray 1546.1a, emphasis mine)</p>\n<p><em>Learning is clearly enhanced by the presence of noise at a high level</em> (around 20%) on both synaptic weights and activities. This result is surprising, in light of the normal assertion alluded to above that back propagation requires up to 16 bit precision during learning. The distinction is that digital inaccuracy, determined by the significance of the least significant bit (LSB), implies that the smallest possible weight change during learning is 1 LSB. Analogue inaccuracy is, however, fundamentally different, in being noise-limited. In principle, <em>infinitesimally small weight changes can be made, and the inaccuracy takes the form of a spread of &lsquo;actual&rsquo; values</em> of that weight as noise enters the forward pass. The underlying &lsquo;accurate&rsquo; weight does, however, maintain its accuracy as a time average, and the learning process is sufficiently slow to effectively &lsquo;see through&rsquo; the noise in an analogue system. (1547.2.bc, emphasis mine)</p>\n<p>The further implication is that <em>drawing parallels in this context between digital inaccuracy and analogue noise</em> is extremely misleading. The former imposes constraints (quantisation) on allowable weight values, whereas the latter merely <em>smears a continuum</em> of allowable weight values. (1547&ndash;1548, emphasis mine)</p>\n</blockquote>\n<p>While it&rsquo;s hard to say in lieu of the full paper, this seems to be the exact same analogue argument as before: analogue supposedly gives a system more degrees of freedom and can answer more questions about similarly free systems, and the brain may be such a free system. Parts of the quotes undermine the idea that analogue offers any additional power in concrete practice (&ldquo;the learning process is sufficiently slow to effectively &lsquo;see through&rsquo; the noise in an analogue system&rdquo;) and to extend this to brains is unwarranted by the same anti-analogue arguments are before - in a quantized universe, you only need more bits to get as much precision <em>as exists</em>.</p>\n<p>In the paper, the neural network apparently uses 8-bit or 16-bit words; perhaps 32-bits would be enough to reach the point where the bit-length quantization is only as bad as the analogue noise, and if it is not, then perhaps 64-bits (as is now standard on commodity computers in 2011) or 128-bit lengths would be enough (commodity computers use <a href=\"http://en.wikipedia.org/wiki/128-bit\">128-bit</a> special-purpose vector registers, and past and present architectures have used them).</p>\n<h1 id=\"even-while-mens-minds-are-wild\"><a href=\"#TOC\"><span class=\"header-section-number\">6</span> Even while men&rsquo;s minds are wild?</a></h1>\n<blockquote>\n<p>Neural noise can result from external interferences like magnetic fields or from internal random fluctuations (Ward 2002, 116&ndash;117). According to Steven Rose, our brain is an &ldquo;uncertain&rdquo; system on account of &ldquo;random, indeterminate, and probabilistic&rdquo; events that are essential to its functioning (Rose 1976, 93). Alex Pouget and his research team recently found that the mind&rsquo;s ability to compute complex calculations has much to do with its noise. Our neurons transmit varying signal-patterns even for the same stimulus (Pouget et al. 2006, 356), which allows us to probabilistically estimate margins of error when making split-second decisions, as for example when deciding what to do if our brakes fail as we speed toward a busy intersection (Pouget et al. 2008, 1142). Hence the brain&rsquo;s noisy irregularities seem to be one reason that it is such a powerful and effective computer.</p>\n<p>Some also theorize that noise is essential to the human brain&rsquo;s creativity. Johnson-Laird claims that creative mental processes are never predictable (Johnson-Laird 1987, 256). On this basis, he suggests that one way to make computers think creatively would be to have them alter their own functioning by submitting their own programs to artificially-generated random variations (Johnson-Laird 1993, 119&ndash;120). This would produce what Ben Goertzel refers to as &ldquo;a complex combination of random chance with strict, deterministic rules&rdquo; (Goertzel 1994, 119). According to Daniel Dennett, this indeterminism is precisely what endows us with what we call free will (Dartnall 1994, 37). Likewise, Bostrom and Sandberg suggest we introduce random noise into our emulation by using pseudo-random number generators. They are not truly random, because eventually the pattern will repeat. However, if it takes a very long time before the repetitions appear, then probably it would be sufficiently close to real randomness.</p>\n<p>&hellip;Lawrence Ward reviews findings that suggest we may characterize our neural irregularities as <em>pink noise</em>, which is also called <em>1/f noise</em> (Ward 2002, 145&ndash;153). Benoit Mandelbrot classifies such 1/f noise as what he terms &ldquo;wild randomness&rdquo; and &ldquo;wild variation&rdquo; (Mandelbrot and Hudson 2004, 39&ndash;41). This sort of random might not be so easily simulated, and Mandelbrot gives two reasons for this.</p>\n<ol style=\"list-style-type: decimal\">\n<li>In wild randomness, there are events that defy the normal random distribution of the bell curve. He cites a number of stock market events that are astronomically improbable, even though such occurrences in fact happen quite frequently in natural systems despite their seeming impossibility. There is no way to predict when they will happen or how drastic they will be (Mandelbrot and Hudson 2004, 4). And</li>\n<li>each event is random and yet it is not independent from the rest, like each toss of a coin is.</li>\n</ol></blockquote>\n<p>The pink noise point seems entirely redundant with the previous 2 paragraphs. If the PRNGs are adequate for the latter kinds of noise, they are adequate for the former, which is merely one of many distributions statisticians employ all the time besides the &lsquo;normal distribution&rsquo;. As well, PRNGs are something of a red herring here: genuine quantum random-number generators for computers are old school, and other kinds of hardware can produce staggering quantities of randomness. (I read a few years ago of laser RNGs producing <a href=\"http://arstechnica.com/science/news/2008/11/random-numbers-at-1-7gbs-courtesy-of-chaotic-lasers.ars\">a gigabyte or two per second</a>.)</p>\n<blockquote>\n<p>The problem is that the brain&rsquo;s 1/f noise is wildly random. So suppose we emulate some person&rsquo;s brain perfectly, and suppose further that the original person and her emulation identify so much that they cannot distinguish themselves from one another. Yet, if both minds are subject to wild variations, then their consciousness and identity might come to differ more than just slightly. They could even veer off wildly. So, to successfully emulate a brain, we might need to emulate this wild neural randomness. However, that seems to remove the possibility that the emulation will continue on as the original person. Perhaps our very effort to emulate a specific human brain results in our producing an entirely different person altogether.</p>\n</blockquote>\n<p>If the inputs are the same to a perfect emulation, the outputs will be the same by definition. This is just retreading the old question of divergence; 1/f noise adds nothing new to the question. If there is a difference in the emulation or in the two inputs, of course there may be arbitrarily small or large differences in outputs. This is easy to see with simple thought-experiments that owe nothing to noise.</p>\n<p>Imagine that the brain is completely devoid of chaos or noise or anything previously suggested. We can still produce arbitrarily large divergences based on arbitrarily small differences, right down to a single bit. Here&rsquo;s an example thought-experiment: the subject resolves, before an uploading procedure, that he will recall a certain memory in which he looks at a bit, and if the bit is 1 he will become a Muslim and if a 0 he will become an atheist. He is uploaded, and the procedure goes <em>perfectly</em> except for one particular bit, which just happens to be the same bit of the memory; the original and the upload then, per their resolution, examine their memories and become an atheist and a Muslim. One proceeds to blow himself up in the local mall and the other spends its time ranting online about the idiocy of theism. Quite a divergence, but one can imagine greater divergences if one must. Now, are they the same people after carrying out their resolution? Or different? An answer to this would seem to cover noise just as well.</p>\n<p>And let&rsquo;s not forget the broader picture. We obviously want the upload to <em>initially</em> be as close as possible to the original. But there being no difference <em>eventually</em> would completely eliminate the desirability of uploads: even if we mirror the upload and original in lockstep, what do we do with the upload when the original dies? Faithfully emulate the process of dying and then erase it, to preserve the moment-to-moment isomorphism? Of course not - we&lsquo;d keep running it, at which point it diverges quite a bit. (&rsquo;What do you mean, I&rsquo;m an upload and the original has just died?!&rsquo;) One could say, with great justice, that for transhumanists, divergence is not an obstacle to personal identity/continuity but the entire reason uploading is desirable in the first place.</p>\n<h1 id=\"further-reading\"><a href=\"#TOC\"><span class=\"header-section-number\">7</span> Further reading</a></h1>\n<p>Currently there doesn&rsquo;t seem to be any discussion of Shores&rsquo;s paper besides <a href=\"http://allatwaste.com/?page_id=2\">J.S. Milam</a>&rsquo;s <a href=\"http://allatwaste.com/?p=22\">&ldquo;Digital Beings Are Different Beings&rdquo;</a>, which is just a mess. For example:</p>\n<blockquote>\n<p>Line segment AC corresponds exactly to line segment AB, and an infinite number of line segments can be drawn this way. This means that although the circles have different diameters, an infinite number of corresponding line segments can be drawn between an infinite number of corresponding points. Each circle represents a different sized infinity. A binary (digital) system can theoretically generate these infinite sets as an ongoing process of constant computation, but the computing resources required to do this are immense. So it seems that the differences between consciousness resulting from a diachronic pattern emergence embodied on a digital system might be quite severe when compared to an emergence resulting from the same set of rules run on an analog or a flesh-and-bone computer.</p>\n</blockquote>\n<p>Perhaps it&rsquo;s just me, but I think the author doesn&rsquo;t grasp Cantorian cardinality at all (different sized infinity? No, they&rsquo;re all the same cardinality, in the same way &lsquo;all powers of 2&rsquo; is the same size as &lsquo;the even integers&rsquo;), and the rest doesn&rsquo;t read much more sensibly.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb2b1": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "zswEixLariPTgxLcC", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 16, "extendedScore": null, "score": 8.231018109615431e-07, "legacy": true, "legacyId": "11772", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><a href=\"http://jetpress.org/v22/shores.htm\">\u201cMisbehaving Machines: The Emulated Brains of Transhumanist Dreams\u201d</a>, by Corry Shores (grad student; <a href=\"https://twitter.com/#!/corryshores\">Twitter</a>, <a href=\"http://piratesandrevolutionaries.blogspot.com/\">blog</a>) is another recent JET paper. Abstract:</p>\n<blockquote>\n<p>Enhancement technologies may someday grant us capacities far beyond what we now consider humanly possible. Nick Bostrom and Anders Sandberg suggest that we might survive the deaths of our physical bodies by living as computer emulations.\u00ad\u00ad In 2008, they issued a report, or \u201croadmap,\u201d from a conference where experts in all relevant fields collaborated to determine the path to \u201cwhole brain emulation.\u201d Advancing this technology could also aid philosophical research. Their \u201croadmap\u201d defends certain philosophical assumptions required for this technology\u2019s success, so by determining the reasons why it succeeds or fails, we can obtain empirical data for philosophical debates regarding our mind and selfhood. The scope ranges widely, so I merely survey some possibilities, namely, I argue that this technology could help us determine</p>\n<ol style=\"list-style-type: decimal\">\n<li>if the mind is an emergent phenomenon,</li>\n<li>if analog technology is necessary for brain emulation, and</li>\n<li>if neural randomness is so wild that a complete emulation is impossible.</li>\n</ol></blockquote>\n<h1 id=\"introduction\"><a id=\"more\"></a></h1>\n<h1 id=\"1_Introduction\"><a href=\"#TOC\"><span class=\"header-section-number\">1</span> Introduction</a></h1>\n<blockquote>\n<p>Whole brain emulation succeeds if it merely replicates human neural functioning. Yet for Nick Bostrom and Anders Sandberg, its success increases when it perfectly replicates a specific person\u2019s brain\u2026In 2008, Nick Bostrom and Anders Sandberg <a href=\"http://www.fhi.ox.ac.uk/reports/2008-3.pdf\">compiled the findings</a> from a conference of philosophers, technicians and other experts who had gathered to formulate a \u201croadmap\u201d of the individual steps and requirements that could plausibly develop this technology\u2026As I proceed, I will look more closely at these philosophical assumptions individually. For now let it suffice to say that I will adopt the basic framework of their philosophy of mind.</p>\n<p>\u2026I will explore research that calls into question certain other ones. For example, although the authors diminish the importance of analog computation and noise interference, there are findings and compelling arguments that suggest otherwise. As well, there is reason to think that the brain\u2019s computational dynamics would not call for Bostrom\u2019s and Sandberg\u2019s hierarchical model for the mind\u2019s emergence. And finally, I will argue on these bases that if brain emulation were to be carried out to its ultimate end of replicating some specific person\u2019s mind, the resulting replica would still over time develop divergently from its original.</p>\n</blockquote>\n<h1 id=\"2_We_are_such_stuff_as_digital_dreams_are_made_on\"><a href=\"#TOC\"><span class=\"header-section-number\">2</span> We are such stuff as digital dreams are made on</a></h1>\n<blockquote>\n<p>Moravec believes that our minds can be transferred this way, because he does not adopt what he calls the <em>body-identity position</em>, which holds that the human individual can only be preserved if the continuity of its \u201cbody stuff\u201d is maintained. He proposes instead what he terms the <em>pattern-identity</em> theory, which defines the essence of personhood as \u201cthe <em>pattern</em> and the <em>process</em> going on in my head and body, not the machinery supporting that process. If the process is preserved, I am preserved. The rest is mere jelly\u201d (Moravec 1988, 108\u2013109). He explains that over the course of our lives, our bodies regenerate themselves</p>\n<p>\u2026For N. Katherine Hayles, Moravec\u2019s description of mind transfer is a nightmare. She observes that mental uploading presupposes a cybernetic concept. Our selfhood extends into intersubjective systems lying beyond our body\u2019s bounds (Hayles 1999, 2). For example, Picasso in a sense places himself into his paintings, and then they reflect and communicate his identity to other selves. This could have been more fully accomplished if we precisely emulated his brain processes\u2026These thinkers whom Krueger refers to as posthumanists would like to overcome the realms of matter and corporeality in which the body resides so as to enter into a pure mental sphere that secures their immortality. They propose that the human mind be \u201cscanned as a perfect simulation\u201d so it may continue forever inside computer hardware (Krueger 2005, 77). In fact, Krueger explains, because posthumanist philosophy seeks the annihilation of biological evolution in favor of computer and machine evolution, their philosophy necessitates there be an immortal existence, and hence, \u201cthe idea of uploading human beings into an absolute virtual existence inside the storage of a computer takes the center stage of the posthumanist philosophy\u201d (Krueger 2005, 80).</p>\n</blockquote>\n<h1 id=\"3_Encoding_all_the_sparks_of_nature\"><a href=\"#TOC\"><span class=\"header-section-number\">3</span> Encoding all the sparks of nature</a></h1>\n<blockquote>\n<p>Bostrom and Sandberg do not favor Moravec\u2019s \u201cinvasive\u201d sort of mind replication that involves surgery and the destruction of brain tissue (Bostrom and Sandberg 2008, 27). They propose instead <em>whole brain emulation</em>. To emulate someone\u2019s neural patterns, we first scan a particular brain to obtain precise detail of its structures and their interactions. Using this data, we program an emulation that will behave essentially the same as the original brain\u2026The emulation will mimic the human brain\u2019s functioning on the cellular level, and then automatically, higher and higher orders of organization should spontaneously arise. Finally human consciousness might emerge at the highest level of organization.</p>\n<p>\u2026There are various levels of successfully attaining a functionally isomorphic mind, beginning with a simple \u201cparts list\u201d of the brain\u2019s components along with the ways they interact. Yet, the highest levels are the most philosophically interesting, write Bostrom and Sandberg. When the technology achieves <em>individual brain emulation</em>, it produces emergent activity characteristic of that of one particular (fully functioning) brain. It is more similar to the activity of the original brain than any other brain. The highest form is a <em>personal identity emulation</em>: \u201ca continuation of the original mind; either as numerically the same person, or as a surviving continuer thereof,\u201d and we achieve such an emulation when it becomes rationally self-concerned for the brain it emulates (Bostrom and Sandberg 2008, 11).</p>\n</blockquote>\n<h1 id=\"4_Arising_minds\"><a href=\"#TOC\"><span class=\"header-section-number\">4</span> Arising minds</a></h1>\n<blockquote>\n<p>Bostrom\u2019s and Sandberg\u2019s \u201cRoadmap\u201d presupposes a physicalist standpoint\u2026Bostrom and Sandberg write that \u201csufficient apparent success with [whole brain emulation] would provide persuasive evidence for <em>multiple realizability</em>\u201d (Bostrom and Sandberg 2008, 14).</p>\n<p>\u2026Our minds emerge from the complex dynamic pattern of all our neurons communicating and computing in parallel. Roger Sperry offers compelling evidence. There are \u201csplit brain\u201d patients whose right and left brain hemispheres are disconnected from one another, and nonetheless, they have maintained unified consciousness. However, there is no good account for this on the basis of neurological activity, because there is no longer normal communication between the two brain-halves (Clayton 2006, 20). For this reason, Sperry concludes that mental phenomena are emergent properties that \u201cgovern the flow of nerve impulse traffic.\u201d According to Sperry, \u201cIndividual nerve impulses and other excitatory components of a cerebral activity pattern are simply carried along or shunted this way and that by the prevailing overall dynamics of the whole active process\u201d (Sperry quoted in Clayton 2006, 20). Yet it works the other way as well:</p>\n<p>The conscious properties of cerebral patterns are directly dependent on the action of the component neural elements. Thus, a mutual interdependence is recognized between the sustaining physico-chemical processes and the enveloping conscious qualities. The neurophysiology, in other words, controls the mental effects, and the mental properties in turn control the neurophysiology. (Sperry quoted in Clayton 2006, 20)</p>\n<p>\u2026Now let\u2019s suppose that whole brain emulation continually fails to produce emergent mental phenomena, despite having developed incredible computational resources for doing so. This might lead us to favor Todd Feinberg\u2019s argument that the mind does not emerge from the brain to a higher order. He builds his argument in part upon Searle\u2019s distinction between two varieties of conscious emergence. Searle first has us consider a system made of a set of components, for example, a rock made up of a conglomerate of molecules. The rock will have features not found in any individual molecule; its weight of ten pounds is not found entirely in any molecular part. However, we can deduce or calculate the weight of the rock on the basis of the weights of its molecules. Yet, what about the solidity of the rock? This is an example of an emergent property that can be explained only in terms of the interactions among the elements (Searle 1992, 111). Consciousness, he argues, is an emergent property based on the interactions of neurons, but he disputes a more \u201cadventurous conception,\u201d which holds that emergent consciousness has capacities not explainable on the basis of the neurons\u2019 interactivity: \u201cthe na\u00efve idea here is that consciousness gets squirted out by the behaviour of the neurons in the brain, but once it has been squirted out, then it has a life of its own\u201d (Searle 1992, 112). Feinberg will build from Searle\u2019s position in order to argue for a non-hierarchical conception of mental emergence. So while Feinberg does in fact think consciousness results from the interaction of many complex layers of neural organization, no level emerges to a superior status. He offers the example of visual recognition and has us consider when we recognize our grandmother. One broad layer of neurons transmits information about the whole visual field. Another more selective layer picks-out lines. Then an even narrower layer detects shapes. Finally the information arrives at the \u201cgrandmother cell,\u201d which only fires when she is the one we see. But this does not make the grandmother cell emergently higher. Rather, all the neural layers of organization must work together simultaneously to achieve this recognition. The brain is a vast network of interconnected circuits, so we cannot say that any layer of organization emerges over-and-above the others (Feinberg 2001, 130\u201331)\u2026Nonetheless, his objection may still be problematic for whole brain emulation, because Bostrom and Sandberg write:</p>\n<p>An important hypothesis for [whole brain emulation] is that in order to emulate the brain we do not need to understand the whole system, but rather we just need a database containing all necessary low-level information about the brain and knowledge of the local update rules that change brain states from moment to moment. (Bostrom and Sandberg 2008, 8)</p>\n<p>\u2026If consciousness emerges from neural activity, perhaps it does so in a way that is not perfectly suited to the sort of emergentism that Bostrom and Sandberg use in their roadmap. Hence, pursuing the development of whole brain emulation might provide evidence indicating whether and how our minds relate to our brains.</p>\n</blockquote>\n<h1 id=\"5_Mental_waves_and_pulses__analog_vs__digital_computation\"><a href=\"#TOC\"><span class=\"header-section-number\">5</span> Mental waves and pulses: analog vs.&nbsp;digital computation</a></h1>\n<blockquote>\n<p>\u2026One notable advantage of analog is its \u201cdensity\u201d (Goodman 1968, 160\u2013161). Between any two variables can be found another, but digital variables will always have gaps between them. For this reason, analog can compute an infinity of different values found within a finite range, while digital will always be missing variables between its units. In fact, Hava Siegelmann argues that analog is capable of a hyper-computation that no digital computer could possibly accomplish (Siegelmann 2003, 109).</p>\n<p>\u2026A \u201crelevant property\u201d of an audiophile\u2019s brain is its ability to discern analog from digital, and prefer one to the other. However, a digital emulation of the audiophile\u2019s brain might not be able to share its appreciation for analog, and also, perhaps digital emulations might even produce a mental awareness quite foreign to what humans normally experience. Bostrom\u2019s and Sandberg\u2019s brain emulation exclusively uses digital computation. Yet, they acknowledge that some argue analog and digital are qualitatively different, and they even admit that implementing analog in brain emulation could present profound difficulties (Bostrom and Sandberg 2008, 39). Nonetheless, they think there is no need to worry.</p>\n<p>They first argue that brains are made of discrete atoms that must obey quantum mechanical rules, which force the atoms into discrete energy states. Moreover, these states could be limited by a discrete time-space (Bostrom and Sandberg 2008, 38). Although I am unable to comment on issues of quantum physics, let\u2019s presume for argument\u2019s sake that the world is fundamentally made-up of discrete parts. Bostrom and Sandberg also say that whole brain emulation\u2019s development would be profoundly hindered if quantum computation were needed to compute such incredibly tiny variations (Bostrom and Sandberg 2008, 39); however, this is where analog now already has the edge (Siegelmann 2003, 111).</p>\n<p>Yet their next argument calls even that notion into question. They pose what is called \u201cthe argument from noise.\u201d Analog devices always take some physical form, and it is unavoidable that interferences and irregularities, called noise, will make the analog device imprecise. So analog might be capable of taking on an infinite range of variations; however, it will never be absolutely accurate, because noise always causes it to veer-off slightly from where it should be\u2026However, soon the magnitude between digital\u2019s smallest values will equal the magnitude that analog veers away from its proper course. Digital\u2019s blind spots would then be no greater than analog\u2019s smallest inaccuracies. So, we only need to wait for digital technology to improve enough so that it can compute the same values with equivalent precision. Both will be equally inaccurate, but for fundamentally different reasons.</p>\n<p>\u2026Note first that our nervous system\u2019s electrical signals are discrete pulses, like Morse code. In that sense they are digital. However, the frequency of the pulses can vary continuously (Jackendoff 1987, 33); for, the interval between two impulses may take any value (M\u00fcller et al. 1995, 5). This applies as well to our sense signals: as the stimulus varies continuously, the signal\u2019s frequency and voltage changes proportionally (Marieb and Hoehn 2007, 401). As well, there are many other neural quantities that are analog in this way. Recent research suggests that the signal\u2019s amplitude is also graded and hence is analog (McCormick et.al 2006, 761). Also consider that our brains learn by adjusting the \u201cweight\u201d or computational significance of certain signal channels. A neuron\u2019s signal-inputs are summed, and when it reaches a specific threshold, the neuron fires its own signal, which then travels to other neurons where the process is repeated. Another way the neurons adapt is by altering this input threshold. Both these adjustments may take on a continuous range of values; hence analog computation seems fundamental to learning (Mead 1989, 353\u201354).</p>\n</blockquote>\n<p>At this point, Shores summarizes an argument from Fred Dretske which seems to me so blatantly false that it could not possibly be what Dretske meant, so I will refrain from excerpting it and passing on my own misconceptions. Continuing on:</p>\n<blockquote>\n<p>\u2026and yet like Bostrom and Sandberg, Schonbein critiques analog using the argument from noise (Schonbein 2005, 60). He says that analog computers are more powerful only in theory, but as soon as we build them, noise from the physical environment diminishes their accuracy (Schonbein 2005, 65\u201366). Curiously, he concludes that we should not for that reason dismiss analog but instead claims that analog neural networks, \u201cwhile not offering greater computational power, may nonetheless offer something else\u201d (2005, 68). However, he leaves it for another effort to say exactly what might be the unique value of analog computation.</p>\n<p>A.F. Murray\u2019s research on neural-network learning supplies an answer: analog noise interference is significantly more effective than digital at aiding adaptation, because being \u201cwrong\u201d allows neurons to explore new possibilities for weights and connections (Murray 1991, 1547). This enables us to learn and adapt to a chaotically changing environment. So using digitally-simulated neural noise might be inadequate. Analog is better, because it affords our neurons an infinite array of alternate configurations (1991, 1547\u20131548). Hence in response to Bostrom\u2019s and Sandberg\u2019s argument from noise, I propose this argument <em>for</em> noise. Analog\u2019s inaccuracies take the form of continuous variation, and in my view, this is precisely what makes it necessary for whole brain emulation.</p>\n</blockquote>\n<p>This is worth examining in more depth. The citation Murray 1991 is <a href=\"http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=132798\">\u201cAnalogue noise-enhanced learning in neural network circuits\u201d</a>; the PDF is not online and the abstract is not particularly helpful:</p>\n<blockquote>\n<p>Experiments are reported which demonstrate that, whereas digital inaccuracy in neural arithmetic, in the form of bit-length limitation, degrades neural learning, analogue noise enhances it dramatically. The classification task chosen is that of vowel recognition within a multilayer perceptron network, but the findings seem to be perfectly general in the neural context, and have ramifications for all learning processes where weights evolve incrementally, and slowly.</p>\n</blockquote>\n<p>Fortunately, Shores has excerpted key bits on his blog in <a href=\"http://piratesandrevolutionaries.blogspot.com/2009/04/deleuzes-guattaris-neurophysiology-and.html\">\u201cDeleuze\u2019s &amp; Guattari\u2019s Neurophysiology and Neurocomputation\u201d</a>; I will reproduce them below:</p>\n<blockquote>\n<p>Analogue techniques allow the essential neural functions of addition and multiplication to be mapped elegantly into small analogue circuits. The compactness introduced allows the use of massively parallel arrays of such operators, but analogue circuits are noise-prone. This is widely held to be tolerable during neural computation, but not during learning. In arriving at this conclusion, parallels are drawn between analogue \u2018noise\u2019 uncertainty, and digital inaccuracy, limited by bit length. This, coupled with dogma which holds that high (\u224316 bit) accuracy is needed during neural learning, has discouraged attempts to develop analogue learning circuitry, although some recent work on hybrid analogue/digital systems suggests that learning is possible, if perhaps not optimal, in low (digital) precision networks. In this Letter results are presented which demonstrate that <em>analogue noise, far from impeding neural learning, actually enhances it</em>. (Murray 1546.1a, emphasis mine)</p>\n<p><em>Learning is clearly enhanced by the presence of noise at a high level</em> (around 20%) on both synaptic weights and activities. This result is surprising, in light of the normal assertion alluded to above that back propagation requires up to 16 bit precision during learning. The distinction is that digital inaccuracy, determined by the significance of the least significant bit (LSB), implies that the smallest possible weight change during learning is 1 LSB. Analogue inaccuracy is, however, fundamentally different, in being noise-limited. In principle, <em>infinitesimally small weight changes can be made, and the inaccuracy takes the form of a spread of \u2018actual\u2019 values</em> of that weight as noise enters the forward pass. The underlying \u2018accurate\u2019 weight does, however, maintain its accuracy as a time average, and the learning process is sufficiently slow to effectively \u2018see through\u2019 the noise in an analogue system. (1547.2.bc, emphasis mine)</p>\n<p>The further implication is that <em>drawing parallels in this context between digital inaccuracy and analogue noise</em> is extremely misleading. The former imposes constraints (quantisation) on allowable weight values, whereas the latter merely <em>smears a continuum</em> of allowable weight values. (1547\u20131548, emphasis mine)</p>\n</blockquote>\n<p>While it\u2019s hard to say in lieu of the full paper, this seems to be the exact same analogue argument as before: analogue supposedly gives a system more degrees of freedom and can answer more questions about similarly free systems, and the brain may be such a free system. Parts of the quotes undermine the idea that analogue offers any additional power in concrete practice (\u201cthe learning process is sufficiently slow to effectively \u2018see through\u2019 the noise in an analogue system\u201d) and to extend this to brains is unwarranted by the same anti-analogue arguments are before - in a quantized universe, you only need more bits to get as much precision <em>as exists</em>.</p>\n<p>In the paper, the neural network apparently uses 8-bit or 16-bit words; perhaps 32-bits would be enough to reach the point where the bit-length quantization is only as bad as the analogue noise, and if it is not, then perhaps 64-bits (as is now standard on commodity computers in 2011) or 128-bit lengths would be enough (commodity computers use <a href=\"http://en.wikipedia.org/wiki/128-bit\">128-bit</a> special-purpose vector registers, and past and present architectures have used them).</p>\n<h1 id=\"6_Even_while_men_s_minds_are_wild_\"><a href=\"#TOC\"><span class=\"header-section-number\">6</span> Even while men\u2019s minds are wild?</a></h1>\n<blockquote>\n<p>Neural noise can result from external interferences like magnetic fields or from internal random fluctuations (Ward 2002, 116\u2013117). According to Steven Rose, our brain is an \u201cuncertain\u201d system on account of \u201crandom, indeterminate, and probabilistic\u201d events that are essential to its functioning (Rose 1976, 93). Alex Pouget and his research team recently found that the mind\u2019s ability to compute complex calculations has much to do with its noise. Our neurons transmit varying signal-patterns even for the same stimulus (Pouget et al. 2006, 356), which allows us to probabilistically estimate margins of error when making split-second decisions, as for example when deciding what to do if our brakes fail as we speed toward a busy intersection (Pouget et al. 2008, 1142). Hence the brain\u2019s noisy irregularities seem to be one reason that it is such a powerful and effective computer.</p>\n<p>Some also theorize that noise is essential to the human brain\u2019s creativity. Johnson-Laird claims that creative mental processes are never predictable (Johnson-Laird 1987, 256). On this basis, he suggests that one way to make computers think creatively would be to have them alter their own functioning by submitting their own programs to artificially-generated random variations (Johnson-Laird 1993, 119\u2013120). This would produce what Ben Goertzel refers to as \u201ca complex combination of random chance with strict, deterministic rules\u201d (Goertzel 1994, 119). According to Daniel Dennett, this indeterminism is precisely what endows us with what we call free will (Dartnall 1994, 37). Likewise, Bostrom and Sandberg suggest we introduce random noise into our emulation by using pseudo-random number generators. They are not truly random, because eventually the pattern will repeat. However, if it takes a very long time before the repetitions appear, then probably it would be sufficiently close to real randomness.</p>\n<p>\u2026Lawrence Ward reviews findings that suggest we may characterize our neural irregularities as <em>pink noise</em>, which is also called <em>1/f noise</em> (Ward 2002, 145\u2013153). Benoit Mandelbrot classifies such 1/f noise as what he terms \u201cwild randomness\u201d and \u201cwild variation\u201d (Mandelbrot and Hudson 2004, 39\u201341). This sort of random might not be so easily simulated, and Mandelbrot gives two reasons for this.</p>\n<ol style=\"list-style-type: decimal\">\n<li>In wild randomness, there are events that defy the normal random distribution of the bell curve. He cites a number of stock market events that are astronomically improbable, even though such occurrences in fact happen quite frequently in natural systems despite their seeming impossibility. There is no way to predict when they will happen or how drastic they will be (Mandelbrot and Hudson 2004, 4). And</li>\n<li>each event is random and yet it is not independent from the rest, like each toss of a coin is.</li>\n</ol></blockquote>\n<p>The pink noise point seems entirely redundant with the previous 2 paragraphs. If the PRNGs are adequate for the latter kinds of noise, they are adequate for the former, which is merely one of many distributions statisticians employ all the time besides the \u2018normal distribution\u2019. As well, PRNGs are something of a red herring here: genuine quantum random-number generators for computers are old school, and other kinds of hardware can produce staggering quantities of randomness. (I read a few years ago of laser RNGs producing <a href=\"http://arstechnica.com/science/news/2008/11/random-numbers-at-1-7gbs-courtesy-of-chaotic-lasers.ars\">a gigabyte or two per second</a>.)</p>\n<blockquote>\n<p>The problem is that the brain\u2019s 1/f noise is wildly random. So suppose we emulate some person\u2019s brain perfectly, and suppose further that the original person and her emulation identify so much that they cannot distinguish themselves from one another. Yet, if both minds are subject to wild variations, then their consciousness and identity might come to differ more than just slightly. They could even veer off wildly. So, to successfully emulate a brain, we might need to emulate this wild neural randomness. However, that seems to remove the possibility that the emulation will continue on as the original person. Perhaps our very effort to emulate a specific human brain results in our producing an entirely different person altogether.</p>\n</blockquote>\n<p>If the inputs are the same to a perfect emulation, the outputs will be the same by definition. This is just retreading the old question of divergence; 1/f noise adds nothing new to the question. If there is a difference in the emulation or in the two inputs, of course there may be arbitrarily small or large differences in outputs. This is easy to see with simple thought-experiments that owe nothing to noise.</p>\n<p>Imagine that the brain is completely devoid of chaos or noise or anything previously suggested. We can still produce arbitrarily large divergences based on arbitrarily small differences, right down to a single bit. Here\u2019s an example thought-experiment: the subject resolves, before an uploading procedure, that he will recall a certain memory in which he looks at a bit, and if the bit is 1 he will become a Muslim and if a 0 he will become an atheist. He is uploaded, and the procedure goes <em>perfectly</em> except for one particular bit, which just happens to be the same bit of the memory; the original and the upload then, per their resolution, examine their memories and become an atheist and a Muslim. One proceeds to blow himself up in the local mall and the other spends its time ranting online about the idiocy of theism. Quite a divergence, but one can imagine greater divergences if one must. Now, are they the same people after carrying out their resolution? Or different? An answer to this would seem to cover noise just as well.</p>\n<p>And let\u2019s not forget the broader picture. We obviously want the upload to <em>initially</em> be as close as possible to the original. But there being no difference <em>eventually</em> would completely eliminate the desirability of uploads: even if we mirror the upload and original in lockstep, what do we do with the upload when the original dies? Faithfully emulate the process of dying and then erase it, to preserve the moment-to-moment isomorphism? Of course not - we\u2018d keep running it, at which point it diverges quite a bit. (\u2019What do you mean, I\u2019m an upload and the original has just died?!\u2019) One could say, with great justice, that for transhumanists, divergence is not an obstacle to personal identity/continuity but the entire reason uploading is desirable in the first place.</p>\n<h1 id=\"7_Further_reading\"><a href=\"#TOC\"><span class=\"header-section-number\">7</span> Further reading</a></h1>\n<p>Currently there doesn\u2019t seem to be any discussion of Shores\u2019s paper besides <a href=\"http://allatwaste.com/?page_id=2\">J.S. Milam</a>\u2019s <a href=\"http://allatwaste.com/?p=22\">\u201cDigital Beings Are Different Beings\u201d</a>, which is just a mess. For example:</p>\n<blockquote>\n<p>Line segment AC corresponds exactly to line segment AB, and an infinite number of line segments can be drawn this way. This means that although the circles have different diameters, an infinite number of corresponding line segments can be drawn between an infinite number of corresponding points. Each circle represents a different sized infinity. A binary (digital) system can theoretically generate these infinite sets as an ongoing process of constant computation, but the computing resources required to do this are immense. So it seems that the differences between consciousness resulting from a diachronic pattern emergence embodied on a digital system might be quite severe when compared to an emergence resulting from the same set of rules run on an analog or a flesh-and-bone computer.</p>\n</blockquote>\n<p>Perhaps it\u2019s just me, but I think the author doesn\u2019t grasp Cantorian cardinality at all (different sized infinity? No, they\u2019re all the same cardinality, in the same way \u2018all powers of 2\u2019 is the same size as \u2018the even integers\u2019), and the rest doesn\u2019t read much more sensibly.</p>", "sections": [{"title": "1 Introduction", "anchor": "1_Introduction", "level": 1}, {"title": "2 We are such stuff as digital dreams are made on", "anchor": "2_We_are_such_stuff_as_digital_dreams_are_made_on", "level": 1}, {"title": "3 Encoding all the sparks of nature", "anchor": "3_Encoding_all_the_sparks_of_nature", "level": 1}, {"title": "4 Arising minds", "anchor": "4_Arising_minds", "level": 1}, {"title": "5 Mental waves and pulses: analog vs.\u00a0digital computation", "anchor": "5_Mental_waves_and_pulses__analog_vs__digital_computation", "level": 1}, {"title": "6 Even while men\u2019s minds are wild?", "anchor": "6_Even_while_men_s_minds_are_wild_", "level": 1}, {"title": "7 Further reading", "anchor": "7_Further_reading", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "4 comments"}], "headingsCount": 9}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-29T23:23:21.412Z", "modifiedAt": null, "url": null, "title": "Stupid Questions Open Thread", "slug": "stupid-questions-open-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:08:54.278Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Costanza", "createdAt": "2010-09-14T16:22:53.235Z", "isAdmin": false, "displayName": "Costanza"}, "userId": "cXudnoTp54SYgqfgF", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/8dijcs9BjxaXE2A9G/stupid-questions-open-thread", "pageUrlRelative": "/posts/8dijcs9BjxaXE2A9G/stupid-questions-open-thread", "linkUrl": "https://www.lesswrong.com/posts/8dijcs9BjxaXE2A9G/stupid-questions-open-thread", "postedAtFormatted": "Thursday, December 29th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Stupid%20Questions%20Open%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AStupid%20Questions%20Open%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8dijcs9BjxaXE2A9G%2Fstupid-questions-open-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Stupid%20Questions%20Open%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8dijcs9BjxaXE2A9G%2Fstupid-questions-open-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F8dijcs9BjxaXE2A9G%2Fstupid-questions-open-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 141, "htmlBody": "<p>This is for anyone in the LessWrong community who has made at least some effort to read the sequences and follow along, but is still confused on some point, and is perhaps feeling a bit embarrassed. Here, newbies and not-so-newbies are free to ask very basic but still relevant questions with the understanding that the answers are probably somewhere in the sequences. Similarly, LessWrong tends to presume a rather high threshold for understanding science and technology. Relevant questions in those areas are welcome as well.&nbsp; Anyone who chooses to respond should respectfully guide the questioner to a helpful resource, and questioners should be appropriately grateful. Good faith should be presumed on both sides, unless and until it is shown to be absent.&nbsp; If a questioner is not sure whether a question is relevant, ask it, and also ask if it's relevant.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "8dijcs9BjxaXE2A9G", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 43, "baseScore": 62, "extendedScore": null, "score": 9.966774737855239e-05, "legacy": true, "legacyId": "11774", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 42, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 268, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-30T05:20:39.802Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Circular Altruism", "slug": "seq-rerun-circular-altruism", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:25.876Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/7t8oM52xMJpGi6MeA/seq-rerun-circular-altruism", "pageUrlRelative": "/posts/7t8oM52xMJpGi6MeA/seq-rerun-circular-altruism", "linkUrl": "https://www.lesswrong.com/posts/7t8oM52xMJpGi6MeA/seq-rerun-circular-altruism", "postedAtFormatted": "Friday, December 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Circular%20Altruism&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Circular%20Altruism%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7t8oM52xMJpGi6MeA%2Fseq-rerun-circular-altruism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Circular%20Altruism%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7t8oM52xMJpGi6MeA%2Fseq-rerun-circular-altruism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F7t8oM52xMJpGi6MeA%2Fseq-rerun-circular-altruism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 180, "htmlBody": "<p>Today's post, <a href=\"/lw/n3/circular_altruism/\">Circular Altruism</a> was originally published on 22 January 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Our moral preferences shouldn't be circular. If a policy A is better than B, and B is better than C, and C is better than D, and so on, then policy A really should be better than policy Z.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/92p/seq_rerun_against_discount_rates/\">Against Discount Rates</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "7t8oM52xMJpGi6MeA", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 7, "extendedScore": null, "score": 8.232534728173868e-07, "legacy": true, "legacyId": "11790", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["4ZzefKQwAtMo5yp99", "5dnAcAGd9bcXAkDzM", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-30T06:52:09.685Z", "modifiedAt": null, "url": null, "title": "The Value (and Danger) of Ritual", "slug": "the-value-and-danger-of-ritual", "viewCount": null, "lastCommentedAt": "2017-06-17T04:21:05.732Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": "Raemon", "user": {"username": "Raemon", "createdAt": "2010-09-09T02:09:20.629Z", "isAdmin": true, "displayName": "Raemon"}, "userId": "r38pkCm7wF4M44MDQ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rijoxTpkSPXcTXRbN/the-value-and-danger-of-ritual", "pageUrlRelative": "/posts/rijoxTpkSPXcTXRbN/the-value-and-danger-of-ritual", "linkUrl": "https://www.lesswrong.com/posts/rijoxTpkSPXcTXRbN/the-value-and-danger-of-ritual", "postedAtFormatted": "Friday, December 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20Value%20(and%20Danger)%20of%20Ritual&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20Value%20(and%20Danger)%20of%20Ritual%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrijoxTpkSPXcTXRbN%2Fthe-value-and-danger-of-ritual%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20Value%20(and%20Danger)%20of%20Ritual%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrijoxTpkSPXcTXRbN%2Fthe-value-and-danger-of-ritual", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrijoxTpkSPXcTXRbN%2Fthe-value-and-danger-of-ritual", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3480, "htmlBody": "<p><em>This is the second part of my Winter Solstice Ritual mini-sequence. The <a href=\"https://www.lesswrong.com/lw/8x5/ritual_report_nyc_less_wrong_solstice_celebration/\">introduction post is here</a>.</em></p><p>-</p><p>Ritual is an interesting phenomenon to me. It can be fun, beautiful, profound, useful... and potentially dangerous.</p><p>Commenters from the previous article fell into two main camps - those who assumed I knew what I was doing and gave me the benefit of the doubt, and those who were afraid I was naively meddling with forces beyond my comprehension. This was a reasonable fear. In this article, I\u2019ll outline why I think ritual is important, why it\u2019s dangerous, why I think it&#x27;s relevant to an aspiring rationalist culture.</p><p></p><p>Before I start arguing how meaningful and transformative ritual can be, I want to argue something simpler:</p><p>It can be really fun.</p><p>This is not to be discounted. For whatever reason, humans tend to appreciate songs, stories and activities that they shared with their tribe. Hedons from ritual can take the form of fun joviality as well as intense, profound experiences.</p><p>Not everything we evolved to do is good. If we feel an urge to hit the enemy tribesman with a huge rock and take their land, we can and should say \u201cNo, there are important game theoretic and moral reasons why this is a bad idea\u201d and suppress the urge. But we can also devise new activities, like <a href=\"http://en.wikipedia.org/wiki/American_football\">knocking the enemy tribesman over and taking their ball</a>, satisfying that urge without the negative consequences of war. I&#x27;d like access to the experience that ritual uniquely offers, if it can be done safely.</p><p>Ritual covers a range of experience. One subset of that is a kind of art. To give you some sense of what I mean here, here&#x27;s a few clusters of activities.</p><ul><li>Art, enjoyed alone for simple aesthetics.</li><li>Art that speaks to your beliefs. </li><li>Art that you enjoy appreciating with other people.</li><li>Beliefs that you enjoy sharing with other people.</li><li>Repetition of activities that you do every year.</li></ul><p>And here are a few songs I like:</p><ul><li><a href=\"http://www.youtube.com/watch?v=9T4WB2zfmps\">Silent Night</a></li><li><a href=\"http://www.youtube.com/watch?v=6tNbsQ8eDbA\">Carol of the Bells</a></li><li><a href=\"http://www.youtube.com/watch?v=Ad7KU9bCTAM\">Do You Hear What I Hear</a></li><li><a href=\"http://www.youtube.com/watch?v=4-vDhYTlCNw\">The Word of God</a></li><li><a href=\"http://reasonableape.blogspot.com/2011/08/singularity-lisps.html\">Singularity</a></li></ul><h2>Art and Belief</h2><p>I like <em><a href=\"http://www.youtube.com/watch?v=9T4WB2zfmps\">Silent Night</a> b</em>ecause it is a simple, tranquil song, often sung with skillful harmonies.</p><p>I like <em><a href=\"http://www.youtube.com/watch?v=6tNbsQ8eDbA\">Carol of the Bell</a></em> because it is a powerful, awe-inspiring song that is performed with immense complexity and skill.</p><p>I like <em><a href=\"http://www.youtube.com/watch?v=Ad7KU9bCTAM\">Do You Hear What I Hear</a></em> partly for the same reasons I like Silent Night - it begins with simple tranquility. But I also like it for ideological reasons - it showcases the power of a meme growing over time, magnifying, evolving and changing the lives of its hosts as they come to believe it. As an artist hoping to craft powerful memes, this is very important to me. I also like the imagery of the proud king, willing to listen to the words of a shepherd boy, acknowledging the importance of a small infant born into poverty far away. </p><p>And the king is able to command the attention of an entire nation: Take a moment, stop what you are doing, and pay attention to this child.</p><p>But <em>Do You Hear What I Hear </em>also bothers me slightly - it lies in the uncanny valley of ideological identification. The song strikes very close to home in my heart, and I want to give myself over to the song, not just to sing the words but to truly feel them in my heart. And I can\u2019t, because there is a slight snag when we get to the proud king. The king is valuing the child for all the wrong reasons. I want the child to be important because <em>all </em>children are important. But this king would not have given the child a second thought if it hadn\u2019t been the son of God. I don\u2019t believe in Jesus, so the intended message of the song clashes with what I <em>want </em>it to be about.</p><p>For the most part I sing the song without thinking about this, but that little snag is there, and it prevents the song from being one of my favorites ever.</p><p>Contrast this with S<em>ilent Night</em>, where the message is largely irrelevant to me. Or <em>Carol of the Bells</em>, whose message is \u201cBells are pretty and people like them.\u201d  I appreciate them aesthetically and I respect skilled performers. Their messages don&#x27;t bother me, but neither do I feel as strongly about them.</p><h2>Art and Tribe</h2><p><em><a href=\"http://www.youtube.com/watch?v=4-vDhYTlCNw\">The Word of God</a></em> is beautiful because the world is an incredible place, and humans have discovered millions of beautiful true things about it. There is exactly one thing I dislike about this song, and it is not a disagreement with its ideology. It\u2019s just the use of the word \u201cGod.\u201d I don\u2019t think it was wrong word to use - it\u2019s a nice, simple word and I read it purely as a metaphor for \u201cthe universe.\u201d </p><p>Like <em>Do you Hear</em>, there is some uncanny-valley effect here. But here it\u2019s about tribal identification. (I draw a distinction between tribal identity and ideology - tribe is about identifying with a group of people, ideology is identifying with a belief-structure).</p><p>My mind snags because \u201cGod\u201d is a word I normally associate with other cultures. This isn\u2019t as big a deal as in <em>Do You Hear. </em>I don\u2019t actually consider the goddists to be my enemy, I just don\u2019t feel connected to them, and the word takes me out of the beauty of the song and reminds me of this disconnection. I did go ahead and include <em>Word of God</em>, verbatim, in the Winter Solstice Celebration. I just want to note that there are different reasons to be moved by (or fail to be moved by) a song.</p><p><em>[Edit in 2018: <a href=\"https://soundcloud.com/raymond-arnold/08-time-wrote-the-rocks\">we&#x27;ve since re-written</a> Word of God (after touching base with the original songwriter) to focus more purely on scientific progress rather than God. This was less because &quot;God&quot; was problematic and more because it kept the focus on </em>political conflict<em> that didn&#x27;t seem good for Solstice longterm]</em></p><p>Finally, we have <em><a href=\"http://reasonableape.blogspot.com/2011/08/singularity-lisps.html\">Singularity</a></em>, which I like for all kinds of reasons. </p><p>The music begins whimsical and fun, but grows more powerful and exciting over time. If you have good speakers, there&#x27;s a heavy but subtle baseline that drives the sound through your bones. It was refreshing to hear an unapologetic vision of how amazing the future could be. And when the sound abruptly cuts out and the song resets, there&#x27;s another image I really like - that humanity is not special because we were created by some God for a grand purpose. Instead, we are special precisely because we were shaped by random forces in an un-extraordinary corner of the universe, and all of our potential power and greatness comes from our own desires, intellect and drive.</p><p>So it&#x27;s ideologically moving to me. But I didn&#x27;t really realize until I sang in a group how tribally moving it could be. I wasn&#x27;t sure people would like the song. The chorus in particular sounds silly when you sing it by yourself. But as a group, everyone got really into it, and yes the chorus was still a little silly but we got up and waved our arms around and belted it out and it felt really good to be part of a group who believed that this weird, outlandish idea was in fact very plausible and important.</p><p>So that was cool.</p><p>I also thought it slightly terrifying. </p><p>Songs like Singularity are what give me the most pause about encouraging Less Wrong culture and rituals.</p><h2>Signaling Issues</h2><p>There are two big issues with ritual. The first is how it makes other people perceive us. </p><p>Rituals are, almost by definition, symbolic actions that look a little weird from the outside. They normally seem okay, because they are ancient and timeless (or at least were created a few years before people started paying attention). But any Less Wrong ritual is going to have all the normal weirdness of &quot;fresh&quot; tradition, <em>and </em>it&#x27;s going to look extra strange because we&#x27;re Less Wrong, and we&#x27;re going to be using words like &quot;ritual&quot; and &quot;tribal identification&quot; to matter-of-factly describe what we&#x27;re doing.</p><p>Some people may be turned off. Skeptics who specifically turned to rationality to escape mindless ritual that was forced upon them may find this all scary. Quality, intelligent individuals may come to our website, see an article about a night of ritual and then tune out and leave.</p><p>I think this is an acceptable cost to pay. Because for good or for ill, most humans <em>like</em> emotional things that aren\u2019t strictly rational. Many people are drawn to the Sequences not just because they say important things, but because Eliezer crafted an emotional narrative around his ideas. He included litanies and parables, which move us in a way that pure logic often can\u2019t. </p><p>There are smart cynics who will be turned off, but there are also smart idealists who will be drawn to recognizable human emotional arcs. I don&#x27;t think ritual should be the FIRST thing potential newcomers see, but I think it is something that will get them fully involved with our community and the important things we believe. I think it may particularly help former theists, who have built their entire lives around a community and ritual infrastructure, make the transition into atheists who are proud of their new beliefs and do productive things.</p><p>It may even help current theists make the transition, if they can see that they WON&#x27;T have to be giving up that community and ritual infrastructure, and all the hedons that went along with it.</p><p>But there\u2019s another cost to ritual, that can\u2019t be resolved quickly with a cost-benefit analysis.</p><p><em>[Update from 2016 - I want to clarify that while I think Less Wrong as a community is a reasonable place for ritual and culture-building, there are other related communities and organizations that are more &quot;PR sensitive&quot; and I don&#x27;t think should be connected to ritual]</em></p><h2>Dangers of Reinforcement</h2><p>Ritual taps into a lot of regions of our brain that are explicitly irrational, or at least a-rational. I don\u2019t think we can afford to ignore those regions - they are too essential to our existence as humans to simply write off. We didn\u2019t evolve to use pure logic to hold together communities and inspire decisions. Some people may be able to do this, but not most.</p><p>I think we need ritual, but I would be a fool to deny that we\u2019re dealing with a dangerous force. Ritual is a self-reinforcing carrier wave for ideas. Those ideas can turn out to be wrong, and the art that was once beautiful and important can turn hollow or even dangerous. Even true ideas can be magnified until you ignite a happy death spiral, giving them far more of your time than they deserve.</p><p>Some of this can be mitigated by making traditions explicitly <em>about </em>the rational process, and building evaluation into the ritual itself. We can review individual elements and remove them if necessary. We can even plan to rewrite them into new parodies that refute the old idea, ceremoniously discarding our old ideas. But this will be a meaningless process unless we are putting in genuine effort - not just doing a dutiful review as good rationalists should.</p><p>We can recite the Litany of Tarski, but unless you are truly considering both possibilities and preparing yourself for them, the words are useless. No amount of pre-planning will change the fact that using ritual will require deliberate effort to protect you from the possibility of insanity. </p><p>You should be doing this anyway. There are plenty of ways to fall into a happy death spiral that don\u2019t involve candle-lit gatherings and weird songs. When you\u2019re dealing with ideas as powerful as the Singularity - a meme that provides a nice, sound-byte word that suggests a solution to all of the most terrible problems humanity faces - you should <em>already </em>be on guard against wishful thinking. When you&#x27;re talking about those ideas in a group, you should already be working hard - genuinely hard, not just performing a dutiful search - to overcome group think and <a href=\"https://www.lesswrong.com/lw/lr/evaporative_cooling_of_group_beliefs/\">evaporative cooling</a> and maintain your objectivity.</p><p>You should be doing that no matter what. <a href=\"https://www.lesswrong.com/lw/lv/every_cause_wants_to_be_a_cult/\">Every cause wants to be a cult</a>, whether or not they have a <a href=\"http://en.wikipedia.org/wiki/God\">nice word that sounds way simpler than it actually is that promises to solve all the world\u2019s problems.</a></p><p>Ritual <em>does </em>make this harder. I\u2019m particularly wary of songs like Singularity, which build up a particular idea that still has a LOT of unknown factors. An anonymous commenter from the Solstice celebration told me they were concerned about the song because it felt like they were \u201cworshipping\u201d the Singularity, and I agree, this is concerning, both for our own sanity and for the signaling it implies to newcomers who stumble upon this discussion. </p><p>I\u2019d go ahead and exclude the song, and any meme that got too specific with too many uncertainties.... except that a lot of our most powerful, beautiful images come from specific ideas about the future. A generic rallying cry of \u201cScience!\u201d, \u201cHumanism!\u201d or \u201cRationality!\u201d is not a satisfying answer to the problems of Death and Global Suffering and Existential Risk. It\u2019s not satisfying on an artistic level, an intellectual level or a tribal level. Having specific ideas about how to steer the future is what gives our group a unique identity. Caring too much about that identity is dangerous, but it can also be extremely motivational.</p><p>As of now, I\u2019m not sure what I think about this particular problem. I look forward to commenters weighing in.</p><p>With all this dire warning, it may seem like a slam dunk case, to abandon the idea of ritual. Obviously, I disagree, for a few reasons. </p><p>The first is that honestly, gathering a few times a year to sing \u201cSingularity! Singularity!\u201d, even <em>without </em>all the preventative measures, simply pales in significance compared to... well... the entire Less Wrong community-memeplex doing what it does on a regular basis.</p><p>If we were genuinely concerned about making bad decisions due to reinforcement rituals, I\u2019d start by worrying about much more <em>mundane </em>rituals, like <em>continuously discussing the Singularity all the time. </em>Constantly talking about an idea trains your brain to think of it as important. Hanging out on forums with a constant stream of news about it creates confirmation and availability bias. If you\u2019re concerned about irrationality, as opposed to weird ceremonies that might signal low status, you should already be putting a lot of effort into protecting yourself against a happy death spiral, and the extra effort you need to expend for a few nights of jubilant celebration shouldn\u2019t be that significant.</p><p>The danger of ceremonial ritual <em>in particular </em>is real, but overestimating it isn\u2019t much better than underestimating it. Even if we\u2019re just talking about ritual as a source of hedons that we\u2019ve previously denied ourselves. Families across the world gather to sing songs about ideas they like, and while this <em>may </em>be a human behavior we need to sacrifice, I\u2019m not going to do so out of fear of what *might* happen without a decent understanding of why.</p><p>But there\u2019s more to it than that. And this is why I\u2019ve worked so hard on this, and why I think the potential upsides are so important.</p><h2>Aspiring Rationalist Culture</h2><p>I had two major motivations for the Solstice celebration. One of them was to produce a fun event for my community, and to inspire similar events for people across the world who share my memes.</p><p>The other was personal: Rationality training has made me better at identifying good solutions, but it hasn&#x27;t made those solutions emotionally salient. This is particularly important when it comes to optimal philanthropy - a million starving people across the world simply can&#x27;t compete with a single smiling orphan I get to personally deliver a christmas present to. And those people have an even harder time if they live in the distant future. </p><p>Scope insensitivity and time discounting can be hard to overcome. Worst of all is when the best solution <em>might not work, </em>and I may not even <em>live </em>to see it work, and I can never get the emotional satisfaction of knowing I actually helped <em>anyone at all.</em> </p><p>I constructed the Solstice celebration around a narrative, based around the interplay between past, present and future. The process of crafting that narrative was extremely valuable to me, and has helped me to finally give Existential Risk the weight it deserves. I haven&#x27;t committed to helping SIAI in particular, but I feel like I&#x27;m at place where if I got better information on how effective SIAI is, I&#x27;d be emotionally able to act on that information. </p><p>I don&#x27;t think the first execution of the Solstice celebration successfully provided other people with that experience, but I think there is tremendous potential in the idea. I&#x27;d like to see the development of a culture that doesn&#x27;t glorify any particular solution, but which takes important rationality concepts and helps people modify their emotions match the actual expected values of actions that would otherwise seem cold, hard and calculating. </p><p>I think this may turn out to be very important.</p><p>In some ways this has me far more scared than ritual-as-hedons. People can gather for a night of jovial fun and come away mostly unchanged. Using ritual *deliberately* to modify yourself is risky, and it is perhaps riskiest if you think you have a good reason for doing so.</p><p>I don&#x27;t think this is that dangerous on the individual level. It was useful to me, I think others may find value in it. Actually allowing yourself to be changed in a meaningful way requires <em>effort </em>beyond an initial, inspiring ceremony. (It took me several weeks of intense work *preparing* the Solstice celebration, cemented by a final moment when a few ideas clicked into place and I came up with a metaphor I could use to alter my emotions. I don&#x27;t know for sure whether this can be distilled into a process that others can use with less effort).</p><p>Next year, I expect the people who come to Solstice for the comfort and hedons will get what they need, and if anyone wants to use it as a springboard for self-modification, they will be able to as well.</p><p>The possibility that most concerns me is a chain of events going something like this:</p><ol><li>Someone (possibly a future version of me, possibly any random person perusing these articles) will decide that this is important enough to deliberately propagate on a mass scale.</li><li>Said person will become good enough at ritual-craft (or use additional dark arts techniques) to make this much more effective than I currently anticipate.</li><li>The result is an effective but low-status self-propagating organization that ends up corrupting the original goal of &quot;help people be better at following through on correct but emotionally unsatisfying choices.&quot;</li></ol><p>This scenario requires someone to put a lot of work in, and they would probably fail uneventfully even if they did. Even if events transpire this way, the problem is less that a hollow self-propagating memeplex exists (it&#x27;s not like we don&#x27;t already have plenty of them, one more won&#x27;t hurt that much) but that its association with Less Wrong and related things may tarnish our reputation.</p><p>I&#x27;d like to think this is a bad thing, although truth be told I think it assumes a level of importance that Less Wrong hasn&#x27;t really carved or itself yet in the greater world. But we <em>are</em> trying to gain status and out of respect for the community I should acknowledge this risk, and sincerely solicit feedback.</p><p>My current assessment is that a) this is unlikely, and b) any organization that&#x27;s trying to accomplish things on a large scale WILL have to accept the risk that it transforms into a hollow, self perpetuating memeplex. If you don&#x27;t want that risk <em>at all</em>, you&#x27;re probably not going to affect the world in a noticeable way. Ritual-driven memeplexes tend to be religions, which many of us consider a rival tribe, so they carry more emotional weight in our risk assessment. But this can also happen to corporations, unions, non-profits and political movements that may have been genuinely valuable at some point.</p><p>I do plan to study this issue in more detail over the coming year. If anyone does have specific literature or examples that I should be aware of, I&#x27;d appreciate it. But my priors are that the few negative reactions I&#x27;ve gotten to this are based more out of emotion than a clear understanding of the risks.</p><p>My final concern is that this simply isn&#x27;t a topic that Less Wrong should encourage <em>that</em> much of, partly because some people find it annoying and partly because we really should be spending our time developing tools for rational thinking and studying scientific literature. I have another article or two that I think would be valuable enough for the main page, and after that I&#x27;ll be inviting people to a separate mailing list if they want to collaborate. </p><p></p><p>This is the second post of the ritual mini-sequence. The next post is <a href=\"https://www.lesswrong.com/lw/9aw/designing_ritual/\">Designing Ritual</a>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"hXTqT62YDTTiqJfxG": 9, "vtozKm5BZ8gf6zd45": 10}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rijoxTpkSPXcTXRbN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 40, "baseScore": 42, "extendedScore": null, "score": 8.7e-05, "legacy": true, "legacyId": "11793", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": "3bbvzoRA8n6ZgbiyK", "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": "designing-ritual", "canonicalPrevPostSlug": "ritual-report-nyc-less-wrong-solstice-celebration", "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 42, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><em>This is the second part of my Winter Solstice Ritual mini-sequence. The <a href=\"https://www.lesswrong.com/lw/8x5/ritual_report_nyc_less_wrong_solstice_celebration/\">introduction post is here</a>.</em></p><p>-</p><p>Ritual is an interesting phenomenon to me. It can be fun, beautiful, profound, useful... and potentially dangerous.</p><p>Commenters from the previous article fell into two main camps - those who assumed I knew what I was doing and gave me the benefit of the doubt, and those who were afraid I was naively meddling with forces beyond my comprehension. This was a reasonable fear. In this article, I\u2019ll outline why I think ritual is important, why it\u2019s dangerous, why I think it's relevant to an aspiring rationalist culture.</p><p></p><p>Before I start arguing how meaningful and transformative ritual can be, I want to argue something simpler:</p><p>It can be really fun.</p><p>This is not to be discounted. For whatever reason, humans tend to appreciate songs, stories and activities that they shared with their tribe. Hedons from ritual can take the form of fun joviality as well as intense, profound experiences.</p><p>Not everything we evolved to do is good. If we feel an urge to hit the enemy tribesman with a huge rock and take their land, we can and should say \u201cNo, there are important game theoretic and moral reasons why this is a bad idea\u201d and suppress the urge. But we can also devise new activities, like <a href=\"http://en.wikipedia.org/wiki/American_football\">knocking the enemy tribesman over and taking their ball</a>, satisfying that urge without the negative consequences of war. I'd like access to the experience that ritual uniquely offers, if it can be done safely.</p><p>Ritual covers a range of experience. One subset of that is a kind of art. To give you some sense of what I mean here, here's a few clusters of activities.</p><ul><li>Art, enjoyed alone for simple aesthetics.</li><li>Art that speaks to your beliefs. </li><li>Art that you enjoy appreciating with other people.</li><li>Beliefs that you enjoy sharing with other people.</li><li>Repetition of activities that you do every year.</li></ul><p>And here are a few songs I like:</p><ul><li><a href=\"http://www.youtube.com/watch?v=9T4WB2zfmps\">Silent Night</a></li><li><a href=\"http://www.youtube.com/watch?v=6tNbsQ8eDbA\">Carol of the Bells</a></li><li><a href=\"http://www.youtube.com/watch?v=Ad7KU9bCTAM\">Do You Hear What I Hear</a></li><li><a href=\"http://www.youtube.com/watch?v=4-vDhYTlCNw\">The Word of God</a></li><li><a href=\"http://reasonableape.blogspot.com/2011/08/singularity-lisps.html\">Singularity</a></li></ul><h2 id=\"Art_and_Belief\">Art and Belief</h2><p>I like <em><a href=\"http://www.youtube.com/watch?v=9T4WB2zfmps\">Silent Night</a> b</em>ecause it is a simple, tranquil song, often sung with skillful harmonies.</p><p>I like <em><a href=\"http://www.youtube.com/watch?v=6tNbsQ8eDbA\">Carol of the Bell</a></em> because it is a powerful, awe-inspiring song that is performed with immense complexity and skill.</p><p>I like <em><a href=\"http://www.youtube.com/watch?v=Ad7KU9bCTAM\">Do You Hear What I Hear</a></em> partly for the same reasons I like Silent Night - it begins with simple tranquility. But I also like it for ideological reasons - it showcases the power of a meme growing over time, magnifying, evolving and changing the lives of its hosts as they come to believe it. As an artist hoping to craft powerful memes, this is very important to me. I also like the imagery of the proud king, willing to listen to the words of a shepherd boy, acknowledging the importance of a small infant born into poverty far away. </p><p>And the king is able to command the attention of an entire nation: Take a moment, stop what you are doing, and pay attention to this child.</p><p>But <em>Do You Hear What I Hear </em>also bothers me slightly - it lies in the uncanny valley of ideological identification. The song strikes very close to home in my heart, and I want to give myself over to the song, not just to sing the words but to truly feel them in my heart. And I can\u2019t, because there is a slight snag when we get to the proud king. The king is valuing the child for all the wrong reasons. I want the child to be important because <em>all </em>children are important. But this king would not have given the child a second thought if it hadn\u2019t been the son of God. I don\u2019t believe in Jesus, so the intended message of the song clashes with what I <em>want </em>it to be about.</p><p>For the most part I sing the song without thinking about this, but that little snag is there, and it prevents the song from being one of my favorites ever.</p><p>Contrast this with S<em>ilent Night</em>, where the message is largely irrelevant to me. Or <em>Carol of the Bells</em>, whose message is \u201cBells are pretty and people like them.\u201d  I appreciate them aesthetically and I respect skilled performers. Their messages don't bother me, but neither do I feel as strongly about them.</p><h2 id=\"Art_and_Tribe\">Art and Tribe</h2><p><em><a href=\"http://www.youtube.com/watch?v=4-vDhYTlCNw\">The Word of God</a></em> is beautiful because the world is an incredible place, and humans have discovered millions of beautiful true things about it. There is exactly one thing I dislike about this song, and it is not a disagreement with its ideology. It\u2019s just the use of the word \u201cGod.\u201d I don\u2019t think it was wrong word to use - it\u2019s a nice, simple word and I read it purely as a metaphor for \u201cthe universe.\u201d </p><p>Like <em>Do you Hear</em>, there is some uncanny-valley effect here. But here it\u2019s about tribal identification. (I draw a distinction between tribal identity and ideology - tribe is about identifying with a group of people, ideology is identifying with a belief-structure).</p><p>My mind snags because \u201cGod\u201d is a word I normally associate with other cultures. This isn\u2019t as big a deal as in <em>Do You Hear. </em>I don\u2019t actually consider the goddists to be my enemy, I just don\u2019t feel connected to them, and the word takes me out of the beauty of the song and reminds me of this disconnection. I did go ahead and include <em>Word of God</em>, verbatim, in the Winter Solstice Celebration. I just want to note that there are different reasons to be moved by (or fail to be moved by) a song.</p><p><em>[Edit in 2018: <a href=\"https://soundcloud.com/raymond-arnold/08-time-wrote-the-rocks\">we've since re-written</a> Word of God (after touching base with the original songwriter) to focus more purely on scientific progress rather than God. This was less because \"God\" was problematic and more because it kept the focus on </em>political conflict<em> that didn't seem good for Solstice longterm]</em></p><p>Finally, we have <em><a href=\"http://reasonableape.blogspot.com/2011/08/singularity-lisps.html\">Singularity</a></em>, which I like for all kinds of reasons. </p><p>The music begins whimsical and fun, but grows more powerful and exciting over time. If you have good speakers, there's a heavy but subtle baseline that drives the sound through your bones. It was refreshing to hear an unapologetic vision of how amazing the future could be. And when the sound abruptly cuts out and the song resets, there's another image I really like - that humanity is not special because we were created by some God for a grand purpose. Instead, we are special precisely because we were shaped by random forces in an un-extraordinary corner of the universe, and all of our potential power and greatness comes from our own desires, intellect and drive.</p><p>So it's ideologically moving to me. But I didn't really realize until I sang in a group how tribally moving it could be. I wasn't sure people would like the song. The chorus in particular sounds silly when you sing it by yourself. But as a group, everyone got really into it, and yes the chorus was still a little silly but we got up and waved our arms around and belted it out and it felt really good to be part of a group who believed that this weird, outlandish idea was in fact very plausible and important.</p><p>So that was cool.</p><p>I also thought it slightly terrifying. </p><p>Songs like Singularity are what give me the most pause about encouraging Less Wrong culture and rituals.</p><h2 id=\"Signaling_Issues\">Signaling Issues</h2><p>There are two big issues with ritual. The first is how it makes other people perceive us. </p><p>Rituals are, almost by definition, symbolic actions that look a little weird from the outside. They normally seem okay, because they are ancient and timeless (or at least were created a few years before people started paying attention). But any Less Wrong ritual is going to have all the normal weirdness of \"fresh\" tradition, <em>and </em>it's going to look extra strange because we're Less Wrong, and we're going to be using words like \"ritual\" and \"tribal identification\" to matter-of-factly describe what we're doing.</p><p>Some people may be turned off. Skeptics who specifically turned to rationality to escape mindless ritual that was forced upon them may find this all scary. Quality, intelligent individuals may come to our website, see an article about a night of ritual and then tune out and leave.</p><p>I think this is an acceptable cost to pay. Because for good or for ill, most humans <em>like</em> emotional things that aren\u2019t strictly rational. Many people are drawn to the Sequences not just because they say important things, but because Eliezer crafted an emotional narrative around his ideas. He included litanies and parables, which move us in a way that pure logic often can\u2019t. </p><p>There are smart cynics who will be turned off, but there are also smart idealists who will be drawn to recognizable human emotional arcs. I don't think ritual should be the FIRST thing potential newcomers see, but I think it is something that will get them fully involved with our community and the important things we believe. I think it may particularly help former theists, who have built their entire lives around a community and ritual infrastructure, make the transition into atheists who are proud of their new beliefs and do productive things.</p><p>It may even help current theists make the transition, if they can see that they WON'T have to be giving up that community and ritual infrastructure, and all the hedons that went along with it.</p><p>But there\u2019s another cost to ritual, that can\u2019t be resolved quickly with a cost-benefit analysis.</p><p><em>[Update from 2016 - I want to clarify that while I think Less Wrong as a community is a reasonable place for ritual and culture-building, there are other related communities and organizations that are more \"PR sensitive\" and I don't think should be connected to ritual]</em></p><h2 id=\"Dangers_of_Reinforcement\">Dangers of Reinforcement</h2><p>Ritual taps into a lot of regions of our brain that are explicitly irrational, or at least a-rational. I don\u2019t think we can afford to ignore those regions - they are too essential to our existence as humans to simply write off. We didn\u2019t evolve to use pure logic to hold together communities and inspire decisions. Some people may be able to do this, but not most.</p><p>I think we need ritual, but I would be a fool to deny that we\u2019re dealing with a dangerous force. Ritual is a self-reinforcing carrier wave for ideas. Those ideas can turn out to be wrong, and the art that was once beautiful and important can turn hollow or even dangerous. Even true ideas can be magnified until you ignite a happy death spiral, giving them far more of your time than they deserve.</p><p>Some of this can be mitigated by making traditions explicitly <em>about </em>the rational process, and building evaluation into the ritual itself. We can review individual elements and remove them if necessary. We can even plan to rewrite them into new parodies that refute the old idea, ceremoniously discarding our old ideas. But this will be a meaningless process unless we are putting in genuine effort - not just doing a dutiful review as good rationalists should.</p><p>We can recite the Litany of Tarski, but unless you are truly considering both possibilities and preparing yourself for them, the words are useless. No amount of pre-planning will change the fact that using ritual will require deliberate effort to protect you from the possibility of insanity. </p><p>You should be doing this anyway. There are plenty of ways to fall into a happy death spiral that don\u2019t involve candle-lit gatherings and weird songs. When you\u2019re dealing with ideas as powerful as the Singularity - a meme that provides a nice, sound-byte word that suggests a solution to all of the most terrible problems humanity faces - you should <em>already </em>be on guard against wishful thinking. When you're talking about those ideas in a group, you should already be working hard - genuinely hard, not just performing a dutiful search - to overcome group think and <a href=\"https://www.lesswrong.com/lw/lr/evaporative_cooling_of_group_beliefs/\">evaporative cooling</a> and maintain your objectivity.</p><p>You should be doing that no matter what. <a href=\"https://www.lesswrong.com/lw/lv/every_cause_wants_to_be_a_cult/\">Every cause wants to be a cult</a>, whether or not they have a <a href=\"http://en.wikipedia.org/wiki/God\">nice word that sounds way simpler than it actually is that promises to solve all the world\u2019s problems.</a></p><p>Ritual <em>does </em>make this harder. I\u2019m particularly wary of songs like Singularity, which build up a particular idea that still has a LOT of unknown factors. An anonymous commenter from the Solstice celebration told me they were concerned about the song because it felt like they were \u201cworshipping\u201d the Singularity, and I agree, this is concerning, both for our own sanity and for the signaling it implies to newcomers who stumble upon this discussion. </p><p>I\u2019d go ahead and exclude the song, and any meme that got too specific with too many uncertainties.... except that a lot of our most powerful, beautiful images come from specific ideas about the future. A generic rallying cry of \u201cScience!\u201d, \u201cHumanism!\u201d or \u201cRationality!\u201d is not a satisfying answer to the problems of Death and Global Suffering and Existential Risk. It\u2019s not satisfying on an artistic level, an intellectual level or a tribal level. Having specific ideas about how to steer the future is what gives our group a unique identity. Caring too much about that identity is dangerous, but it can also be extremely motivational.</p><p>As of now, I\u2019m not sure what I think about this particular problem. I look forward to commenters weighing in.</p><p>With all this dire warning, it may seem like a slam dunk case, to abandon the idea of ritual. Obviously, I disagree, for a few reasons. </p><p>The first is that honestly, gathering a few times a year to sing \u201cSingularity! Singularity!\u201d, even <em>without </em>all the preventative measures, simply pales in significance compared to... well... the entire Less Wrong community-memeplex doing what it does on a regular basis.</p><p>If we were genuinely concerned about making bad decisions due to reinforcement rituals, I\u2019d start by worrying about much more <em>mundane </em>rituals, like <em>continuously discussing the Singularity all the time. </em>Constantly talking about an idea trains your brain to think of it as important. Hanging out on forums with a constant stream of news about it creates confirmation and availability bias. If you\u2019re concerned about irrationality, as opposed to weird ceremonies that might signal low status, you should already be putting a lot of effort into protecting yourself against a happy death spiral, and the extra effort you need to expend for a few nights of jubilant celebration shouldn\u2019t be that significant.</p><p>The danger of ceremonial ritual <em>in particular </em>is real, but overestimating it isn\u2019t much better than underestimating it. Even if we\u2019re just talking about ritual as a source of hedons that we\u2019ve previously denied ourselves. Families across the world gather to sing songs about ideas they like, and while this <em>may </em>be a human behavior we need to sacrifice, I\u2019m not going to do so out of fear of what *might* happen without a decent understanding of why.</p><p>But there\u2019s more to it than that. And this is why I\u2019ve worked so hard on this, and why I think the potential upsides are so important.</p><h2 id=\"Aspiring_Rationalist_Culture\">Aspiring Rationalist Culture</h2><p>I had two major motivations for the Solstice celebration. One of them was to produce a fun event for my community, and to inspire similar events for people across the world who share my memes.</p><p>The other was personal: Rationality training has made me better at identifying good solutions, but it hasn't made those solutions emotionally salient. This is particularly important when it comes to optimal philanthropy - a million starving people across the world simply can't compete with a single smiling orphan I get to personally deliver a christmas present to. And those people have an even harder time if they live in the distant future. </p><p>Scope insensitivity and time discounting can be hard to overcome. Worst of all is when the best solution <em>might not work, </em>and I may not even <em>live </em>to see it work, and I can never get the emotional satisfaction of knowing I actually helped <em>anyone at all.</em> </p><p>I constructed the Solstice celebration around a narrative, based around the interplay between past, present and future. The process of crafting that narrative was extremely valuable to me, and has helped me to finally give Existential Risk the weight it deserves. I haven't committed to helping SIAI in particular, but I feel like I'm at place where if I got better information on how effective SIAI is, I'd be emotionally able to act on that information. </p><p>I don't think the first execution of the Solstice celebration successfully provided other people with that experience, but I think there is tremendous potential in the idea. I'd like to see the development of a culture that doesn't glorify any particular solution, but which takes important rationality concepts and helps people modify their emotions match the actual expected values of actions that would otherwise seem cold, hard and calculating. </p><p>I think this may turn out to be very important.</p><p>In some ways this has me far more scared than ritual-as-hedons. People can gather for a night of jovial fun and come away mostly unchanged. Using ritual *deliberately* to modify yourself is risky, and it is perhaps riskiest if you think you have a good reason for doing so.</p><p>I don't think this is that dangerous on the individual level. It was useful to me, I think others may find value in it. Actually allowing yourself to be changed in a meaningful way requires <em>effort </em>beyond an initial, inspiring ceremony. (It took me several weeks of intense work *preparing* the Solstice celebration, cemented by a final moment when a few ideas clicked into place and I came up with a metaphor I could use to alter my emotions. I don't know for sure whether this can be distilled into a process that others can use with less effort).</p><p>Next year, I expect the people who come to Solstice for the comfort and hedons will get what they need, and if anyone wants to use it as a springboard for self-modification, they will be able to as well.</p><p>The possibility that most concerns me is a chain of events going something like this:</p><ol><li>Someone (possibly a future version of me, possibly any random person perusing these articles) will decide that this is important enough to deliberately propagate on a mass scale.</li><li>Said person will become good enough at ritual-craft (or use additional dark arts techniques) to make this much more effective than I currently anticipate.</li><li>The result is an effective but low-status self-propagating organization that ends up corrupting the original goal of \"help people be better at following through on correct but emotionally unsatisfying choices.\"</li></ol><p>This scenario requires someone to put a lot of work in, and they would probably fail uneventfully even if they did. Even if events transpire this way, the problem is less that a hollow self-propagating memeplex exists (it's not like we don't already have plenty of them, one more won't hurt that much) but that its association with Less Wrong and related things may tarnish our reputation.</p><p>I'd like to think this is a bad thing, although truth be told I think it assumes a level of importance that Less Wrong hasn't really carved or itself yet in the greater world. But we <em>are</em> trying to gain status and out of respect for the community I should acknowledge this risk, and sincerely solicit feedback.</p><p>My current assessment is that a) this is unlikely, and b) any organization that's trying to accomplish things on a large scale WILL have to accept the risk that it transforms into a hollow, self perpetuating memeplex. If you don't want that risk <em>at all</em>, you're probably not going to affect the world in a noticeable way. Ritual-driven memeplexes tend to be religions, which many of us consider a rival tribe, so they carry more emotional weight in our risk assessment. But this can also happen to corporations, unions, non-profits and political movements that may have been genuinely valuable at some point.</p><p>I do plan to study this issue in more detail over the coming year. If anyone does have specific literature or examples that I should be aware of, I'd appreciate it. But my priors are that the few negative reactions I've gotten to this are based more out of emotion than a clear understanding of the risks.</p><p>My final concern is that this simply isn't a topic that Less Wrong should encourage <em>that</em> much of, partly because some people find it annoying and partly because we really should be spending our time developing tools for rational thinking and studying scientific literature. I have another article or two that I think would be valuable enough for the main page, and after that I'll be inviting people to a separate mailing list if they want to collaborate. </p><p></p><p>This is the second post of the ritual mini-sequence. The next post is <a href=\"https://www.lesswrong.com/lw/9aw/designing_ritual/\">Designing Ritual</a>.</p>", "sections": [{"title": "Art and Belief", "anchor": "Art_and_Belief", "level": 1}, {"title": "Art and Tribe", "anchor": "Art_and_Tribe", "level": 1}, {"title": "Signaling Issues", "anchor": "Signaling_Issues", "level": 1}, {"title": "Dangers of Reinforcement", "anchor": "Dangers_of_Reinforcement", "level": 1}, {"title": "Aspiring Rationalist Culture", "anchor": "Aspiring_Rationalist_Culture", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "68 comments"}], "headingsCount": 7}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 68, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["jES7mcPvKpfmzMTgC", "ZQG9cwKbct2LtmL3p", "yEjaj7PWacno5EvWa", "GEQyCqgu5Yhi5dTdb"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 10, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-30T07:33:20.190Z", "modifiedAt": null, "url": null, "title": "Would a FAI reward us for helping create it?", "slug": "would-a-fai-reward-us-for-helping-create-it", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:03.699Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "TwistingFingers", "createdAt": "2011-09-16T02:07:04.963Z", "isAdmin": false, "displayName": "TwistingFingers"}, "userId": "tWYLp5wq8caRLNiSJ", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/W5GPNYAWMx6SWeJuy/would-a-fai-reward-us-for-helping-create-it", "pageUrlRelative": "/posts/W5GPNYAWMx6SWeJuy/would-a-fai-reward-us-for-helping-create-it", "linkUrl": "https://www.lesswrong.com/posts/W5GPNYAWMx6SWeJuy/would-a-fai-reward-us-for-helping-create-it", "postedAtFormatted": "Friday, December 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Would%20a%20FAI%20reward%20us%20for%20helping%20create%20it%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWould%20a%20FAI%20reward%20us%20for%20helping%20create%20it%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW5GPNYAWMx6SWeJuy%2Fwould-a-fai-reward-us-for-helping-create-it%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Would%20a%20FAI%20reward%20us%20for%20helping%20create%20it%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW5GPNYAWMx6SWeJuy%2Fwould-a-fai-reward-us-for-helping-create-it", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FW5GPNYAWMx6SWeJuy%2Fwould-a-fai-reward-us-for-helping-create-it", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 92, "htmlBody": "<p>We expect that post-singularity there will still be limited resources in the form of available computational resources until heat death.</p>\n<p>Those resources do not necessarily need to be allocated fairly. In fact, I would guess that if they were allocated unfairly the most like beneficiaries would be those people that helped contribute to the creation of a friendly AI.</p>\n<p>Now for some open questions:</p>\n<p>What probability distribution of extra resources do you expect with respect to various possible contributions to the creation of friendly AI?</p>\n<p>Would donating to the SIAI suffice for acquiring these extra resources?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "W5GPNYAWMx6SWeJuy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 23, "baseScore": -11, "extendedScore": null, "score": 8.233027078404324e-07, "legacy": true, "legacyId": "11796", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -8, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 25, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-30T08:54:19.642Z", "modifiedAt": null, "url": null, "title": "What are good apps for rationalists?", "slug": "what-are-good-apps-for-rationalists", "viewCount": null, "lastCommentedAt": "2017-06-17T04:29:34.748Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "dekelron", "createdAt": "2011-11-10T09:58:55.323Z", "isAdmin": false, "displayName": "dekelron"}, "userId": "L2EhSfSLAw2PMhAM6", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Pe6ahfAXMRf9HYrJF/what-are-good-apps-for-rationalists", "pageUrlRelative": "/posts/Pe6ahfAXMRf9HYrJF/what-are-good-apps-for-rationalists", "linkUrl": "https://www.lesswrong.com/posts/Pe6ahfAXMRf9HYrJF/what-are-good-apps-for-rationalists", "postedAtFormatted": "Friday, December 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20What%20are%20good%20apps%20for%20rationalists%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhat%20are%20good%20apps%20for%20rationalists%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPe6ahfAXMRf9HYrJF%2Fwhat-are-good-apps-for-rationalists%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=What%20are%20good%20apps%20for%20rationalists%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPe6ahfAXMRf9HYrJF%2Fwhat-are-good-apps-for-rationalists", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPe6ahfAXMRf9HYrJF%2Fwhat-are-good-apps-for-rationalists", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 256, "htmlBody": "<p>The smartphone world is relatively new, so I suspect there are amazing (yet sometimes unknown) apps that can help various aspects of our lives. I cannot think of a better discussion place for this topic. So,</p>\n<p style=\"padding-left: 30px;\">Which app help you optimize life?</p>\n<p style=\"padding-left: 30px;\">Is there an app you keep searching for in vain? (though it might exist!)</p>\n<p>For me (I was searching for both apps for months!! until they were finally created):</p>\n<p><a title=\"anki\" href=\"http://ankisrs.net/\"><strong>Anki</strong></a>, A flashcard memorization app with many useful features.</p>\n<blockquote>\n<p>[From  the link] Anki is a program which makes remembering things easy.  Because it is a lot more efficient than traditional study methods, you  can either greatly decrease your time spent studying, or greatly  increase the amount you learn. <br />Anyone who needs to remember things  in their daily life can benefit from Anki. Since it is content-agnostic  and supports images, audio, videos and scientific markup (via LaTeX),  the possibilities are endless.</p>\n</blockquote>\n<p><a href=\"https://market.android.com/details?id=de.bigbyte.tools.simplecounterwidget&amp;hl=en\">Simple Counter Widget</a>, A counter widget. Sounds simple, but it's amazingly helpful.<br />I  use it as a tool to better myself. Every time I eat my gum (an annoying  mostly unconscious habit), the gum-eating-widget is increased by one.  Because the widget is so accessible, I rarely cheat. This gives the  habit immediate negative feedback, and allows me to check improvement  over time. (Also my desktop looks cool, with many intriguing numbers  popping from everywhere)</p>\n<p>(less interesting: Record Mic and Call - which includes a useful recording widget, and <a href=\"http://www.waze.com/\">Waze</a> - which is amazing but well known GPS app (at least here))</p>\n<p>&nbsp;</p>\n<p>please share to make this interesting :)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Pe6ahfAXMRf9HYrJF", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 4, "extendedScore": null, "score": 8.233326998557623e-07, "legacy": true, "legacyId": "11797", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 26, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-30T10:39:10.249Z", "modifiedAt": null, "url": null, "title": "I'd like to talk to some LGBT LWers.", "slug": "i-d-like-to-talk-to-some-lgbt-lwers", "viewCount": null, "lastCommentedAt": "2017-06-17T04:10:28.489Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Solvent", "createdAt": "2011-07-19T07:12:44.132Z", "isAdmin": false, "displayName": "Solvent"}, "userId": "a3sBsZXtAQacMDHfK", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/bZJBzenHgw33Qeknk/i-d-like-to-talk-to-some-lgbt-lwers", "pageUrlRelative": "/posts/bZJBzenHgw33Qeknk/i-d-like-to-talk-to-some-lgbt-lwers", "linkUrl": "https://www.lesswrong.com/posts/bZJBzenHgw33Qeknk/i-d-like-to-talk-to-some-lgbt-lwers", "postedAtFormatted": "Friday, December 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20I'd%20like%20to%20talk%20to%20some%20LGBT%20LWers.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AI'd%20like%20to%20talk%20to%20some%20LGBT%20LWers.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbZJBzenHgw33Qeknk%2Fi-d-like-to-talk-to-some-lgbt-lwers%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=I'd%20like%20to%20talk%20to%20some%20LGBT%20LWers.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbZJBzenHgw33Qeknk%2Fi-d-like-to-talk-to-some-lgbt-lwers", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FbZJBzenHgw33Qeknk%2Fi-d-like-to-talk-to-some-lgbt-lwers", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 232, "htmlBody": "<p>When _ozymandias posted zir introduction post a few days ago, I went off and binged on blogs from the trans/men's rights/feminist spectrum. I found them absolutely fascinating. I've always had lots of sympathy for transgendered people in particular, and care a lot about all those issues. I don't know what I think of making up new pronouns, and I get a bit offput by trying to remember the non-offensive terms for everything. For example, I'm sure that LGBT as a term offends people, and I agree that lumping the T with the LGB is a bit dubious, but I don't know any other equivalent term that everyone will understand. I'm going to keep using it.</p>\n<p>However, I don't currently know any LGBT people who I can talk to about these things. In particular, the whole LGBT and feminist and so on community seems to be prone to taking unnecessary offense, and believing in subjectivism and silly things like that.</p>\n<p>So I'd really like to talk with some LWers who have experience with these things. I've got questions that I think would be better answered by an IM conversation than by just reading blogs.</p>\n<p>If anyone wants to have an IM conversation about this, please message me. I'd be very grateful.</p>\n<p>EDIT: Wow, that's an amazing response. Thank you all for your kind offers. I'll talk to as many of you as I can get around to.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "bZJBzenHgw33Qeknk", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 5, "extendedScore": null, "score": 2e-05, "legacy": true, "legacyId": "11799", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 6, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 72, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-30T15:07:44.084Z", "modifiedAt": null, "url": null, "title": "Weekly LW Meetups", "slug": "weekly-lw-meetups-45", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:03.297Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "FrankAdamek", "createdAt": "2009-07-10T09:21:16.400Z", "isAdmin": false, "displayName": "FrankAdamek"}, "userId": "u4ciX8qr47d9EiSvD", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/AqywCpwPmvdLcQcEN/weekly-lw-meetups-45", "pageUrlRelative": "/posts/AqywCpwPmvdLcQcEN/weekly-lw-meetups-45", "linkUrl": "https://www.lesswrong.com/posts/AqywCpwPmvdLcQcEN/weekly-lw-meetups-45", "postedAtFormatted": "Friday, December 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Weekly%20LW%20Meetups&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWeekly%20LW%20Meetups%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAqywCpwPmvdLcQcEN%2Fweekly-lw-meetups-45%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Weekly%20LW%20Meetups%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAqywCpwPmvdLcQcEN%2Fweekly-lw-meetups-45", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FAqywCpwPmvdLcQcEN%2Fweekly-lw-meetups-45", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 361, "htmlBody": "<p>There are no LW meetups posted for THIS week, but...</p>\n<p>There are upcoming irregularly scheduled Less Wrong meetups in:</p>\n<ul>\n<li><a href=\"/meetups/5m\">Fort Collins, Colorado Meetup:&nbsp;<span class=\"date\">04 January 2012 07:00PM</span></a></li>\n<li><a href=\"/meetups/5j\">Salt Lake City, Late January 2012</a></li>\n<li><a href=\"/meetups/5n\">Columbus or Cincinnati Meetup:&nbsp;<span class=\"date\">21 January 2012 08:34PM</span></a></li>\n<li><a href=\"/meetups/5f\">First Brussels meetup:&nbsp;<span class=\"date\">18 February 2012 11:00AM</span></a></li>\n</ul>\n<p>The following meetups take place in cities with regularly scheduled meetups, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>\n<ul>\n<li><a href=\"/meetups/5o\">Melbourne practical rationality meetup:&nbsp;<span class=\"date\">06 January 2012 07:00AM</span></a></li>\n</ul>\n<p>The Mountain View meetup is taking a hiatus for the rest of December, see the mailing list for details.</p>\n<p>Cities with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>,</strong><strong> <a href=\"/r/discussion/lw/5pd/southern_california_meetup_may_21_weekly_irvine\">Irvine</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison, WI</a></strong>,<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin, CA</a> </strong>(uses the Bay Area List)<strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Oxford.2C_UK\">Oxford</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#San_Francisco\">San Francisco</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>, and <strong><a href=\"/r/discussion/lw/6at/west_la_biweekly_meetups\">West Los Angeles</a></strong>.<a id=\"more\"></a></p>\n<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, and have fun!</p>\n<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>\n<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>\n<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetup&nbsp;<em>before </em>the Friday before your meetup!</p>\n<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong><strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,&nbsp;<a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a></strong><strong>.</strong></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "AqywCpwPmvdLcQcEN", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 2, "baseScore": 2, "extendedScore": null, "score": 8.234721279536454e-07, "legacy": true, "legacyId": "11637", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pAHo9zSFXygp5A5dL", "tHFu6kvy2HMvQBEhW", "d28mWBMrFt8nwpXLp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-30T16:23:34.614Z", "modifiedAt": null, "url": null, "title": "[RESEARCH] Marijuana may prevent Alzheimer's disease", "slug": "research-marijuana-may-prevent-alzheimer-s-disease", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:09.258Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "bfq5YorFxpih9j6nL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/agLhNuarH4N7X67en/research-marijuana-may-prevent-alzheimer-s-disease", "pageUrlRelative": "/posts/agLhNuarH4N7X67en/research-marijuana-may-prevent-alzheimer-s-disease", "linkUrl": "https://www.lesswrong.com/posts/agLhNuarH4N7X67en/research-marijuana-may-prevent-alzheimer-s-disease", "postedAtFormatted": "Friday, December 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BRESEARCH%5D%20Marijuana%20may%20prevent%20Alzheimer's%20disease&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BRESEARCH%5D%20Marijuana%20may%20prevent%20Alzheimer's%20disease%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FagLhNuarH4N7X67en%2Fresearch-marijuana-may-prevent-alzheimer-s-disease%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BRESEARCH%5D%20Marijuana%20may%20prevent%20Alzheimer's%20disease%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FagLhNuarH4N7X67en%2Fresearch-marijuana-may-prevent-alzheimer-s-disease", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FagLhNuarH4N7X67en%2Fresearch-marijuana-may-prevent-alzheimer-s-disease", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 435, "htmlBody": "<p><span style=\"font-family: arial; \">In a post last August called <a href=\"/lw/7aj/alzheimers_vs_cryonics/\">Alzheimer's vs. Cryonics</a>, I left the following comment:</span></p>\n<blockquote>\n<p><span style=\"font-family: arial; \"><a href=\"/23andme.com\">23andMe</a>&nbsp;recently showed that people of my genotype are more than twice as likely to develop Alzheimer's as others in my ethnic group. I have a 15% chance of getting Alzheimer's before I'm 80, up from 7%. See&nbsp;<a href=\"http://patrissimo.livejournal.com/1453175.html?thread=13409911\">Patri's post</a>&nbsp;about this.</span></p>\n<p><span style=\"font-family: arial; \">An initial Googling has generated things like 'eat paleo', 'get caffeine', 'exercise', and 'use your brain'. I'm planning to do further research about decreasing Alzheimer's risk.</span></p>\n<p><span style=\"font-family: arial; \">I&nbsp;<em>really</em>&nbsp;recommend that everyone do 23andMe for this precise reason.</span></p>\n</blockquote>\n<p><span style=\"font-family: arial; \"><a href=\"/user/wedrifid/\">wedrifid</a> made an insightful response:</span></p>\n<blockquote>\n<p><span style=\"font-family: arial; \">I wonder, how much does that single bit of information (doubling the chance) matter to those decisions? Should you have been doing those things anyway, for the Alzheimer's prevention and the other benefits? Is it the&nbsp;<em>motivational</em>&nbsp;factor of the formal personal certification that is important or the actual information?</span></p>\n</blockquote>\n<p>wedrifid nailed it. Transhumanists have a huge interest in making the necessary lifestyle adjustments to prevent Alzheimer's. Even if cryonics were certain to work, I'd like it to be <em>me</em> that is resurrected when the technology is available, and I won't still be <em>me</em> if Alzheimer's gets its way.</p>\n<p><span style=\"font-family: arial; \">So I've been keeping an eye out for relevant information. I haven't done rigorous research yet, but a friend recently sent me a study: \"<a href=\"http://www.ncbi.nlm.nih.gov/pubmed/17140265\">A molecular link between the active component of marijuana and Alzheimer's disease pathology</a>.\"&nbsp;</span><span style=\"font-family: arial; \">I've uploaded the full text of the article&nbsp;</span><a style=\"font-family: arial; \" href=\"http://michaelcurzi.com/media/Alzheimers%20and%20marijuana.pdf\">here</a><span style=\"font-family: arial; \">.</span></p>\n<p><span style=\"font-family: arial; \">A sample from the abstract:</span></p>\n<blockquote>\n<p><span style=\"font-family: arial; \">Here, we demonstrate that the active component of marijuana, Delta9-tetrahydrocannabinol (THC), competitively inhibits the enzyme acetylcholinesterase (AChE) as well as prevents AChE-induced amyloid beta-peptide (Abeta) aggregation, the key pathological marker of Alzheimer's disease. [...] Compared to currently approved drugs prescribed for the treatment of Alzheimer's disease, THC is a considerably superior inhibitor of Abeta aggregation, and this study provides a previously unrecognized molecular mechanism through which cannabinoid molecules may directly impact the progression of this debilitating disease.</span></p>\n</blockquote>\n<p>From the conclusion:</p>\n<blockquote>\n<p>It is noteworthy that THC is a considerably more effective&nbsp;inhibitor of AChE-induced [Abeta] deposition than the approved&nbsp;drugs for Alzheimer&rsquo;s disease treatment, donepezil and&nbsp;tacrine, which reduced [Abeta] aggregation by only 22% and 7%,&nbsp;respectively, at twice the concentration used in our studies.</p>\n<p>Therefore, AChE inhibitors such as THC and its analogues&nbsp;may provide an improved therapeutic for Alzheimer&rsquo;s&nbsp;disease, augmenting acetylcholine levels by preventing&nbsp;neurotransmitter degradation and reducing [Abeta] aggregation,&nbsp;thereby simultaneously treating both the symptoms and&nbsp;progression of Alzheimer&rsquo;s disease.</p>\n</blockquote>\n<p>I, for one, would like to know if smoking weed could help prevent a fate that's plausibly worse than death-and-cryonics. So f<span style=\"font-family: arial;\">or those who know more about biology than I do: how promising are these results? What else has been shown to help prevent Alzheimer's?</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "agLhNuarH4N7X67en", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 14, "extendedScore": null, "score": 8.23500382910919e-07, "legacy": true, "legacyId": "11801", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 9, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["HPWod9Zrph8CCdR86"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-30T18:25:19.455Z", "modifiedAt": null, "url": null, "title": "Last chance to donate for 2011", "slug": "last-chance-to-donate-for-2011", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:07.832Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "multifoliaterose", "createdAt": "2010-06-13T08:56:10.885Z", "isAdmin": false, "displayName": "multifoliaterose"}, "userId": "747HfTZFyfTqGyoPM", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/WoHsqnehcBTPiJMaR/last-chance-to-donate-for-2011", "pageUrlRelative": "/posts/WoHsqnehcBTPiJMaR/last-chance-to-donate-for-2011", "linkUrl": "https://www.lesswrong.com/posts/WoHsqnehcBTPiJMaR/last-chance-to-donate-for-2011", "postedAtFormatted": "Friday, December 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Last%20chance%20to%20donate%20for%202011&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ALast%20chance%20to%20donate%20for%202011%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWoHsqnehcBTPiJMaR%2Flast-chance-to-donate-for-2011%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Last%20chance%20to%20donate%20for%202011%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWoHsqnehcBTPiJMaR%2Flast-chance-to-donate-for-2011", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWoHsqnehcBTPiJMaR%2Flast-chance-to-donate-for-2011", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 323, "htmlBody": "<p><span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.292969); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\">Many LW readers choose to direct their charitable donations to SingInst with a view toward reducing existential risk. Others do not, whether because they feel they lack an understanding of the relevant issues, because they value present day humans more than future humans or because they have concern as to the incentive effects that would be created by donating to SingInst at present. I personally feel that there's a strong case for saving money to donate later on account of better information being available in the future.</span></p>\n<p><span style=\"-webkit-tap-highlight-color: rgba(26, 26, 26, 0.292969); -webkit-composition-fill-color: rgba(175, 192, 227, 0.230469); -webkit-composition-frame-color: rgba(77, 128, 180, 0.230469);\">However, I feel cognitive dissonance attached to saving to donate later rather than now. If you are in this camp you might consider&nbsp;donating to GiveWell's top ranked charities. Also note that spreading the word about GiveWell promotes a culture of effective philanthropy which is likely to have spin off effect of interesting people in x-risk reduction, reducing x-risk.&nbsp;</span></p>\n<p>See Holden's article on last minute donations http://blog.givewell.org/2011/12/30/last-minute-donations/ :</p>\n<blockquote>\n<p style=\"margin-top: 0.6em; margin-right: 0px; margin-bottom: 1.2em; margin-left: 0px; padding: 0px;\">\"Of the&nbsp;<a style=\"padding: 0px; margin: 0px;\" href=\"http://www.givewell.org/about/impact\">money moved</a>&nbsp;to our&nbsp;<a style=\"padding: 0px; margin: 0px;\" href=\"http://www.givewell.org/charities/top-charities\">top charities</a>&nbsp;through our website in 2010,&nbsp;<span style=\"padding: 0px; margin: 0px;\">25% was on December 31st alone</span>. We know that lots of people will be looking to make last-minute donations.</p>\n<p style=\"margin-top: 0.6em; margin-right: 0px; margin-bottom: 1.2em; margin-left: 0px; padding: 0px;\">If you only have five minutes but you want to take advantage of the thousands of hours of work we put into finding the best giving opportunities, consider giving to our <a style=\"padding: 0px; margin: 0px;\" href=\"http://www.givewell.org/charities/top-charities\">top charities</a>. They have strong track records, accomplish a lot of good per dollar spent, and have good concrete plans for how to use additional donations.</p>\n<p style=\"margin-top: 0.6em; margin-right: 0px; margin-bottom: 1.2em; margin-left: 0px; padding: 0px;\">A couple of things to keep in mind:</p>\n<ul style=\"padding-top: 0px; padding-right: 0px; padding-bottom: 0px; padding-left: 20px; margin: 0px;\">\n<li style=\"margin-top: 0.15em; margin-right: 0px; margin-bottom: 0.15em; margin-left: 0.5em; padding-top: 0px; padding-right: 0px; padding-bottom: 0.2em; padding-left: 0px; list-style-image: none; list-style-type: disc;\"><span style=\"padding: 0px; margin: 0px;\">After you give, spread the word.</span>&nbsp;This is the perfect time to remind people (via&nbsp;<a style=\"padding: 0px; margin: 0px;\" href=\"http://www.facebook.com/sharer.php?u=http://blog.givewell.org/2011/12/30/last-minute-donations/\">Facebook sharing</a>,&nbsp;<a style=\"padding: 0px; margin: 0px;\" href=\"http://www.twitter.com/share?url=http://blog.givewell.org/2011/12/30/last-minute-donations/\">tweeting</a>, etc.) to give before the year ends. And people making last-minute gifts are likely to be receptive to suggestions.</li>\n<li style=\"margin-top: 0.15em; margin-right: 0px; margin-bottom: 0.15em; margin-left: 0.5em; padding-top: 0px; padding-right: 0px; padding-bottom: 0.2em; padding-left: 0px; list-style-image: none; list-style-type: disc;\"><span style=\"padding: 0px; margin: 0px;\">If you have any questions, we&rsquo;re here to help.</span>&nbsp;We should be available by phone for most of the day, and responding to email when we&rsquo;re not. (See our&nbsp;<a style=\"padding: 0px; margin: 0px;\" href=\"http://www.givewell.org/contact\">contact page</a>). Our&nbsp;<a style=\"padding: 0px; margin: 0px;\" href=\"http://www.givewell.org/about/FAQ/research\">research FAQ</a>&nbsp;may also be a good resource.\"</li>\n</ul>\n</blockquote>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "WoHsqnehcBTPiJMaR", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 11, "baseScore": 7, "extendedScore": null, "score": 8.235457434194715e-07, "legacy": true, "legacyId": "11804", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 4, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 4, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-30T21:07:50.916Z", "modifiedAt": null, "url": null, "title": "[LINK] \"Prediction Audits\" for Nate Silver, Dave Weigel", "slug": "link-prediction-audits-for-nate-silver-dave-weigel", "viewCount": null, "lastCommentedAt": "2017-06-17T04:07:35.331Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "orthonormal", "createdAt": "2009-03-22T16:06:51.665Z", "isAdmin": false, "displayName": "orthonormal"}, "userId": "4fh2AAe3n7oBviyxx", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/D9vBfQ2dXvCSkYgmL/link-prediction-audits-for-nate-silver-dave-weigel", "pageUrlRelative": "/posts/D9vBfQ2dXvCSkYgmL/link-prediction-audits-for-nate-silver-dave-weigel", "linkUrl": "https://www.lesswrong.com/posts/D9vBfQ2dXvCSkYgmL/link-prediction-audits-for-nate-silver-dave-weigel", "postedAtFormatted": "Friday, December 30th 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLINK%5D%20%22Prediction%20Audits%22%20for%20Nate%20Silver%2C%20Dave%20Weigel&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLINK%5D%20%22Prediction%20Audits%22%20for%20Nate%20Silver%2C%20Dave%20Weigel%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD9vBfQ2dXvCSkYgmL%2Flink-prediction-audits-for-nate-silver-dave-weigel%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLINK%5D%20%22Prediction%20Audits%22%20for%20Nate%20Silver%2C%20Dave%20Weigel%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD9vBfQ2dXvCSkYgmL%2Flink-prediction-audits-for-nate-silver-dave-weigel", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FD9vBfQ2dXvCSkYgmL%2Flink-prediction-audits-for-nate-silver-dave-weigel", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 39, "htmlBody": "<p><a href=\"http://fivethirtyeight.blogs.nytimes.com/2011/12/30/our-smartest-and-dumbest-columns-of-2011/\">Nate Silver</a> (the NYT quantitative political analyst) and <a href=\"http://www.slate.com/articles/news_and_politics/politics/2011/12/david_weigel_chooses_his_four_dumbest_political_predictions_of_the_year.html\">Dave Weigel</a> (the Slate columnist) have started a good tradition, listing their worst predictions of 2011. (Silver also listed his best.)</p>\n<p>If any other pundits are doing the same, link them here.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "D9vBfQ2dXvCSkYgmL", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 13, "baseScore": 18, "extendedScore": null, "score": 8.236063034684296e-07, "legacy": true, "legacyId": "11806", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 12, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-31T05:07:13.879Z", "modifiedAt": null, "url": null, "title": ".", "slug": "", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:21.718Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "a7xJQpZ55R6SxFTik", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/w9eELo9krcjwjNGGK/", "pageUrlRelative": "/posts/w9eELo9krcjwjNGGK/", "linkUrl": "https://www.lesswrong.com/posts/w9eELo9krcjwjNGGK/", "postedAtFormatted": "Saturday, December 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw9eELo9krcjwjNGGK%2F%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw9eELo9krcjwjNGGK%2F", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fw9eELo9krcjwjNGGK%2F", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1, "htmlBody": "<p>.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"NDjABjx6DRfSKcqED": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "w9eELo9krcjwjNGGK", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 36, "extendedScore": null, "score": 8.237848071659407e-07, "legacy": true, "legacyId": "11819", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 25, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-31T06:09:44.928Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] The \"Intuitions\" Behind \"Utilitarianism\"", "slug": "seq-rerun-the-intuitions-behind-utilitarianism", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/uRHo4TMxkR27P5xbQ/seq-rerun-the-intuitions-behind-utilitarianism", "pageUrlRelative": "/posts/uRHo4TMxkR27P5xbQ/seq-rerun-the-intuitions-behind-utilitarianism", "linkUrl": "https://www.lesswrong.com/posts/uRHo4TMxkR27P5xbQ/seq-rerun-the-intuitions-behind-utilitarianism", "postedAtFormatted": "Saturday, December 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20The%20%22Intuitions%22%20Behind%20%22Utilitarianism%22&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20The%20%22Intuitions%22%20Behind%20%22Utilitarianism%22%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuRHo4TMxkR27P5xbQ%2Fseq-rerun-the-intuitions-behind-utilitarianism%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20The%20%22Intuitions%22%20Behind%20%22Utilitarianism%22%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuRHo4TMxkR27P5xbQ%2Fseq-rerun-the-intuitions-behind-utilitarianism", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FuRHo4TMxkR27P5xbQ%2Fseq-rerun-the-intuitions-behind-utilitarianism", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 193, "htmlBody": "<p>Today's post, <a href=\"/lw/n9/the_intuitions_behind_utilitarianism/\">The \"Intuitions\" Behind \"Utilitarianism\"</a> was originally published on 28 January 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>Our intuitions, the underlying cognitive tricks that we use to build our thoughts, are an indespensible part of our cognition. The problem is that many of those intuitions are incoherent, or are undesirable upon reflection. But if you try to \"renormalize\" your intuition, you wind up with what is essentially utilitarianism.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/93i/seq_rerun_circular_altruism/\">Circular Altruism</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb187": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "uRHo4TMxkR27P5xbQ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 4, "baseScore": 6, "extendedScore": null, "score": 8.238082838159277e-07, "legacy": true, "legacyId": "11824", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 3, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["r5MSQ83gtbjWRBDWJ", "7t8oM52xMJpGi6MeA", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-31T13:38:19.077Z", "modifiedAt": null, "url": null, "title": "Why some people seem to be proud of their ignorance?", "slug": "why-some-people-seem-to-be-proud-of-their-ignorance", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:21.588Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "uzalud", "createdAt": "2011-07-16T12:16:46.510Z", "isAdmin": false, "displayName": "uzalud"}, "userId": "LtYeek58ACZWdRy8n", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/nM2j2AFcbANdpJ9es/why-some-people-seem-to-be-proud-of-their-ignorance", "pageUrlRelative": "/posts/nM2j2AFcbANdpJ9es/why-some-people-seem-to-be-proud-of-their-ignorance", "linkUrl": "https://www.lesswrong.com/posts/nM2j2AFcbANdpJ9es/why-some-people-seem-to-be-proud-of-their-ignorance", "postedAtFormatted": "Saturday, December 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Why%20some%20people%20seem%20to%20be%20proud%20of%20their%20ignorance%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AWhy%20some%20people%20seem%20to%20be%20proud%20of%20their%20ignorance%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnM2j2AFcbANdpJ9es%2Fwhy-some-people-seem-to-be-proud-of-their-ignorance%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Why%20some%20people%20seem%20to%20be%20proud%20of%20their%20ignorance%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnM2j2AFcbANdpJ9es%2Fwhy-some-people-seem-to-be-proud-of-their-ignorance", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FnM2j2AFcbANdpJ9es%2Fwhy-some-people-seem-to-be-proud-of-their-ignorance", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 255, "htmlBody": "<p>Sometimes I run into people that have rather strong&nbsp;opinions&nbsp;on some topic, and it turns out that they are basing them on quite shallow and biased information. They are aware that their knowledge is quite limited compared to mine, and they admit that they don't want to put in the effort needed to learn enough to level the field.</p>\n<p>But that's not really a problem. What is bothering me is that, sometimes, that declaration of ignorance is expressed with some kind of <em>pride</em>.&nbsp;</p>\n<p>This&nbsp;behaviour&nbsp;is noticeable&nbsp;on other levels too, in politics or in the sciences-humanities culture clash.</p>\n<p>I came up with several hypotheses which might account for this:</p>\n<ol>\n<li>Being opinionated on a topic you know little about is a sign of <strong>confidence </strong>and <strong>bravery</strong>. Any fool can play it safe and carefully form opinions based on solid knowledge, but it takes <em>a real man</em>&nbsp;to do it quickly and&nbsp;decidedly,&nbsp;with only partial information.</li>\n<li>Knowing something is an&nbsp;<strong>identity badge</strong>. In-depth knowledge of science, or computers, or any number of other fields is a sign that you are a geek. People are proud of not being geeks, or are a proud member of some other group that does not care for that particular knowledge.</li>\n<li>Knowledge is <strong>relative </strong>and/or <strong>unimportant</strong>. Not caring about concrete knowledge is a sign of post-modernist sophistication, or an avant-garde, non-mainstream thinking, which is something to be proud of.</li>\n<li>Displaying pride <strong>overcompensates </strong>for shame one normally feels when forced to acknowledge one's ignorance.</li>\n</ol>\n<p>Do you notice this behaviour too? What do you think causes it?</p>\n<p><em>EDIT: formatting, style, grammar</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "nM2j2AFcbANdpJ9es", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 20, "extendedScore": null, "score": 5.6e-05, "legacy": true, "legacyId": "11826", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 17, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 34, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-31T15:27:01.797Z", "modifiedAt": null, "url": null, "title": "[wild nonsense] How about marketing rationality along the lines of \"Feeling Rational\"?", "slug": "wild-nonsense-how-about-marketing-rationality-along-the", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:08.109Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Multiheaded", "createdAt": "2011-07-02T10:10:20.692Z", "isAdmin": false, "displayName": "Multiheaded"}, "userId": "moGiw35FowgiAnzfg", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rtjPeggZ6TSmQKKoD/wild-nonsense-how-about-marketing-rationality-along-the", "pageUrlRelative": "/posts/rtjPeggZ6TSmQKKoD/wild-nonsense-how-about-marketing-rationality-along-the", "linkUrl": "https://www.lesswrong.com/posts/rtjPeggZ6TSmQKKoD/wild-nonsense-how-about-marketing-rationality-along-the", "postedAtFormatted": "Saturday, December 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Bwild%20nonsense%5D%20How%20about%20marketing%20rationality%20along%20the%20lines%20of%20%22Feeling%20Rational%22%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Bwild%20nonsense%5D%20How%20about%20marketing%20rationality%20along%20the%20lines%20of%20%22Feeling%20Rational%22%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrtjPeggZ6TSmQKKoD%2Fwild-nonsense-how-about-marketing-rationality-along-the%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Bwild%20nonsense%5D%20How%20about%20marketing%20rationality%20along%20the%20lines%20of%20%22Feeling%20Rational%22%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrtjPeggZ6TSmQKKoD%2Fwild-nonsense-how-about-marketing-rationality-along-the", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FrtjPeggZ6TSmQKKoD%2Fwild-nonsense-how-about-marketing-rationality-along-the", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 340, "htmlBody": "<p><a href=\"/lw/hp/feeling_rational/\">An excellent piece.</a> I just saw it again, after skimming through the sequences once more today, having nothing better to do on New Year's Eve, and had a thought. What if Less Wrong could do a... mildly cultish thing and, when preaching rationality to random technically inclined folks, start not from its utility, beauty and necessity, but from its ability to make oneself *truer?*</p>\r\n<p>Appeal to idealism, or, as the correct term here goes, to terminal values! If an ape desires to be an angel and, seeing no way to become one but many ways to make the pain bearable, abuses all kinds of opium for the masses, don't just come on to him like a boring ol' rehab worker when you can start the conversation by promising him actual grafted wings, and explain how it's in his interest to at least survive while they're being tested - oh, and take some theoretical flight lessons. Imagine if LW was disguised as a transhumanist propaganda resource first and foremost. It could ignite people's imagination with quality fiction and passionate calls to arms... then explain that there's only one way, twisting and narrow, way forward, and you need St. Bayes to hold your hand even if you want to make the first step.</p>\r\n<p>You stumble in here, and are made to envision and clarify your core values, to become more aware of your actual highest desires - the ones that most ordinary people are probably ashamed to even display: they will likely involve ending all misery, grasping for the stars, casting off the shackles of biology, growing up in the best way possible. And *then* you're shown the path, step by step, to the launch pad where the ships are being built, so you can, too, contribute. Couldn't this get a certain subset of people - like some science fiction fans - seriously motivated?</p>\r\n<p>Yeah, I'm just a little high on cough syrup. Is this incoherent?</p>\r\n<p>&nbsp;</p>\r\n<p>&nbsp;</p>\r\n<p>tl;dr: what if We The LW had a full transhumanist side to us, as a source of cohesion and inspiration.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rtjPeggZ6TSmQKKoD", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 5, "baseScore": -2, "extendedScore": null, "score": -2e-06, "legacy": true, "legacyId": "11827", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": -2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": true, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["SqF8cHjJv43mvJJzx"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-31T17:44:54.125Z", "modifiedAt": null, "url": null, "title": "The bias shield", "slug": "the-bias-shield", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:25.031Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "PhilGoetz", "createdAt": "2009-03-01T05:11:37.246Z", "isAdmin": false, "displayName": "PhilGoetz"}, "userId": "BvoQtwkppeooDTDmh", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/4sTmZC9NPjo9REzFY/the-bias-shield", "pageUrlRelative": "/posts/4sTmZC9NPjo9REzFY/the-bias-shield", "linkUrl": "https://www.lesswrong.com/posts/4sTmZC9NPjo9REzFY/the-bias-shield", "postedAtFormatted": "Saturday, December 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20The%20bias%20shield&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AThe%20bias%20shield%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4sTmZC9NPjo9REzFY%2Fthe-bias-shield%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=The%20bias%20shield%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4sTmZC9NPjo9REzFY%2Fthe-bias-shield", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F4sTmZC9NPjo9REzFY%2Fthe-bias-shield", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1743, "htmlBody": "<p>A friend asked me to get her Bill O'Reilly's new book <em>Killing Lincoln </em>for Christmas.&nbsp; I read its reviews on Amazon, and found several that said it wasn't as good as another book about the assassination, <em>Blood on the Moon</em>.&nbsp; This seemed like a believable conclusion to me.&nbsp; <em>Killing Lincoln</em> has no footnotes to document any of its claims, and is not in the Ford's Theatre national park service bookstore because the NPS <a href=\"http://www.washingtonpost.com/entertainment/books/bill-oreillys-lincoln-book-banned-from-fords-theatre-because-of-mistakes/2011/11/11/gIQAhJpyFN_story.html\">decided it was too historically inaccurate</a> to sell.&nbsp; Nearly 200 books have been written about the Lincoln assassination, including some by professional Lincoln scholars.&nbsp; So the odds seemed good that at least one of these was better than a book written by a TV talk show host.</p>\n<p>But I was wrong.&nbsp; To many people, this was not a believable conclusion.</p>\n<p>(This is not about the irrationality of Fox network fans.&nbsp; They are just a useful case study.)<a id=\"more\"></a></p>\n<p><a href=\"http://www.amazon.com/review/R29GG9ZHFHH40P/ref=cm_cr_dp_perm?ie=UTF8&amp;ASIN=0805093079&amp;nodeID=283155\">One review</a> ended like this:</p>\n<p style=\"padding-left: 30px;\">I hope people are not writing off an honest review because they think I'm picking on O'Reilly. The only POSSIBLE reason that this book took off so fast on the bestseller lists is because it was publicized on the O'Reilly Factor, not because it was so much better than any of the other books written about the Lincoln assassination. There has been much back-and-forth about this for some time. Dishonest people who didn't read the book but hate O'Reilly gave it one-star reviews without ever opening it. O'Reilly fans have an attack of the vapors at anything less than a five-star review. The purpose of this review was to inform, not to express ideology. I stand by this review. If you don't like it, that's fine, but don't attack me simply because you're sticking up for Bill O'Reilly (a futile wish, apparently). Again -- I watch The O'Reilly Factor. I am also a Lincoln scholar. Take this review at face value.</p>\n<p>And Amazon readers responded:</p>\n<p style=\"padding-left: 30px;\">Ted says:&nbsp; My guess is that your review is based on the same thing as every other liberal here... partisan hatred and the huffington post.</p>\n<p style=\"padding-left: 30px;\">Robes says:&nbsp; More time was spent by Mr. Ford going after O'Reilly than reviewing the merits of the content of the book itself. Smacks of a political agenda by Mr. Ford.&nbsp; Best to leave serious reviews to the pro's.</p>\n<p style=\"padding-left: 30px;\">(There was a further exchange where the reviewer gave his conservative credentials by saying he had worked for Sarah Palin, and readers responded by calling him a liar, but that has sadly been deleted by Amazon.)</p>\n<p>Another review said in part:</p>\n<p style=\"padding-left: 30px;\">It seems there are forces here who believe people who gave this book 1 star reviews are 1. anti-Bill O'Reilly, 2. never read the book, and 3. Partisan. One can follow this same illogical rationale and make the statement that the people who gave this book 5 star reviews are 1. pro-Bill O'Reilly, 2. never read the book, and 3. Partisan.<br /><br />There are too many mistakes in this book for it to be considered factual. Mary Surratt was never aboard the Montauk; she was never hooded while imprisoned, etc. Daniel Sickles killed his wife's lover, not the husband of his mistress. Booth and Herold spent 5 days in the Pine Thicket, not 6. etc. Since this work is non-fiction, these cannot be cavalierly brushed off as nitpicking or minor details.</p>\n<p>To which the readers responded:</p>\n<p style=\"padding-left: 30px;\">Zascha Marie says:&nbsp; Shoddy and lazy is writing a review on a book you did not read.</p>\n<p style=\"padding-left: 30px;\">Greg E. Garcia says:&nbsp; Swamp Poodle, please get a clue and try to not let your partisanship shine through. Just because you disagree with someone, does not mean that you have to make up false reviews and smear them. If the book is so bad, then why is it selling so well? I reply to your post not to get a no spin mug, but to try to bring reason into your partisan-addled brain.</p>\n<p style=\"padding-left: 30px;\">Jscichi says:&nbsp; Not a Verified Purchase so you didn't even read the book.. Typical Left Wing Loon!</p>\n<p style=\"padding-left: 30px;\">Ted says:&nbsp; Pitiful.... Nothing more than a vacuous, vapid, partisan rant, completely bereft of detail, analysis, example or reference.</p>\n<p>The facts in contention were questions such as, When was the Oval Office built? What is the proper spelling of Der(r)inger?<sup>1</sup>&nbsp; Was there a hole in the wall or in the door?&nbsp; The readers defending O'Reilly made it clear that they were conservatives and considered anyone who disagreed with O'Reilly about these facts to be a liberal; yet a conservative bias would not cause one to believe these facts.<sup>2</sup></p>\n<p>Any of these readers would have been willing to believe that Bill O'Reilly had written a bad book, <em>if they did not believe that Bill O'Reilly was strongly biased</em>.&nbsp; O'Reilly's strong bias about political matters made him <em>more </em>believable when writing about non-political matters.&nbsp; He is invulnerable to criticism because he is known to be biased.&nbsp; This is the Bias Shield:&nbsp; Beyond some level of bias, the more biased you act, and the more publicly you do it, the more your statements will be taken by the audience you still have as objective and unbiased.&nbsp; Not because they can't see your bias - because they <em>can</em> see it.&nbsp; Any objections can be dismissed as ad hominem attacks by people who don't agree with your bias.&nbsp; Claims by the criticizer to share the same bias will be dismissed as lies.</p>\n<h2>Optional mathy part<br /></h2>\n<p>This is an opportunity to start thinking about how to model bias mathematically, for this and other problems.</p>\n<ol>\n<li>Take out an n-dimensional piece of graph paper, and label its axes with political, religious, or other biases that are largely independent, so that you can describe a person's biases with a single point.&nbsp; Each axis will range from -1 to 1.&nbsp; Person i's biases are described by a point <em>p<sub>i</sub></em>.</li>\n<li>Define the <em>agreement </em>a(<em>p<sub>i</sub></em>,<em>p<sub>j</sub></em>) between points <em>p<sub>i</sub></em> and <em>p<sub>j</sub></em> as the square of the inverse of the length of the vector <em>p<sub>i</sub>p<sub>j</sub></em> (a(i,j) = 1 / |<em>p<sub>i</sub>p<sub>j</sub></em>|<sup>2</sup>).&nbsp; (Having a function that approaches infinity is inconvenient for some purposes, but I can't think of anything better.&nbsp; The squaring makes taking derivatives easier.)</li>\n<li>Plot the origin (0, 0) at the center of the paper.&nbsp; Define the vector <em>v<sub>i</sub></em> as the vector from 0 to <em>p<sub>i</sub></em>.</li>\n<li>Define a weight <em>w<sub>x</sub></em> for each axis <em>x</em> that is monotonic increasing in the average conditional prior P(opinion<sub>i</sub> | x<sub>i</sub>) people use to guess the opinion of person <em>i</em> based on their position on that axis, w<sub>x</sub> &le; 1.&nbsp; Define a function w(v) as <em>v</em> scaled along each dimension <em>x</em> by w<sub>x</sub>.&nbsp; (Biases perceived as less-important will be scaled down by w.)</li>\n<li>Define a <em>bias shield </em>function s(w), which operates over vectors, and returns a multiplicative factor that is larger the larger |w| is.&nbsp; We can start by assuming s(w) = |w|.</li>\n<li>Define the <em>believability </em>b(<em>p<sub>i</sub></em>,<em>p<sub>j</sub></em>), the probability a person at <em>p<sub>i</sub></em><em> </em>assigns to statements made by a person at <em>p<sub>j</sub></em>, as a(<em>w(p<sub>i</sub></em>),w(<em>p<sub>j</sub></em>))(1 + s(w(v<sub>j</sub>))(v<sub>i</sub>&middot;v<sub>j</sub>)).&nbsp; The dot-product scales the bias shield function by the degree to which <em>i</em> and <em>j</em> are biased in the same direction (negative if in opposite directions).&nbsp; The right-hand side is 1 + s(w(v<sub>j</sub>))(v<sub>i</sub>&middot;v<sub>j</sub>) because agreement can go to infinity, while max(s(w(v<sub>j</sub>))(v<sub>i</sub>&middot;v<sub>j</sub>)) = 1, and we don't want agreement to be infinitely larger than the bias shield.&nbsp; Note that b(<em>p<sub>i</sub></em>,<em>-p<sub>i</sub></em>) = (1 - s(w(v)))/4|p<sub>i</sub>|, which is zero if <em>|w(v<sub>i</sub></em>)| = 1.&nbsp; <em>b</em> can be negative, which is a good property if you can disagree with someone so much that, hearing them assert <em>P</em> will make you consider <em>P</em> less likely.</li>\n<li>Shade each point <em>p</em> on the paper with the <em>expected believability</em> at that point, which is the sum over all <em>i </em>in the population of b(<em>p<sub>i</sub></em>,p).</li>\n</ol>\n<p>Believability is concentrated at the outer fringes of every dimension <em>x </em>where&nbsp;&Sigma;<sub>i</sub> &delta;(b(<em>p<sub>i</sub></em>,x))/&delta;x, evaluated at <em>x = w<sub>x</sub></em>, is &gt; 0.&nbsp; That is, if even at the farthest fringe, moving further out on dimension <em>x</em> makes person <em>j</em> more believable on average.&nbsp; (This is a sufficient but perhaps not necessary condition.)&nbsp; This turns out to be the case... always, for every dimension.&nbsp; That surprised me.&nbsp; The figure you drew on your n-dimensional paper will look like a hollow ball.</p>\n<p>However, the density of that hollow ball at the fringes can vary.&nbsp; The degree to which being more biased makes someone more believable is proportional to the derivative given above.&nbsp; With the function definitions given above, the partial derivative along dimension <em>x<sub>d</sub></em> works out to be <em>x<sub>d</sub></em> / [(w<sub>d</sub> - x<sub>d</sub>)<sup>2</sup> + &Sigma;<sub>i&ne;d</sub>x<sub>i</sub><sup>2</sup>].&nbsp; If people's opinions are distributed randomly, and we assume no two people have exactly the same biases (to avoid infinities), the expected believability at &lt;0, ... max<sub>d</sub>, 0, 0, ...&gt; is proportional to the multiple integral over all the dimensions of that partial derivative.&nbsp; You can work it out if you want to, but the main observation is that the bias shield effect is nearly proportional to a non-linear increasing function of 1 /&Sigma;<sub>d</sub>w<sub>d</sub><sup>2</sup>.&nbsp; The effect is therefore huge if there is only one dimension, but becomes smaller when there are more dimensions, or when other dimensions have a larger w<sub>x</sub>.</p>\n<p>In this model, there is no hump in the believability function at (0,0).&nbsp; That says that people don't give speakers credit for being unbiased. Whether this is generally true is an important open question.</p>\n<h2>Conclusion (end of mathy part)<br /></h2>\n<p>The take-home messages are</p>\n<ol>\n<li>When someone publicly displays a strong bias, it can have the counter-intuitive effect of giving them more credibility, not because people don't acknowledge the bias, but because a known bias can be used to justify ad hominem attacks on critics.</li>\n<li>This effect (and all bias effects, although that isn't established here) is strongly diminished if people categorize speakers in two rather than just one dimension.&nbsp; It might be as simple a matter as reminding someone that Bill O'Reilly is a Harvard graduate, or Irish, just to get them thinking in more dimensions.</li>\n</ol>\n<h2>Footnotes</h2>\n<ol>\n<li>O'Reilly was right on this one.&nbsp; \"Derringer\" describes guns that are generic knock-offs of small pistols made by Henry Deringer.&nbsp; Booth shot Lincoln with a Deringer.</li>\n<li>It's stranger than that, because the O'Reilly defenders seldom responded by defending his statements.&nbsp; The facts did not seem important to them.&nbsp; Sometimes they said so: \"He is not writing to get his PhD in history or to impress a small group of <span class=\"matches\">academics</span> with his erudition\", \"I don't think O'Reilly was targeting \"serious historians\" when he wrote the book,\" \"Sounds like you are too educated to have wasted your time reading this book.\"</li>\n</ol>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"DdgSyQoZXjj3KnF4N": 1, "4R8JYu4QF2FqzJxE5": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "4sTmZC9NPjo9REzFY", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 26, "baseScore": 29, "extendedScore": null, "score": 6.7e-05, "legacy": true, "legacyId": "11805", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p>A friend asked me to get her Bill O'Reilly's new book <em>Killing Lincoln </em>for Christmas.&nbsp; I read its reviews on Amazon, and found several that said it wasn't as good as another book about the assassination, <em>Blood on the Moon</em>.&nbsp; This seemed like a believable conclusion to me.&nbsp; <em>Killing Lincoln</em> has no footnotes to document any of its claims, and is not in the Ford's Theatre national park service bookstore because the NPS <a href=\"http://www.washingtonpost.com/entertainment/books/bill-oreillys-lincoln-book-banned-from-fords-theatre-because-of-mistakes/2011/11/11/gIQAhJpyFN_story.html\">decided it was too historically inaccurate</a> to sell.&nbsp; Nearly 200 books have been written about the Lincoln assassination, including some by professional Lincoln scholars.&nbsp; So the odds seemed good that at least one of these was better than a book written by a TV talk show host.</p>\n<p>But I was wrong.&nbsp; To many people, this was not a believable conclusion.</p>\n<p>(This is not about the irrationality of Fox network fans.&nbsp; They are just a useful case study.)<a id=\"more\"></a></p>\n<p><a href=\"http://www.amazon.com/review/R29GG9ZHFHH40P/ref=cm_cr_dp_perm?ie=UTF8&amp;ASIN=0805093079&amp;nodeID=283155\">One review</a> ended like this:</p>\n<p style=\"padding-left: 30px;\">I hope people are not writing off an honest review because they think I'm picking on O'Reilly. The only POSSIBLE reason that this book took off so fast on the bestseller lists is because it was publicized on the O'Reilly Factor, not because it was so much better than any of the other books written about the Lincoln assassination. There has been much back-and-forth about this for some time. Dishonest people who didn't read the book but hate O'Reilly gave it one-star reviews without ever opening it. O'Reilly fans have an attack of the vapors at anything less than a five-star review. The purpose of this review was to inform, not to express ideology. I stand by this review. If you don't like it, that's fine, but don't attack me simply because you're sticking up for Bill O'Reilly (a futile wish, apparently). Again -- I watch The O'Reilly Factor. I am also a Lincoln scholar. Take this review at face value.</p>\n<p>And Amazon readers responded:</p>\n<p style=\"padding-left: 30px;\">Ted says:&nbsp; My guess is that your review is based on the same thing as every other liberal here... partisan hatred and the huffington post.</p>\n<p style=\"padding-left: 30px;\">Robes says:&nbsp; More time was spent by Mr. Ford going after O'Reilly than reviewing the merits of the content of the book itself. Smacks of a political agenda by Mr. Ford.&nbsp; Best to leave serious reviews to the pro's.</p>\n<p style=\"padding-left: 30px;\">(There was a further exchange where the reviewer gave his conservative credentials by saying he had worked for Sarah Palin, and readers responded by calling him a liar, but that has sadly been deleted by Amazon.)</p>\n<p>Another review said in part:</p>\n<p style=\"padding-left: 30px;\">It seems there are forces here who believe people who gave this book 1 star reviews are 1. anti-Bill O'Reilly, 2. never read the book, and 3. Partisan. One can follow this same illogical rationale and make the statement that the people who gave this book 5 star reviews are 1. pro-Bill O'Reilly, 2. never read the book, and 3. Partisan.<br><br>There are too many mistakes in this book for it to be considered factual. Mary Surratt was never aboard the Montauk; she was never hooded while imprisoned, etc. Daniel Sickles killed his wife's lover, not the husband of his mistress. Booth and Herold spent 5 days in the Pine Thicket, not 6. etc. Since this work is non-fiction, these cannot be cavalierly brushed off as nitpicking or minor details.</p>\n<p>To which the readers responded:</p>\n<p style=\"padding-left: 30px;\">Zascha Marie says:&nbsp; Shoddy and lazy is writing a review on a book you did not read.</p>\n<p style=\"padding-left: 30px;\">Greg E. Garcia says:&nbsp; Swamp Poodle, please get a clue and try to not let your partisanship shine through. Just because you disagree with someone, does not mean that you have to make up false reviews and smear them. If the book is so bad, then why is it selling so well? I reply to your post not to get a no spin mug, but to try to bring reason into your partisan-addled brain.</p>\n<p style=\"padding-left: 30px;\">Jscichi says:&nbsp; Not a Verified Purchase so you didn't even read the book.. Typical Left Wing Loon!</p>\n<p style=\"padding-left: 30px;\">Ted says:&nbsp; Pitiful.... Nothing more than a vacuous, vapid, partisan rant, completely bereft of detail, analysis, example or reference.</p>\n<p>The facts in contention were questions such as, When was the Oval Office built? What is the proper spelling of Der(r)inger?<sup>1</sup>&nbsp; Was there a hole in the wall or in the door?&nbsp; The readers defending O'Reilly made it clear that they were conservatives and considered anyone who disagreed with O'Reilly about these facts to be a liberal; yet a conservative bias would not cause one to believe these facts.<sup>2</sup></p>\n<p>Any of these readers would have been willing to believe that Bill O'Reilly had written a bad book, <em>if they did not believe that Bill O'Reilly was strongly biased</em>.&nbsp; O'Reilly's strong bias about political matters made him <em>more </em>believable when writing about non-political matters.&nbsp; He is invulnerable to criticism because he is known to be biased.&nbsp; This is the Bias Shield:&nbsp; Beyond some level of bias, the more biased you act, and the more publicly you do it, the more your statements will be taken by the audience you still have as objective and unbiased.&nbsp; Not because they can't see your bias - because they <em>can</em> see it.&nbsp; Any objections can be dismissed as ad hominem attacks by people who don't agree with your bias.&nbsp; Claims by the criticizer to share the same bias will be dismissed as lies.</p>\n<h2 id=\"Optional_mathy_part\">Optional mathy part<br></h2>\n<p>This is an opportunity to start thinking about how to model bias mathematically, for this and other problems.</p>\n<ol>\n<li>Take out an n-dimensional piece of graph paper, and label its axes with political, religious, or other biases that are largely independent, so that you can describe a person's biases with a single point.&nbsp; Each axis will range from -1 to 1.&nbsp; Person i's biases are described by a point <em>p<sub>i</sub></em>.</li>\n<li>Define the <em>agreement </em>a(<em>p<sub>i</sub></em>,<em>p<sub>j</sub></em>) between points <em>p<sub>i</sub></em> and <em>p<sub>j</sub></em> as the square of the inverse of the length of the vector <em>p<sub>i</sub>p<sub>j</sub></em> (a(i,j) = 1 / |<em>p<sub>i</sub>p<sub>j</sub></em>|<sup>2</sup>).&nbsp; (Having a function that approaches infinity is inconvenient for some purposes, but I can't think of anything better.&nbsp; The squaring makes taking derivatives easier.)</li>\n<li>Plot the origin (0, 0) at the center of the paper.&nbsp; Define the vector <em>v<sub>i</sub></em> as the vector from 0 to <em>p<sub>i</sub></em>.</li>\n<li>Define a weight <em>w<sub>x</sub></em> for each axis <em>x</em> that is monotonic increasing in the average conditional prior P(opinion<sub>i</sub> | x<sub>i</sub>) people use to guess the opinion of person <em>i</em> based on their position on that axis, w<sub>x</sub> \u2264 1.&nbsp; Define a function w(v) as <em>v</em> scaled along each dimension <em>x</em> by w<sub>x</sub>.&nbsp; (Biases perceived as less-important will be scaled down by w.)</li>\n<li>Define a <em>bias shield </em>function s(w), which operates over vectors, and returns a multiplicative factor that is larger the larger |w| is.&nbsp; We can start by assuming s(w) = |w|.</li>\n<li>Define the <em>believability </em>b(<em>p<sub>i</sub></em>,<em>p<sub>j</sub></em>), the probability a person at <em>p<sub>i</sub></em><em> </em>assigns to statements made by a person at <em>p<sub>j</sub></em>, as a(<em>w(p<sub>i</sub></em>),w(<em>p<sub>j</sub></em>))(1 + s(w(v<sub>j</sub>))(v<sub>i</sub>\u00b7v<sub>j</sub>)).&nbsp; The dot-product scales the bias shield function by the degree to which <em>i</em> and <em>j</em> are biased in the same direction (negative if in opposite directions).&nbsp; The right-hand side is 1 + s(w(v<sub>j</sub>))(v<sub>i</sub>\u00b7v<sub>j</sub>) because agreement can go to infinity, while max(s(w(v<sub>j</sub>))(v<sub>i</sub>\u00b7v<sub>j</sub>)) = 1, and we don't want agreement to be infinitely larger than the bias shield.&nbsp; Note that b(<em>p<sub>i</sub></em>,<em>-p<sub>i</sub></em>) = (1 - s(w(v)))/4|p<sub>i</sub>|, which is zero if <em>|w(v<sub>i</sub></em>)| = 1.&nbsp; <em>b</em> can be negative, which is a good property if you can disagree with someone so much that, hearing them assert <em>P</em> will make you consider <em>P</em> less likely.</li>\n<li>Shade each point <em>p</em> on the paper with the <em>expected believability</em> at that point, which is the sum over all <em>i </em>in the population of b(<em>p<sub>i</sub></em>,p).</li>\n</ol>\n<p>Believability is concentrated at the outer fringes of every dimension <em>x </em>where&nbsp;\u03a3<sub>i</sub> \u03b4(b(<em>p<sub>i</sub></em>,x))/\u03b4x, evaluated at <em>x = w<sub>x</sub></em>, is &gt; 0.&nbsp; That is, if even at the farthest fringe, moving further out on dimension <em>x</em> makes person <em>j</em> more believable on average.&nbsp; (This is a sufficient but perhaps not necessary condition.)&nbsp; This turns out to be the case... always, for every dimension.&nbsp; That surprised me.&nbsp; The figure you drew on your n-dimensional paper will look like a hollow ball.</p>\n<p>However, the density of that hollow ball at the fringes can vary.&nbsp; The degree to which being more biased makes someone more believable is proportional to the derivative given above.&nbsp; With the function definitions given above, the partial derivative along dimension <em>x<sub>d</sub></em> works out to be <em>x<sub>d</sub></em> / [(w<sub>d</sub> - x<sub>d</sub>)<sup>2</sup> + \u03a3<sub>i\u2260d</sub>x<sub>i</sub><sup>2</sup>].&nbsp; If people's opinions are distributed randomly, and we assume no two people have exactly the same biases (to avoid infinities), the expected believability at &lt;0, ... max<sub>d</sub>, 0, 0, ...&gt; is proportional to the multiple integral over all the dimensions of that partial derivative.&nbsp; You can work it out if you want to, but the main observation is that the bias shield effect is nearly proportional to a non-linear increasing function of 1 /\u03a3<sub>d</sub>w<sub>d</sub><sup>2</sup>.&nbsp; The effect is therefore huge if there is only one dimension, but becomes smaller when there are more dimensions, or when other dimensions have a larger w<sub>x</sub>.</p>\n<p>In this model, there is no hump in the believability function at (0,0).&nbsp; That says that people don't give speakers credit for being unbiased. Whether this is generally true is an important open question.</p>\n<h2 id=\"Conclusion__end_of_mathy_part_\">Conclusion (end of mathy part)<br></h2>\n<p>The take-home messages are</p>\n<ol>\n<li>When someone publicly displays a strong bias, it can have the counter-intuitive effect of giving them more credibility, not because people don't acknowledge the bias, but because a known bias can be used to justify ad hominem attacks on critics.</li>\n<li>This effect (and all bias effects, although that isn't established here) is strongly diminished if people categorize speakers in two rather than just one dimension.&nbsp; It might be as simple a matter as reminding someone that Bill O'Reilly is a Harvard graduate, or Irish, just to get them thinking in more dimensions.</li>\n</ol>\n<h2 id=\"Footnotes\">Footnotes</h2>\n<ol>\n<li>O'Reilly was right on this one.&nbsp; \"Derringer\" describes guns that are generic knock-offs of small pistols made by Henry Deringer.&nbsp; Booth shot Lincoln with a Deringer.</li>\n<li>It's stranger than that, because the O'Reilly defenders seldom responded by defending his statements.&nbsp; The facts did not seem important to them.&nbsp; Sometimes they said so: \"He is not writing to get his PhD in history or to impress a small group of <span class=\"matches\">academics</span> with his erudition\", \"I don't think O'Reilly was targeting \"serious historians\" when he wrote the book,\" \"Sounds like you are too educated to have wasted your time reading this book.\"</li>\n</ol>", "sections": [{"title": "Optional mathy part", "anchor": "Optional_mathy_part", "level": 1}, {"title": "Conclusion (end of mathy part)", "anchor": "Conclusion__end_of_mathy_part_", "level": 1}, {"title": "Footnotes", "anchor": "Footnotes", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "65 comments"}], "headingsCount": 5}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 65, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-31T18:09:46.578Z", "modifiedAt": null, "url": null, "title": "Best wishes for 2012", "slug": "best-wishes-for-2012", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Thomas", "createdAt": "2009-03-02T17:47:09.607Z", "isAdmin": false, "displayName": "Thomas"}, "userId": "GrAKeuxT4e9AKyHdE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/gdbk9AhfD5RsosgRS/best-wishes-for-2012", "pageUrlRelative": "/posts/gdbk9AhfD5RsosgRS/best-wishes-for-2012", "linkUrl": "https://www.lesswrong.com/posts/gdbk9AhfD5RsosgRS/best-wishes-for-2012", "postedAtFormatted": "Saturday, December 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Best%20wishes%20for%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ABest%20wishes%20for%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgdbk9AhfD5RsosgRS%2Fbest-wishes-for-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Best%20wishes%20for%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgdbk9AhfD5RsosgRS%2Fbest-wishes-for-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fgdbk9AhfD5RsosgRS%2Fbest-wishes-for-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 20, "htmlBody": "<p>To all the writers and readers of this site and all the people behind the site, who make it operational.</p>\n<p>&nbsp;</p>\n<p>Cheers!</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "gdbk9AhfD5RsosgRS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 14, "extendedScore": null, "score": 8.240767927450478e-07, "legacy": true, "legacyId": "11829", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-31T18:58:20.004Z", "modifiedAt": null, "url": null, "title": "Intuition and Mathematics", "slug": "intuition-and-mathematics", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:20.829Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "XiXiDu", "createdAt": "2009-03-07T18:49:18.890Z", "isAdmin": false, "displayName": "XiXiDu"}, "userId": "DH3Hiv6kJp93dDF4J", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5twHzuzCxgFWHZRsg/intuition-and-mathematics", "pageUrlRelative": "/posts/5twHzuzCxgFWHZRsg/intuition-and-mathematics", "linkUrl": "https://www.lesswrong.com/posts/5twHzuzCxgFWHZRsg/intuition-and-mathematics", "postedAtFormatted": "Saturday, December 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Intuition%20and%20Mathematics&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AIntuition%20and%20Mathematics%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5twHzuzCxgFWHZRsg%2Fintuition-and-mathematics%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Intuition%20and%20Mathematics%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5twHzuzCxgFWHZRsg%2Fintuition-and-mathematics", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5twHzuzCxgFWHZRsg%2Fintuition-and-mathematics", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 1016, "htmlBody": "<p>While reading <a href=\"/r/discussion/lw/94b/link_what_is_it_like_to_have_an_understanding_of/\">the answer</a> to the question <em>'What is it like to have an understanding of very advanced mathematics?'</em> I became curious about the value of intuition in mathematics and why it might be useful.</p>\n<p>It usually seems to be a bad idea to try to solve problems intuitively or use our intuition as evidence to judge issues that our evolutionary ancestors never encountered and therefore were never optimized to judge by natural selection.</p>\n<p>And so it seems to be especially strange to suggest that intuition might be a good tool to make mathematical conjectures. Yet people like fields medalist <a href=\"http://en.wikipedia.org/wiki/Terence_Tao\">Terence Tao</a> <a href=\"http://terrytao.wordpress.com/career-advice/there%E2%80%99s-more-to-mathematics-than-rigour-and-proofs/\">seem to believe</a> that intuition should not be disregarded when doing mathematics,</p>\n<blockquote>\n<p>...&ldquo;fuzzier&rdquo; or &ldquo;intuitive&rdquo; thinking (such as heuristic reasoning, judicious extrapolation from examples, or analogies with other contexts such as physics) gets deprecated as &ldquo;non-rigorous&rdquo;. All too often, one ends up discarding one&rsquo;s initial intuition and is only able to process mathematics at a formal level, thus getting stalled at the second stage of one&rsquo;s mathematical education.</p>\n<p>The point of rigour is not to destroy all intuition; instead, it should be used to destroy bad intuition while clarifying and elevating good intuition. It is only with a combination of both rigorous formalism and good intuition that one can tackle complex mathematical problems;</p>\n</blockquote>\n<p>The author mentioned at the beginning also makes the case that intuition is an important tool,</p>\n<blockquote>\n<p>You are often confident that something is true long before you have an airtight proof for it (this happens especially often in geometry). The main reason is that you have a large catalogue of connections between concepts, and you can quickly intuit that if X were to be false, that would create tensions with other things you know to be true, so you are inclined to believe X is probably true to maintain the harmony of the conceptual space. It's not so much that you can imagine the situation perfectly, but you can quickly imagine many other things that are logically connected to it.</p>\n</blockquote>\n<p>But what do those people mean when they talk about 'intuition', what exactly is its advantage? The author hints at an answer,</p>\n<blockquote>\n<p>You go up in abstraction, \"higher and higher\". The main object of study yesterday becomes just an example or a tiny part of what you are considering today. For example, in calculus classes you think about functions or curves. In functional analysis or algebraic geometry, you think of spaces whose points are functions or curves -- that is, you \"zoom out\" so that every function is just a point in a space, surrounded by many other \"nearby\" functions. Using this kind of zooming out technique, you can say very complex things in short sentences -- things that, if unpacked and said at the zoomed-in level, would take up pages. Abstracting and compressing in this way allows you to consider extremely complicated issues while using your limited memory and processing power.</p>\n</blockquote>\n<p>At this point I was reminded of something Scott Aaronson wrote in his essay '<a href=\"http://eccc.hpi-web.de/report/2011/108/\">Why Philosophers Should Care About Computational Complexity</a>',</p>\n<blockquote>\n<p>...even if computers were better than humans at factoring large numbers or at solving randomly-generated Sudoku puzzles, humans might still be better at search problems with &ldquo;higher-level structure&rdquo; or &ldquo;semantics,&rdquo; such as proving Fermat&rsquo;s Last Theorem or (ironically) designing faster computer algorithms. Indeed, even in limited domains such as puzzle-solving, while computers can examine solutions millions of times faster, humans (for now) are vastly better at noticing <em>global patterns</em> or <em>symmetries</em> in the puzzle that make a solution either trivial or impossible. As an amusing example, consider the <em>Pigeonhole Principle</em>, which says that n+1 pigeons can&rsquo;t be placed into n holes, with at most one pigeon per hole. It&rsquo;s not hard to construct a propositional Boolean formula <span class=\"texhtml\" dir=\"ltr\">&Phi;</span> that encodes the Pigeonhole Principle for some fixed value of n (say, 1000). However, if you then feed <span class=\"texhtml\" dir=\"ltr\">&Phi;</span> to current Boolean satisfiability algorithms, they&rsquo;ll assiduously set to work trying out possibilities: &ldquo;let&rsquo;s see, if I put this pigeon here, and that one there ... darn, it still doesn&rsquo;t work!&rdquo; And they&rsquo;ll continue trying out possibilities for an exponential number of steps, oblivious to the &ldquo;global&rdquo; reason why the goal can never be achieved. Indeed, beginning in the 1980s, the field of <em>proof complexity</em>&mdash;a close cousin of computational complexity&mdash;has been able to show that large classes of algorithms <em>require</em> exponential time to prove the Pigeonhole Principle and similar propositional tautologies.</p>\n</blockquote>\n<p>Again back to the answer on <em>'what it is like to have an understanding of very advanced mathematics'</em>.<em> </em>The author writes,</p>\n<blockquote>\n<p><em>...</em>you are good at modularizing a conceptual space and taking certain calculations or arguments you don't understand as \"black boxes\" and considering their implications anyway. You can sometimes make statements you know are true and have good intuition for, without understanding all the details. You can often detect where the delicate or interesting part of something is based on only a very high-level explanation.</p>\n</blockquote>\n<p>Humans are good at 'zooming out' to detect global patterns. Humans can jump conceptual gaps by treating them as \"black boxes\".&nbsp;</p>\n<p>Intuition is a conceptual bird's-eye view that allows humans to draw inferences from high-level abstractions without having to systematically trace out each step. Intuition is a wormhole. Intuition allows us get from here to there given limited computational resources.</p>\n<p>If true, it also explains many of our shortcomings and biases. Intuitions greatest feature is also our biggest flaw.</p>\n<blockquote>\n<p>The introduction of suitable abstractions is our only mental aid to organize and master complexity. &mdash; Edsger W. Dijkstra</p>\n</blockquote>\n<p>Our computational limitations make it necessary to take shortcuts and view the world as a simplified model. That heuristic is naturally prone to error and introduces biases. We draw connections without establishing them systematically. We recognize patterns in random noise.</p>\n<p>Many of our biases can be seen as a side-effect of making judgments under computational restrictions. A trade off between optimization power and resource use.</p>\n<p>It it possible to correct for the shortcomings of intuition other than by refining rationality and becoming aware of our biases? That's up to how optimization power scales with resources and if there are more efficient algorithms that work under limited resources.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"z95PGFXtPpwakqkTA": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5twHzuzCxgFWHZRsg", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 8, "baseScore": 8, "extendedScore": null, "score": 8.240949059714165e-07, "legacy": true, "legacyId": "11830", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 17, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["w9eELo9krcjwjNGGK"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-31T19:50:39.518Z", "modifiedAt": null, "url": null, "title": "[link] A attempt to reduce Epistemic Viciousness in the martial arts/ an empirical analysis of WSD training.", "slug": "link-a-attempt-to-reduce-epistemic-viciousness-in-the", "viewCount": null, "lastCommentedAt": null, "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "beoShaffer", "createdAt": "2011-05-29T15:52:29.240Z", "isAdmin": false, "displayName": "beoShaffer"}, "userId": "589WwYp3jytZqATFL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/QPwqdLQcSMuXTD272/link-a-attempt-to-reduce-epistemic-viciousness-in-the", "pageUrlRelative": "/posts/QPwqdLQcSMuXTD272/link-a-attempt-to-reduce-epistemic-viciousness-in-the", "linkUrl": "https://www.lesswrong.com/posts/QPwqdLQcSMuXTD272/link-a-attempt-to-reduce-epistemic-viciousness-in-the", "postedAtFormatted": "Saturday, December 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5Blink%5D%20A%20attempt%20to%20reduce%20Epistemic%20Viciousness%20in%20the%20martial%20arts%2F%20an%20empirical%20analysis%20of%20WSD%20training.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5Blink%5D%20A%20attempt%20to%20reduce%20Epistemic%20Viciousness%20in%20the%20martial%20arts%2F%20an%20empirical%20analysis%20of%20WSD%20training.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQPwqdLQcSMuXTD272%2Flink-a-attempt-to-reduce-epistemic-viciousness-in-the%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5Blink%5D%20A%20attempt%20to%20reduce%20Epistemic%20Viciousness%20in%20the%20martial%20arts%2F%20an%20empirical%20analysis%20of%20WSD%20training.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQPwqdLQcSMuXTD272%2Flink-a-attempt-to-reduce-epistemic-viciousness-in-the", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FQPwqdLQcSMuXTD272%2Flink-a-attempt-to-reduce-epistemic-viciousness-in-the", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 100, "htmlBody": "<p>http://kojutsukan.blogspot.com/2011/12/womens-self-defence-courses-effective.html</p>\n<p>http://kojutsukan.blogspot.com/2011/12/womens-self-defence-effective-or-not-pt.html</p>\n<p>&nbsp;</p>\n<p>The linked articles' theoretical topic, the effectiveness of women's self defense(WDS) courses, is not of particular interest to LW. However, they are also a pushback against <a href=\"/lw/2i/epistemic_viciousness/\">epistemic viciousness</a>&nbsp;in the martial arts. &nbsp;The author analyzes WDS courses in the light of actual studies on sexual violence and the effectiveness of various methods of resistance. &nbsp;They also make several direct references to the problems with martial arts epistemology and some of the causes. &nbsp;Thus, I recommend them anyone interested in the martial arts and rationality. &nbsp;For that matter I recommend the entire blog, which is largely about the science of martial arts.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "QPwqdLQcSMuXTD272", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 11, "extendedScore": null, "score": 8.241144256118146e-07, "legacy": true, "legacyId": "11831", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": null, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["T8ddXNtmNSHexhQh8"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-31T21:01:55.152Z", "modifiedAt": null, "url": null, "title": "[Link] Social dance and cognitive maintenance ", "slug": "link-social-dance-and-cognitive-maintenance", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:56.993Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "NancyLebovitz", "createdAt": "2009-03-24T11:25:00.619Z", "isAdmin": false, "displayName": "NancyLebovitz"}, "userId": "oxTHYnSBbLZP9F25d", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/pDbuhcrsvz7vvuSrb/link-social-dance-and-cognitive-maintenance", "pageUrlRelative": "/posts/pDbuhcrsvz7vvuSrb/link-social-dance-and-cognitive-maintenance", "linkUrl": "https://www.lesswrong.com/posts/pDbuhcrsvz7vvuSrb/link-social-dance-and-cognitive-maintenance", "postedAtFormatted": "Saturday, December 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BLink%5D%20Social%20dance%20and%20cognitive%20maintenance%20&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BLink%5D%20Social%20dance%20and%20cognitive%20maintenance%20%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpDbuhcrsvz7vvuSrb%2Flink-social-dance-and-cognitive-maintenance%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BLink%5D%20Social%20dance%20and%20cognitive%20maintenance%20%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpDbuhcrsvz7vvuSrb%2Flink-social-dance-and-cognitive-maintenance", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FpDbuhcrsvz7vvuSrb%2Flink-social-dance-and-cognitive-maintenance", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 134, "htmlBody": "<p><a href=\"http://www.stumbleupon.com/su/1d49Jo/socialdance.stanford.edu/syllabi/smarter.htm\">Frequent dancing causes a 76% reduction in the risk of dementia</a>. The benefit seems to be from freestyle social dancing, though I don't think that part is as well-verified. The benefit seems to be from making large numbers of quick decisions.</p>\n<p>I wonder if playing jazz has similar good effects.</p>\n<p>For something a little geekier, <a href=\"http://shivanata.com/\">Shiva Nata</a>, a system of keeping the mind fresh by doing more and more complex movements. The author of the blog is a silly person, but the system seems to be for real.</p>\n<p>Edited to correct html which was entered in the text field instead of the html field. It wouldn't surprise me if having to deal with electronics and computers tends to prevent dementia, considering the number of fiddly and changing details one has to deal with. Or is fun required?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "pDbuhcrsvz7vvuSrb", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 2, "extendedScore": null, "score": 8.241410102818046e-07, "legacy": true, "legacyId": "11832", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 18, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2011-12-31T21:21:22.949Z", "modifiedAt": null, "url": null, "title": "Evaporative cooling of group beliefs: current example", "slug": "evaporative-cooling-of-group-beliefs-current-example", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:22.326Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "calcsam", "createdAt": "2011-04-30T17:07:18.622Z", "isAdmin": false, "displayName": "calcsam"}, "userId": "YpbtzJj8Qwi4PHGm9", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/Wyzy36ExMYHofu6HW/evaporative-cooling-of-group-beliefs-current-example", "pageUrlRelative": "/posts/Wyzy36ExMYHofu6HW/evaporative-cooling-of-group-beliefs-current-example", "linkUrl": "https://www.lesswrong.com/posts/Wyzy36ExMYHofu6HW/evaporative-cooling-of-group-beliefs-current-example", "postedAtFormatted": "Saturday, December 31st 2011", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Evaporative%20cooling%20of%20group%20beliefs%3A%20current%20example&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AEvaporative%20cooling%20of%20group%20beliefs%3A%20current%20example%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWyzy36ExMYHofu6HW%2Fevaporative-cooling-of-group-beliefs-current-example%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Evaporative%20cooling%20of%20group%20beliefs%3A%20current%20example%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWyzy36ExMYHofu6HW%2Fevaporative-cooling-of-group-beliefs-current-example", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FWyzy36ExMYHofu6HW%2Fevaporative-cooling-of-group-beliefs-current-example", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 478, "htmlBody": "<p>Background info: a splinter group, which broke off from the LDS (\"Mormon\") church ~100 years ago, refusing to give up polygamy, has been in the headlines over the last year; their leader was sentenced to life in prison for rape of teenage girls he took as plural wives.</p>\n<p>Deseret News,&nbsp;<a href=\"http://www.deseretnews.com/article/705396590/Sex-banned-until-Warren-Jeffs-prison-walls-crumble-FLDS-relatives-say.html?pg=1\">Sex banned until Warren Jeffs' prison walls crumble, FLDS relatives say</a></p>\n<blockquote>\n<p>\"As the year comes to an end and the followers of Warren Jeffs await the apocalypse he has predicted, they're living under a challenging edict: they're forbidden to have sex until Jeffs is sprung from a Texas prison.</p>\n<p>It's one of the strangest edicts in a season full of them. Jeffs has issued a stream of revelations, prophecies and orders to his congregation in the border community of Hildale, Utah, and Colorado City, Ariz.</p>\n<p><strong>The recent edicts from Jeffs' prison cell seem to be having two contradictory effects. Many are leaving the FLDS faith in disgust. Those who stay are reported to be increasingly devoted</strong> to a man who is serving a lifetime sentence for raping underage girls.</p>\n<p>According to numerous critics and outside observers, the imprisoned FLDS leader has sometimes acted through his brother Lyle and other times has spoken directly to his congregation over the phone from prison. He recently banned many of the things his followers enjoy: bicycles, ATVs, trampolines, even children's toys. But the sex edict reaches into the bedrooms of all his devoted followers.</p>\n<p>According to Holm, Jeffs declared all existing marriages to be void.....\"<span style=\"font-family: arial, sans-serif; font-size: 13px; line-height: 17px;\">they have all been told that they are not to live as husband and wife\"....</span><span style=\"font-family: arial, sans-serif; font-size: 14px; line-height: 17px;\">Holm thinks about 100 members have left in recent weeks from the community of 10,000.</span></p>\n</blockquote>\n<p>Eliezer, <a href=\"/lw/lr/evaporative_cooling_of_group_beliefs/\">Evaporative Cooling of Group Beliefs</a></p>\n<blockquote>\n<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; font-size: small; \">Why would a group belief become&nbsp;<em style=\"font-style: italic; \">stronger</em>&nbsp;after encountering crushing counterevidence?</span></p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; \">In Festinger's classic \"When Prophecy Fails\", one of the cult members walked out the door immediately after the flying saucer failed to land.&nbsp; Who gets fed up and leaves&nbsp;<em style=\"font-style: italic; \">first?</em>&nbsp; An&nbsp;<em style=\"font-style: italic; \">average</em>&nbsp;cult member?&nbsp; Or a relatively more skeptical member, who previously might have been acting as a voice of moderation, a brake on the more fanatic members?</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; \">After the members with the highest kinetic energy escape, the remaining discussions will be between the extreme fanatics on one end and the slightly less extreme fanatics on the other end, with the group consensus somewhere in the \"middle\".</p>\n</blockquote>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; \">This doesn't simply seem to be a case of a new weighted average after some skeptics are gone (only 1% of FLDS have left). There are other dynamics going on among those remaining.</p>\n<p style=\"margin-top: 0px; margin-right: 0px; margin-bottom: 1em; margin-left: 0px; \">The image that comes to my mind is a lot of points scattered along a skepticism/fanaticism axis, and a repelling magnet placed on that line. This magnet pushes the already-skeptical values into greater skepticism (and out) and pushing the more fanatical members into greater fanaticism. How well does that actually represent what's going on? Not sure.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "Wyzy36ExMYHofu6HW", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 22, "baseScore": 28, "extendedScore": null, "score": 8.24148271580318e-07, "legacy": true, "legacyId": "11833", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 19, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 15, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["ZQG9cwKbct2LtmL3p"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-01T00:09:33.815Z", "modifiedAt": null, "url": null, "title": "Role-reversal education for math and science students (pilot project)", "slug": "role-reversal-education-for-math-and-science-students-pilot", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:08.566Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "EvelynM", "createdAt": "2010-01-03T23:18:02.364Z", "isAdmin": false, "displayName": "EvelynM"}, "userId": "gigfo2RbZBC2Nvg3T", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/PqXrzScit7DCedD3P/role-reversal-education-for-math-and-science-students-pilot", "pageUrlRelative": "/posts/PqXrzScit7DCedD3P/role-reversal-education-for-math-and-science-students-pilot", "linkUrl": "https://www.lesswrong.com/posts/PqXrzScit7DCedD3P/role-reversal-education-for-math-and-science-students-pilot", "postedAtFormatted": "Sunday, January 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Role-reversal%20education%20for%20math%20and%20science%20students%20(pilot%20project)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARole-reversal%20education%20for%20math%20and%20science%20students%20(pilot%20project)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPqXrzScit7DCedD3P%2Frole-reversal-education-for-math-and-science-students-pilot%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Role-reversal%20education%20for%20math%20and%20science%20students%20(pilot%20project)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPqXrzScit7DCedD3P%2Frole-reversal-education-for-math-and-science-students-pilot", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FPqXrzScit7DCedD3P%2Frole-reversal-education-for-math-and-science-students-pilot", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 285, "htmlBody": "<p>tldr; Role-reversal education: by explaining to others, we learn, and improve in confidence.</p>\n<p>&nbsp;</p>\n<p>http://solutionfocusedchange.blogspot.com/2011/12/betacoach-promising-project-to-improve.html</p>\n<p>\"<strong>The project B&egrave;tacoach</strong> seems to me to be a well thought-out and  promising way to give improve math and science education in secondary  education. Here is a pointwise explanation of the project:</p>\n<ul>\n<li><span style=\"text-decoration: underline;\">What is B&egrave;tacoach?</span> In September 2010 a pilot started in which  third-graders with low self-confidence in math and science were asked  to become coaches of four to five first-graders during math class. Once a  week the b&egrave;tacoaches joined the lesson which was prepared by the  teacher, to help their group of first-graders.&nbsp;</li>\n<li><span style=\"text-decoration: underline;\">Role-reversal education</span>: an important principle which is used  in the project is that of role-reversal education. Research has shown  that by explaning things, people construct knowledge again which helps  anchor this knowlegde better and which makes it easier to connect it to  information.&nbsp;</li>\n<li><span style=\"text-decoration: underline;\">Choosing b&egrave;tacoaches</span>: the following steps help to choose the  b&egrave;tacoaches: 1) choose students for whom there is room for improvement  with respect to their grades and/or their self-confidence, 2) discuss  their suitability for the b&egrave;tacoach role with your colleagues, 3) aks  the students for the role and make clear that the role is an important  one, 4) express that you expect that the student will be able to fulfill  the role well, 5) be demanding: make it clear that the role requires  commitment and effort.&nbsp;</li>\n<li><span style=\"text-decoration: underline;\">Preliminary findings</span>: both the first-graders and the  third-graders turn out to be enthusiast about the project. First-graders  said the could concentrate better, felt more comfortable to ask  questions and understood the material better. The b&egrave;tacoaches  themselves, the third-graders, said they learned from the experience and  that they had become more active. \"</li>\n</ul>\n<p>http://moniquepijls.com/2011/08/16/betacoach-leren-in-een-nieuwe-rol/ (Dutch)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"fH8jPjHF2R27sRTTG": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "PqXrzScit7DCedD3P", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 10, "baseScore": 10, "extendedScore": null, "score": 8.242110207413653e-07, "legacy": true, "legacyId": "11836", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 2, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-01T02:02:38.425Z", "modifiedAt": null, "url": null, "title": "Meta analysis of Writing Therapy", "slug": "meta-analysis-of-writing-therapy", "viewCount": null, "lastCommentedAt": "2019-09-08T11:22:00.280Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "jsalvatier", "createdAt": "2009-03-02T09:27:42.415Z", "isAdmin": false, "displayName": "jsalvatier"}, "userId": "r5LffMcjHLHZXtvKt", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/o7nRiBP9W8xR5E4v5/meta-analysis-of-writing-therapy", "pageUrlRelative": "/posts/o7nRiBP9W8xR5E4v5/meta-analysis-of-writing-therapy", "linkUrl": "https://www.lesswrong.com/posts/o7nRiBP9W8xR5E4v5/meta-analysis-of-writing-therapy", "postedAtFormatted": "Sunday, January 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Meta%20analysis%20of%20Writing%20Therapy&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AMeta%20analysis%20of%20Writing%20Therapy%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo7nRiBP9W8xR5E4v5%2Fmeta-analysis-of-writing-therapy%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Meta%20analysis%20of%20Writing%20Therapy%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo7nRiBP9W8xR5E4v5%2Fmeta-analysis-of-writing-therapy", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Fo7nRiBP9W8xR5E4v5%2Fmeta-analysis-of-writing-therapy", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 132, "htmlBody": "<p>Robin Hanson&nbsp;<a href=\"http://www.overcomingbias.com/2011/12/easy-job-fix.html\">recently mentioned \"writing therapy\"</a> as potentially having surprisingly large benefits. In the example he gives, recently unemployed engineers who write about their experience find jobs more quickly than those that did not.</p>\n<p>The meta-analysis paper he links to was pretty lame, but I found another meta-analysis, <a href=\"http://dl.dropbox.com/u/29304719/Papers/Experimental%20Disclosure%20and%20its%20moderators%3A%20a%20meta%20analysis.pdf\">\"Experimental disclosure and its moderators: A meta-analysis\"</a>, on a somewhat broader topic of Experimental Disclosure that appears to be much better.</p>\n<p>My judgment is non-expert, but it looks to me like a very high quality meta-analysis. The authors use a large number of studies (146) and include a large number of potential moderators,&nbsp;discuss their methodology in detail, and&nbsp;address publication bias intelligently.&nbsp;</p>\n<p>The authors find small to moderate positive effects on measures of psychological health,&nbsp;physiological health and general life outcomes. They also find a number of interesting moderating factors.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "o7nRiBP9W8xR5E4v5", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "bigUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 15, "baseScore": 22, "extendedScore": null, "score": 5e-05, "legacy": true, "legacyId": "11837", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 18, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 12, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-01T04:20:37.303Z", "modifiedAt": null, "url": null, "title": "[SEQ RERUN] Trust in Bayes", "slug": "seq-rerun-trust-in-bayes", "viewCount": null, "lastCommentedAt": "2012-01-01T21:46:44.228Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "MinibearRex", "createdAt": "2011-02-03T20:01:25.670Z", "isAdmin": false, "displayName": "MinibearRex"}, "userId": "So4JxeRr9GArTAK5D", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/cuasP7oujr2etWpNJ/seq-rerun-trust-in-bayes", "pageUrlRelative": "/posts/cuasP7oujr2etWpNJ/seq-rerun-trust-in-bayes", "linkUrl": "https://www.lesswrong.com/posts/cuasP7oujr2etWpNJ/seq-rerun-trust-in-bayes", "postedAtFormatted": "Sunday, January 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20%5BSEQ%20RERUN%5D%20Trust%20in%20Bayes&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0A%5BSEQ%20RERUN%5D%20Trust%20in%20Bayes%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcuasP7oujr2etWpNJ%2Fseq-rerun-trust-in-bayes%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=%5BSEQ%20RERUN%5D%20Trust%20in%20Bayes%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcuasP7oujr2etWpNJ%2Fseq-rerun-trust-in-bayes", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FcuasP7oujr2etWpNJ%2Fseq-rerun-trust-in-bayes", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 219, "htmlBody": "<p>Today's post, <a href=\"/lw/na/trust_in_bayes/\">Trust in Bayes</a> was originally published on 29 January 2008.  A summary (taken from the <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong/2008_Articles/Summaries\">LW wiki</a>):</p>\n<p>&nbsp;</p>\n<blockquote>There is a long history of people claiming to have found paradoxes in Bayesian Probability Theory. Typically, these proofs are fallacious, but correct seeming, just as apparent proofs that 2 = 1 are. But in probability theory, the illegal operation is usually not a hidden division by zero, but rather an infinity that is not arrived as a limit of a finite calculation. Once you are more careful with your math, these paradoxes typically go away.</blockquote>\n<p><br />Discuss the post here (rather than in the comments to the original post).<br /><br /><em>This post is part of the Rerunning the Sequences series, where we'll be going through Eliezer Yudkowsky's old posts in order so that people who are interested can (re-)read and discuss them.  The previous post was <a href=\"/lw/94g/seq_rerun_the_intuitions_behind_utilitarianism/\">The \"Intuitions\" Behind \"Utilitarianism\"</a>, and you can use the <a href=\"/r/discussion/tag/sequence_reruns/\">sequence_reruns tag</a> or <a href=\"/r/discussion/tag/sequence_reruns/.rss\">rss feed</a> to follow the rest of the series.<br /><br />Sequence reruns are a community-driven effort.  You can participate by re-reading the sequence post, discussing it here, posting the next day's sequence reruns post, or summarizing forthcoming articles on the wiki. Go <a href=\"/r/discussion/lw/5as/introduction_to_the_sequence_reruns/\">here</a> for more details, or to have meta discussions about the Rerunning the Sequences series.</em></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "cuasP7oujr2etWpNJ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 3, "baseScore": 5, "extendedScore": null, "score": 8.243047074300734e-07, "legacy": true, "legacyId": "11838", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 2, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 1, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["BL9DuE2iTCkrnuYzx", "uRHo4TMxkR27P5xbQ", "m9PsRxu65FuL9c8xp"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-01T05:40:16.848Z", "modifiedAt": null, "url": null, "title": "January 1-14, 2012 Open Thread", "slug": "january-1-14-2012-open-thread", "viewCount": null, "lastCommentedAt": "2017-06-17T04:05:02.788Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "daenerys", "createdAt": "2011-11-08T02:18:14.528Z", "isAdmin": false, "displayName": "daenerys"}, "userId": "KWkCEqaju3xRPA2ka", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/rf36badZFR4PzfGwT/january-1-14-2012-open-thread", "pageUrlRelative": "/posts/rf36badZFR4PzfGwT/january-1-14-2012-open-thread", "linkUrl": "https://www.lesswrong.com/posts/rf36badZFR4PzfGwT/january-1-14-2012-open-thread", "postedAtFormatted": "Sunday, January 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20January%201-14%2C%202012%20Open%20Thread&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AJanuary%201-14%2C%202012%20Open%20Thread%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frf36badZFR4PzfGwT%2Fjanuary-1-14-2012-open-thread%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=January%201-14%2C%202012%20Open%20Thread%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frf36badZFR4PzfGwT%2Fjanuary-1-14-2012-open-thread", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2Frf36badZFR4PzfGwT%2Fjanuary-1-14-2012-open-thread", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 58, "htmlBody": "<p><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">If it's worth saying, but not worth its own post (even in Discussion), then it goes here.</span><br style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\" /><br style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\" /><span style=\"font-family: Arial, Helvetica, sans-serif; line-height: 19px; text-align: justify;\">If continuing the discussion becomes impractical, that means you win at open threads; a celebratory top-level post on the topic is traditional.<br /><br /><br />Poster's Note: omg, it felt so weird typing \"2012\" up there.</span></p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"ABG8vt87eW4FFA6gD": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "rf36badZFR4PzfGwT", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 11, "extendedScore": null, "score": 8.243344375310233e-07, "legacy": true, "legacyId": "11839", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 7, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 129, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-01T07:39:53.314Z", "modifiedAt": null, "url": null, "title": "So You Want to Save the World", "slug": "so-you-want-to-save-the-world", "viewCount": null, "lastCommentedAt": "2017-06-17T04:15:32.716Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "lukeprog", "createdAt": "2009-03-19T08:07:34.230Z", "isAdmin": false, "displayName": "lukeprog"}, "userId": "SdZmP36R37riQrHAw", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/5BJvusxdwNXYQ4L9L/so-you-want-to-save-the-world", "pageUrlRelative": "/posts/5BJvusxdwNXYQ4L9L/so-you-want-to-save-the-world", "linkUrl": "https://www.lesswrong.com/posts/5BJvusxdwNXYQ4L9L/so-you-want-to-save-the-world", "postedAtFormatted": "Sunday, January 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20So%20You%20Want%20to%20Save%20the%20World&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ASo%20You%20Want%20to%20Save%20the%20World%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5BJvusxdwNXYQ4L9L%2Fso-you-want-to-save-the-world%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=So%20You%20Want%20to%20Save%20the%20World%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5BJvusxdwNXYQ4L9L%2Fso-you-want-to-save-the-world", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5BJvusxdwNXYQ4L9L%2Fso-you-want-to-save-the-world", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 3725, "htmlBody": "<p><strong>This post is very out-of-date. See <a href=\"http://intelligence.org/research/\">MIRI's research page</a> for the current research agenda.</strong></p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/superman-vs-robot.jpg\" alt=\"\" /></p>\n<p>So you want to save the world. As it turns out, the world cannot be saved by caped crusaders with great strength and the power of flight. No, the world must be saved by mathematicians, computer scientists, and philosophers.</p>\n<p>This is because the creation of <a href=\"http://facingthesingularity.com/\">machine superintelligence</a> this century will determine the future of our planet, and in order for this \"<a href=\"http://en.wikipedia.org/wiki/Technological_singularity\">technological Singularity</a>\" to go <em>well</em> for us, we need to solve a particular set of technical problems in mathematics, computer science, and philosophy <em>before</em> the Singularity happens.</p>\n<p>The best way for most people to save the world is to <a href=\"/lw/3gj/efficient_charity_do_unto_others/\">donate</a> to an organization working to solve these problems, an organization like the <a href=\"http://intelligence.org/donate/\">Singularity Institute</a> or the <a href=\"http://www.fhi.ox.ac.uk/donate\">Future of Humanity Institute</a>.</p>\n<p>Don't underestimate the importance of donation. <a href=\"http://vimeo.com/32787159\">You can do more good as a philanthropic banker than as a charity worker or researcher.</a></p>\n<p>But if you <em>are</em> a capable researcher, then you may also be able to contribute by working directly on one or more of the open problems humanity needs to solve. If so, read on...</p>\n<p><a id=\"more\"></a></p>\n<h3><br /></h3>\n<h3>Preliminaries</h3>\n<p>At this point, I'll need to assume some familiarity with the subject matter. If you haven't already, take a few hours to <strong>read these five articles</strong>, and then come back:</p>\n<ol>\n<li><a href=\"http://intelligence.org/upload/artificial-intelligence-risk.pdf\">Yudkowsky (2008a)</a></li>\n<li><a href=\"http://www.fhi.ox.ac.uk/__data/assets/pdf_file/0019/3853/brain-emulation-roadmap-report.pdf\">Sandberg &amp; Bostrom (2008)</a></li>\n<li><a href=\"http://consc.net/papers/singularityjcs.pdf\">Chalmers (2010)</a></li>\n<li><a href=\"http://selfawaresystems.files.wordpress.com/2011/10/rationally_shaped_ai.pdf\">Omohundro (2011)</a></li>\n<li><a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Armstrong et al. (2011)</a></li>\n</ol>\n<p>Or at the very least, read my shorter and more accessible summary of the main points in my online book-in-progress, <a href=\"http://facingthesingularity.com/\"><em>Facing the Singularity</em></a>.</p>\n<p><a href=\"http://www.danieldewey.net/\">Daniel Dewey</a>'s highly compressed summary of several key points is:</p>\n<blockquote>\n<p>Hardware and software are improving, there are no signs that we will stop this, and human biology and biases indicate that we are far below the upper limit on intelligence. Economic arguments indicate that most AIs would act to become more intelligent. Therefore, intelligence explosion is very likely. The apparent diversity and irreducibility of information about \"what is good\" suggests that value is complex and fragile; therefore, an AI is unlikely to have any significant overlap with human values if that is not engineered in at significant cost. Therefore, a bad AI explosion is our default future.</p>\n<p>The <a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">VNM utility</a> theorem suggests that there is some formally stated goal that we most prefer. The CEV thought experiment suggests that we could program a metaethics that would generate a good goal. The <a href=\"http://yudkowsky.net/singularity\">Gandhi's pill argument</a> indicates that goal-preserving self-improvement is possible, and the reliability of formal proof suggests that long chains of self-improvement are possible. Therefore, a good AI explosion is likely possible.</p>\n</blockquote>\n<p>Next, I need to make a few important points:</p>\n<p><strong>1. Defining each problem is part of the problem.</strong> As <a href=\"http://www.amazon.com/Adaptive-Control-Processes-Guided-Tour/dp/B000X9B8N6/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Bellman (1961)</a> said, \"the very construction of a precise mathematical statement of a verbal problem is itself a problem of major difficulty.\" Many of the problems related to navigating the Singularity have not yet been stated with mathematical precision, and the need for a precise statement of the problem is <em>part</em> of the problem. But there is reason for optimism. Many times, particular heroes have managed to formalize a previously fuzzy and mysterious concept: see Kolmogorov on complexity and simplicity (<a href=\"http://www.tandfonline.com.proxy.lib.siu.edu/doi/abs/10.1080/00207166808803030\">Kolmogorov 1965</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Grunwald-Vitanyi-Kolmogorov-complexity-and-information-theory.pdf\">Grunwald &amp; Vitanyi 2003</a>; <a href=\"http://www.amazon.com/Introduction-Kolmogorov-Complexity-Applications-Computer/dp/0387339981/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Li &amp; Vit&aacute;nyi 2008</a>), Solomonoff on induction (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Solomonoff-A-formal-theory-of-inductive-inference-part-1.pdf\">Solomonoff 1964a</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Solomonoff-A-formal-theory-of-inductive-inference-part-2.pdf\">1964b</a>; <a href=\"http://www.hutter1.net/publ/uiphil.pdf\">Rathmanner &amp; Hutter 2011</a>), Von Neumann and Morgenstern on rationality (<a href=\"http://en.wikipedia.org/wiki/Theory_of_Games_and_Economic_Behavior\">Von Neumann &amp; Morgenstern 1947</a>; <a href=\"http://www.amazon.com/Foundations-Rational-Choice-Under-Risk/dp/0198774427/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Anand 1995</a>), and Shannon on information (<a href=\"http://makseq.com/materials/lib/Articles-Books/General/InformationTheory/p3-shannon.pdf\">Shannon 1948</a>; <a href=\"http://www.amazon.com/Information-Measures-Description-Engineering-Communication/dp/354040855X/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Arndt 2004</a>).</p>\n<p><strong>2. The nature of the problem space is unclear.</strong> Which problems will biological humans need to solve, and which problems can a successful Friendly AI (FAI) solve on its own (perhaps with the help of human uploads it creates to solve the remaining open problems)? Are Friendly AI (<a href=\"http://intelligence.org/upload/CFAI.html\">Yudkowsky 2001</a>) and CEV (<a href=\"http://intelligence.org/upload/CEV.html\">Yudkowsky 2004</a>) coherent ideas, given the <a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/\">confused</a> <a href=\"/r/discussion/lw/8oy/why_study_the_cognitive_science_of_concepts/\">nature</a> of human \"values\"? Should we aim instead for something like Oracle AI (<a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Armstrong et al. 2011</a>)? Which problems are we unable to state with precision because they are irreparably confused, and which problems are we unable to state due to a lack of insight?</p>\n<p><strong>3. Our intervention priorities are unclear.</strong> There are a limited number of capable researchers who will work on these problems. Which are the most important problems they should be working on, if they are capable of doing so? Should we focus on \"control problem\" theory (FAI, AI-boxing, oracle AI, etc.), or on strategic considerations (<a href=\"http://en.wikipedia.org/wiki/Differential_technological_development\">differential technological development</a>, <a href=\"/lw/76x/is_rationality_teachable/\">methods</a> for <a href=\"/lw/1e/raising_the_sanity_waterline/\">raising the sanity waterline</a>, methods for bringing more funding to existential risk reduction and growing the community of x-risk reducers, reducing the odds of <a href=\"http://intelligence.org/armscontrolintelligenceexplosions.pdf\">AI arms races</a>, etc.)? Is AI more urgent than other existential risks, especially synthetic biology? Is research the most urgent thing to be done, or should we focus on growing the community of x-risk reducers, raising the sanity waterline, bringing in more funding for x-risk reduction, etc.? Can we make better research progress in the next 10 years if we work to improve sanity and funding for 7 years and <em>then</em> have the resources to grab more and better researchers, or can we make better research progress by focusing on research now?</p>\n<h3><br /></h3>\n<h3>Problem Categories</h3>\n<p>There are many ways to categorize our open problems; I'll divide them into three groups:</p>\n<p><strong>Safe AI Architectures</strong>. This may include architectures for securely confined or \"boxed\" AIs (<a href=\"http://www.cs.umd.edu/~jkatz/TEACHING/comp_sec_F04/downloads/confinement.pdf\">Lampson 1973</a>), including Oracle AIs, and also AI architectures capable of using a safe set of goals (resulting in Friendly AI).</p>\n<p><strong>Safe AI Goals</strong>. What could it mean to have a Friendly AI with \"good\" goals?</p>\n<p><strong>Strategy</strong>. How do we predict the future and make recommendations for differential technological development? Do we aim for Friendly AI or Oracle AI or both? Should we focus on growing support now, or do we focus on research? How should we interact with the public and with governments?</p>\n<p>The list of open problems on this page is <em>very</em> preliminary. I'm sure there are many problems I've forgotten, and many problems I'm unaware of. Probably <em>all</em> of the problems are stated poorly: this is only a \"first step\" document. Certainly, all listed problems are described at a very \"high\" level, far away (so far) from mathematical precision, and can themselves be broken down into several and often <em>dozens</em> of subproblems.</p>\n<h3><br /></h3>\n<h3>Safe AI Architectures</h3>\n<p><strong>Is \"rationally-shaped\" transparent AI the only potentially safe AI architecture?</strong> Omohundro (<a href=\"http://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf\">2007</a>, <a href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\">2008</a>, <a href=\"http://selfawaresystems.files.wordpress.com/2011/10/rationally_shaped_ai.pdf\">2011</a>) describes \"rationally shaped\" AI as AI that is as economically rational as possible given its limitations. A rationally shaped AI has beliefs and desires, its desires are defined by a utility function, and it seeks to maximize its expected utility. If an AI doesn't use a utility function, then it's hard to predict its actions, including whether they will be \"friendly.\" The same problem can arise if the decision mechanism or the utility function is not transparent to humans. At least, this <em>seems</em> to be the case, but perhaps there are strong attractors that would allow us to predict friendliness even without the AI having a transparent utility function, or even a utility function at all? Or, perhaps a new decision theory could show the way to a different AI architecture that would allow us to predict the AI's behavior without it having a transparent utility function?</p>\n<p><strong>How can we develop a reflective decision theory?</strong> When an agent considers radical modification of its own decision mechanism, how can it ensure that doing so will keep constant or increase its expected utility? <a href=\"http://www.youtube.com/watch?v=MwriJqBZyoM\">Yudkowsky (2011a)</a> argues that current decision theories stumble over L&ouml;b's Theorem at this point, and that a new, \"reflectively consistent\" decision theory is needed.</p>\n<p><strong>How can we develop a timeless decision theory with the bugs worked out?</strong> Paradoxes like Newcomb's Problem (<a href=\"http://kops.ub.uni-konstanz.de/bitstream/handle/urn:nbn:de:bsz:352-opus-5241/ledwig.pdf?sequence=1\">Ledwig 2000</a>) and Solomon's Problem (<a href=\"https://www.kellogg.northwestern.edu/research/math/papers/194.pdf\">Gibbard &amp; Harper 1978</a>) seem to show that neither causal decision theory nor evidential decision theory is ideal. <a href=\"http://intelligence.org/upload/TDT-v01o.pdf\">Yudkowsky (2010)</a> proposes an apparently superior alternative, timeless decision theory. But it, too, has bugs that need to be worked out, for example the \"5-and-10 problem\" (described <a href=\"/lw/15z/ingredients_of_timeless_decision_theory/1217\">here</a> by Gary Drescher, who doesn't use the 5-and-10 example illustration).</p>\n<p><strong>How can we modify a transparent AI architecture to have a utility function over the external world?</strong> Reinforcement learning can only be used to define agents whose goal is to maximize expected rewards. But this doesn't match human goals, so advanced reinforcement learning agents will diverge from our wishes. Thus, we need a class of agents called \"value learners\" (<a href=\"http://www.danieldewey.net/learning-what-to-value.pdf\">Dewey 2011</a>) that \"can be designed to learn and maximize any initially unknown utility function\" (see <a href=\"http://arxiv.org/pdf/1111.3934v1\">Hibbard 2011</a> for clarifications). Dewey's paper, however, is only the first step in this direction.</p>\n<p><strong>How can an agent keep a stable utility function through ontological shifts?</strong> An agent's utility function may refer to states of, or entities within, its ontology. As <a href=\"http://arxiv.org/pdf/1105.3821v1\">De Blanc (2011)</a> notes, \"If the agent may upgrade or replace its ontology, it faces a crisis: the agent's original [utility function] may not be well-defined with respect to its new ontology.\" De Blanc points toward some possible solutions for these problems, but they need to be developed further.</p>\n<p><strong>How can an agent choose an ideal prior?</strong> We want a Friendly AI's model of the world to be as accurate as possible so that it successfully does friendly things if we can figure out how to give it friendly goals. Solomonoff induction (<a href=\"http://www.amazon.com/Introduction-Kolmogorov-Complexity-Applications-Computer/dp/0387339981/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Li &amp; Vitanyi 2008</a>) may be our best formalization of induction yet, but it could be improved upon.</p>\n<p>First, we may need to solve the problem of observation selection effects or \"anthropic bias\" (<a href=\"http://www.amazon.com/Anthropic-Bias-Studies-Philosophy-Bostrom/dp/0415883946/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Bostrom 2002b</a>): even an agent using a powerful approximation of Solomonoff induction may, due to anthropic bias, make radically incorrect inferences when it does not encounter sufficient evidence to update far enough away from its priors. Several solutions have been proposed (<a href=\"http://arxiv.org/pdf/math/0608592v1\">Neal 2006</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Grace-Anthropic-Reasoning-in-the-Great-Filter.pdf\">Grace 2010;</a>, <a href=\"http://arxiv.org/abs/1110.6437\">Armstrong 2011</a>), but none are as yet widely persuasive.</p>\n<p>Second, we need improvements to Solomonoff induction. <a href=\"http://www.mdpi.com/1999-4893/2/3/879/pdf\">Hutter (2009)</a> discusses many of these problems. We may also need a version of Solmonoff induction in second-order logic because second-order logic with binary predicates can simulate higher-order logics with nth-order predicates. This kind of Solomonoff induction would be able to imagine even, for example, <a href=\"http://en.wikipedia.org/wiki/Hypercomputation\">hypercomputers</a> and time machines.</p>\n<p>Third, we would need computable approximations for this improvement to Solomonoff induction.</p>\n<p><strong>What is the ideal theory of how to handle logical uncertainty?</strong> Even an AI will be uncertain about the true value of certain logical propositions or long chains of logical reasoning. What is the best way to handle this problem? Partial solutions are offered by <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.115.6859&amp;rep=rep1&amp;type=pdf\">Gaifman (2004)</a>, <a href=\"http://www.aaai.org/Papers/Symposia/Fall/2001/FS-01-04/FS01-04-021.pdf\">Williamson (2001)</a>, and <a href=\"http://www.iam.unibe.ch/~haenni/Homepage/PAPERS/ECSQARU05.pdf\">Haenni (2005)</a>, among others.</p>\n<p><strong>What is the ideal computable approximation of perfect Bayesianism?</strong> As explained elsewhere, we want a Friendly AI's model of the world to be as accurate as possible. Thus, we need ideal computable theories of priors and of logical uncertainty, but we also need computable approximations of Bayesian inference. <a href=\"https://bmir.stanford.edu/file_asset/index.php/644/BMIR-1990-0317.pdf\">Cooper (1990)</a> showed that inference in unconstrained Bayesian networks is NP-hard, and <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Dagum-Luby-Approximating-probabilistic-inference-in-Bayesian-belief-networks-is-NP-hard.pdf\">Dagum &amp; Luby (1993)</a> showed that the corresponding approximation problem is also NP-hard. The most common solution is to use randomized sampling methods, also known as \"Monte Carlo\" algorithms (<a href=\"http://www.amazon.com/Monte-Statistical-Methods-Springer-Statistics/dp/1441919392/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Robert &amp; Casella 2010</a>). Another approach is variational approximation (<a href=\"http://webproject.cse.ust.hk:8014/pub/Main/Seminar2010Spring/WaiJor08_FTML.pdf\">Wainwright &amp; Jordan 2008</a>), which works with a simpler but similar version of the original problem. Another approach is called \"belief propagation\" &mdash; for example, loopy belief propagation (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Weiss-Correctness-of-local-probability-propagation-in-graphical-models-with-loops.pdf\">Weiss 2000</a>).</p>\n<p><strong>Can we develop a safely confined AI? Can we develop Oracle AI?</strong> One approach to constraining a powerful AI is to give it \"good\" goals. Another is to externally constrain it, creating a \"boxed\" AI and thereby \"leakproofing the singularity\" (<a href=\"http://consc.net/papers/singularityjcs.pdf\">Chalmers 2010</a>). A <em>fully</em> leakproof singularity is impossible or pointless: \"For an AI system to be useful... to us at all, it must have some effects on us. At a minimum, we must be able to observe it.\" Still, there may be a way to constrain a superhuman AI such that it is useful but not dangerous. <a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Armstrong et al. (2011)</a> offer a detailed proposal for constraining an AI, but there remain many worries about how safe and sustainable such a solution is. The question remains: Can a superhuman AI be safely confined, and can humans managed to safely confine <em>all</em> superhuman AIs that are created?</p>\n<p><strong>What convergent AI architectures and convergent instrumental goals can we expect from superintelligent machines?</strong> Omohundro (<a href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\">2008</a>, <a href=\"http://selfawaresystems.files.wordpress.com/2011/10/rationally_shaped_ai.pdf\">2011</a>) argues that we can expect that \"as computational resources increase, there is a natural progress through stimulus-response systems, learning systems, reasoning systems, self-improving systems, to fully rational systems,\" and that for rational systems there are several convergent instrumental goals: self-protection, resource acquisition, replication, goal preservation, efficiency, and self-improvement. Are these claims true? Are there additional convergent AI architectures or instrumental goals that we can use to predict the implications of machine superintelligence?</p>\n<h3><br /></h3>\n<h3>Safe AI Goals</h3>\n<p><strong>Can \"safe\" AI goals only be derived from contingent \"desires\" and \"goals\"? Might a single procedure for responding to goals be uniquely determined by reason?</strong> A natural approach to selecting goals for a Friendly AI is to ground them in an extrapolation of current human goals, for this approach works even if we assume the naturalist's standard Humean division between motives and reason. But might a sophisticated Kantian approach work, such that some combination of decision theory, game theory, and algorithmic information theory provides a uniquely dictated response to goals? <a href=\"http://www.amazon.com/Good-Real-Demystifying-Paradoxes-Bradford/dp/0262042339/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Drescher (2006)</a> attempts something like this, though his particular approach seems to fail.</p>\n<p><strong>How do we construe a utility function from what humans \"want\"?</strong> A natural approach to Friendly AI is to program a powerful AI with a utility function that accurately represents an extrapolation of what humans want. Unfortunately, humans do not seem to have coherent utility functions, as demonstrated by the neurobiological mechanisms of choice (<a href=\"http://books.google.com/books?id=C02xtHiHJd8C&amp;lpg=PP1&amp;dq=neuroscience%20of%20preference%20and%20choice&amp;pg=PA33#v=onepage&amp;q&amp;f=false\">Dayan 2011</a>) and behavioral violations of the axioms of utility theory (<a href=\"http://pages.stern.nyu.edu/~dbackus/Exotic/1Other/KahnemanTversky%20Ec%2079.pdf\">Kahneman &amp; Tversky 1979</a>). Economists and computer scientists have tried to extract utility theories from human behavior with choice modelling (<a href=\"http://www.amazon.com/Choice-Modelling-State---art--practice/dp/1849507724/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Hess &amp; Daly 2010</a>) and preference elicitation (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/Domshlak-et-al-Preferences-in-AI-an-overview.pdf\">Domshlak et al. 2011</a>), but these attempts have focused on extracting utility functions over a narrow range of human preferences, for example those relevant to developing a particular <a href=\"http://en.wikipedia.org/wiki/Decision_support_system\">decision support system</a>. We need new more powerful and universal methods for preference extraction. Or, perhaps we must allow actual humans to reason about their own preferences for a very long time until they reach a kind of \"reflective equilibrium\" in their preferences (<a href=\"http://intelligence.org/upload/CEV.html\">Yudkowsky 2004</a>). The best path may be to upload a certain set of humans, which would allow them to reason through their preferences with greater speed and introspective access. Unfortunately, the development of human uploads may spin off dangerous neuromorphic AI before this can be done.</p>\n<p><strong>How should human values be extrapolated?</strong> Value extrapolation is an old subject in philosophy (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">Muehlhauser &amp; Helm 2011</a>), but the major results of the field so far have been to show that certain approaches <em>won't</em> work (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Sobel-Full-Information-Accounts-of-Well-Being.pdf\">Sobel 1994</a>); we still have no value extrapolation algorithms that might plausibly work.</p>\n<p><strong>Why extrapolate the values of humans alone? What counts as a human? Do values converge if extrapolated?</strong> Would the choice to extrapolate the values of humans alone be an unjustified act of speciesism, or is it justified because humans are special in some way &mdash; perhaps because humans are the only beings who can reason about their own preferences? And what counts as a human? The problem is more complicated than one might imagine (<a href=\"http://www.nickbostrom.com/papers/experience.pdf\">Bostrom 2006</a>; <a href=\"http://www.nickbostrom.com/views/identity.pdf\">Bostrom &amp; Sandberg 2011</a>). Moreover, do we need to scan the values of all humans, or only some? These problems are less important if values converge upon extrapolation for a wide variety of agents, but it is far from clear that this is the case (<a href=\"http://www.unl.edu/philosop/people/faculty/sobel/DotheDesires.pdf\">Sobel 1999</a>, <a href=\"http://www.uni-tuebingen.de/uploads/media/Steinhoff_Converging_Desires.pdf\">Doring &amp; Steinhoff 2009</a>).</p>\n<p><strong>How do aggregate or assess value in an infinite universe? What can we make of other possible laws of physics?</strong> Our best model of the physical universe predicts that the universe is spatially infinite, meaning that all possible \"bubble universes\" are realized an infinite number of times. Given this, how do we make value calculations? The problem is discussed by <a href=\"http://www.anthropic-principle.com/preprints/knobe.pdf\">Knobe (2006)</a> and <a href=\"http://www.nickbostrom.com/ethics/infinite.pdf\">Bostrom (2009)</a>, but more work remains to be done. These difficulties may be exacerbated if the universe is infinite in a stronger sense, for example if all possible mathematical objects exist (<a href=\"http://arxiv.org/pdf/0905.1283\">Tegmark 2005</a>).</p>\n<p><strong>How should we deal with normative uncertainty?</strong> We may not solve the problems of value or morality in time to build Friendly AI. Perhaps instead we need a theory of how to handle this normative uncertainty. <a href=\"http://www.fil.lu.se/files/conference117.pdf\">Sepielli (2009)</a> and <a href=\"http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html\">Bostrom (2009)</a> have made the initial steps, here.</p>\n<p><strong>Is it possible to program an AI to do what is \"morally right\" rather than give it an extrapolation of human goals?</strong> Perhaps the only way to solve the Friendly AI problem is to get an AI to do moral philosophy and come to the correct answer. But perhaps this exercise would only result in the conclusion that our moral concepts are incoherent (<a href=\"http://faculty.evansville.edu/tb2/PDFs/Moral%20Machines%20and%20the%20Threat%20of%20Ethical%20Nihilism.pdf\">Beavers 2011</a>).</p>\n<h3><br /></h3>\n<h3>Strategy</h3>\n<p><strong>What methods can we use to predict technological development?</strong> Predicting progress in powerful technologies (AI, synthetic biology, nanotechnology) can help us decide which existential threats are most urgent, and can inform our efforts in differential technological development (<a href=\"http://www.nickbostrom.com/existential/risks.html\">Bostrom 2002a</a>). The stability of Moore's law may give us limited predictive hope (<a href=\"https://nanohub.org/resource_files/2005/11/00271/2005.07.13-lundstrom-nclt.pdf\">Lundstrom 2003</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Mack-Fifty-Years-of-Moores-Law.pdf\">Mack 2011</a>), but in general we have no proven method for long-term technological forecasting, including expert elicitation (<a href=\"http://www.amazon.com/Long-range-Forecasting-Crystal-Ball-Computer/dp/0471823600/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Armstrong 1985</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Woudenberg-An-evaluation-of-Delphi.pdf\">Woudenberg 1991</a>; <a href=\"http://teaching.p-design.ch/forecasting07/texts/RoweWright2001_Delphi_Technique.pdf\">Rowe &amp; Wright 2001</a>) and prediction markets (<a href=\"http://www.amazon.com/Prediction-Markets-Applications-Routledge-International/dp/041557286X/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Williams 2011</a>). Nagy's performance curves database (<a href=\"http://tuvalu.santafe.edu/~bn/workingpapers/NagyFarmerTrancikBui.pdf\">Nagy 2010</a>) may aid our forecasting efforts, as may \"big data\" in general (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Weinberger-The-machine-tha-would-predict-the-future.pdf\">Weinberger 2011</a>).</p>\n<p><strong>Which kinds of differential technological development should we encourage, and how?</strong> <a href=\"http://www.nickbostrom.com/existential/risks.html\">Bostrom (2002)</a> proposes a course of <em>differential technological development</em>: \"trying to retard the implementation of dangerous technologies and accelerate implementation of beneficial technologies, especially those that ameliorate the hazards posed by other technologies.\" Many examples are obvious: we should retard the development of technologies that pose an existential risk, and accelerate the development of technologies that help protect us from existential risk, such as vaccines and protective structures. Some potential applications are less obvious. Should we accelerate the development of whole brain emulation technology so that uploaded humans can solve the problems of Friendly AI, or will the development of WBEs spin off dangerous neuromorphic AI first? (<a href=\"http://www.youtube.com/watch?v=cY7v1d06LwM&amp;t=12m37s\">Shulman &amp; Salamon 2011</a>)</p>\n<p><strong>Which open problems are safe to discuss, and which are potentially highly dangerous.</strong> There was a recent debate on whether a certain scientist should publish his discovery of a virus that \"<a href=\"http://rt.com/news/bird-flu-killer-strain-119/?utm_medium=referral&amp;utm_source=pulsenews\">could kill half of humanity</a>.\" (The answer in this case was \"<a href=\"http://news.yahoo.com/details-lab-made-bird-flu-wont-revealed-223114982.html\">no</a>.\") The question of whether to publish results is particularly thorny when it comes to AI research, because most of the work in the \"Safe AI Architectures\" section above would, if completed, bring us closer to developing <em>both</em> uFAI and FAI, but in particular it would make it easier to develop uFAI. Unfortunately, it looks like that work must be done to develop <em>any</em> kind of FAI, while if it is <em>not</em> done then <em>only</em> uFAI can be developed (<a href=\"http://www.danieldewey.net/learning-what-to-value.pdf\">Dewey 2011</a>).</p>\n<p><strong>What can we do to reduce the risk of an AI arms race?</strong> AGI is, in one sense, a powerful weapon for dominating the globe. Once it is seen by governments as a feasible technology goal, we may predict an arms race for AGI. <a href=\"http://intelligence.org/armscontrolintelligenceexplosions.pdf\">Shulman (2009)</a> gives several reasons to recommend \"cooperative control of the development of software entities\" over other methods for arms race risk mitigation, but these scenarios require more extensive analysis.</p>\n<p><strong>What can we do to raise the \"sanity waterline,\" and how much will this help?</strong> The Singularity Institute is a strong advocate of <a href=\"/lw/7e5/the_cognitive_science_of_rationality/\">rationality training</a>, in part so that both AI safety researchers and supporters of x-risk reduction can avoid the usual thinking failures that occur when thinking about those issues (<a href=\"http://intelligence.org/upload/cognitive-biases.pdf\">Yudkowsky 2008b</a>). This raises the question of <a href=\"/lw/76x/is_rationality_teachable/\">how well rationality can be taught</a>, and how much difference it will make for existential risk reduction.</p>\n<p><strong>What can we do to attract more funding, support, and research to x-risk reduction and to specific sub-problems of successful Singularity navigation?</strong> Much is known about how to raise funding (<a href=\"http://www.amazon.com/Science-Giving-Experimental-Approaches-Judgment/dp/1848728859/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Oppenheimer &amp; Olivola 2010</a>) and awareness (<a href=\"http://www.amazon.com/Principles-Marketing-13th-Philip-Kotler/dp/0136079415/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Kotler &amp; Armstrong 2009</a>), but applying these principles is always a challenge, and x-risk reduction may pose unique problems for these tasks.</p>\n<p><strong>Which interventions should we prioritize?</strong> There are limited resources available for existential risk reduction work, and for AI safety research in particular. How should these resources be allocated? Should the focus be on direct research, or on making it easier for a wider pool of researchers to contribute, or on fundraising and awareness-raising, or on other types of interventions?</p>\n<p><strong>How should x-risk reducers and AI safety researchers interact with governments and corporations?</strong> Governments and corporations are potential sources of funding for x-risk reduction work, but they may also endanger the x-risk reduction community. AI development labs will be unfriendly to certain kinds of differential technological development advocated by the AI safety community, and governments may face pressures to nationalize advanced AI research groups (including AI safety researchers) once AGI draws nearer.</p>\n<p><strong>How can optimal philanthropists get the most x-risk reduction for their philanthropic buck?</strong> <a href=\"/lw/3gj/efficient_charity_do_unto_others/\">Optimal philanthropists</a> aim not just to make a difference, but to make the <em>most possible</em> positive difference. <a href=\"http://www.existential-risk.org/concept.pdf\">Bostrom (2011)</a> makes a good case for existential risk reduction as optimal philanthropy, but more detailed questions remain. <em>Which</em> x-risk reduction interventions and organizations should be funded? Should new organizations be formed, or should resources be pooled in one or more of the existing organizations working on x-risk reduction?</p>\n<p><strong>How does AI risk compare to other existential risks?</strong> <a href=\"http://intelligence.org/upload/artificial-intelligence-risk.pdf\">Yudkowsky (2008a)</a> notes that AI poses a special kind of existential risk, for it can surely destroy the human species but, if done right, it also has the unique capacity to <em>save</em> our species from all other existential risks. But will AI come before other existential risks, especially the risks of synthetic biology? How should efforts be allocated between safe AI and the mitigation of other existential risks? Is Oracle AI enough to mitigate other existential risks?</p>\n<p><strong>Which problems do we need to solve, and which ones can we have an AI solve?</strong> Can we get an AI to do Friendly AI philosophy <em>before</em> it takes over the world? Which problems must be solved by humans, and which ones can we hand off to the AI?</p>\n<p><strong>How can we develop microeconomic models of WBEs and self-improving systems?</strong> Hanson (<a href=\"http://hanson.gmu.edu/uploads.html\">1994</a>, <a href=\"http://hanson.gmu.edu/fastgrow.html\">1998</a>, <a href=\"http://hanson.gmu.edu/collapse.pdf\">2008a</a>, <a href=\"http://hanson.gmu.edu/EconOfBrainEmulations.pdf\">2008b</a>, <a href=\"http://hanson.gmu.edu/IEEESpectrum-6-08.pdf\">2008c</a>, <a href=\"http://hanson.gmu.edu/aigrow.pdf\">forthcoming</a>) provides some preliminary steps. Might such models help us predict takeoff speed and the likelihood of monopolar (<a href=\"http://www.nickbostrom.com/fut/singleton.html\">singleton</a>) vs. multipolar outcomes?.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"sYm3HiWcfZvrGu3ui": 1, "NrvXXL3iGjjxu5B7d": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "5BJvusxdwNXYQ4L9L", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 51, "baseScore": 54, "extendedScore": null, "score": 0.000163, "legacy": true, "legacyId": "11712", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": "2018-01-30T00:32:03.501Z", "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 43, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": {"html": "<p><strong id=\"This_post_is_very_out_of_date__See_MIRI_s_research_page_for_the_current_research_agenda_\">This post is very out-of-date. See <a href=\"http://intelligence.org/research/\">MIRI's research page</a> for the current research agenda.</strong></p>\n<p align=\"center\"><img src=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/superman-vs-robot.jpg\" alt=\"\"></p>\n<p>So you want to save the world. As it turns out, the world cannot be saved by caped crusaders with great strength and the power of flight. No, the world must be saved by mathematicians, computer scientists, and philosophers.</p>\n<p>This is because the creation of <a href=\"http://facingthesingularity.com/\">machine superintelligence</a> this century will determine the future of our planet, and in order for this \"<a href=\"http://en.wikipedia.org/wiki/Technological_singularity\">technological Singularity</a>\" to go <em>well</em> for us, we need to solve a particular set of technical problems in mathematics, computer science, and philosophy <em>before</em> the Singularity happens.</p>\n<p>The best way for most people to save the world is to <a href=\"/lw/3gj/efficient_charity_do_unto_others/\">donate</a> to an organization working to solve these problems, an organization like the <a href=\"http://intelligence.org/donate/\">Singularity Institute</a> or the <a href=\"http://www.fhi.ox.ac.uk/donate\">Future of Humanity Institute</a>.</p>\n<p>Don't underestimate the importance of donation. <a href=\"http://vimeo.com/32787159\">You can do more good as a philanthropic banker than as a charity worker or researcher.</a></p>\n<p>But if you <em>are</em> a capable researcher, then you may also be able to contribute by working directly on one or more of the open problems humanity needs to solve. If so, read on...</p>\n<p><a id=\"more\"></a></p>\n<h3><br></h3>\n<h3 id=\"Preliminaries\">Preliminaries</h3>\n<p>At this point, I'll need to assume some familiarity with the subject matter. If you haven't already, take a few hours to <strong>read these five articles</strong>, and then come back:</p>\n<ol>\n<li><a href=\"http://intelligence.org/upload/artificial-intelligence-risk.pdf\">Yudkowsky (2008a)</a></li>\n<li><a href=\"http://www.fhi.ox.ac.uk/__data/assets/pdf_file/0019/3853/brain-emulation-roadmap-report.pdf\">Sandberg &amp; Bostrom (2008)</a></li>\n<li><a href=\"http://consc.net/papers/singularityjcs.pdf\">Chalmers (2010)</a></li>\n<li><a href=\"http://selfawaresystems.files.wordpress.com/2011/10/rationally_shaped_ai.pdf\">Omohundro (2011)</a></li>\n<li><a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Armstrong et al. (2011)</a></li>\n</ol>\n<p>Or at the very least, read my shorter and more accessible summary of the main points in my online book-in-progress, <a href=\"http://facingthesingularity.com/\"><em>Facing the Singularity</em></a>.</p>\n<p><a href=\"http://www.danieldewey.net/\">Daniel Dewey</a>'s highly compressed summary of several key points is:</p>\n<blockquote>\n<p>Hardware and software are improving, there are no signs that we will stop this, and human biology and biases indicate that we are far below the upper limit on intelligence. Economic arguments indicate that most AIs would act to become more intelligent. Therefore, intelligence explosion is very likely. The apparent diversity and irreducibility of information about \"what is good\" suggests that value is complex and fragile; therefore, an AI is unlikely to have any significant overlap with human values if that is not engineered in at significant cost. Therefore, a bad AI explosion is our default future.</p>\n<p>The <a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\">VNM utility</a> theorem suggests that there is some formally stated goal that we most prefer. The CEV thought experiment suggests that we could program a metaethics that would generate a good goal. The <a href=\"http://yudkowsky.net/singularity\">Gandhi's pill argument</a> indicates that goal-preserving self-improvement is possible, and the reliability of formal proof suggests that long chains of self-improvement are possible. Therefore, a good AI explosion is likely possible.</p>\n</blockquote>\n<p>Next, I need to make a few important points:</p>\n<p><strong>1. Defining each problem is part of the problem.</strong> As <a href=\"http://www.amazon.com/Adaptive-Control-Processes-Guided-Tour/dp/B000X9B8N6/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Bellman (1961)</a> said, \"the very construction of a precise mathematical statement of a verbal problem is itself a problem of major difficulty.\" Many of the problems related to navigating the Singularity have not yet been stated with mathematical precision, and the need for a precise statement of the problem is <em>part</em> of the problem. But there is reason for optimism. Many times, particular heroes have managed to formalize a previously fuzzy and mysterious concept: see Kolmogorov on complexity and simplicity (<a href=\"http://www.tandfonline.com.proxy.lib.siu.edu/doi/abs/10.1080/00207166808803030\">Kolmogorov 1965</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Grunwald-Vitanyi-Kolmogorov-complexity-and-information-theory.pdf\">Grunwald &amp; Vitanyi 2003</a>; <a href=\"http://www.amazon.com/Introduction-Kolmogorov-Complexity-Applications-Computer/dp/0387339981/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Li &amp; Vit\u00e1nyi 2008</a>), Solomonoff on induction (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Solomonoff-A-formal-theory-of-inductive-inference-part-1.pdf\">Solomonoff 1964a</a>, <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Solomonoff-A-formal-theory-of-inductive-inference-part-2.pdf\">1964b</a>; <a href=\"http://www.hutter1.net/publ/uiphil.pdf\">Rathmanner &amp; Hutter 2011</a>), Von Neumann and Morgenstern on rationality (<a href=\"http://en.wikipedia.org/wiki/Theory_of_Games_and_Economic_Behavior\">Von Neumann &amp; Morgenstern 1947</a>; <a href=\"http://www.amazon.com/Foundations-Rational-Choice-Under-Risk/dp/0198774427/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Anand 1995</a>), and Shannon on information (<a href=\"http://makseq.com/materials/lib/Articles-Books/General/InformationTheory/p3-shannon.pdf\">Shannon 1948</a>; <a href=\"http://www.amazon.com/Information-Measures-Description-Engineering-Communication/dp/354040855X/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Arndt 2004</a>).</p>\n<p><strong>2. The nature of the problem space is unclear.</strong> Which problems will biological humans need to solve, and which problems can a successful Friendly AI (FAI) solve on its own (perhaps with the help of human uploads it creates to solve the remaining open problems)? Are Friendly AI (<a href=\"http://intelligence.org/upload/CFAI.html\">Yudkowsky 2001</a>) and CEV (<a href=\"http://intelligence.org/upload/CEV.html\">Yudkowsky 2004</a>) coherent ideas, given the <a href=\"/lw/71x/a_crash_course_in_the_neuroscience_of_human/\">confused</a> <a href=\"/r/discussion/lw/8oy/why_study_the_cognitive_science_of_concepts/\">nature</a> of human \"values\"? Should we aim instead for something like Oracle AI (<a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Armstrong et al. 2011</a>)? Which problems are we unable to state with precision because they are irreparably confused, and which problems are we unable to state due to a lack of insight?</p>\n<p><strong>3. Our intervention priorities are unclear.</strong> There are a limited number of capable researchers who will work on these problems. Which are the most important problems they should be working on, if they are capable of doing so? Should we focus on \"control problem\" theory (FAI, AI-boxing, oracle AI, etc.), or on strategic considerations (<a href=\"http://en.wikipedia.org/wiki/Differential_technological_development\">differential technological development</a>, <a href=\"/lw/76x/is_rationality_teachable/\">methods</a> for <a href=\"/lw/1e/raising_the_sanity_waterline/\">raising the sanity waterline</a>, methods for bringing more funding to existential risk reduction and growing the community of x-risk reducers, reducing the odds of <a href=\"http://intelligence.org/armscontrolintelligenceexplosions.pdf\">AI arms races</a>, etc.)? Is AI more urgent than other existential risks, especially synthetic biology? Is research the most urgent thing to be done, or should we focus on growing the community of x-risk reducers, raising the sanity waterline, bringing in more funding for x-risk reduction, etc.? Can we make better research progress in the next 10 years if we work to improve sanity and funding for 7 years and <em>then</em> have the resources to grab more and better researchers, or can we make better research progress by focusing on research now?</p>\n<h3><br></h3>\n<h3 id=\"Problem_Categories\">Problem Categories</h3>\n<p>There are many ways to categorize our open problems; I'll divide them into three groups:</p>\n<p><strong>Safe AI Architectures</strong>. This may include architectures for securely confined or \"boxed\" AIs (<a href=\"http://www.cs.umd.edu/~jkatz/TEACHING/comp_sec_F04/downloads/confinement.pdf\">Lampson 1973</a>), including Oracle AIs, and also AI architectures capable of using a safe set of goals (resulting in Friendly AI).</p>\n<p><strong>Safe AI Goals</strong>. What could it mean to have a Friendly AI with \"good\" goals?</p>\n<p><strong>Strategy</strong>. How do we predict the future and make recommendations for differential technological development? Do we aim for Friendly AI or Oracle AI or both? Should we focus on growing support now, or do we focus on research? How should we interact with the public and with governments?</p>\n<p>The list of open problems on this page is <em>very</em> preliminary. I'm sure there are many problems I've forgotten, and many problems I'm unaware of. Probably <em>all</em> of the problems are stated poorly: this is only a \"first step\" document. Certainly, all listed problems are described at a very \"high\" level, far away (so far) from mathematical precision, and can themselves be broken down into several and often <em>dozens</em> of subproblems.</p>\n<h3><br></h3>\n<h3 id=\"Safe_AI_Architectures\">Safe AI Architectures</h3>\n<p><strong>Is \"rationally-shaped\" transparent AI the only potentially safe AI architecture?</strong> Omohundro (<a href=\"http://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf\">2007</a>, <a href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\">2008</a>, <a href=\"http://selfawaresystems.files.wordpress.com/2011/10/rationally_shaped_ai.pdf\">2011</a>) describes \"rationally shaped\" AI as AI that is as economically rational as possible given its limitations. A rationally shaped AI has beliefs and desires, its desires are defined by a utility function, and it seeks to maximize its expected utility. If an AI doesn't use a utility function, then it's hard to predict its actions, including whether they will be \"friendly.\" The same problem can arise if the decision mechanism or the utility function is not transparent to humans. At least, this <em>seems</em> to be the case, but perhaps there are strong attractors that would allow us to predict friendliness even without the AI having a transparent utility function, or even a utility function at all? Or, perhaps a new decision theory could show the way to a different AI architecture that would allow us to predict the AI's behavior without it having a transparent utility function?</p>\n<p><strong>How can we develop a reflective decision theory?</strong> When an agent considers radical modification of its own decision mechanism, how can it ensure that doing so will keep constant or increase its expected utility? <a href=\"http://www.youtube.com/watch?v=MwriJqBZyoM\">Yudkowsky (2011a)</a> argues that current decision theories stumble over L\u00f6b's Theorem at this point, and that a new, \"reflectively consistent\" decision theory is needed.</p>\n<p><strong>How can we develop a timeless decision theory with the bugs worked out?</strong> Paradoxes like Newcomb's Problem (<a href=\"http://kops.ub.uni-konstanz.de/bitstream/handle/urn:nbn:de:bsz:352-opus-5241/ledwig.pdf?sequence=1\">Ledwig 2000</a>) and Solomon's Problem (<a href=\"https://www.kellogg.northwestern.edu/research/math/papers/194.pdf\">Gibbard &amp; Harper 1978</a>) seem to show that neither causal decision theory nor evidential decision theory is ideal. <a href=\"http://intelligence.org/upload/TDT-v01o.pdf\">Yudkowsky (2010)</a> proposes an apparently superior alternative, timeless decision theory. But it, too, has bugs that need to be worked out, for example the \"5-and-10 problem\" (described <a href=\"/lw/15z/ingredients_of_timeless_decision_theory/1217\">here</a> by Gary Drescher, who doesn't use the 5-and-10 example illustration).</p>\n<p><strong>How can we modify a transparent AI architecture to have a utility function over the external world?</strong> Reinforcement learning can only be used to define agents whose goal is to maximize expected rewards. But this doesn't match human goals, so advanced reinforcement learning agents will diverge from our wishes. Thus, we need a class of agents called \"value learners\" (<a href=\"http://www.danieldewey.net/learning-what-to-value.pdf\">Dewey 2011</a>) that \"can be designed to learn and maximize any initially unknown utility function\" (see <a href=\"http://arxiv.org/pdf/1111.3934v1\">Hibbard 2011</a> for clarifications). Dewey's paper, however, is only the first step in this direction.</p>\n<p><strong>How can an agent keep a stable utility function through ontological shifts?</strong> An agent's utility function may refer to states of, or entities within, its ontology. As <a href=\"http://arxiv.org/pdf/1105.3821v1\">De Blanc (2011)</a> notes, \"If the agent may upgrade or replace its ontology, it faces a crisis: the agent's original [utility function] may not be well-defined with respect to its new ontology.\" De Blanc points toward some possible solutions for these problems, but they need to be developed further.</p>\n<p><strong>How can an agent choose an ideal prior?</strong> We want a Friendly AI's model of the world to be as accurate as possible so that it successfully does friendly things if we can figure out how to give it friendly goals. Solomonoff induction (<a href=\"http://www.amazon.com/Introduction-Kolmogorov-Complexity-Applications-Computer/dp/0387339981/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Li &amp; Vitanyi 2008</a>) may be our best formalization of induction yet, but it could be improved upon.</p>\n<p>First, we may need to solve the problem of observation selection effects or \"anthropic bias\" (<a href=\"http://www.amazon.com/Anthropic-Bias-Studies-Philosophy-Bostrom/dp/0415883946/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Bostrom 2002b</a>): even an agent using a powerful approximation of Solomonoff induction may, due to anthropic bias, make radically incorrect inferences when it does not encounter sufficient evidence to update far enough away from its priors. Several solutions have been proposed (<a href=\"http://arxiv.org/pdf/math/0608592v1\">Neal 2006</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Grace-Anthropic-Reasoning-in-the-Great-Filter.pdf\">Grace 2010;</a>, <a href=\"http://arxiv.org/abs/1110.6437\">Armstrong 2011</a>), but none are as yet widely persuasive.</p>\n<p>Second, we need improvements to Solomonoff induction. <a href=\"http://www.mdpi.com/1999-4893/2/3/879/pdf\">Hutter (2009)</a> discusses many of these problems. We may also need a version of Solmonoff induction in second-order logic because second-order logic with binary predicates can simulate higher-order logics with nth-order predicates. This kind of Solomonoff induction would be able to imagine even, for example, <a href=\"http://en.wikipedia.org/wiki/Hypercomputation\">hypercomputers</a> and time machines.</p>\n<p>Third, we would need computable approximations for this improvement to Solomonoff induction.</p>\n<p><strong>What is the ideal theory of how to handle logical uncertainty?</strong> Even an AI will be uncertain about the true value of certain logical propositions or long chains of logical reasoning. What is the best way to handle this problem? Partial solutions are offered by <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.115.6859&amp;rep=rep1&amp;type=pdf\">Gaifman (2004)</a>, <a href=\"http://www.aaai.org/Papers/Symposia/Fall/2001/FS-01-04/FS01-04-021.pdf\">Williamson (2001)</a>, and <a href=\"http://www.iam.unibe.ch/~haenni/Homepage/PAPERS/ECSQARU05.pdf\">Haenni (2005)</a>, among others.</p>\n<p><strong>What is the ideal computable approximation of perfect Bayesianism?</strong> As explained elsewhere, we want a Friendly AI's model of the world to be as accurate as possible. Thus, we need ideal computable theories of priors and of logical uncertainty, but we also need computable approximations of Bayesian inference. <a href=\"https://bmir.stanford.edu/file_asset/index.php/644/BMIR-1990-0317.pdf\">Cooper (1990)</a> showed that inference in unconstrained Bayesian networks is NP-hard, and <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Dagum-Luby-Approximating-probabilistic-inference-in-Bayesian-belief-networks-is-NP-hard.pdf\">Dagum &amp; Luby (1993)</a> showed that the corresponding approximation problem is also NP-hard. The most common solution is to use randomized sampling methods, also known as \"Monte Carlo\" algorithms (<a href=\"http://www.amazon.com/Monte-Statistical-Methods-Springer-Statistics/dp/1441919392/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Robert &amp; Casella 2010</a>). Another approach is variational approximation (<a href=\"http://webproject.cse.ust.hk:8014/pub/Main/Seminar2010Spring/WaiJor08_FTML.pdf\">Wainwright &amp; Jordan 2008</a>), which works with a simpler but similar version of the original problem. Another approach is called \"belief propagation\" \u2014 for example, loopy belief propagation (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Weiss-Correctness-of-local-probability-propagation-in-graphical-models-with-loops.pdf\">Weiss 2000</a>).</p>\n<p><strong>Can we develop a safely confined AI? Can we develop Oracle AI?</strong> One approach to constraining a powerful AI is to give it \"good\" goals. Another is to externally constrain it, creating a \"boxed\" AI and thereby \"leakproofing the singularity\" (<a href=\"http://consc.net/papers/singularityjcs.pdf\">Chalmers 2010</a>). A <em>fully</em> leakproof singularity is impossible or pointless: \"For an AI system to be useful... to us at all, it must have some effects on us. At a minimum, we must be able to observe it.\" Still, there may be a way to constrain a superhuman AI such that it is useful but not dangerous. <a href=\"http://www.aleph.se/papers/oracleAI.pdf\">Armstrong et al. (2011)</a> offer a detailed proposal for constraining an AI, but there remain many worries about how safe and sustainable such a solution is. The question remains: Can a superhuman AI be safely confined, and can humans managed to safely confine <em>all</em> superhuman AIs that are created?</p>\n<p><strong>What convergent AI architectures and convergent instrumental goals can we expect from superintelligent machines?</strong> Omohundro (<a href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\">2008</a>, <a href=\"http://selfawaresystems.files.wordpress.com/2011/10/rationally_shaped_ai.pdf\">2011</a>) argues that we can expect that \"as computational resources increase, there is a natural progress through stimulus-response systems, learning systems, reasoning systems, self-improving systems, to fully rational systems,\" and that for rational systems there are several convergent instrumental goals: self-protection, resource acquisition, replication, goal preservation, efficiency, and self-improvement. Are these claims true? Are there additional convergent AI architectures or instrumental goals that we can use to predict the implications of machine superintelligence?</p>\n<h3><br></h3>\n<h3 id=\"Safe_AI_Goals\">Safe AI Goals</h3>\n<p><strong>Can \"safe\" AI goals only be derived from contingent \"desires\" and \"goals\"? Might a single procedure for responding to goals be uniquely determined by reason?</strong> A natural approach to selecting goals for a Friendly AI is to ground them in an extrapolation of current human goals, for this approach works even if we assume the naturalist's standard Humean division between motives and reason. But might a sophisticated Kantian approach work, such that some combination of decision theory, game theory, and algorithmic information theory provides a uniquely dictated response to goals? <a href=\"http://www.amazon.com/Good-Real-Demystifying-Paradoxes-Bradford/dp/0262042339/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Drescher (2006)</a> attempts something like this, though his particular approach seems to fail.</p>\n<p><strong>How do we construe a utility function from what humans \"want\"?</strong> A natural approach to Friendly AI is to program a powerful AI with a utility function that accurately represents an extrapolation of what humans want. Unfortunately, humans do not seem to have coherent utility functions, as demonstrated by the neurobiological mechanisms of choice (<a href=\"http://books.google.com/books?id=C02xtHiHJd8C&amp;lpg=PP1&amp;dq=neuroscience%20of%20preference%20and%20choice&amp;pg=PA33#v=onepage&amp;q&amp;f=false\">Dayan 2011</a>) and behavioral violations of the axioms of utility theory (<a href=\"http://pages.stern.nyu.edu/~dbackus/Exotic/1Other/KahnemanTversky%20Ec%2079.pdf\">Kahneman &amp; Tversky 1979</a>). Economists and computer scientists have tried to extract utility theories from human behavior with choice modelling (<a href=\"http://www.amazon.com/Choice-Modelling-State---art--practice/dp/1849507724/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Hess &amp; Daly 2010</a>) and preference elicitation (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/08/Domshlak-et-al-Preferences-in-AI-an-overview.pdf\">Domshlak et al. 2011</a>), but these attempts have focused on extracting utility functions over a narrow range of human preferences, for example those relevant to developing a particular <a href=\"http://en.wikipedia.org/wiki/Decision_support_system\">decision support system</a>. We need new more powerful and universal methods for preference extraction. Or, perhaps we must allow actual humans to reason about their own preferences for a very long time until they reach a kind of \"reflective equilibrium\" in their preferences (<a href=\"http://intelligence.org/upload/CEV.html\">Yudkowsky 2004</a>). The best path may be to upload a certain set of humans, which would allow them to reason through their preferences with greater speed and introspective access. Unfortunately, the development of human uploads may spin off dangerous neuromorphic AI before this can be done.</p>\n<p><strong>How should human values be extrapolated?</strong> Value extrapolation is an old subject in philosophy (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/11/Muehlhauser-Helm-The-Singularity-and-Machine-Ethics-draft.pdf\">Muehlhauser &amp; Helm 2011</a>), but the major results of the field so far have been to show that certain approaches <em>won't</em> work (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/03/Sobel-Full-Information-Accounts-of-Well-Being.pdf\">Sobel 1994</a>); we still have no value extrapolation algorithms that might plausibly work.</p>\n<p><strong>Why extrapolate the values of humans alone? What counts as a human? Do values converge if extrapolated?</strong> Would the choice to extrapolate the values of humans alone be an unjustified act of speciesism, or is it justified because humans are special in some way \u2014 perhaps because humans are the only beings who can reason about their own preferences? And what counts as a human? The problem is more complicated than one might imagine (<a href=\"http://www.nickbostrom.com/papers/experience.pdf\">Bostrom 2006</a>; <a href=\"http://www.nickbostrom.com/views/identity.pdf\">Bostrom &amp; Sandberg 2011</a>). Moreover, do we need to scan the values of all humans, or only some? These problems are less important if values converge upon extrapolation for a wide variety of agents, but it is far from clear that this is the case (<a href=\"http://www.unl.edu/philosop/people/faculty/sobel/DotheDesires.pdf\">Sobel 1999</a>, <a href=\"http://www.uni-tuebingen.de/uploads/media/Steinhoff_Converging_Desires.pdf\">Doring &amp; Steinhoff 2009</a>).</p>\n<p><strong>How do aggregate or assess value in an infinite universe? What can we make of other possible laws of physics?</strong> Our best model of the physical universe predicts that the universe is spatially infinite, meaning that all possible \"bubble universes\" are realized an infinite number of times. Given this, how do we make value calculations? The problem is discussed by <a href=\"http://www.anthropic-principle.com/preprints/knobe.pdf\">Knobe (2006)</a> and <a href=\"http://www.nickbostrom.com/ethics/infinite.pdf\">Bostrom (2009)</a>, but more work remains to be done. These difficulties may be exacerbated if the universe is infinite in a stronger sense, for example if all possible mathematical objects exist (<a href=\"http://arxiv.org/pdf/0905.1283\">Tegmark 2005</a>).</p>\n<p><strong>How should we deal with normative uncertainty?</strong> We may not solve the problems of value or morality in time to build Friendly AI. Perhaps instead we need a theory of how to handle this normative uncertainty. <a href=\"http://www.fil.lu.se/files/conference117.pdf\">Sepielli (2009)</a> and <a href=\"http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html\">Bostrom (2009)</a> have made the initial steps, here.</p>\n<p><strong>Is it possible to program an AI to do what is \"morally right\" rather than give it an extrapolation of human goals?</strong> Perhaps the only way to solve the Friendly AI problem is to get an AI to do moral philosophy and come to the correct answer. But perhaps this exercise would only result in the conclusion that our moral concepts are incoherent (<a href=\"http://faculty.evansville.edu/tb2/PDFs/Moral%20Machines%20and%20the%20Threat%20of%20Ethical%20Nihilism.pdf\">Beavers 2011</a>).</p>\n<h3><br></h3>\n<h3 id=\"Strategy\">Strategy</h3>\n<p><strong>What methods can we use to predict technological development?</strong> Predicting progress in powerful technologies (AI, synthetic biology, nanotechnology) can help us decide which existential threats are most urgent, and can inform our efforts in differential technological development (<a href=\"http://www.nickbostrom.com/existential/risks.html\">Bostrom 2002a</a>). The stability of Moore's law may give us limited predictive hope (<a href=\"https://nanohub.org/resource_files/2005/11/00271/2005.07.13-lundstrom-nclt.pdf\">Lundstrom 2003</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Mack-Fifty-Years-of-Moores-Law.pdf\">Mack 2011</a>), but in general we have no proven method for long-term technological forecasting, including expert elicitation (<a href=\"http://www.amazon.com/Long-range-Forecasting-Crystal-Ball-Computer/dp/0471823600/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Armstrong 1985</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Woudenberg-An-evaluation-of-Delphi.pdf\">Woudenberg 1991</a>; <a href=\"http://teaching.p-design.ch/forecasting07/texts/RoweWright2001_Delphi_Technique.pdf\">Rowe &amp; Wright 2001</a>) and prediction markets (<a href=\"http://www.amazon.com/Prediction-Markets-Applications-Routledge-International/dp/041557286X/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Williams 2011</a>). Nagy's performance curves database (<a href=\"http://tuvalu.santafe.edu/~bn/workingpapers/NagyFarmerTrancikBui.pdf\">Nagy 2010</a>) may aid our forecasting efforts, as may \"big data\" in general (<a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/12/Weinberger-The-machine-tha-would-predict-the-future.pdf\">Weinberger 2011</a>).</p>\n<p><strong>Which kinds of differential technological development should we encourage, and how?</strong> <a href=\"http://www.nickbostrom.com/existential/risks.html\">Bostrom (2002)</a> proposes a course of <em>differential technological development</em>: \"trying to retard the implementation of dangerous technologies and accelerate implementation of beneficial technologies, especially those that ameliorate the hazards posed by other technologies.\" Many examples are obvious: we should retard the development of technologies that pose an existential risk, and accelerate the development of technologies that help protect us from existential risk, such as vaccines and protective structures. Some potential applications are less obvious. Should we accelerate the development of whole brain emulation technology so that uploaded humans can solve the problems of Friendly AI, or will the development of WBEs spin off dangerous neuromorphic AI first? (<a href=\"http://www.youtube.com/watch?v=cY7v1d06LwM&amp;t=12m37s\">Shulman &amp; Salamon 2011</a>)</p>\n<p><strong>Which open problems are safe to discuss, and which are potentially highly dangerous.</strong> There was a recent debate on whether a certain scientist should publish his discovery of a virus that \"<a href=\"http://rt.com/news/bird-flu-killer-strain-119/?utm_medium=referral&amp;utm_source=pulsenews\">could kill half of humanity</a>.\" (The answer in this case was \"<a href=\"http://news.yahoo.com/details-lab-made-bird-flu-wont-revealed-223114982.html\">no</a>.\") The question of whether to publish results is particularly thorny when it comes to AI research, because most of the work in the \"Safe AI Architectures\" section above would, if completed, bring us closer to developing <em>both</em> uFAI and FAI, but in particular it would make it easier to develop uFAI. Unfortunately, it looks like that work must be done to develop <em>any</em> kind of FAI, while if it is <em>not</em> done then <em>only</em> uFAI can be developed (<a href=\"http://www.danieldewey.net/learning-what-to-value.pdf\">Dewey 2011</a>).</p>\n<p><strong>What can we do to reduce the risk of an AI arms race?</strong> AGI is, in one sense, a powerful weapon for dominating the globe. Once it is seen by governments as a feasible technology goal, we may predict an arms race for AGI. <a href=\"http://intelligence.org/armscontrolintelligenceexplosions.pdf\">Shulman (2009)</a> gives several reasons to recommend \"cooperative control of the development of software entities\" over other methods for arms race risk mitigation, but these scenarios require more extensive analysis.</p>\n<p><strong>What can we do to raise the \"sanity waterline,\" and how much will this help?</strong> The Singularity Institute is a strong advocate of <a href=\"/lw/7e5/the_cognitive_science_of_rationality/\">rationality training</a>, in part so that both AI safety researchers and supporters of x-risk reduction can avoid the usual thinking failures that occur when thinking about those issues (<a href=\"http://intelligence.org/upload/cognitive-biases.pdf\">Yudkowsky 2008b</a>). This raises the question of <a href=\"/lw/76x/is_rationality_teachable/\">how well rationality can be taught</a>, and how much difference it will make for existential risk reduction.</p>\n<p><strong>What can we do to attract more funding, support, and research to x-risk reduction and to specific sub-problems of successful Singularity navigation?</strong> Much is known about how to raise funding (<a href=\"http://www.amazon.com/Science-Giving-Experimental-Approaches-Judgment/dp/1848728859/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Oppenheimer &amp; Olivola 2010</a>) and awareness (<a href=\"http://www.amazon.com/Principles-Marketing-13th-Philip-Kotler/dp/0136079415/ref=as_li_ss_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0321928423&amp;linkCode=as2&amp;tag=lesswrong-20\">Kotler &amp; Armstrong 2009</a>), but applying these principles is always a challenge, and x-risk reduction may pose unique problems for these tasks.</p>\n<p><strong>Which interventions should we prioritize?</strong> There are limited resources available for existential risk reduction work, and for AI safety research in particular. How should these resources be allocated? Should the focus be on direct research, or on making it easier for a wider pool of researchers to contribute, or on fundraising and awareness-raising, or on other types of interventions?</p>\n<p><strong>How should x-risk reducers and AI safety researchers interact with governments and corporations?</strong> Governments and corporations are potential sources of funding for x-risk reduction work, but they may also endanger the x-risk reduction community. AI development labs will be unfriendly to certain kinds of differential technological development advocated by the AI safety community, and governments may face pressures to nationalize advanced AI research groups (including AI safety researchers) once AGI draws nearer.</p>\n<p><strong>How can optimal philanthropists get the most x-risk reduction for their philanthropic buck?</strong> <a href=\"/lw/3gj/efficient_charity_do_unto_others/\">Optimal philanthropists</a> aim not just to make a difference, but to make the <em>most possible</em> positive difference. <a href=\"http://www.existential-risk.org/concept.pdf\">Bostrom (2011)</a> makes a good case for existential risk reduction as optimal philanthropy, but more detailed questions remain. <em>Which</em> x-risk reduction interventions and organizations should be funded? Should new organizations be formed, or should resources be pooled in one or more of the existing organizations working on x-risk reduction?</p>\n<p><strong>How does AI risk compare to other existential risks?</strong> <a href=\"http://intelligence.org/upload/artificial-intelligence-risk.pdf\">Yudkowsky (2008a)</a> notes that AI poses a special kind of existential risk, for it can surely destroy the human species but, if done right, it also has the unique capacity to <em>save</em> our species from all other existential risks. But will AI come before other existential risks, especially the risks of synthetic biology? How should efforts be allocated between safe AI and the mitigation of other existential risks? Is Oracle AI enough to mitigate other existential risks?</p>\n<p><strong>Which problems do we need to solve, and which ones can we have an AI solve?</strong> Can we get an AI to do Friendly AI philosophy <em>before</em> it takes over the world? Which problems must be solved by humans, and which ones can we hand off to the AI?</p>\n<p><strong>How can we develop microeconomic models of WBEs and self-improving systems?</strong> Hanson (<a href=\"http://hanson.gmu.edu/uploads.html\">1994</a>, <a href=\"http://hanson.gmu.edu/fastgrow.html\">1998</a>, <a href=\"http://hanson.gmu.edu/collapse.pdf\">2008a</a>, <a href=\"http://hanson.gmu.edu/EconOfBrainEmulations.pdf\">2008b</a>, <a href=\"http://hanson.gmu.edu/IEEESpectrum-6-08.pdf\">2008c</a>, <a href=\"http://hanson.gmu.edu/aigrow.pdf\">forthcoming</a>) provides some preliminary steps. Might such models help us predict takeoff speed and the likelihood of monopolar (<a href=\"http://www.nickbostrom.com/fut/singleton.html\">singleton</a>) vs. multipolar outcomes?.</p>", "sections": [{"title": "This post is very out-of-date. See MIRI's research page for the current research agenda.", "anchor": "This_post_is_very_out_of_date__See_MIRI_s_research_page_for_the_current_research_agenda_", "level": 2}, {"title": "Preliminaries", "anchor": "Preliminaries", "level": 1}, {"title": "Problem Categories", "anchor": "Problem_Categories", "level": 1}, {"title": "Safe AI Architectures", "anchor": "Safe_AI_Architectures", "level": 1}, {"title": "Safe AI Goals", "anchor": "Safe_AI_Goals", "level": 1}, {"title": "Strategy", "anchor": "Strategy", "level": 1}, {"divider": true, "level": 0, "anchor": "postHeadingsDivider"}, {"anchor": "comments", "level": 0, "title": "149 comments"}], "headingsCount": 8}, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 149, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pC47ZTsPNAkjavkXs", "hN2aRnu798yas5b2k", "aMfa3Lf9CN86qF64k", "H2zKAfiSJR6WJQ8pn", "XqmjdBKa4ZaXJtNmf", "xLm9mgJRPvmPGpo7Q"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-01T09:35:32.899Z", "modifiedAt": null, "url": null, "title": "New Year's Prediction Thread (2012)", "slug": "new-year-s-prediction-thread-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:09:31.010Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "gwern", "createdAt": "2009-02-27T22:16:11.237Z", "isAdmin": false, "displayName": "gwern"}, "userId": "BtbwfsEyeT4P2eqXu", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/ApDNK9vrQPDe2oZCy/new-year-s-prediction-thread-2012", "pageUrlRelative": "/posts/ApDNK9vrQPDe2oZCy/new-year-s-prediction-thread-2012", "linkUrl": "https://www.lesswrong.com/posts/ApDNK9vrQPDe2oZCy/new-year-s-prediction-thread-2012", "postedAtFormatted": "Sunday, January 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20New%20Year's%20Prediction%20Thread%20(2012)&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANew%20Year's%20Prediction%20Thread%20(2012)%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FApDNK9vrQPDe2oZCy%2Fnew-year-s-prediction-thread-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=New%20Year's%20Prediction%20Thread%20(2012)%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FApDNK9vrQPDe2oZCy%2Fnew-year-s-prediction-thread-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FApDNK9vrQPDe2oZCy%2Fnew-year-s-prediction-thread-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 168, "htmlBody": "<p>Going through <a href=\"http://predictionbook.com/predictions/judged\">expiring predictions</a> reminded me. Just as we did for <a href=\"/lw/1la/new_years_predictions_thread/\">2010</a> and <a href=\"/lw/3kz/new_years_predictions_thread_2010/\">2011</a>, it's time for LessWrong to make its beliefs pay rent and give hostages to fortune in making predictions for events in 2012 and beyond.</p>\n<p><a id=\"more\"></a></p>\n<p>Suggested topics include: <em>Methods of Rationality</em> updates (eg. \"will there be any?\"), economic benchmarks (price of gold has been an educational one for me this past year), medical advances (but be careful not to be <a href=\"/lw/8yp/prediction_is_hard_especially_of_medicine/\">too optimistic</a>!), personal precommitments (signing up for cryonics?), being <a href=\"/lw/925/link_g%C3%B6del_escher_bach_read_through_starting_on/5jkw\">curmudgeonly</a> about self-improvement, making <a href=\"http://predictionbook.com/predictions/5037\">daring predictions</a> about the future of AGI, and so on.</p>\n<p>As before, please be fairly specific. I intend to put most predictions on <a href=\"http://predictionbook.com/happenstance\">PredictionBook.com</a> and it'd be nice if they weren't too hard to judge in the future.</p>\n<p>(If you want advice on making good predictions, I've tried to write up a few <a href=\"http://www.gwern.net/Prediction%20markets#how-i-make-predictions\">useful heuristics</a> I've learned. So far in the judging process, I've done pretty well this year, although I'm a little annoyed I got a <a href=\"http://predictionbook.com/predictions/3242\">Yemen prediction</a> right but for the wrong reasons.)</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"33BrBRSrRQS4jEHdk": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "ApDNK9vrQPDe2oZCy", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 21, "baseScore": 27, "extendedScore": null, "score": 8.244222543938783e-07, "legacy": true, "legacyId": "11828", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 20, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 340, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["PybtwazftXzvcQSiQ", "KdxC34w596fMat3pk", "qNxPRh5jzrLorak6B"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-01T10:28:45.564Z", "modifiedAt": null, "url": null, "title": "Rationality quotes January 2012", "slug": "rationality-quotes-january-2012", "viewCount": null, "lastCommentedAt": "2017-06-17T04:23:39.616Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Thomas", "createdAt": "2009-03-02T17:47:09.607Z", "isAdmin": false, "displayName": "Thomas"}, "userId": "GrAKeuxT4e9AKyHdE", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/NuKSh7zkW6fMAYnG6/rationality-quotes-january-2012", "pageUrlRelative": "/posts/NuKSh7zkW6fMAYnG6/rationality-quotes-january-2012", "linkUrl": "https://www.lesswrong.com/posts/NuKSh7zkW6fMAYnG6/rationality-quotes-january-2012", "postedAtFormatted": "Sunday, January 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Rationality%20quotes%20January%202012&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ARationality%20quotes%20January%202012%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNuKSh7zkW6fMAYnG6%2Frationality-quotes-january-2012%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Rationality%20quotes%20January%202012%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNuKSh7zkW6fMAYnG6%2Frationality-quotes-january-2012", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FNuKSh7zkW6fMAYnG6%2Frationality-quotes-january-2012", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 71, "htmlBody": "<p>Here's the new thread for posting quotes, with the usual rules:</p>\n<ul>\n<li>Please post all quotes separately, so that they can be voted up/down separately.&nbsp;&nbsp;(If they are strongly related, reply to your own comments.&nbsp;&nbsp;If strongly ordered, then go ahead and post them together.)<span style=\"white-space: pre;\"> </span></li>\n<li>Do not quote yourself.</li>\n<li>Do not quote comments/posts on LW/OB.</li>\n<li>No more than 5 quotes per person per monthly thread, please.</li>\n</ul>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"Zwc2JcT5az4e5YpJy": 1}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "NuKSh7zkW6fMAYnG6", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 12, "baseScore": 11, "extendedScore": null, "score": 8.244421184616783e-07, "legacy": true, "legacyId": "11835", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 9, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 467, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-01T15:16:40.793Z", "modifiedAt": null, "url": null, "title": "Newcomb's problem - one boxer's introspection.", "slug": "newcomb-s-problem-one-boxer-s-introspection", "viewCount": null, "lastCommentedAt": "2017-06-17T04:03:19.942Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": {"username": "Dmytry", "createdAt": "2009-12-03T17:11:53.492Z", "isAdmin": false, "displayName": "Dmytry"}, "userId": "AjtmA2qtA8sdiMbru", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/yD8CTFbrQRe5AvcgS/newcomb-s-problem-one-boxer-s-introspection", "pageUrlRelative": "/posts/yD8CTFbrQRe5AvcgS/newcomb-s-problem-one-boxer-s-introspection", "linkUrl": "https://www.lesswrong.com/posts/yD8CTFbrQRe5AvcgS/newcomb-s-problem-one-boxer-s-introspection", "postedAtFormatted": "Sunday, January 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20Newcomb's%20problem%20-%20one%20boxer's%20introspection.&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0ANewcomb's%20problem%20-%20one%20boxer's%20introspection.%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyD8CTFbrQRe5AvcgS%2Fnewcomb-s-problem-one-boxer-s-introspection%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=Newcomb's%20problem%20-%20one%20boxer's%20introspection.%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyD8CTFbrQRe5AvcgS%2Fnewcomb-s-problem-one-boxer-s-introspection", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FyD8CTFbrQRe5AvcgS%2Fnewcomb-s-problem-one-boxer-s-introspection", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 365, "htmlBody": "<p>So, just a small observation about <a href=\"http://wiki.lesswrong.com/wiki/Newcomb%27s_problem\">Newcomb's problem</a>:</p>\n<p>It does matter to me who the predictor is.</p>\n<p>If it is a substantially magical Omega, that predicts without fail, I will onebox - gamble that my decision might in fact cause a million in that box somehow (via simulation, via timetravel, via some handwavy sciencefictiony quantum mechanical stuff where the box content is entangled with me, via quantum murder even (like quantum suicide), it does not matter). I don't need to change anything about myself - I will win, unless I was wrong about how predictions are done and Omega failed.</p>\n<p>If it is a human psychologist, or equivalent - well in that case I should make up here some rationalization to one box which looks like I truly believe it. I'm not going to do that because I see utility of writing a better post here to be larger than utility of winning in a future Newcomb's game show that is exceedingly unlikely to happen.</p>\n<p>The situation with a fairly accurate human psychologist is <em>drastically different</em>.</p>\n<p>The psychologist may have to put nothing into box B because you did well on particular subset of a test you did decades ago, or nothing because you did poorly. He can do it based on your relative grades for particular problems back in elementary school. One thing he isn't doing, is replicating non-trivial, complicated computation that you do in your head (assuming those aren't a mere rationalization fitted to arrive at otherwise preset conclusion). He may have been correct with previous 100 subjects via combination of sheer luck with unwillingness of previous 100 participants to actually think about it on spot, rather than solve it via cached thoughts and memes, requiring mere lookup of their personal history (they might have complex after the fact rationalizations of that decision but those are irrelevant). You can't in advance make yourself 'win' this by adjusting your Newcomb paradox specific strategy. You would have to adjust your normal life. E.g. I may have to change content of this post to win future Newcomb's paradox. Even that may not work if the prediction is based to events that happened to you and which shaped the way you think.</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": null, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "yD8CTFbrQRe5AvcgS", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallDownvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 16, "baseScore": 1, "extendedScore": null, "score": 8.245496158795813e-07, "legacy": true, "legacyId": "11841", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 1, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}, {"createdAt": null, "postedAt": "2012-01-01T15:57:19.602Z", "modifiedAt": null, "url": null, "title": "How confident should we be?", "slug": "how-confident-should-we-be", "viewCount": null, "lastCommentedAt": "2017-06-17T04:02:22.569Z", "clickCount": null, "deletedDraft": false, "status": 2, "isFuture": false, "sticky": false, "stickyPriority": 2, "userIP": null, "userAgent": null, "referrer": null, "author": null, "user": null, "userId": "bfq5YorFxpih9j6nL", "domain": null, "pageUrl": "https://www.lesswrong.com/posts/itKvzPmatBDvtgXTZ/how-confident-should-we-be", "pageUrlRelative": "/posts/itKvzPmatBDvtgXTZ/how-confident-should-we-be", "linkUrl": "https://www.lesswrong.com/posts/itKvzPmatBDvtgXTZ/how-confident-should-we-be", "postedAtFormatted": "Sunday, January 1st 2012", "emailShareUrl": "mailto:?subject=Interesting%20link%3A%20How%20confident%20should%20we%20be%3F&body=I%20thought%20you%20might%20find%20this%20interesting%3A%0A%0AHow%20confident%20should%20we%20be%3F%0Ahttps%3A%2F%2Fwww.lesswrong.com%2Fposts%2FitKvzPmatBDvtgXTZ%2Fhow-confident-should-we-be%0A%0A(found%20via%20https%3A%2F%2Fwww.lesswrong.com)%0A%20%20", "twitterShareUrl": "https://twitter.com/intent/tweet?text=How%20confident%20should%20we%20be%3F%20https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FitKvzPmatBDvtgXTZ%2Fhow-confident-should-we-be", "facebookShareUrl": "https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2FitKvzPmatBDvtgXTZ%2Fhow-confident-should-we-be", "socialPreviewImageUrl": "", "question": false, "authorIsUnreviewed": false, "wordCount": 307, "htmlBody": "<p>What should a rationalist do about <em>confidence</em>? Should he lean harder towards</p>\n<ol>\n<li>relentlessly psyching himself up to feel like he can do anything, <em>or</em></li>\n<li>having true beliefs about his abilities in all areas, coldly predicting his <a href=\"http://en.wikipedia.org/wiki/Confidence_interval\">likelihood</a> of success in a given domain?</li>\n</ol>\n<p>I don't want to&nbsp;falsely construe these as dichotomous. The real answer will probably dissolve 'confidence' into smaller parts and indicate which parts go where. So which&nbsp;parts of 'confidence' correctly belong in our<em>&nbsp;models of the world</em>&nbsp;(which must never be corrupted) or our<em> motivational systems</em>&nbsp;(which we may cut apart and put together however helps us achieve our goals)? Note that this follows the distinction between epistemic and instrumental rationality.</p>\n<p>Eliezer offers a decision criterion in&nbsp;<a href=\"/lw/c3/the_sin_of_underconfidence/\">The Sin of Underconfidence</a>:</p>\n<blockquote>\n<p><em>Does this way of thinking make me stronger, or weaker? &nbsp;Really truly?</em></p>\n</blockquote>\n<p>It makes us stronger to know when to <a href=\"/lw/gx/just_lose_hope_already/\">lose hope already</a>, and it makes us stronger to have the mental fortitude to <a href=\"/lw/ui/use_the_try_harder_luke/\">kick our asses into shape</a>&nbsp;so we can <a href=\"http://wiki.lesswrong.com/wiki/Challenging_the_Difficult\">do the impossible</a>.&nbsp;Lukeprog <a href=\"/lw/3w3/how_to_beat_procrastination/\">prescribes</a> boosting optimism \"by watching inspirational movies, reading inspirational biographies, and listening to motivational speakers.\" That probably makes you stronger too.</p>\n<p>But I don't know what to do about&nbsp;saying '<a href=\"http://www.youtube.com/watch?v=Yx9xO98kcBU&amp;feature=related\">I can do it</a>' when the odds are against me. What do you do when you probably <em>won't</em>&nbsp;succeed, but&nbsp;believing that Heaven's army is at your back would increase your chances?</p>\n<p>My default answer has always been to maximize confidence, but I acted this way long before I discovered rationality, and I've probably generated confidence for bad reasons as often as I have for good reasons.&nbsp;I'd like to have an answer that prescribes the right action, all of the time. I want know when confidence steers me wrong, and know when to stop increasing my confidence. I want the real answer, not the historically-generated heuristic.</p>\n<p>I can't help but feeling like I'm missing something basic here. What do you think?</p>", "submitToFrontpage": true, "hiddenRelatedQuestion": false, "originalPostRelationSourceId": null, "shortform": false, "canonicalSource": null, "nominationCount2018": null, "nominationCount2019": 0, "reviewCount2018": null, "reviewCount2019": null, "reviewCount": 0, "reviewVoteCount": 0, "positiveReviewVoteCount": 0, "reviewVoteScoreAF": null, "reviewVotesAF": null, "reviewVoteScoreHighKarma": null, "reviewVotesHighKarma": null, "reviewVoteScoreAllKarma": null, "reviewVotesAllKarma": null, "finalReviewVoteScoreHighKarma": null, "finalReviewVotesHighKarma": null, "finalReviewVoteScoreAllKarma": null, "finalReviewVotesAllKarma": null, "finalReviewVoteScoreAF": null, "finalReviewVotesAF": null, "lastCommentPromotedAt": null, "tagRelevance": {"5f5c37ee1b5cdee568cfb118": 2}, "noIndex": null, "rsvps": null, "activateRSVPs": null, "nextDayReminderSent": null, "onlyVisibleToLoggedIn": null, "onlyVisibleToEstablishedAccounts": null, "votingSystem": null, "myEditorAccess": "none", "_id": "itKvzPmatBDvtgXTZ", "schemaVersion": 2, "currentUserVote": null, "currentUserExtendedVote": null, "allVotes": [{"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}, {"voteType": "smallUpvote", "cancelled": false, "isUnvote": false}], "voteCount": 6, "baseScore": 10, "extendedScore": null, "score": 8.245647936764508e-07, "legacy": true, "legacyId": "11842", "legacySpam": null, "feedId": null, "feedLink": null, "lastVisitedAt": null, "isRead": false, "curatedDate": null, "metaDate": null, "suggestForCuratedUsernames": null, "suggestForCuratedUserIds": null, "frontpageDate": null, "collectionTitle": null, "coauthorUserIds": null, "socialPreviewImageId": null, "socialPreviewImageAutoUrl": null, "canonicalSequenceId": null, "canonicalCollectionSlug": null, "canonicalBookId": null, "canonicalNextPostSlug": null, "canonicalPrevPostSlug": null, "unlisted": false, "disableRecommendation": null, "defaultRecommendation": null, "draft": null, "meta": false, "hideFrontpageComments": false, "maxBaseScore": 5, "bannedUserIds": null, "commentsLocked": null, "organizerIds": null, "groupId": null, "eventType": null, "isEvent": false, "reviewedByUserId": "XtphY3uYHwruKqDyG", "reviewForCuratedUserId": null, "startTime": null, "localStartTime": null, "endTime": null, "localEndTime": null, "eventRegistrationLink": null, "joinEventLink": null, "onlineEvent": null, "globalEvent": null, "mongoLocation": null, "googleLocation": null, "location": null, "contactInfo": null, "facebookLink": null, "meetupLink": null, "website": null, "eventImageId": null, "types": null, "metaSticky": false, "sharingSettings": null, "shareWithUsers": null, "linkSharingKey": null, "linkSharingKeyUsedBy": null, "commentSortOrder": null, "hideAuthor": false, "tableOfContents": null, "showModerationGuidelines": false, "moderationStyle": null, "hideCommentKarma": null, "commentCount": 19, "af": false, "version": "1.0.0", "pingbacks": {"Posts": ["pkFazhcTErMw7TFtT", "waqC6FihC2ryAZuAq", "fhEPnveFhb9tmd7Pe", "RWo4LwFzpHNQCTcYt"]}, "moderationGuidelinesVersion": null, "customHighlightVersion": null, "afDate": null, "afBaseScore": 0, "afExtendedScore": null, "afCommentCount": null, "afLastCommentedAt": null, "afSticky": false, "suggestForAlignmentUserIds": null, "reviewForAlignmentUserId": null}]}